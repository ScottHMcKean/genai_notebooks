{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DsPy Prompt Optimization with Azure AI Search\n",
        "## Information Extraction and Summarization with MLFlow 3\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Azure AI Search integration with DsPy\n",
        "2. Structured information extraction\n",
        "3. Google-like summarization\n",
        "4. Prompt optimization with MIPROv2\n",
        "5. MLFlow 3 tracking and GENAI evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# Uncomment to install dependencies\n",
        "# !uv pip install -U dspy-ai mlflow>=3.1.0 azure-search-documents azure-identity openai pydantic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import dspy\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Add src to path for module imports\n",
        "sys.path.insert(0, os.path.abspath('../src'))\n",
        "\n",
        "from azure_search_dspy import AzureSearchRM, InformationExtractionAgent, evaluate_quality, evaluate_agent\n",
        "\n",
        "# Set MLflow experiment\n",
        "mlflow.set_experiment(\"/Users/your_user/dspy_azure_search_extraction\")\n",
        "print(\"‚úì Imports complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n",
        "\n",
        "Set up your Azure AI Search and Azure OpenAI credentials.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Azure AI Search Configuration\n",
        "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\", \"https://your-search-service.search.windows.net\")\n",
        "AZURE_SEARCH_KEY = os.getenv(\"AZURE_SEARCH_KEY\", \"your-key\")\n",
        "AZURE_SEARCH_INDEX = os.getenv(\"AZURE_SEARCH_INDEX\", \"your-index-name\")\n",
        "\n",
        "# Azure OpenAI Configuration (for LLM)\n",
        "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"https://your-openai.openai.azure.com/\")\n",
        "AZURE_OPENAI_KEY = os.getenv(\"AZURE_OPENAI_KEY\", \"your-key\")\n",
        "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4\")\n",
        "AZURE_OPENAI_API_VERSION = \"2024-02-15-preview\"\n",
        "\n",
        "print(f\"Azure Search Index: {AZURE_SEARCH_INDEX}\")\n",
        "print(f\"Azure OpenAI Deployment: {AZURE_OPENAI_DEPLOYMENT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Azure Search Retriever and LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Azure Search retriever\n",
        "azure_rm = AzureSearchRM(\n",
        "    search_endpoint=AZURE_SEARCH_ENDPOINT,\n",
        "    search_key=AZURE_SEARCH_KEY,\n",
        "    index_name=AZURE_SEARCH_INDEX,\n",
        "    k=5,\n",
        "    content_field=\"content\",  # Adjust to your index schema\n",
        "    title_field=\"title\",      # Adjust to your index schema\n",
        "    use_semantic_search=True\n",
        ")\n",
        "\n",
        "# Configure Azure OpenAI LLM for DsPy\n",
        "lm = dspy.AzureOpenAI(\n",
        "    api_base=AZURE_OPENAI_ENDPOINT,\n",
        "    api_key=AZURE_OPENAI_KEY,\n",
        "    api_version=AZURE_OPENAI_API_VERSION,\n",
        "    deployment_id=AZURE_OPENAI_DEPLOYMENT,\n",
        "    model_type=\"chat\",\n",
        "    max_tokens=2000,\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "dspy.settings.configure(lm=lm)\n",
        "print(\"‚úì Azure Search retriever and LLM initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Information Extraction Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the information extraction agent\n",
        "agent = InformationExtractionAgent(retriever=azure_rm)\n",
        "print(\"‚úì Agent created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test the Agent\n",
        "\n",
        "Let's test the agent with a sample query before optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_query = \"What are the key features of Azure AI Search?\"\n",
        "\n",
        "with mlflow.start_run(run_name=\"baseline_test\") as run:\n",
        "    result = agent(test_query)\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Query: {result.query}\")\n",
        "    print(f\"Rewritten Query: {result.rewritten_query}\")\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SUMMARY:\")\n",
        "    print(result.summary)\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"KEY POINTS:\")\n",
        "    print(result.key_points)\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ENTITIES:\")\n",
        "    print(result.entities_json[:500])  # Truncate for display\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SOURCES:\")\n",
        "    for i, source in enumerate(result.sources, 1):\n",
        "        print(f\"{i}. {source}\")\n",
        "    \n",
        "    # Log to MLflow\n",
        "    mlflow.log_param(\"query\", test_query)\n",
        "    mlflow.log_text(result.summary, \"summary.txt\")\n",
        "    mlflow.log_text(result.key_points, \"key_points.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create Evaluation Dataset\n",
        "\n",
        "Create a curated dataset for evaluation. In production, this should be a comprehensive golden dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create evaluation dataset\n",
        "eval_dataset = [\n",
        "    dspy.Example(\n",
        "        query=\"What is Azure AI Search?\",\n",
        "        expected_topics=[\"search service\", \"cognitive search\", \"AI-powered\"],\n",
        "    ).with_inputs(\"query\"),\n",
        "    dspy.Example(\n",
        "        query=\"How does semantic search work?\",\n",
        "        expected_topics=[\"semantic\", \"ranking\", \"understanding\"],\n",
        "    ).with_inputs(\"query\"),\n",
        "    dspy.Example(\n",
        "        query=\"What are vector search capabilities?\",\n",
        "        expected_topics=[\"vector\", \"embeddings\", \"similarity\"],\n",
        "    ).with_inputs(\"query\"),\n",
        "    dspy.Example(\n",
        "        query=\"How can I enrich my documents during indexing?\",\n",
        "        expected_topics=[\"enrichment\", \"skills\", \"cognitive\"],\n",
        "    ).with_inputs(\"query\"),\n",
        "    dspy.Example(\n",
        "        query=\"What pricing models are available?\",\n",
        "        expected_topics=[\"pricing\", \"tiers\", \"cost\"],\n",
        "    ).with_inputs(\"query\"),\n",
        "]\n",
        "\n",
        "# Split into train and test sets\n",
        "train_size = int(len(eval_dataset) * 0.6)\n",
        "trainset = eval_dataset[:train_size]\n",
        "testset = eval_dataset[train_size:]\n",
        "\n",
        "print(f\"Training examples: {len(trainset)}\")\n",
        "print(f\"Test examples: {len(testset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Baseline Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Running baseline evaluation...\")\n",
        "with mlflow.start_run(run_name=\"baseline_evaluation\") as run:\n",
        "    baseline_results = evaluate_agent(agent, testset, \"baseline\")\n",
        "    \n",
        "    print(f\"\\nBaseline Score: {baseline_results['average_score']:.2%}\")\n",
        "    print(f\"Individual scores: {[f'{s:.2%}' for s in baseline_results['scores']]}\")\n",
        "    \n",
        "    # Log to MLflow\n",
        "    mlflow.log_metric(\"baseline_score\", baseline_results['average_score'])\n",
        "    mlflow.log_metric(\"num_test_examples\", baseline_results['num_examples'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. DsPy Prompt Optimization with MIPROv2\n",
        "\n",
        "Now let's optimize the prompts using DsPy's MIPROv2 optimizer. This will automatically improve the prompts based on our evaluation metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dspy.teleprompt import MIPROv2\n",
        "\n",
        "print(\"Starting prompt optimization with MIPROv2...\")\n",
        "print(\"This may take several minutes...\\n\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"mipro_optimization\") as run:\n",
        "    # Initialize optimizer\n",
        "    optimizer = MIPROv2(\n",
        "        prompt_model=lm,\n",
        "        task_model=lm,\n",
        "        metric=evaluate_quality,\n",
        "        num_candidates=5,  # Number of prompt candidates to try\n",
        "        init_temperature=0.2\n",
        "    )\n",
        "    \n",
        "    # Optimize the agent\n",
        "    kwargs = dict(num_threads=2, display_progress=True, display_table=0)\n",
        "    \n",
        "    optimized_agent = optimizer.compile(\n",
        "        student=agent,\n",
        "        trainset=trainset,\n",
        "        eval_kwargs=kwargs,\n",
        "        requires_permission_to_run=False,\n",
        "    )\n",
        "    \n",
        "    mlflow.log_param(\"optimizer\", \"MIPROv2\")\n",
        "    mlflow.log_param(\"num_candidates\", 5)\n",
        "    mlflow.log_param(\"train_size\", len(trainset))\n",
        "    \n",
        "    print(\"\\n‚úì Optimization complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate Optimized Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluating optimized agent...\")\n",
        "with mlflow.start_run(run_name=\"optimized_evaluation\") as run:\n",
        "    optimized_results = evaluate_agent(optimized_agent, testset, \"optimized\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"RESULTS COMPARISON\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Optimized Score: {optimized_results['average_score']:.2%}\")\n",
        "    print(f\"Baseline Score:  {baseline_results['average_score']:.2%}\")\n",
        "    improvement = optimized_results['average_score'] - baseline_results['average_score']\n",
        "    print(f\"Improvement:     {improvement:+.2%}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Log to MLflow\n",
        "    mlflow.log_metric(\"optimized_score\", optimized_results['average_score'])\n",
        "    mlflow.log_metric(\"improvement\", improvement)\n",
        "    mlflow.log_metric(\"num_test_examples\", optimized_results['num_examples'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. MLflow GENAI Evaluation\n",
        "\n",
        "Use MLflow 3's built-in GENAI evaluation metrics for additional insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_mlflow_eval_data(dataset, predictions):\n",
        "    \"\"\"Prepare data for MLflow evaluation\"\"\"\n",
        "    eval_data = []\n",
        "    for example, pred in zip(dataset, predictions):\n",
        "        if pred is not None:\n",
        "            eval_data.append({\n",
        "                \"request\": example.query,\n",
        "                \"response\": pred.summary,\n",
        "                \"retrieved_context\": [{\"content\": pred.context[:1000]}],  # Truncate\n",
        "                \"key_points\": pred.key_points,\n",
        "                \"entities\": pred.entities_json[:500],  # Truncate\n",
        "                \"sources\": str(pred.sources)\n",
        "            })\n",
        "    return pd.DataFrame(eval_data)\n",
        "\n",
        "# Prepare evaluation data\n",
        "baseline_eval_df = prepare_mlflow_eval_data(testset, baseline_results['predictions'])\n",
        "optimized_eval_df = prepare_mlflow_eval_data(testset, optimized_results['predictions'])\n",
        "\n",
        "print(\"Running MLflow GENAI evaluation...\\n\")\n",
        "\n",
        "# Evaluate baseline\n",
        "print(\"Baseline GENAI Metrics:\")\n",
        "with mlflow.start_run(run_name=\"mlflow_baseline_genai_eval\") as run:\n",
        "    baseline_mlflow = mlflow.evaluate(\n",
        "        data=baseline_eval_df,\n",
        "        model_type=\"question-answering\",\n",
        "        targets=\"request\",\n",
        "        predictions=\"response\",\n",
        "        extra_metrics=[\n",
        "            mlflow.metrics.genai.answer_relevance(),\n",
        "            mlflow.metrics.genai.answer_correctness(),\n",
        "        ]\n",
        "    )\n",
        "    print(baseline_mlflow.metrics)\n",
        "\n",
        "# Evaluate optimized\n",
        "print(\"\\nOptimized GENAI Metrics:\")\n",
        "with mlflow.start_run(run_name=\"mlflow_optimized_genai_eval\") as run:\n",
        "    optimized_mlflow = mlflow.evaluate(\n",
        "        data=optimized_eval_df,\n",
        "        model_type=\"question-answering\",\n",
        "        targets=\"request\",\n",
        "        predictions=\"response\",\n",
        "        extra_metrics=[\n",
        "            mlflow.metrics.genai.answer_relevance(),\n",
        "            mlflow.metrics.genai.answer_correctness(),\n",
        "        ]\n",
        "    )\n",
        "    print(optimized_mlflow.metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Optimized Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"save_optimized_model\") as run:\n",
        "    # Save agent state\n",
        "    optimized_agent.save(\"optimized_extraction_agent.json\")\n",
        "    \n",
        "    # Log as artifact\n",
        "    mlflow.log_artifact(\"optimized_extraction_agent.json\")\n",
        "    \n",
        "    # Log configuration\n",
        "    config = {\n",
        "        \"model\": \"information_extraction_agent\",\n",
        "        \"optimizer\": \"MIPROv2\",\n",
        "        \"baseline_score\": baseline_results['average_score'],\n",
        "        \"optimized_score\": optimized_results['average_score'],\n",
        "        \"improvement\": improvement,\n",
        "        \"azure_search_index\": AZURE_SEARCH_INDEX,\n",
        "        \"llm_model\": AZURE_OPENAI_DEPLOYMENT\n",
        "    }\n",
        "    \n",
        "    mlflow.log_dict(config, \"model_config.json\")\n",
        "    \n",
        "    print(\"\\n‚úì Optimized agent saved!\")\n",
        "    print(f\"MLflow Run ID: {run.info.run_id}\")\n",
        "    print(f\"Artifact URI: {run.info.artifact_uri}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Interactive Testing\n",
        "\n",
        "Use the optimized agent interactively with your own queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def interactive_query(query_text: str, use_optimized: bool = True):\n",
        "    \"\"\"Run an interactive query and display results\"\"\"\n",
        "    selected_agent = optimized_agent if use_optimized else agent\n",
        "    agent_name = \"Optimized\" if use_optimized else \"Baseline\"\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Using {agent_name} Agent\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Query: {query_text}\\n\")\n",
        "    \n",
        "    result = selected_agent(query_text)\n",
        "    \n",
        "    print(\"üìù SUMMARY:\")\n",
        "    print(result.summary)\n",
        "    \n",
        "    print(\"\\nüîë KEY POINTS:\")\n",
        "    print(result.key_points)\n",
        "    \n",
        "    print(\"\\nüè∑Ô∏è EXTRACTED ENTITIES:\")\n",
        "    try:\n",
        "        entities = json.loads(result.entities_json)\n",
        "        print(json.dumps(entities, indent=2)[:500])  # Truncate\n",
        "    except:\n",
        "        print(result.entities_json[:500])\n",
        "    \n",
        "    print(\"\\nüìö SOURCES:\")\n",
        "    for i, source in enumerate(result.sources, 1):\n",
        "        print(f\"  {i}. {source}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Try it with your own query!\n",
        "# result = interactive_query(\"What are the security features of Azure AI Search?\", use_optimized=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. ‚úÖ **Azure AI Search Integration**: Custom DsPy retrieval module for Azure AI Search with semantic search\n",
        "2. ‚úÖ **Structured Information Extraction**: Extracting entities, summaries, and key points using DsPy signatures\n",
        "3. ‚úÖ **Google-like Summarization**: Concise, informative summaries with sources\n",
        "4. ‚úÖ **DsPy Prompt Optimization**: Using MIPROv2 to automatically optimize prompts\n",
        "5. ‚úÖ **MLFlow 3 Integration**: Comprehensive experiment tracking and GENAI evaluation metrics\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- **Expand Dataset**: Add more diverse queries to your evaluation dataset\n",
        "- **Custom Metrics**: Fine-tune evaluation metrics for your specific domain\n",
        "- **Deploy**: Package the optimized agent for production deployment\n",
        "- **Continuous Improvement**: Monitor production queries and retrain periodically\n",
        "- **Advanced Features**: Add caching, async processing, and batch inference\n",
        "\n",
        "### Key Benefits of this Approach:\n",
        "\n",
        "- **Modular Design**: Clean separation of concerns with reusable components in `src/`\n",
        "- **Testable**: Easy to write unit tests for each component\n",
        "- **Optimizable**: Automatic prompt improvement with DsPy optimizers\n",
        "- **Observable**: Full MLflow tracking for reproducibility\n",
        "- **Scalable**: Ready for Databricks deployment with minimal changes\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
