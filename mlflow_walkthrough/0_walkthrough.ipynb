{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bd833b4-dffd-49a2-a58d-1e3d57d94b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MLFLow 3 GenAI Walkthrough\n",
    "This code walks through some of the key features of MLFlow for generative AI applications. Some additional documentation can be found here:\n",
    "\n",
    "- [MLFlow 3 for Generative AI](https://docs.databricks.com/aws/en/mlflow3/genai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b3baf9a-b183-413b-92c8-b17ca32c6ac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U --quiet mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "775287de-0644-453d-a392-fb7436a61048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.deployments\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16c70add-713a-47da-9028-388df6809b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "MLFlow has two key places it tracks experiments and models - the tracking server and the registry. These are automatically set with MLFLow 3 and we can verify below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2277b16-7c86-43be-8e7c-ee59bd3f395d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"MLflow Registry URI: {mlflow.get_registry_uri()}\")\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29b24252-cf6a-4166-a5de-c44786b29b12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Using the Deploy Client\n",
    "The main way to interact with custom agents and foundation models when using a serving endpoint or application is the deploy client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6cced27-ad7a-4858-a463-24be76fb7224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "# List available endpoints\n",
    "endpoints = client.list_endpoints()\n",
    "\n",
    "print(\"Available endpoints:\")\n",
    "for endpoint in endpoints:\n",
    "    if 'databricks' in endpoint['name']:\n",
    "        print(f\"- {endpoint['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9adca01f-b4b5-4249-8986-ec4d96d1ec4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tracing\n",
    "One of the most important parts of agent development is the ability to trace the path (and latency) of each agent step. In MLFLow 3, experiments are the main container and we recommend one experiment per application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df2495da-8d4e-4aef-9136-078d8e981d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MODEL_ENDPOINT = \"databricks-llama-4-maverick\"\n",
    "\n",
    "@mlflow.trace\n",
    "def query_foundation_model(prompt, temperature=0.7, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Query the foundation model via MLflow deployments.\n",
    "\n",
    "    Parameters:\n",
    "    prompt (str): The input prompt to send to the model.\n",
    "    temperature (float): Sampling temperature for the model's response. Higher values mean more random completions.\n",
    "                         Lower values make the output more focused and deterministic. Adjusting the temperature \n",
    "                         allows control over the creativity and diversity of the generated responses.\n",
    "    max_tokens (int): The maximum number of tokens to generate in the response.\n",
    "    \"\"\"\n",
    "    \"\"\"Query the foundation model via MLflow deployments\"\"\"\n",
    "    try:\n",
    "        response = client.predict(\n",
    "            endpoint=MODEL_ENDPOINT,\n",
    "            inputs={\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens\n",
    "            }\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1b6651e-2281-4e39-a418-c10f5f28d334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Set the experiment\n",
    "\n",
    "# Experiments are generally stored under usernames in the workspace\n",
    "username = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "\n",
    "# Create or set experiment\n",
    "experiment_name = f\"/Users/{username}/gen_ai_prompt_engineering\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "532a6af7-f8d0-4615-8d5c-54920101f3ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.autolog()\n",
    "query_foundation_model(\"Quote Marcus Aurelius\", temperature=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "170e9e46-9e37-4276-b579-66421beca557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define different prompt templates for customer service\n",
    "prompt_templates = {\n",
    "    \"basic\": \"Answer this customer question: {question}\",\n",
    "    \"friendly\": \"As a helpful customer service representative, please answer this question with a warm and friendly tone: {question}\",\n",
    "    \"detailed\": \"Provide a comprehensive and detailed answer to this customer service question. Include relevant context and next steps: {question}\",\n",
    "    \"concise\": \"Give a brief, direct answer to this customer question: {question}\"\n",
    "}\n",
    "\n",
    "# Sample customer questions\n",
    "customer_questions = [\n",
    "    \"How do I return a product?\",\n",
    "    \"What is your refund policy?\",\n",
    "    \"My order hasn't arrived yet, what should I do?\",\n",
    "    \"Can I change my shipping address?\"\n",
    "]\n",
    "\n",
    "# Function to run prompt engineering experiment\n",
    "def run_prompt_experiment(template_name, template, questions, temperature=0.7):\n",
    "    \"\"\"Run an experiment with a specific prompt template\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"prompt_template_{template_name}\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"template_name\", template_name)\n",
    "        mlflow.log_param(\"template\", template)\n",
    "        mlflow.log_param(\"temperature\", temperature)\n",
    "        mlflow.log_param(\"model_endpoint\", MODEL_ENDPOINT)\n",
    "        mlflow.log_param(\"num_questions\", len(questions))\n",
    "        \n",
    "        results = []\n",
    "        total_tokens = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, question in enumerate(questions):  # enumerate provides a counter (i) and the item (question) from the iterable (questions)\n",
    "            # Format prompt\n",
    "            formatted_prompt = template.format(question=question)\n",
    "            \n",
    "            # Get model response\n",
    "            response = query_foundation_model(formatted_prompt, temperature=temperature)\n",
    "            \n",
    "            if response:\n",
    "                result = {\n",
    "                    \"question\": question,\n",
    "                    \"formatted_prompt\": formatted_prompt,\n",
    "                    \"response\": response,\n",
    "                    \"response_length\": len(response),\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                results.append(result)\n",
    "                total_tokens += len(response.split())\n",
    "            else:\n",
    "                # Handle case where model response failed\n",
    "                result = {\n",
    "                    \"question\": question,\n",
    "                    \"formatted_prompt\": formatted_prompt,\n",
    "                    \"response\": \"ERROR: No response received\",\n",
    "                    \"response_length\": 0,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                results.append(result)\n",
    "            \n",
    "            # Small delay to avoid rate limiting\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"total_responses\", len(results))\n",
    "        if results:  # Avoid division by zero\n",
    "            mlflow.log_metric(\"avg_response_length\", sum(r[\"response_length\"] for r in results) / len(results))\n",
    "        mlflow.log_metric(\"total_tokens\", total_tokens)\n",
    "        mlflow.log_metric(\"execution_time_seconds\", end_time - start_time)\n",
    "        \n",
    "        # Log results as artifact\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(\"prompt_results.csv\", index=False)\n",
    "        mlflow.log_artifact(\"prompt_results.csv\")\n",
    "        \n",
    "        # Log individual responses as text files\n",
    "        for i, result in enumerate(results):\n",
    "            mlflow.log_text(\n",
    "                f\"Q: {result['question']}\\n\\nPrompt: {result['formatted_prompt']}\\n\\nA: {result['response']}\", \n",
    "                f\"qa_pair_{i}.txt\"\n",
    "            )\n",
    "        \n",
    "        print(f\"Completed experiment for template: {template_name}\")\n",
    "        return results\n",
    "    \n",
    "# Run experiments for each prompt template\n",
    "all_results = {}\n",
    "for template_name, template in prompt_templates.items():\n",
    "    print(f\"\\n--- Running experiment: {template_name} ---\")\n",
    "    results = run_prompt_experiment(template_name, template, customer_questions)\n",
    "    all_results[template_name] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "533d96b8-f73e-4537-aaea-6e3b0ef1ff5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Evaluation\n",
    "This notebook goes through some basic model evaluation with custom metrics. We can also use `databricks-agent` to use tuned and efficient LLM judges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9541213a-ccf7-4422-a45f-28111cd5aae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define custom evaluation functions\n",
    "def calculate_response_quality_score(question, response):\n",
    "    \"\"\"Simple heuristic scoring for response quality\"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Length check (not too short, not too long)\n",
    "    if 20 <= len(response) <= 500:\n",
    "        score += 25\n",
    "    \n",
    "    # Politeness indicators\n",
    "    polite_words = ['please', 'thank you', 'sorry', 'help', 'assist']\n",
    "    if any(word in response.lower() for word in polite_words):\n",
    "        score += 25\n",
    "    \n",
    "    # Relevance check (contains key terms from question)\n",
    "    question_words = set(question.lower().split())\n",
    "    response_words = set(response.lower().split())\n",
    "    if len(question_words.intersection(response_words)) > 0:\n",
    "        score += 25\n",
    "    \n",
    "    # Completeness (contains actionable information)\n",
    "    action_words = ['contact', 'visit', 'call', 'email', 'process', 'steps']\n",
    "    if any(word in response.lower() for word in action_words):\n",
    "        score += 25\n",
    "    \n",
    "    return score\n",
    "\n",
    "def evaluate_template_performance(template_name, results):\n",
    "    \"\"\"Evaluate overall performance of a prompt template\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"evaluation_{template_name}\"):\n",
    "        mlflow.log_param(\"evaluation_template\", template_name)\n",
    "        mlflow.log_param(\"evaluation_timestamp\", datetime.now().isoformat())\n",
    "        mlflow.log_param(\"total_responses_evaluated\", len(results))\n",
    "        \n",
    "        scores = []\n",
    "        detailed_scores = []\n",
    "        error_count = 0\n",
    "        \n",
    "        # Evaluate each response\n",
    "        for i, result in enumerate(results):\n",
    "            # Skip error responses\n",
    "            if result[\"response\"].startswith(\"ERROR:\"):\n",
    "                error_count += 1\n",
    "                continue\n",
    "                \n",
    "            quality_score = calculate_response_quality_score(\n",
    "                result[\"question\"], \n",
    "                result[\"response\"]\n",
    "            )\n",
    "            scores.append(quality_score)\n",
    "            \n",
    "            detailed_scores.append({\n",
    "                \"question_id\": i,\n",
    "                \"question\": result[\"question\"],\n",
    "                \"response\": result[\"response\"],\n",
    "                \"quality_score\": quality_score,\n",
    "                \"response_length\": result[\"response_length\"],\n",
    "                \"timestamp\": result[\"timestamp\"]\n",
    "            })\n",
    "        \n",
    "        # Handle case where no valid responses exist\n",
    "        if not scores:\n",
    "            print(f\"Warning: No valid responses found for template '{template_name}'\")\n",
    "            mlflow.log_metric(\"avg_quality_score\", 0)\n",
    "            mlflow.log_metric(\"error_rate\", 1.0)\n",
    "            mlflow.log_metric(\"valid_responses\", 0)\n",
    "            return {\n",
    "                \"template_name\": template_name,\n",
    "                \"avg_quality_score\": 0,\n",
    "                \"total_questions\": len(results),\n",
    "                \"valid_responses\": 0,\n",
    "                \"error_count\": error_count,\n",
    "                \"evaluation_date\": datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        min_score = min(scores)\n",
    "        max_score = max(scores)\n",
    "        score_std = pd.Series(scores).std() if len(scores) > 1 else 0\n",
    "        error_rate = error_count / len(results)\n",
    "        \n",
    "        # Log evaluation metrics\n",
    "        mlflow.log_metric(\"avg_quality_score\", avg_score)\n",
    "        mlflow.log_metric(\"min_quality_score\", min_score)\n",
    "        mlflow.log_metric(\"max_quality_score\", max_score)\n",
    "        mlflow.log_metric(\"score_std\", score_std)\n",
    "        mlflow.log_metric(\"error_rate\", error_rate)\n",
    "        mlflow.log_metric(\"valid_responses\", len(scores))\n",
    "        mlflow.log_metric(\"error_count\", error_count)\n",
    "        \n",
    "        # Calculate additional quality metrics\n",
    "        high_quality_responses = sum(1 for score in scores if score >= 75)\n",
    "        medium_quality_responses = sum(1 for score in scores if 50 <= score < 75)\n",
    "        low_quality_responses = sum(1 for score in scores if score < 50)\n",
    "        \n",
    "        mlflow.log_metric(\"high_quality_count\", high_quality_responses)\n",
    "        mlflow.log_metric(\"medium_quality_count\", medium_quality_responses)\n",
    "        mlflow.log_metric(\"low_quality_count\", low_quality_responses)\n",
    "        mlflow.log_metric(\"high_quality_percentage\", (high_quality_responses / len(scores)) * 100)\n",
    "        \n",
    "        # Create evaluation summary\n",
    "        eval_summary = {\n",
    "            \"template_name\": template_name,\n",
    "            \"avg_quality_score\": avg_score,\n",
    "            \"min_quality_score\": min_score,\n",
    "            \"max_quality_score\": max_score,\n",
    "            \"score_std\": score_std,\n",
    "            \"total_questions\": len(results),\n",
    "            \"valid_responses\": len(scores),\n",
    "            \"error_count\": error_count,\n",
    "            \"error_rate\": error_rate,\n",
    "            \"high_quality_count\": high_quality_responses,\n",
    "            \"medium_quality_count\": medium_quality_responses,\n",
    "            \"low_quality_count\": low_quality_responses,\n",
    "            \"high_quality_percentage\": (high_quality_responses / len(scores)) * 100,\n",
    "            \"evaluation_date\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Log detailed evaluation results\n",
    "        if detailed_scores:\n",
    "            eval_df = pd.DataFrame(detailed_scores)\n",
    "            eval_df.to_csv(\"detailed_evaluation.csv\", index=False)\n",
    "            mlflow.log_artifact(\"detailed_evaluation.csv\")\n",
    "            \n",
    "            # Create score distribution summary\n",
    "            score_distribution = {\n",
    "                \"score_bins\": [0, 25, 50, 75, 100],\n",
    "                \"score_counts\": [\n",
    "                    sum(1 for score in scores if 0 <= score < 25),\n",
    "                    sum(1 for score in scores if 25 <= score < 50),\n",
    "                    sum(1 for score in scores if 50 <= score < 75),\n",
    "                    sum(1 for score in scores if 75 <= score <= 100)\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Log score distribution\n",
    "            with open(\"score_distribution.json\", \"w\") as f:\n",
    "                json.dump(score_distribution, f, indent=2)\n",
    "            mlflow.log_artifact(\"score_distribution.json\")\n",
    "        \n",
    "        # Log summary as JSON\n",
    "        with open(\"evaluation_summary.json\", \"w\") as f:\n",
    "            json.dump(eval_summary, f, indent=2)\n",
    "        mlflow.log_artifact(\"evaluation_summary.json\")\n",
    "        \n",
    "        # Create human-readable evaluation report\n",
    "        report = f\"\"\"=== EVALUATION REPORT: {template_name.upper()} ===\n",
    "Total Questions: {len(results)}\n",
    "Valid Responses: {len(scores)}\n",
    "Error Rate: {error_rate:.2%}\n",
    "\n",
    "QUALITY SCORES:\n",
    "- Average: {avg_score:.2f}/100\n",
    "- Range: {min_score:.0f} - {max_score:.0f}\n",
    "- Standard Deviation: {score_std:.2f}\n",
    "\n",
    "QUALITY DISTRIBUTION:\n",
    "- High Quality (75-100): {high_quality_responses} ({(high_quality_responses/len(scores)*100):.1f}%)\n",
    "- Medium Quality (50-74): {medium_quality_responses} ({(medium_quality_responses/len(scores)*100):.1f}%)\n",
    "- Low Quality (0-49): {low_quality_responses} ({(low_quality_responses/len(scores)*100):.1f}%)\n",
    "\n",
    "Evaluation completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\"\"\n",
    "        \n",
    "        # Log the report\n",
    "        mlflow.log_text(report, \"evaluation_report.txt\")\n",
    "        \n",
    "        print(f\"Template '{template_name}' - Average Quality Score: {avg_score:.2f}/100\")\n",
    "        print(f\"Valid Responses: {len(scores)}/{len(results)} (Error Rate: {error_rate:.2%})\")\n",
    "        \n",
    "        return eval_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b610ec-6408-4588-a1d1-345b8a3366ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # COMMAND ----------\n",
    "\n",
    "# Evaluate all templates\n",
    "evaluation_results = {}\n",
    "print(\"=== EVALUATION RESULTS ===\")\n",
    "for template_name, results in all_results.items():\n",
    "    eval_result = evaluate_template_performance(template_name, results)\n",
    "    evaluation_results[template_name] = eval_result\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "531fe875-0830-4182-be5e-f3a4380e3394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare all templates\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Template\": name,\n",
    "        \"Avg Quality Score\": result[\"avg_quality_score\"],\n",
    "        \"Valid Responses\": result[\"valid_responses\"],\n",
    "        \"Error Rate\": f\"{result['error_rate']:.1%}\",\n",
    "        \"High Quality %\": f\"{result['high_quality_percentage']:.1f}%\"\n",
    "    }\n",
    "    for name, result in evaluation_results.items()\n",
    "]).sort_values(\"Avg Quality Score\", ascending=False)\n",
    "\n",
    "print(\"\\n=== TEMPLATE COMPARISON ===\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Log comparison results in the same experiment\n",
    "with mlflow.start_run(run_name=\"template_comparison\"):\n",
    "    mlflow.log_param(\"comparison_timestamp\", datetime.now().isoformat())\n",
    "    mlflow.log_param(\"templates_compared\", list(evaluation_results.keys()))\n",
    "    \n",
    "    # Log best performing template\n",
    "    best_template = comparison_df.iloc[0][\"Template\"]\n",
    "    best_score = comparison_df.iloc[0][\"Avg Quality Score\"]\n",
    "    \n",
    "    mlflow.log_metric(\"best_template_score\", best_score)\n",
    "    mlflow.log_param(\"best_template_name\", best_template)\n",
    "    \n",
    "    # Save comparison table\n",
    "    comparison_df.to_csv(\"template_comparison.csv\", index=False)\n",
    "    mlflow.log_artifact(\"template_comparison.csv\")\n",
    "    \n",
    "    print(f\"\\nðŸ† Best performing template: {best_template} (Score: {best_score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18fb9b95-2ce6-49b1-9579-eaee7d308e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deployment and Monitoring\n",
    "Once we've done some development and evaluation, the next thing we will want to do is deploy the chatbot so it can easily be used by an application. The deployment abstracts away the logic of the agent / generative AI application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a7525e-a802-4046-872b-80e9d5c19876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from typing import Any, Generator, Optional, Sequence, Union\n",
    "\n",
    "# Deploy the best template for production use\n",
    "best_template_name = comparison_df.iloc[0][\"Template\"]\n",
    "best_template = prompt_templates[best_template_name]\n",
    "\n",
    "print(f\"ðŸš€ PRODUCTION DEPLOYMENT\")\n",
    "print(f\"Selected template: {best_template_name}\")\n",
    "print(f\"Template: {best_template}\")\n",
    "\n",
    "\n",
    "\n",
    "class ProdChatbot(ChatAgent):\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages[-1])}\n",
    "\n",
    "        messages = []\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        return ChatAgentResponse(messages=messages)\n",
    "\n",
    "def production_chatbot(question, user_id=None):\n",
    "    \"\"\"Production-ready chatbot function\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # The 'Logic'\n",
    "    formatted_prompt = best_template.format(question=question)\n",
    "    response = query_foundation_model(formatted_prompt, temperature=0.7)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test production deployment\n",
    "test_questions = [\n",
    "    \"I want to return my order, how do I do that?\",\n",
    "    \"What's your customer service phone number?\",\n",
    "    \"My package is late, what should I do?\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== PRODUCTION TESTING ===\")\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nTest {i}: {question}\")\n",
    "    response = production_chatbot(question, user_id=f\"test_user_{i}\")\n",
    "    print(f\"Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "0_walkthrough",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
