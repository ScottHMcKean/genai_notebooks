{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93720d31-cbdd-4798-8560-f70adef062e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Les MisÃ©r-AI-bles: Batching Through the Barricades\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efa9edf7-291e-4533-8fef-6fd156b3debf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install bs4 langchain-text-splitters databricks-agents --quiet\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e478c9e-7d0f-4b0a-8234-4e7e24eb21e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A Chunky View on Les Miserables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f70c1186-938b-4cc6-88b4-9915bfed667c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook takes Les Miserables from Project Gutenberg and prepared a spark dataframe with the chunked sections. It works fine on standard clusters or serverless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4be51c3-7b67-472b-ad28-86b0a3bae54d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters.html import HTMLHeaderTextSplitter\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://www.gutenberg.org/cache/epub/135/pg135-images.html')\n",
    "miserables_text = response.text\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h2\", \"Header 1\"),\n",
    "    (\"h3\", \"Header 2\"),\n",
    "    (\"h4\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(miserables_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0ccdda1-0131-41d2-bf83-2ab450e967d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The main reason I love this dataset is that it has a nice distribution of length and provides a reasonable small test of LLM query performance. See the graph below. We filter to chunks where the page content has at least 1,000 characters. This gives us 368 chunks, 365 of which are chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40a4f43b-7072-4aee-a6b6-da4ce35c32ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "valid_chunks = [x for x in html_header_splits if len(x.page_content) > 1000][1:]\n",
    "valid_chunk_lengths = [len(x.page_content) for x in valid_chunks]\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.hist(valid_chunk_lengths, bins=20, edgecolor='black')\n",
    "plt.title('Histogram of Valid Chunk Lengths')\n",
    "plt.xlabel('Length of Valid Chunks')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "804eabc7-698c-424b-b819-686aea72b84e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3913962-d767-4917-83c0-90f42c85ae13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_passage(passage):\n",
    "  return {\n",
    "    \"header_1\": passage.metadata.get('Header 1',\"\"),\n",
    "    \"header_2\": passage.metadata.get('Header 2',\"\"),\n",
    "    \"page_content\": passage.page_content\n",
    "}\n",
    "  \n",
    "extracted_passages = [extract_passage(x) for x in valid_chunks]\n",
    "les_mis_df = pd.DataFrame(extracted_passages).query(\"header_2 != ''\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f67d6c96-2eed-48cb-8b66-bdd28d603e61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We also want to ensure we have a consistent prompt for extracting structured data, so we pregenerate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6e1f94-a100-4113-8b5b-fe0ee3cf1169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def create_extraction_prompt(header1, header2, content):\n",
    "    \"\"\"\n",
    "    Create a prompt for structured data extraction from Les Miserables passages.\n",
    "    \n",
    "    Args:\n",
    "        header1 (str): The Header 1 content\n",
    "        header2 (str): The Header 2 content\n",
    "        content (str): The page content\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted prompt string\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "         Take this passage from Les Miserables and do structured data extraction in JSON. I want you to provide the title of the chapter, a list of characters, a synopsis of the chapter, and the overall sentiment of the chapter - positive, neutral, or negative. Do not make up anything if the passage isn't part of the novel.\n",
    "         \n",
    "         {header1}\n",
    "         {header2}\n",
    "         {content}\n",
    "         \"\"\"\n",
    "    return prompt\n",
    "\n",
    "extraction_prompt_udf = udf(create_extraction_prompt, StringType())\n",
    "\n",
    "les_mis_sp = spark.createDataFrame(les_mis_df).withColumn(\"extraction_prompt\", extraction_prompt_udf(\n",
    "  col(\"header_1\"), \n",
    "  col(\"header_2\"), \n",
    "  col(\"page_content\")\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aa9cdba-37cb-4a84-9b6b-7f8b8faf50cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "les_mis_sp.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(\"shm.default.les_mis_w_prompt\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3244118383129326,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "data_prep",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
