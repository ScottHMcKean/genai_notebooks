{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e478c9e-7d0f-4b0a-8234-4e7e24eb21e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Les Mis√©r-AI-bles: Batching Through the Barricades\n",
    "## Ray Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c131d17-d8ba-43f2-837b-017eaa336fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "One of the concerns with parallel execution is that we can quickly exhaust cluster memory with large tables. In order to evaluate this, we are going to multiply our LesMis table by a factor of 10. We also choose the small possible cluster in terms of memory / core - using drivers and workers that only have 2 GB per core. If we distribute our LesMis table to each worker directly, it will quickly exhaust the memory. We setup a simple task in Ray - to count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1c76533-0dcd-4eaa-b4b2-838e57649fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are going to hijack the Spark cluster immediately here to ensure we are in full Ray mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f27ab75-c550-4f3a-b196-d1e7d5c1439a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb5da09-faf3-4641-bb8b-e3766f4e58d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# call `ray.util.spark.shutdown_ray_cluster()` before initiating a new Ray cluster on spark\n",
    "# ray.util.spark.shutdown_ray_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fa22718-5f22-4cd4-8b61-bb787e84caea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ray.util.spark import setup_ray_cluster\n",
    "import ray\n",
    "\n",
    "setup_ray_cluster(\n",
    "  min_worker_nodes=3,\n",
    "  max_worker_nodes=3,\n",
    "  num_cpus_head_node=0,\n",
    "  num_gpus_worker_node=0,\n",
    "  num_cpus_worker_node=4,\n",
    "  num_gpus_head_node=0\n",
    ")\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c77aab6d-609b-4323-bf5b-1bfab9adfd53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Les Mis dataframe is tiny (13 MB in memory). So we want to make it much, much larger. We multiply the dataframe by 30 or 300 to make it larger for testing and performance loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59b26d28-8ae1-4d80-9fc1-16ebd6581012",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "les_mis = pd.read_parquet('./les_mis_w_prompt.parquet')\n",
    "print(les_mis.shape[0])\n",
    "les_mis.memory_usage(deep=True).sum() / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e66cc3-c3b0-4934-8d1f-95b8feec3db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mid_mis = pd.concat([les_mis] * 30, ignore_index=True)\n",
    "print(mid_mis.shape[0])\n",
    "mid_mis.memory_usage(deep=True).sum() / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f127c37-650a-45b8-aa53-2323def0fa94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "more_mis = pd.concat([les_mis] * 300, ignore_index=True)\n",
    "print(more_mis.shape[0])\n",
    "more_mis.memory_usage(deep=True).sum() / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f288bb3d-8096-4765-9a7d-74e2ee84b071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sentiment Analysis Many Tasks\n",
    "\n",
    "In order to test pure performance or many tasks, we use a small function that does sentiment analysis from the NLTK library. This is a very fast function that we want to run on every row of the dataframe, similar to an external model call. In a single thread, it takes around 5 seconds for the original `Les Mis` dataset and 146 seconds for the `Mid Mis` dataset. We don't even try the `More Mis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b877309-6a9f-4618-bc73-11906fb4fa6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(les_mis.iloc[0]['page_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d34954b7-fc77-43c6-abfa-eaf43b2cf9c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "def analyze_sentiment(row):\n",
    "    return sia.polarity_scores(row['page_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9f208f1-8270-4ca9-a89c-7c5291f752e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# les_mis_exec_time\n",
    "timeit.timeit(lambda: les_mis.apply(analyze_sentiment, axis=1), number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a72c545d-c10e-4be8-a90b-54abb37069c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mid_mis_exec_time\n",
    "timeit.timeit(lambda: mid_mis.apply(analyze_sentiment, axis=1), number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef86b737-5568-4ea0-b50c-3ff75cd30216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is where we create single references to the data using `ray.put()` This is essential for memory management, as if we pass the dataframes directly in the `ray.remote()` call, it can cause major issues on the cluster. Even with the `More Mis` dataset, we had no issues with the worker nodes due to passing small chunks of the large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d27a2476-5a8b-4443-9768-1659bc606db5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "les_mis_ref = ray.put(les_mis)\n",
    "mid_mis_ref = ray.put(mid_mis)\n",
    "more_mis_ref = ray.put(more_mis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b09a4740-95fc-4531-ad1d-6a52b48a48b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Approach 1 - Simple Map\n",
    "This simple map is the well, the simplest way to create a Ray job, and it works surprisingly well. The Ray Futures mechanisms deals with the data shuffling and avoids overloading the memory. It also is very fast and universally applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c7254e8-fd45-41c7-bfbe-eec77d7e0859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=1)\n",
    "def map_analyze_sentiment(data_dict):\n",
    "    data_dict['sentiment'] = analyze_sentiment(data_dict)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5697d4f8-8108-4e8e-bf07-4a1010d572b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "futures = [\n",
    "  map_analyze_sentiment.remote(data) \n",
    "  for data in more_mis.to_dict(orient='records')\n",
    "  ]\n",
    "results = ray.get(futures)\n",
    "print(f\"Processing time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e49bcd83-c0c1-4195-a71c-767d99feceda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Approach 2 - put() with Reference\n",
    "This code is meant to demonstrate the hazard of refering to an external object within a remote. If we use a `ray.put()` reference, it works well (although not as fast as the simple map above), but if we directly use `remote(i, df)`, then we start to see a lot of memory problems. This is generally a poor way to reference Ray objects, but gets the point across."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a516052-f675-43e6-bcff-755de470b02f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=1)\n",
    "def analyze_sentiment_row_idx(row_idx, df_ref):\n",
    "  row = df_ref.iloc[row_idx]\n",
    "  return analyze_sentiment(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b92b59fb-6f4b-4984-87e3-a469e71ee7ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ray.get(analyze_sentiment_row_idx.remote(0, more_mis_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed33d563-a952-4bc8-b230-f2b9d6ca839e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "row_indices = list(range(len(more_mis)))\n",
    "results_entire_df = ray.get(\n",
    "  [analyze_sentiment_row_idx.remote(i, more_mis_ref) \n",
    "   for i in row_indices\n",
    "   ])\n",
    "print(f\"Processing time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69248c92-d47a-4d40-98a9-d0f1cf01003a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Approach 3 - Ray Dataset\n",
    "This approach uses a Ray Dataset to manage mapping. There are a couple tricks here to make this work. First, we need to set the .map() concurrency to number of our cores with `num_cpus` = 1. You also need to repartition the dataset to make this work. I recommend very small partitions, for example size/(num_workers x 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c6abd76-ad1f-477f-a4c9-0aa5b3ad97e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment(row):\n",
    "    return sia.polarity_scores(row['page_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3224d074-877b-4194-8d70-7cbb7ba2773a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6717ea1d-2f39-43b2-97ec-a740c4d9a1f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partitions = int(more_mis.shape[0]/(12*10))\n",
    "ray_dataset = ray.data.from_pandas(more_mis).repartition(partitions)\n",
    "ray_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cdf88c8-f3e5-4798-b268-a51c63227676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "results = ray_dataset.map(\n",
    "  analyze_sentiment, \n",
    "  concurrency=12, \n",
    "  num_cpus=1,\n",
    "  ).take_all()\n",
    "print(f\"Processing time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8722e7ec-81ca-4bd9-854e-93b69aab7402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## LLM Calls\n",
    "Now that we've shown some of the options for controlling Ray memory and compute, let's make some LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94e9221-5c46-4239-b954-3e261e0a4aaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "workspace_client = WorkspaceClient()\n",
    "workspace_url = workspace_client.config.host\n",
    "\n",
    "# Check if running in Databricks\n",
    "import os\n",
    "\n",
    "if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n",
    "    token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "else:\n",
    "    token = workspace_client.config.token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fad26f56-498e-4154-b0f9-f2c15a3f5d23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "#@ray.remote(num_cpus=1, num_gpus=1)\n",
    "def extract_data_from_passage_ray(data_dict):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "    \n",
    "        client = OpenAI(\n",
    "            api_key=token,\n",
    "            base_url=f\"{workspace_url}/serving-endpoints\",\n",
    "        )\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model='azure-gpt-4o-mini',\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": data_dict['extraction_prompt']}\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        data_dict['extracted_data'] = response.choices[0].message.content.replace(\"json\\n\",\"\").replace(\"\",\"\") + f\"time: {elapsed_time:.2f}\"\n",
    "    except: \n",
    "        data_dict['extracted_data'] = \"ERROR\"\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a554f649-e0b5-4861-9a2e-150f8e210d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "futures = [\n",
    "  extract_data_from_passage_ray.remote(data) \n",
    "  for data in les_mis.to_dict(orient='records')\n",
    "  ] \n",
    "results = ray.get(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b2e7a4-8af6-4858-8eff-6d823371f2e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(results)\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d56e74d4-cb12-43b1-bd5b-eac35ea611c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save as files since we've hijacked our spark clusters\n",
    "result_df.to_csv(\"/Volumes/shm/default/llm_profiling/les_mis_df-azure-o1-ray.csv\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "les_miserables_ray",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
