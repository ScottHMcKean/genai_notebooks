{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e98a9680-927e-4169-9bb4-cb580e745dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kagglehub openai\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "233597d2-cb80-4478-8cc4-0e93958ec725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "from openai import OpenAI\n",
    "import kagglehub\n",
    "\n",
    "TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "URL = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "154f5bd2-b1e7-4333-a1ce-81f87aad790b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Download Check Data\n",
    "We are going to use the Kaggle check dataset and Unity Catalog volumes for our first dataset. We download the files to temporary directory and then copy them over to a Volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c08f8924-69f7-4573-b972-6d3364cdbb0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Setup our volume\n",
    "CREATE VOLUME IF NOT EXISTS shm.default.bank_checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4518f4c1-9f94-43cc-8187-20968657c9cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Download the dataset and save it to the volume\n",
    "path = Path(kagglehub.dataset_download(\"saifkhichi96/bank-checks-signatures-segmentation-dataset\"))\n",
    "\n",
    "subdir = \"TestSet/X/\"\n",
    "file_dir = path / subdir\n",
    "for file_name in os.listdir(file_dir):\n",
    "    file_path = path / subdir / file_name\n",
    "    shutil.copy(file_path, f\"/Volumes/shm/default/bank_checks/{file_name}\")\n",
    "\n",
    "# Use Spark's read binary file to load all the images into a table\n",
    "images_df = (\n",
    "  spark.read.format(\"binaryFile\")\n",
    "  .load(\"/Volumes/shm/default/bank_checks/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4c6c12c-19dd-447d-8759-fc7c1602d6cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parse Check Data\n",
    "Now that we have a table of images (this could also be PDF pages etc.!), we simply pass the images in batch to a multimodal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ac56db9-16f1-4fdf-97dd-544b8e2423a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "image_data= base64.b64encode(\n",
    "  images_df.collect()[0]['content']\n",
    "  ).decode(\"utf-8\")\n",
    "\n",
    "display(Image.open(io.BytesIO(base64.b64decode(image_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5744f93d-4a60-4beb-9fb7-7732db57227a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=TOKEN, base_url=f\"{URL}/serving-endpoints/\")\n",
    "\n",
    "structured_prompt = \"\"\"Extract all information from this check and return ONLY valid JSON with:\n",
    "{\n",
    "  'date': 'yyyy-mm-dd',\n",
    "  'payee_name': 'string',\n",
    "  'dollar_amount': 'float',\n",
    "  'reason_for_payment': 'string',\n",
    "  'check_number': 'integer'\n",
    "  'sender_name': 'string',\n",
    "  'sender_address': 'string'\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": structured_prompt\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Extract all infromation from this image\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}}\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  model=\"databricks-claude-3-7-sonnet\",\n",
    "  max_tokens=512,\n",
    ")\n",
    "\n",
    "parsed_text = chat_completion.choices[0].message.content\n",
    "print(parsed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82762d12-0e61-45ee-9a6d-c76a2cf03b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run it at Scale\n",
    "We now move from a single call to operating at scale. The beauty here is that we can scale horizontally to massively reduce our compute time. \n",
    "\n",
    "What if you could parse every document in your legal department in a couple of days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60a209bd-a437-4d18-8463-36e56d902499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define It!\n",
    "output_schema = StructType([\n",
    "    StructField(\"date\", StringType(), nullable=True),\n",
    "    StructField(\"payee_name\", StringType(), nullable=True),\n",
    "    StructField(\"dollar_amount\", FloatType(), nullable=True),\n",
    "    StructField(\"reason_for_payment\", StringType(), nullable=True),\n",
    "    StructField(\"check_number\", IntegerType(), nullable=True),\n",
    "    StructField(\"sender_name\", StringType(), nullable=True),\n",
    "    StructField(\"sender_address\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "def process_image(content):\n",
    "    client = OpenAI(api_key=TOKEN, base_url=f\"{URL}/serving-endpoints/\")\n",
    "\n",
    "    image_data = base64.b64encode(content).decode(\"utf-8\")\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": structured_prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{image_data}\"\n",
    "                }}\n",
    "            ]\n",
    "        }],\n",
    "        model=\"databricks-claude-3-7-sonnet\",\n",
    "        max_tokens=512\n",
    "    )\n",
    "    \n",
    "    raw_text = response.choices[0].message.content\n",
    "    clean_text = raw_text.replace('```','').replace('json','')\n",
    "    return json.loads(clean_text)\n",
    "\n",
    "# Scale It!\n",
    "process_image_udf = F.udf(process_image, output_schema)\n",
    "\n",
    "# Run It!\n",
    "(\n",
    "  images_df\n",
    "  .withColumn('extracted_info', process_image_udf(F.col('content')))\n",
    "  .select('path','extracted_info')\n",
    "  .display()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2275659825069930,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "bank_checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
