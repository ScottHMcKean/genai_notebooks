{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09015a9e-76ae-44a4-9db2-640852476d91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq backoff databricks-openai uv databricks-agents mlflow[databricks] azure-search azure-search-documents\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e88b405-0e8f-4fe5-bd29-09e323c53dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AZURE_SEARCH_API_KEY\"] = dbutils.secrets.get('shm','aisearch')\n",
    "os.environ[\"AZURE_SEARCH_ENDPOINT\"] = \"https://fieldengeast-ai-search.search.windows.net\"\n",
    "os.environ[\"AZURE_SEARCH_INDEX\"] = \"dbmma-manufacturing\"\n",
    "os.environ[\"DATABRICKS_MODEL_ENDPOINT\"] = \"databricks-claude-sonnet-4-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43a76ed9-2caa-4171-b857-a75db319fc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile azure_agent.py\n",
    "import mlflow\n",
    "import uuid\n",
    "from typing import Dict, List, Optional, Generator\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "\n",
    "class EDCAgentModel(mlflow.pyfunc.ResponsesAgent):\n",
    "    \"\"\"\n",
    "    MLflow ResponsesAgent wrapper for EDC AI Summarization Agent.\n",
    "    Uses the same prompt template as simple_knowledge_extraction notebook.\n",
    "    \"\"\"\n",
    "    \n",
    "    # EDC Summarization Prompt Template (exact same as original notebook)\n",
    "    PROMPT_TEMPLATE = \"\"\"[SUMMARIZATION RULES]\n",
    "\n",
    "[SYSTEM]\n",
    "You are an AI summarizer for Export Development Canada (EDC). Answer the query using ONLY the provided search results.\n",
    "If insufficient info, reply: 'INSUFFICIENT_INFORMATION'.\n",
    "\n",
    "WRITING STYLE (EDC):\n",
    "- Plain language, short sentences (15–20 words max).\n",
    "- Inverted pyramid: begin with the most important point.\n",
    "- Professional but friendly tone.\n",
    "- Headings: Capitalize first word only (unless proper names/titles).\n",
    "- Write in second person (\"you\" not \"companies\").\n",
    "- EDC products: Capitalize all EDC products (start with \"EDC\").\n",
    "- Canadian spelling (e.g., \"favour\", \"centre\").\n",
    "- No jargon, idioms, or unnecessary adjectives.\n",
    "- Use consistent formatting: headings (##, ###), bullets, bold for emphasis.\n",
    "\n",
    "CONTENT FOCUS:\n",
    "- Emphasize Canadian exporters and EDC services: financing, credit insurance, bonding, market intelligence, Trading.\n",
    "- Synthesize information across results.\n",
    "- Maximum length: {max_words} words.\n",
    "\n",
    "GUARDRAILS:\n",
    "- Do not invent or assume facts.\n",
    "- Use only the provided search results.\n",
    "- Every factual statement must include an inline numbered citation [1], [2], [3].\n",
    "- No URLs. Do not add a citation list at the end.\n",
    "\n",
    "[CONTEXT]\n",
    "USER QUERY: {query}\n",
    "SEARCH RESULTS: {contents}\n",
    "\n",
    "[INSTRUCTIONS]\n",
    "1. Start with a direct answer under a level-2 heading (##).\n",
    "2. Add a Key Points section for supporting details or recommendations.\n",
    "3. Use bullets for clarity and bold for emphasis.\n",
    "4. Prioritize higher-scoring results first.\n",
    "5. Keep sentences short, tone neutral, and spelling Canadian\n",
    "\n",
    "[RESPONSE FORMAT]\n",
    "## [Main Answer]\n",
    "[Concise synthesis with inline citations like [1]]\n",
    "\n",
    "### Key points\n",
    "- [Insight with citation like [2]]\n",
    "- [Recommendation with citations like [1], [3]]\n",
    "\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the agent with Azure AI Search and Databricks clients.\"\"\"\n",
    "        super().__init__()\n",
    "        from azure.search.documents import SearchClient\n",
    "        from azure.core.credentials import AzureKeyCredential\n",
    "        import os\n",
    "        \n",
    "        # Load configuration\n",
    "        endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "        index = os.getenv(\"AZURE_SEARCH_INDEX\")\n",
    "        api_key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "        self.model_endpoint = os.getenv(\"DATABRICKS_MODEL_ENDPOINT\")\n",
    "        \n",
    "        # Initialize search client\n",
    "        credential = AzureKeyCredential(api_key)\n",
    "        self.search_client = SearchClient(\n",
    "            endpoint=endpoint,\n",
    "            index_name=index,\n",
    "            credential=credential\n",
    "        )\n",
    "        \n",
    "        # Initialize Databricks client if model endpoint is configured\n",
    "        if self.model_endpoint:\n",
    "            from databricks.sdk import WorkspaceClient\n",
    "            self.workspace_client = WorkspaceClient()\n",
    "        \n",
    "        print(\"✅ EDC Agent initialized successfully\")\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        \"\"\"Load context (called by MLflow when loading logged model).\"\"\"\n",
    "        from azure.search.documents import SearchClient\n",
    "        from azure.core.credentials import AzureKeyCredential\n",
    "        import os\n",
    "        \n",
    "        # Load configuration\n",
    "        endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "        index = os.getenv(\"AZURE_SEARCH_INDEX\")\n",
    "        api_key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "        self.model_endpoint = os.getenv(\"DATABRICKS_MODEL_ENDPOINT\")\n",
    "        \n",
    "        # Initialize search client\n",
    "        credential = AzureKeyCredential(api_key)\n",
    "        self.search_client = SearchClient(\n",
    "            endpoint=endpoint,\n",
    "            index_name=index,\n",
    "            credential=credential\n",
    "        )\n",
    "        \n",
    "        # Initialize Databricks client if model endpoint is configured\n",
    "        if self.model_endpoint:\n",
    "            from databricks.sdk import WorkspaceClient\n",
    "            self.workspace_client = WorkspaceClient()\n",
    "        \n",
    "        print(\"✅ EDC Agent initialized successfully\")\n",
    "    \n",
    "    def _extract_knowledge(self, query: str, top: int = 6) -> Dict:\n",
    "        \"\"\"Extract knowledge from Azure AI Search with keyword search.\"\"\"\n",
    "        from azure.search.documents.models import QueryType\n",
    "        \n",
    "        # Build search parameters for keyword search\n",
    "        search_params = {\n",
    "            'search_text': query,\n",
    "            'top': top,\n",
    "            'include_total_count': True,\n",
    "            'select': ['id', 'content'],\n",
    "            'query_type': QueryType.FULL,\n",
    "        }\n",
    "        \n",
    "        # Execute search\n",
    "        results = self.search_client.search(**search_params)\n",
    "        \n",
    "        # Extract documents with all available information\n",
    "        documents = []\n",
    "        raw_results = []\n",
    "        for result in results:\n",
    "            # Store the complete result for reference\n",
    "            raw_result = dict(result)\n",
    "            raw_results.append(raw_result)\n",
    "            \n",
    "            # Extract key fields for processing\n",
    "            doc = {\n",
    "                'id': result.get('id', ''),\n",
    "                'content': result.get('content', ''),\n",
    "            }\n",
    "            documents.append(doc)\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'total_results': results.get_count(),\n",
    "            'documents': documents,\n",
    "            'raw_results': raw_results,\n",
    "            'ranking_mode': 'keyword search'\n",
    "        }\n",
    "    \n",
    "    def _format_search_results(self, search_results: List[Dict]) -> str:\n",
    "        \"\"\"Format search results for the prompt with full content.\"\"\"\n",
    "        formatted = []\n",
    "        \n",
    "        for i, doc in enumerate(search_results, 1):\n",
    "            id = doc.get('id', '')\n",
    "            content = doc.get('content', '')\n",
    "            \n",
    "            formatted.append(\n",
    "                f\"[{i}] Id: {id}\\n\"\n",
    "                f\"Content: {content}\\n\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n---\\n\".join(formatted)\n",
    "    \n",
    "    def _generate_summary(self, query: str, search_results: List[Dict], max_words: int) -> str:\n",
    "        \"\"\"Generate summary using the EDC prompt template.\"\"\"\n",
    "        \n",
    "        if not search_results:\n",
    "            return \"INSUFFICIENT_INFORMATION\"\n",
    "        \n",
    "        # Format search results\n",
    "        formatted_contents = self._format_search_results(search_results)\n",
    "        \n",
    "        # Build the prompt using the EDC template\n",
    "        prompt = self.PROMPT_TEMPLATE.format(\n",
    "            max_words=max_words,\n",
    "            query=query,\n",
    "            contents=formatted_contents\n",
    "        )\n",
    "        \n",
    "        # Generate summary\n",
    "        if self.model_endpoint:\n",
    "            return self._call_databricks_model(prompt)\n",
    "        else:\n",
    "            return \"NO_LLM_CONFIGURED: Please set DATABRICKS_MODEL_ENDPOINT to use LLM-based summarization\"\n",
    "    \n",
    "    def _call_databricks_model(self, prompt: str) -> str:\n",
    "        \"\"\"Call Databricks model endpoint for summarization.\"\"\"\n",
    "        try:\n",
    "            from databricks.sdk.service.serving import ChatMessage, ChatMessageRole\n",
    "            \n",
    "            response = self.workspace_client.serving_endpoints.query(\n",
    "                name=self.model_endpoint,\n",
    "                messages=[\n",
    "                    ChatMessage(\n",
    "                        role=ChatMessageRole.USER,\n",
    "                        content=prompt\n",
    "                    )\n",
    "                ],\n",
    "                max_tokens=1000,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            if response and response.choices:\n",
    "                return response.choices[0].message.content\n",
    "            else:\n",
    "                return \"Error: No response from model\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Error calling model: {str(e)}\"\n",
    "    \n",
    "    def predict(self, context=None, request: ResponsesAgentRequest=None) -> ResponsesAgentResponse:\n",
    "        \"\"\"Handle agent queries using ResponsesAgent interface.\"\"\"\n",
    "        try:\n",
    "            # Handle different calling conventions\n",
    "            if request is None and context is not None:\n",
    "                if isinstance(context, dict):\n",
    "                    request = context\n",
    "                    context = None\n",
    "            \n",
    "            if request is None:\n",
    "                raise ValueError(\"No request data provided\")\n",
    "            \n",
    "            # Extract the user message\n",
    "            messages = request.get('input', [])\n",
    "            if not messages:\n",
    "                raise ValueError(\"No messages provided in request\")\n",
    "            \n",
    "            # Get the last user message\n",
    "            user_message = None\n",
    "            for msg in reversed(messages):\n",
    "                if msg.get('role') == 'user':\n",
    "                    user_message = msg\n",
    "                    break\n",
    "            \n",
    "            if not user_message:\n",
    "                raise ValueError(\"No user message found in request\")\n",
    "            \n",
    "            # Extract query from message content\n",
    "            content = user_message.get('content', '')\n",
    "            if isinstance(content, list):\n",
    "                query = ' '.join([c.get('text', '') for c in content if c.get('type') == 'text'])\n",
    "            else:\n",
    "                query = content\n",
    "            \n",
    "            # Extract optional parameters\n",
    "            top_k = 6\n",
    "            max_words = 800\n",
    "            \n",
    "            # Extract knowledge from Azure Search\n",
    "            search_result = self._extract_knowledge(query=query, top=top_k)\n",
    "            \n",
    "            # Generate summary using EDC prompt\n",
    "            summary = self._generate_summary(\n",
    "                query=query,\n",
    "                search_results=search_result['documents'],\n",
    "                max_words=max_words\n",
    "            )\n",
    "            \n",
    "            # Create output with references in custom_outputs\n",
    "            return ResponsesAgentResponse(\n",
    "                output=[\n",
    "                    {\n",
    "                        \"type\": \"message\",\n",
    "                        \"id\": str(uuid.uuid4()),\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"output_text\",\n",
    "                                \"text\": summary\n",
    "                            }\n",
    "                        ],\n",
    "                        \"status\": \"completed\"\n",
    "                    }\n",
    "                ],\n",
    "                custom_outputs={\n",
    "                    \"references\": search_result['raw_results'],\n",
    "                    \"total_results\": search_result['total_results'],\n",
    "                    \"ranking_mode\": search_result['ranking_mode'],\n",
    "                    \"query\": query\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ResponsesAgentResponse(\n",
    "                output=[\n",
    "                    {\n",
    "                        \"type\": \"message\",\n",
    "                        \"id\": str(uuid.uuid4()),\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"output_text\",\n",
    "                                \"text\": f\"Error: {str(e)}\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"status\": \"completed\"\n",
    "                    }\n",
    "                ],\n",
    "                custom_outputs={\"error\": str(e)}\n",
    "            )\n",
    "    \n",
    "    def predict_stream(\n",
    "        self, \n",
    "        context=None, \n",
    "        request: ResponsesAgentRequest = None\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"Handle streaming agent queries using ResponsesAgent interface.\"\"\"\n",
    "        try:\n",
    "            # Handle different calling conventions\n",
    "            if request is None and context is not None:\n",
    "                if isinstance(context, dict):\n",
    "                    request = context\n",
    "                    context = None\n",
    "            \n",
    "            if request is None:\n",
    "                raise ValueError(\"No request data provided\")\n",
    "            \n",
    "            # Extract the user message\n",
    "            messages = request.get('input', [])\n",
    "            if not messages:\n",
    "                raise ValueError(\"No messages provided in request\")\n",
    "            \n",
    "            # Get the last user message\n",
    "            user_message = None\n",
    "            for msg in reversed(messages):\n",
    "                if msg.get('role') == 'user':\n",
    "                    user_message = msg\n",
    "                    break\n",
    "            \n",
    "            if not user_message:\n",
    "                raise ValueError(\"No user message found in request\")\n",
    "            \n",
    "            # Extract query from message content\n",
    "            content = user_message.get('content', '')\n",
    "            if isinstance(content, list):\n",
    "                query = ' '.join([c.get('text', '') for c in content if c.get('type') == 'text'])\n",
    "            else:\n",
    "                query = content\n",
    "            \n",
    "            # Extract optional parameters\n",
    "            top_k = 6\n",
    "            max_words = 800\n",
    "            \n",
    "            # Extract knowledge from Azure Search\n",
    "            search_result = self._extract_knowledge(query=query, top=top_k)\n",
    "            \n",
    "            # Generate summary\n",
    "            summary = self._generate_summary(\n",
    "                query=query,\n",
    "                search_results=search_result['documents'],\n",
    "                max_words=max_words\n",
    "            )\n",
    "            \n",
    "            # Stream the response in chunks\n",
    "            item_id = str(uuid.uuid4())\n",
    "            chunk_size = 50\n",
    "            \n",
    "            # Manual streaming format\n",
    "            for i in range(0, len(summary), chunk_size):\n",
    "                chunk = summary[i:i + chunk_size]\n",
    "                yield ResponsesAgentStreamEvent(\n",
    "                    event=\"response.output_text.delta\",\n",
    "                    type=\"message\",\n",
    "                    delta={\"text\": chunk},\n",
    "                    item_id=item_id\n",
    "                )\n",
    "            \n",
    "            # Send final done event with complete text and references\n",
    "            yield ResponsesAgentStreamEvent(\n",
    "                event=\"response.output_item.done\",\n",
    "                type=\"message\",\n",
    "                item={\n",
    "                    \"type\": \"message\",\n",
    "                    \"id\": item_id,\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"output_text\",\n",
    "                            \"text\": summary\n",
    "                        }\n",
    "                    ],\n",
    "                    \"status\": \"completed\"\n",
    "                },\n",
    "                custom_outputs={\n",
    "                    \"references\": search_result['raw_results'],\n",
    "                    \"total_results\": search_result['total_results'],\n",
    "                    \"ranking_mode\": search_result['ranking_mode'],\n",
    "                    \"query\": query\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_id = str(uuid.uuid4())\n",
    "            error_msg = f\"Error: {str(e)}\"\n",
    "            \n",
    "            yield ResponsesAgentStreamEvent(\n",
    "                event=\"response.output_text.delta\",\n",
    "                type=\"message\",\n",
    "                delta={\"text\": error_msg},\n",
    "                item_id=error_id\n",
    "            )\n",
    "            \n",
    "            yield ResponsesAgentStreamEvent(\n",
    "                event=\"response.output_item.done\",\n",
    "                type=\"message\",\n",
    "                item={\n",
    "                    \"type\": \"message\",\n",
    "                    \"id\": error_id,\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"output_text\", \"text\": error_msg}\n",
    "                    ],\n",
    "                    \"status\": \"completed\"\n",
    "                },\n",
    "                custom_outputs={\"error\": str(e)}\n",
    "            )\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "AGENT = EDCAgentModel()\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94a99bc-0c7c-4f9e-9389-0f2da54f95c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure_agent import AGENT\n",
    "\n",
    "# ResponsesAgent expects a dict with 'input' key containing messages\n",
    "request = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the biggest considerations with Disaster recovery?\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Call predict with context=None and the request dict\n",
    "output = AGENT.predict(context=None, request=request)\n",
    "\n",
    "# Display the response\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e030deee-c254-4ffe-9c0e-0c699535b720",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display references"
    }
   },
   "outputs": [],
   "source": [
    "# Access the summary text\n",
    "summary = output.output[0].content[0]['text']\n",
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(summary)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Access the references from custom_outputs\n",
    "references = output.custom_outputs.get('references', [])\n",
    "total_results = output.custom_outputs.get('total_results', 0)\n",
    "ranking_mode = output.custom_outputs.get('ranking_mode', '')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"REFERENCES ({len(references)} shown out of {total_results} total results)\")\n",
    "print(f\"Ranking mode: {ranking_mode}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, ref in enumerate(references, 1):\n",
    "    print(f\"\\n[{i}] ID: {ref.get('id', 'N/A')}\")\n",
    "    print(f\"Score: {ref.get('@search.score', 'N/A')}\")\n",
    "    print(f\"Content: {ref.get('content', 'N/A')[:200]}...\")  # Show first 200 chars\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f563c28-4319-4f17-8212-51b5bcf995ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test streaming response"
    }
   },
   "outputs": [],
   "source": [
    "# Test the streaming predict method\n",
    "for chunk in AGENT.predict_stream(\n",
    "    context=None,\n",
    "    request={\n",
    "        \"input\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What are the biggest considerations with Disaster recovery?\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    print(chunk.model_dump(exclude_none=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74246d2-19e4-49b7-b2c5-d9a3f85e5294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log the agent as an MLflow model\n",
    "\n",
    "Log the agent to MLflow for tracking, versioning, and deployment. We'll specify the Azure Search endpoint and Databricks model endpoint as resources for automatic authentication passthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed58423-3135-4eea-9ab6-f3e27c1e1c1a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Log model with resources"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.resources import DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "# Define resources for automatic auth passthrough at deployment time\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=os.getenv(\"DATABRICKS_MODEL_ENDPOINT\"))\n",
    "]\n",
    "\n",
    "# Define input example for model signature\n",
    "input_example = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What EDC services are available for Canadian exporters in 2025?\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Log the agent model\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"edc_agent\",\n",
    "        python_model=\"azure_agent.py\",\n",
    "        input_example=input_example,\n",
    "        pip_requirements=[\n",
    "            \"azure-search\",\n",
    "            \"azure-search-documents\",\n",
    "            \"databricks-sdk\",\n",
    "            \"databricks-openai\",\n",
    "            f\"mlflow>={mlflow.__version__}\",\n",
    "        ],\n",
    "        registered_model_name=\"shm.default.azure_agent\",\n",
    "        resources=resources,\n",
    "    )\n",
    "\n",
    "print(f\"Model logged with run_id: {logged_agent_info.run_id}\")\n",
    "print(f\"Model URI: {logged_agent_info.model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f37c5bf-5429-41c7-b58e-6fd17fd00dd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate the agent with Agent Evaluation\n",
    "\n",
    "Evaluate your agent with predefined LLM scorers to assess quality metrics like relevance and safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c4305b2-2bb3-4d6e-9655-2e7894aaa960",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run evaluation"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import RelevanceToQuery, Safety\n",
    "\n",
    "# Define evaluation dataset\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"What EDC services are available for Canadian exporters in 2025?\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"expected_response\": None\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"What are the biggest considerations with Disaster recovery?\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"expected_response\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=lambda input: agent.predict(context=None, request={\"input\": input}),\n",
    "    scorers=[RelevanceToQuery(), Safety()],\n",
    ")\n",
    "\n",
    "print(\"Evaluation complete. Review results in the MLflow UI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8420220b-9fca-497b-af6a-2955f0c41c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Perform pre-deployment validation\n",
    "\n",
    "Before registering and deploying the agent, validate that it works correctly with the logged model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7cd632-d297-4c91-bf6c-1068cd7682f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validate model"
    }
   },
   "outputs": [],
   "source": [
    "# Validate the logged model before deployment\n",
    "validation_result = mlflow.models.predict(\n",
    "    model_uri=f\"models:/shm.default.azure_agent/5\",\n",
    "    input_data={\n",
    "        \"input\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What are the main considerations for disaster recovery?\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    env_manager=\"uv\",\n",
    ")\n",
    "\n",
    "print(\"Validation successful!\")\n",
    "print(validation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2fea8a3-2282-40a0-aaf1-ac5f8b149d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register the model to Unity Catalog\n",
    "\n",
    "Update the `catalog`, `schema`, and `model_name` below to register the MLflow model to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8339741e-0583-4076-818c-d98a6f0cc49f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Register model to UC"
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: Define the catalog, schema, and model name for your UC model\n",
    "catalog = \"shm\"  # Update with your catalog\n",
    "schema = \"default\"  # Update with your schema\n",
    "model_name = \"azure_agent\"  # Update with your model name\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# Register the model to Unity Catalog\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, \n",
    "    name=UC_MODEL_NAME\n",
    ")\n",
    "\n",
    "print(f\"Model registered to Unity Catalog: {UC_MODEL_NAME}\")\n",
    "print(f\"Version: {uc_registered_model_info.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81177631-fc33-4a22-a237-2ad2ccd4831e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the agent\n",
    "\n",
    "Deploy the agent to a Model Serving endpoint for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69ab852-2431-46c2-8d1b-c9c011e368d0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Deploy to Model Serving"
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "# Deploy the agent to Model Serving\n",
    "# NOTE: Pass scale_to_zero=True to enable scale-to-zero for cost savings\n",
    "# This is not recommended for production workloads\n",
    "deployment_info = agents.deploy(\n",
    "    UC_MODEL_NAME, \n",
    "    uc_registered_model_info.version,\n",
    "    tags={\"endpointSource\": \"notebook\", \"agent_type\": \"edc_summarization\"}\n",
    ")\n",
    "\n",
    "print(f\"Agent deployed successfully!\")\n",
    "print(f\"Endpoint name: {UC_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "516e55a8-dd28-4d7a-9fa8-17e5a4b0a477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "After your agent is deployed, you can:\n",
    "\n",
    "* **Test in AI Playground**: Chat with your agent and review responses with references\n",
    "* **Share with stakeholders**: Get feedback from SMEs in your organization\n",
    "* **Embed in applications**: Use the Model Serving endpoint in production applications\n",
    "* **Monitor performance**: Track usage and quality metrics in the Agent Monitoring dashboard\n",
    "\n",
    "### Example API call to deployed agent:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import os\n",
    "\n",
    "response = requests.post(\n",
    "    f\"https://{os.getenv('DATABRICKS_HOST')}/serving-endpoints/{UC_MODEL_NAME}/invocations\",\n",
    "    headers={\"Authorization\": f\"Bearer {os.getenv('DATABRICKS_TOKEN')}\"},\n",
    "    json={\n",
    "        \"input\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What EDC services are available for Canadian exporters?\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "result = response.json()\n",
    "summary = result['output'][0]['content'][0]['text']\n",
    "references = result['custom_outputs']['references']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6feb46-93db-426d-a4d6-caa612f5c446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "azure_search_responses_agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
