[
    {
        "id": 1,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/audit-aws-credentials.html",
        "content": "Step 2: Configure credentials for audit log delivery  \nThis article describes how to set up IAM services for audit log delivery. To use different credentials for different workspaces, repeat the procedures in this article for each workspace or group of workspaces.  \nNote  \nTo use different S3 bucket names, you need to create separate IAM roles.  \nCreate the IAM role\nCreate the IAM role\nLog into your AWS Console as a user with administrator privileges and go to the IAM service.  \nClick the Roles tab in the sidebar.  \nClick Create role.  \nIn Select type of trusted entity, click AWS service.  \nUnder Use Case, select EC2.  \nClick the Next button.  \nClick the Next button.  \nIn the Role name field, enter a role name.  \nClick Create role. The list of roles displays.\n\nCreate the inline policy"
    },
    {
        "id": 2,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/audit-aws-credentials.html",
        "content": "Create the inline policy\nIn the list of roles, click the role you created.  \nAdd an inline policy.  \nOn the Permissions tab, click Add permissions then Create inline policy.  \nIn the policy editor, click the JSON tab.  \nCopy this access policy and modify it. Replace the following values in the policy with your own configuration values:  \n<s3-bucket-name>: The bucket name of your AWS S3 bucket.  \n<s3-bucket-path-prefix>: (Optional) The path to the delivery location in the S3 bucket. If unspecified, the logs are delivered to the root of the bucket. This path must match the delivery_path_prefix argument when you call the log delivery API.  \n{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetBucketLocation\" ], \"Resource\":[ \"arn:aws:s3:::<s3-bucket-name>\" ] }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:PutObject\", \"s3:GetObject\", \"s3:DeleteObject\", \"s3:PutObjectAcl\", \"s3:AbortMultipartUpload\" ], \"Resource\":[ \"arn:aws:s3:::<s3-bucket-name>/<s3-bucket-path-prefix>/\", \"arn:aws:s3:::<s3-bucket-name>/<s3-bucket-path-prefix>/*\" ] }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:ListMultipartUploadParts\", \"s3:ListBucketMultipartUploads\" ], \"Resource\":\"arn:aws:s3:::<s3-bucket-name>\", \"Condition\":{ \"StringLike\":{ \"s3:prefix\":[ \"<s3-bucket-path-prefix>\", \"<s3-bucket-path-prefix>/*\" ] } } } ] }"
    },
    {
        "id": 3,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/audit-aws-credentials.html",
        "content": "You can customize the policy usage of the path prefix in the following ways:  \nIf you do not want to use the bucket path prefix, remove <s3-bucket-path-prefix>/ (including the final slash) from the policy each time it appears.  \nIf you want log delivery configurations for different workspaces that share the S3 bucket but use different path prefixes, you can include multiple path prefixes. There are two separate parts of the policy that reference <s3-bucket-path-prefix>. For each case, duplicate the two lines that reference the path prefix. For example:  \n{ \"Resource\":[ \"arn:aws:s3:::<mybucketname>/field-team/\", \"arn:aws:s3:::<mybucketname>/field-team/*\", \"arn:aws:s3:::<mybucketname>/finance-team/\", \"arn:aws:s3:::<mybucketname>/finance-team/*\" ] }  \nClick Review policy.  \nIn the Name field, enter a policy name.  \nClick Create policy.  \nIf you use service control policies to deny certain actions at the AWS account level, ensure that sts:AssumeRole is whitelisted so Databricks can assume the cross-account role."
    },
    {
        "id": 4,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/audit-aws-credentials.html",
        "content": "Create the trust policy\nOn the role summary page, click the Trust Relationships tab.  \nPaste this access policy into the editor, replacing <databricks-account-id> with your Databricks account ID. Note the policy is slightly different if you use Databricks on AWS GovCloud.  \n{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Principal\":{ \"AWS\":\"arn:aws:iam::414351767826:role/SaasUsageDeliveryRole-prod-IAMRole-3PLHICCRR1TK\" }, \"Action\":\"sts:AssumeRole\", \"Condition\":{ \"StringEquals\":{ \"sts:ExternalId\":[ \"<databricks-account-id>\" ] } } } ] }  \n{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Principal\":{ \"AWS\":\"arn:aws-us-gov:iam::044793339203:role/SaasUsageDeliveryRole-prod-aws-gov-IAMRole-L4QM0RCHYQ1G\" }, \"Action\":\"sts:AssumeRole\", \"Condition\":{ \"StringEquals\":{ \"sts:ExternalId\":[ \"<databricks-account-id>\" ] } } } ] }  \nIn the role summary, copy the Role ARN. You need this value to call the create credential configuration API in the next step."
    },
    {
        "id": 5,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/audit-aws-credentials.html",
        "content": "Call the create\u00a0credential configuration\u00a0API\nTo finish settings up your credentials, call the Create\u00a0credential configuration\u00a0API.  \nThis request establishes cross-account trust and returns a reference ID you can use when creating a new workspace.  \nReplace <account-id> with your Databricks account ID.  \nSet credentials_name to a name that is unique within your account.  \nSet role_arn to the role ARN that you just created.  \nThe response body includes a credentials_id field. Copy this field so you can use it to create the log delivery configuration in Step 4.  \nFor example:  \ncurl -X POST -n \\ 'https://accounts.cloud.databricks.com/api/2.0/accounts/<databricks-account-id>/credentials' \\ -d '{ \"credentials_name\": \"databricks-credentials-v1\", \"aws_credentials\": { \"sts_role\": { \"role_arn\": \"arn:aws:iam::<aws-account-id>:role/my-company-example-role\" } } }'  \nExample response:  \n{ \"credentials_id\": \"<databricks-credentials-id>\", \"account_id\": \"<databricks-account-id>\", \"aws_credentials\": { \"sts_role\": { \"role_arn\": \"arn:aws:iam::<aws-account-id>:role/my-company-example-role\", \"external_id\": \"<databricks-account-id>\" } }, \"credentials_name\": \"databricks-credentials-v1\", \"creation_time\": 1579753556257 }  \nAgain, copy the credentials_id field from the response for later use."
    },
    {
        "id": 6,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/audit-aws-credentials.html",
        "content": "Next steps\nNext steps\nIf you need to set up cross-account delivery (your S3 bucket is in a different AWS account than the IAM role used for log delivery), see Step 3: Configure cross-account support (Optional).  \nIf your S3 bucket is in the same AWS account as your IAM role used for log delivery, skip to the final step of calling the log delivery API. See Step 4: Call the log delivery API."
    },
    {
        "id": 7,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "Databricks Runtime 8.3 (EoS)  \nNote  \nSupport for this Databricks Runtime version has ended. For the end-of-support date, see End-of-support history. For all supported Databricks Runtime versions, see Databricks Runtime release notes versions and compatibility.  \nThe following release notes provide information about Databricks Runtime 8.3 and Databricks Runtime 8.3 Photon, powered by Apache Spark 3.1.1. Databricks released these images in June 2021. Photon is in Public Preview.  \nNew features and improvements"
    },
    {
        "id": 8,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "New features and improvements\nAccelerate SQL workloads with Photon (Public Preview)  \nGenerated columns in Delta tables (Public Preview)  \nAuto Loader features and improvements  \nCreate Delta tables with new programmatic APIs (Public Preview)  \nCorrect calculation of Delta table sizes in SQL ANALYZE  \nDetailed metrics of RocksDB performance when using RocksDBStateStore  \nAutomatic Optimized Writes  \nEnable bucketed joins if only one join side is bucketed  \nImproved security when defining Spark UDFs (Public Preview)  \nCSE-KMS endpoint supported in the new S3 client  \nEasy migration of OSS Hadoop configurations to Databricks clusters, thanks to support for both OSS and shaded class names for S3 AWS credentials providers  \nReduced number of requests to schema registry for queries with from_avro  \nMultiple results in R with ListResults (Public Preview)  \nBug fixes  \nAccelerate SQL workloads with Photon (Public Preview)  \nPhoton is the new native vectorized engine on Databricks, directly compatible with Apache Spark APIs. It is included as part of a new high-performance runtime designed to run your SQL workloads faster and reduce your total cost per workload. This runtime is a variant of the standard Databricks Runtime and is selectable when you create clusters in the UI, the Clusters API, or the Jobs API (specified in the APIs as 8.3.x-photon-scala2.12).  \nPhoton supports a limited set of instance types on the driver and worker nodes. Instance types with Photon consume DBUs at a different rate than the same instance type running a non-Photon runtime. For more information about Photon instances and DBU consumption, see the Databricks pricing page.  \nGenerated columns in Delta tables (Public Preview)"
    },
    {
        "id": 9,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "Generated columns in Delta tables (Public Preview)  \nDelta Lake now supports generated columns, which are a special type of column whose values are automatically generated based on a user-specified function over other columns in the Delta table. You can use most built-in SQL functions to generate the values of these generated columns. For example, you can automatically generate a date column (for partitioning the table by date) from the timestamp column; any writes into the table need only specify the data for the timestamp column. You can create Delta tables with generated columns using SQL, Scala, Java or Python APIs.  \nFor more information, see Delta Lake generated columns.  \nAuto Loader features and improvements  \nSchema inference for CSV files in Auto Loader  \nImproved startup time for Auto Loader streams  \nFaster directory listing in Auto Loader  \nReduced storage overhead for Auto Loader checkpoints  \nAuto Loader includes the file path in the rescued data column when available  \nAuto Loader supports file renames in Azure Data Lake Storage Gen2 in file notification mode  \nSchema inference for CSV files in Auto Loader  \nAuto Loader now supports schema inference and evolution on CSV files. Auto Loader provides the following capabilities on top of the existing CSV parser in Apache Spark:  \nSchema merging: Auto Loader can ingest CSV files that have different schema (different number of columns, different ordering of columns) across files.  \nRescued data column: You can use the rescued data column to rescue unexpected data that may appear in your CSV files. This includes data that can not be parsed in the data type that\u2019s expected, columns that have a different casing or null values in the header, or additional columns that were not part of the expected schema.  \nFor details, see Configure schema inference and evolution in Auto Loader.  \nImproved startup time for Auto Loader streams  \nAuto Loader streams now perform the initial backfill for the stream asynchronously when starting for the first time, leading to a much faster start up time for the stream. This can allow you to iterate quickly on your code with production data, especially when you have to ingest data from directories that contain millions or billions of files."
    },
    {
        "id": 10,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "In addition, the bootstrap time of streams that are restarting are also improved, because we have parallelized the download and upload of the RocksDB files that Auto Loader leverages for providing exactly-once semantics.  \nFaster directory listing in Auto Loader  \nWe greatly improved the efficiency of directory listing in Auto Loader. A side effect of this performance improvement is that the stream may emit more list requests to the storage system when there is no new data to process, which can lead to a surge on list request charges. As a general best practice, Databricks recommends that you set a reasonable trigger interval for production streaming pipelines. See Production considerations for Structured Streaming.  \nReduced storage overhead for Auto Loader checkpoints  \nAuto Loader streams now automatically clean up stale files in the checkpoint directory asynchronously to keep the checkpoint directory size from growing indefinitely and reduce storage costs.  \nAuto Loader includes the file path in the rescued data column when available  \nThe rescued data column automatically provides the file path of the rescued data when applicable in a column named _file_ path. This can help you track down the root cause of data quality issues. The column is not included if the data schema contains a column called _file_path. You can use the SQL configuration spark.databricks.sql.rescuedDataColumn.filePath.name to rename the column if necessary.  \nAuto Loader supports file renames in Azure Data Lake Storage Gen2 in file notification mode  \nAuto Loader now supports BlobRenamed events for Azure Data Lake Storage Gen2 when running in file notification mode. To process files that are uploaded to an Azure Data Lake Storage Gen2 container through a rename operation with file notifications, start a new stream with Auto Loader using Databricks Runtime 8.3. To ensure that a file is processed exactly once, make sure that the source directory that the file is being renamed from is not watched by Auto Loader.  \nCreate Delta tables with new programmatic APIs (Public Preview)  \nYou can now create new Delta tables programmatically (using Scala, Java, and Python) without using DataFrame APIs. New DeltaTableBuilder and DeltaColumnBuilder APIs let you specify all of the table details that you can specify using SQL CREATE TABLE.  \nFor more information, see Create a table."
    },
    {
        "id": 11,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "For more information, see Create a table.  \nCorrect calculation of Delta table sizes in SQL ANALYZE  \nExisting analyze logic incorrectly computes the table size for Delta tables and updates the catalog with incorrect size. The fix is to get the size of a Delta table from the Delta log.  \nDetailed metrics of RocksDB performance when using RocksDBStateStore  \nIf you have configured your Structured Streaming query to use RocksDB as the state store, you can now get better visibility into the performance of RocksDB, with detailed metrics on get/put latencies, compaction latencies, cache hits, and so on. These metrics are available through the StreamingQueryProgress and StreamingQueryListener APIs for monitoring a streaming query.  \nFor more information, see Configure RocksDB state store on Databricks.  \nAutomatic Optimized Writes  \nOptimized Writes on partitioned Delta tables are now automatically enabled for update and delete queries containing subqueries.  \nEnable bucketed joins if only one join side is bucketed  \nA new config spark.databricks.sql.minBucketsForBucketedJoin enables a bucketed join if only one join side is bucketed and the number of buckets is not less than this config value. By default, this config value is the same as the default shuffle partitions number (200).  \nImproved security when defining Spark UDFs (Public Preview)  \nUser info functions current_user and is_member can no longer be overridden by temporary functions including Python spark.udf.register or SQL create or replace temp function.  \nCSE-KMS endpoint supported in the new S3 client  \nThe S3 connector now supports the fs.s3a.cse.kms.endpoint option to configure a custom KMS endpoint when using AWS S3 client-side encryption with keys managed by AWS KMS service (CSE-KMS). See the AWS article AWS Key Management Service endpoints and quotas for the list of valid endpoints. If the option is not specified, CSE-KMS defaults to us-east-1 region by default. If you use S3 client-side encryption with a customer-provided key, this option has no effect."
    },
    {
        "id": 12,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "Easy migration of OSS Hadoop configurations to Databricks clusters, thanks to support for both OSS and shaded class names for S3 AWS credentials providers  \nOpen-source Hadoop AWS credentials providers, for example, org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider, org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider, and org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider, can be configured as part of Hadoop configuration for S3 Storage Connector in addition to existing authentication methods. This enables easy migration of OSS Hadoop configuration to a Databricks cluster.  \nReduced number of requests to schema registry for queries with from_avro  \nQueries with from_avro with schema registry support no longer generate as many requests to the schema registry service, saving operational cost.  \nMultiple results in R with ListResults (Public Preview)  \nDatabricks R notebooks now support multiple results in each cell. Previously only a single result was rendered for each notebook cell. Currently the results of a single cell in R notebooks are displayed in the following order:  \nRShiny URL  \nPlot  \ndisplayHTML outputs  \nTables  \nstdout  \nBug fixes  \nDisabled a list of pushed down predicates (StartsWith, EndsWith, Contains, Not(EqualTo()), and DataType) for AWS Glue Catalog since they are not supported in Glue yet."
    },
    {
        "id": 13,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "Library upgrades\nLibrary upgrades\nUpgraded Python library:  \nkoalas upgraded from 1.7.0 to 1.8.0.  \npandas upgraded from 1.1.3 to 1.1.5.  \ns3transfer upgraded from 0.3.4 to 0.3.6.  \nUpgraded R library:  \nSparkR upgraded from 3.1.1 to 3.1.2.  \nUpgraded Java library:  \nmariadb-java-client from 2.1.2 to 2.2.5.  \nparquet-column from 1.10.1-databricks6 to 1.10.1-databricks9  \nparquet-common from 1.10.1-databricks6 to 1.10.1-databricks9  \nparquet-encoding from 1.10.1-databricks6 to 1.10.1-databricks9  \nparquet-hadoop from 1.10.1-databricks6 to 1.10.1-databricks9  \nparquet-jackson from 1.10.1-databricks6 to 1.10.1-databricks9\n\nApache Spark"
    },
    {
        "id": 14,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "Apache Spark\nDatabricks Runtime 8.3 includes Apache Spark 3.1.1. This release includes all Spark fixes and improvements included in Databricks Runtime 8.2 (EoS), as well as the following additional bug fixes and improvements made to Spark:  \n[SPARK-34614] [SQL] ANSI mode: Casting String to Boolean should throw exception on parse error  \n[SPARK-34246] [FOLLOWUP] Change the definition of `findTightestCommonT\u2026  \n[SPARK-35213] [SQL] Keep the correct ordering of nested structs in chained withField operations  \n[SPARK-35096] [SQL] SchemaPruning should adhere spark.sql.caseSensitive config  \n[SPARK-35227] [BUILD] Update the resolver for spark-packages in SparkSubmit  \n[SPARK-35224] [SQL] Fix buffer overflow in MutableProjectionSuite  \n[SPARK-34245] [CORE] Ensure Master removes executors that failed to send finished state  \n[SPARK-34856] [SQL] ANSI mode: Allow casting complex types as string type  \n[SPARK-34946] [SQL] Block unsupported correlated scalar subquery in Aggregate  \n[SPARK-35014] Fix the PhysicalAggregation pattern to not rewrite foldable expressions  \n[SPARK-34769] [SQL] AnsiTypeCoercion: return closest con\u2026"
    },
    {
        "id": 15,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "System environment\nOperating System: Ubuntu 18.04.5 LTS  \nJava: Zulu 8.52.0.23-CA-linux64 (build 1.8.0_282-b08)  \nScala: 2.12.10  \nPython: 3.8.8  \nR: R version 4.0.4 (2021-02-15)  \nDelta Lake 1.0.0  \nInstalled Python libraries  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nappdirs  \n1.4.4  \nasn1crypto  \n1.4.0  \nbackcall  \n0.2.0  \nboto3  \n1.16.7  \nbotocore  \n1.19.7  \nbrotlipy  \n0.7.0  \ncertifi  \n2020.12.5  \ncffi  \n1.14.3  \nchardet  \n3.0.4  \ncryptography  \n3.1.1  \ncycler  \n0.10.0  \nCython  \n0.29.21  \ndecorator  \n4.4.2  \ndistlib  \n0.3.1  \ndocutils  \n0.15.2  \nentrypoints  \n0.3  \nfacets-overview  \n1.0.0  \nfilelock  \n3.0.12  \nidna  \n2.10  \nipykernel  \n5.3.4  \nipython  \n7.19.0  \nipython-genutils  \n0.2.0  \njedi  \n0.17.2  \njmespath  \n0.10.0  \njoblib  \n0.17.0  \njupyter-client  \n6.1.7  \njupyter-core  \n4.6.3  \nkiwisolver  \n1.3.0  \nkoalas  \n1.8.0  \nmatplotlib  \n3.2.2  \nnumpy  \n1.19.2  \npandas  \n1.1.5  \nparso  \n0.7.0  \npatsy  \n0.5.1  \npexpect  \n4.8.0  \npickleshare  \n0.7.5  \npip  \n20.2.4  \nplotly  \n4.14.3  \nprompt-toolkit  \n3.0.8  \nprotobuf  \n3.17.0  \npsycopg2  \n2.8.5  \nptyprocess  \n0.6.0  \npyarrow  \n1.0.1  \npycparser  \n2.20  \nPygments  \n2.7.2  \npyOpenSSL  \n19.1.0  \npyparsing  \n2.4.7  \nPySocks  \n1.7.1  \npython-dateutil  \n2.8.1  \npytz  \n2020.5  \npyzmq  \n19.0.2"
    },
    {
        "id": 16,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "pyOpenSSL  \n19.1.0  \npyparsing  \n2.4.7  \nPySocks  \n1.7.1  \npython-dateutil  \n2.8.1  \npytz  \n2020.5  \npyzmq  \n19.0.2  \nrequests  \n2.24.0  \nretrying  \n1.3.3  \ns3transfer  \n0.3.6  \nscikit-learn  \n0.23.2  \nscipy  \n1.5.2  \nseaborn  \n0.10.0  \nsetuptools  \n50.3.1  \nsix  \n1.15.0  \nstatsmodels  \n0.12.0  \nthreadpoolctl  \n2.1.0  \ntornado  \n6.0.4  \ntraitlets  \n5.0.5  \nurllib3  \n1.25.11  \nvirtualenv  \n20.2.1  \nwcwidth  \n0.2.5  \nwheel  \n0.35.1  \nInstalled R libraries  \nR libraries are installed from the Microsoft CRAN snapshot on 2020-11-02.  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \naskpass  \n1.1  \nassertthat  \n0.2.1  \nbackports  \n1.2.1  \nbase  \n4.0.4  \nbase64enc  \n0.1-3  \nBH  \n1.72.0-3  \nbit  \n4.0.4  \nbit64  \n4.0.5  \nblob  \n1.2.1  \nboot  \n1.3-27  \nbrew  \n1.0-6  \nbrio  \n1.1.0  \nbroom  \n0.7.2  \ncallr  \n3.5.1  \ncaret  \n6.0-86  \ncellranger  \n1.1.0  \nchron  \n2.3-56  \nclass  \n7.3-18  \ncli  \n2.2.0  \nclipr  \n0.7.1  \ncluster  \n2.1.1  \ncodetools  \n0.2-18  \ncolorspace  \n2.0-0  \ncommonmark  \n1.7  \ncompiler  \n4.0.4  \nconfig  \n0.3  \ncovr  \n3.5.1  \ncpp11  \n0.2.4  \ncrayon  \n1.3.4  \ncredentials  \n1.3.0  \ncrosstalk  \n1.1.0.1  \ncurl  \n4.3  \ndata.table  \n1.13.4  \ndatasets  \n4.0.4  \nDBI  \n1.1.0  \ndbplyr  \n2.0.0  \ndesc  \n1.2.0  \ndevtools  \n2.3.2  \ndiffobj  \n0.3.2  \ndigest  \n0.6.27  \ndplyr  \n1.0.2  \nDT  \n0.16  \nellipsis  \n0.3.1  \nevaluate  \n0.14"
    },
    {
        "id": 17,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "1.2.0  \ndevtools  \n2.3.2  \ndiffobj  \n0.3.2  \ndigest  \n0.6.27  \ndplyr  \n1.0.2  \nDT  \n0.16  \nellipsis  \n0.3.1  \nevaluate  \n0.14  \nfansi  \n0.4.1  \nfarver  \n2.0.3  \nfastmap  \n1.0.1  \nforcats  \n0.5.0  \nforeach  \n1.5.1  \nforeign  \n0.8-81  \nforge  \n0.2.0  \nfs  \n1.5.0  \nfuture  \n1.21.0  \ngenerics  \n0.1.0  \ngert  \n1.0.2  \nggplot2  \n3.3.2  \ngh  \n1.2.0  \ngitcreds  \n0.1.1  \nglmnet  \n4.0-2  \nglobals  \n0.14.0  \nglue  \n1.4.2  \ngower  \n0.2.2  \ngraphics  \n4.0.4  \ngrDevices  \n4.0.4  \ngrid  \n4.0.4  \ngridExtra  \n2.3  \ngsubfn  \n0.7  \ngtable  \n0.3.0  \nhaven  \n2.3.1  \nhighr  \n0.8  \nhms  \n0.5.3  \nhtmltools  \n0.5.0  \nhtmlwidgets  \n1.5.3  \nhttpuv  \n1.5.4  \nhttr  \n1.4.2  \nhwriter  \n1.3.2  \nhwriterPlus  \n1.0-3  \nini  \n0.3.1  \nipred  \n0.9-9  \nisoband  \n0.2.3  \niterators  \n1.0.13  \njsonlite  \n1.7.2  \nKernSmooth  \n2.23-18  \nknitr  \n1.30  \nlabeling  \n0.4.2  \nlater  \n1.1.0.1  \nlattice  \n0.20-41  \nlava  \n1.6.8.1  \nlazyeval  \n0.2.2  \nlifecycle  \n0.2.0  \nlistenv  \n0.8.0  \nlubridate  \n1.7.9.2  \nmagrittr  \n2.0.1  \nmarkdown  \n1.1  \nMASS  \n7.3-53.1  \nMatrix  \n1.3-2  \nmemoise  \n1.1.0  \nmethods  \n4.0.4  \nmgcv  \n1.8-33  \nmime  \n0.9  \nModelMetrics  \n1.2.2.2  \nmodelr  \n0.1.8  \nmunsell  \n0.5.0  \nnlme  \n3.1-152  \nnnet  \n7.3-15  \nnumDeriv"
    },
    {
        "id": 18,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "mime  \n0.9  \nModelMetrics  \n1.2.2.2  \nmodelr  \n0.1.8  \nmunsell  \n0.5.0  \nnlme  \n3.1-152  \nnnet  \n7.3-15  \nnumDeriv  \n2016.8-1.1  \nopenssl  \n1.4.3  \nparallel  \n4.0.4  \nparallelly  \n1.22.0  \npillar  \n1.4.7  \npkgbuild  \n1.1.0  \npkgconfig  \n2.0.3  \npkgload  \n1.1.0  \nplogr  \n0.2.0  \nplyr  \n1.8.6  \npraise  \n1.0.0  \nprettyunits  \n1.1.1  \npROC  \n1.16.2  \nprocessx  \n3.4.5  \nprodlim  \n2019.11.13  \nprogress  \n1.2.2  \npromises  \n1.1.1  \nproto  \n1.0.0  \nps  \n1.5.0  \npurrr  \n0.3.4  \nr2d3  \n0.2.3  \nR6  \n2.5.0  \nrandomForest  \n4.6-14  \nrappdirs  \n0.3.1  \nrcmdcheck  \n1.3.3  \nRColorBrewer  \n1.1-2  \nRcpp  \n1.0.5  \nreadr  \n1.4.0  \nreadxl  \n1.3.1  \nrecipes  \n0.1.15  \nrematch  \n1.0.1  \nrematch2  \n2.1.2  \nremotes  \n2.2.0  \nreprex  \n0.3.0  \nreshape2  \n1.4.4  \nrex  \n1.2.0  \nrlang  \n0.4.9  \nrmarkdown  \n2.6  \nRODBC  \n1.3-17  \nroxygen2  \n7.1.1  \nrpart  \n4.1-15  \nrprojroot  \n2.0.2  \nRserve  \n1.8-7  \nRSQLite  \n2.2.1  \nrstudioapi  \n0.13  \nrversions  \n2.0.2  \nrvest  \n0.3.6  \nscales  \n1.1.1  \nselectr  \n0.4-2  \nsessioninfo  \n1.1.1  \nshape  \n1.4.5  \nshiny  \n1.5.0  \nsourcetools  \n0.1.7  \nsparklyr  \n1.5.2  \nSparkR  \n3.1.2  \nspatial  \n7.3-11  \nsplines  \n4.0.4  \nsqldf  \n0.4-11  \nSQUAREM  \n2020.5  \nstats  \n4.0.4  \nstats4"
    },
    {
        "id": 19,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "SparkR  \n3.1.2  \nspatial  \n7.3-11  \nsplines  \n4.0.4  \nsqldf  \n0.4-11  \nSQUAREM  \n2020.5  \nstats  \n4.0.4  \nstats4  \n4.0.4  \nstringi  \n1.5.3  \nstringr  \n1.4.0  \nsurvival  \n3.2-7  \nsys  \n3.4  \ntcltk  \n4.0.4  \nTeachingDemos  \n2.10  \ntestthat  \n3.0.0  \ntibble  \n3.0.4  \ntidyr  \n1.1.2  \ntidyselect  \n1.1.0  \ntidyverse  \n1.3.0  \ntimeDate  \n3043.102  \ntinytex  \n0.28  \ntools  \n4.0.4  \nusethis  \n2.0.0  \nutf8  \n1.1.4  \nutils  \n4.0.4  \nuuid  \n0.1-4  \nvctrs  \n0.3.5  \nviridisLite  \n0.3.0  \nwaldo  \n0.2.3  \nwhisker  \n0.4  \nwithr  \n2.3.0  \nxfun  \n0.19  \nxml2  \n1.3.2  \nxopen  \n1.0.0  \nxtable  \n1.8-4  \nyaml  \n2.2.1  \nzip  \n2.1.1  \nInstalled Java and Scala libraries (Scala 2.12 cluster version)  \nGroup ID  \nArtifact ID  \nVersion  \nantlr  \nantlr  \n2.7.7  \ncom.amazonaws  \namazon-kinesis-client  \n1.12.0  \ncom.amazonaws  \naws-java-sdk-autoscaling  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudformation  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudfront  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudhsm  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudsearch  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudtrail  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudwatch  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudwatchmetrics  \n1.11.655  \ncom.amazonaws"
    },
    {
        "id": 20,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "aws-java-sdk-cloudwatch  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudwatchmetrics  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-codedeploy  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cognitoidentity  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cognitosync  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-config  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-core  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-datapipeline  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-directconnect  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-directory  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-dynamodb  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-ec2  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-ecs  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-efs  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-elasticache  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-elasticbeanstalk  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-elasticloadbalancing  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-elastictranscoder  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-emr  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-glacier  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-iam  \n1.11.655  \ncom.amazonaws"
    },
    {
        "id": 21,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "com.amazonaws  \naws-java-sdk-glacier  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-iam  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-importexport  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-kinesis  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-kms  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-lambda  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-logs  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-machinelearning  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-opsworks  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-rds  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-redshift  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-route53  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-s3  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-ses  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-simpledb  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-simpleworkflow  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-sns  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-sqs  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-ssm  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-storagegateway  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-sts  \n1.11.655  \ncom.amazonaws"
    },
    {
        "id": 22,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "aws-java-sdk-storagegateway  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-sts  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-support  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-swf-libraries  \n1.11.22  \ncom.amazonaws  \naws-java-sdk-workspaces  \n1.11.655  \ncom.amazonaws  \njmespath-java  \n1.11.655  \ncom.chuusai  \nshapeless_2.12  \n2.3.3  \ncom.clearspring.analytics  \nstream  \n2.9.6  \ncom.databricks  \nRserve  \n1.8-3  \ncom.databricks  \njets3t  \n0.7.1-0  \ncom.databricks.scalapb  \ncompilerplugin_2.12  \n0.4.15-10  \ncom.databricks.scalapb  \nscalapb-runtime_2.12  \n0.4.15-10  \ncom.esotericsoftware  \nkryo-shaded  \n4.0.2  \ncom.esotericsoftware  \nminlog  \n1.3.0  \ncom.fasterxml  \nclassmate  \n1.3.4  \ncom.fasterxml.jackson.core  \njackson-annotations  \n2.10.0  \ncom.fasterxml.jackson.core  \njackson-core  \n2.10.0  \ncom.fasterxml.jackson.core  \njackson-databind  \n2.10.0  \ncom.fasterxml.jackson.dataformat  \njackson-dataformat-cbor  \n2.10.0  \ncom.fasterxml.jackson.datatype  \njackson-datatype-joda  \n2.10.0  \ncom.fasterxml.jackson.module  \njackson-module-paranamer  \n2.10.0  \ncom.fasterxml.jackson.module  \njackson-module-scala_2.12  \n2.10.0  \ncom.github.ben-manes.caffeine  \ncaffeine  \n2.3.4  \ncom.github.fommil  \njniloader  \n1.1  \ncom.github.fommil.netlib  \ncore  \n1.1.2  \ncom.github.fommil.netlib  \nnative_ref-java"
    },
    {
        "id": 23,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "jniloader  \n1.1  \ncom.github.fommil.netlib  \ncore  \n1.1.2  \ncom.github.fommil.netlib  \nnative_ref-java  \n1.1  \ncom.github.fommil.netlib  \nnative_ref-java-natives  \n1.1  \ncom.github.fommil.netlib  \nnative_system-java  \n1.1  \ncom.github.fommil.netlib  \nnative_system-java-natives  \n1.1  \ncom.github.fommil.netlib  \nnetlib-native_ref-linux-x86_64-natives  \n1.1  \ncom.github.fommil.netlib  \nnetlib-native_system-linux-x86_64-natives  \n1.1  \ncom.github.joshelser  \ndropwizard-metrics-hadoop-metrics2-reporter  \n0.1.2  \ncom.github.luben  \nzstd-jni  \n1.4.8-1  \ncom.github.wendykierp  \nJTransforms  \n3.1  \ncom.google.code.findbugs  \njsr305  \n3.0.0  \ncom.google.code.gson  \ngson  \n2.2.4  \ncom.google.flatbuffers  \nflatbuffers-java  \n1.9.0  \ncom.google.guava  \nguava  \n15.0  \ncom.google.protobuf  \nprotobuf-java  \n2.6.1  \ncom.h2database  \nh2  \n1.4.195  \ncom.helger  \nprofiler  \n1.1.1  \ncom.jcraft  \njsch  \n0.1.50  \ncom.jolbox  \nbonecp  \n0.8.0.RELEASE  \ncom.lihaoyi  \nsourcecode_2.12  \n0.1.9  \ncom.microsoft.azure  \nazure-data-lake-store-sdk  \n2.3.9  \ncom.microsoft.sqlserver  \nmssql-jdbc  \n9.2.1.jre8  \ncom.ning  \ncompress-lzf  \n1.0.3  \ncom.sun.mail  \njavax.mail  \n1.5.2  \ncom.tdunning  \njson  \n1.8  \ncom.thoughtworks.paranamer  \nparanamer  \n2.8"
    },
    {
        "id": 24,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "1.0.3  \ncom.sun.mail  \njavax.mail  \n1.5.2  \ncom.tdunning  \njson  \n1.8  \ncom.thoughtworks.paranamer  \nparanamer  \n2.8  \ncom.trueaccord.lenses  \nlenses_2.12  \n0.4.12  \ncom.twitter  \nchill-java  \n0.9.5  \ncom.twitter  \nchill_2.12  \n0.9.5  \ncom.twitter  \nutil-app_2.12  \n7.1.0  \ncom.twitter  \nutil-core_2.12  \n7.1.0  \ncom.twitter  \nutil-function_2.12  \n7.1.0  \ncom.twitter  \nutil-jvm_2.12  \n7.1.0  \ncom.twitter  \nutil-lint_2.12  \n7.1.0  \ncom.twitter  \nutil-registry_2.12  \n7.1.0  \ncom.twitter  \nutil-stats_2.12  \n7.1.0  \ncom.typesafe  \nconfig  \n1.2.1  \ncom.typesafe.scala-logging  \nscala-logging_2.12  \n3.7.2  \ncom.univocity  \nunivocity-parsers  \n2.9.1  \ncom.zaxxer  \nHikariCP  \n3.1.0  \ncommons-beanutils  \ncommons-beanutils  \n1.9.4  \ncommons-cli  \ncommons-cli  \n1.2  \ncommons-codec  \ncommons-codec  \n1.10  \ncommons-collections  \ncommons-collections  \n3.2.2  \ncommons-configuration  \ncommons-configuration  \n1.6  \ncommons-dbcp  \ncommons-dbcp  \n1.4  \ncommons-digester  \ncommons-digester  \n1.8  \ncommons-fileupload  \ncommons-fileupload  \n1.3.3  \ncommons-httpclient  \ncommons-httpclient  \n3.1  \ncommons-io  \ncommons-io  \n2.4  \ncommons-lang  \ncommons-lang  \n2.6  \ncommons-logging  \ncommons-logging  \n1.1.3  \ncommons-net  \ncommons-net  \n3.1  \ncommons-pool  \ncommons-pool  \n1.5.4  \nhive-2.3__hadoop-2.7  \njets3t-0.7  \nliball_deps_2.12  \nhive-2.3__hadoop-2.7  \nzookeeper-3.4  \nliball_deps_2.12  \ninfo.ganglia.gmetric4j"
    },
    {
        "id": 25,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "liball_deps_2.12  \nhive-2.3__hadoop-2.7  \nzookeeper-3.4  \nliball_deps_2.12  \ninfo.ganglia.gmetric4j  \ngmetric4j  \n1.0.10  \nio.airlift  \naircompressor  \n0.10  \nio.dropwizard.metrics  \nmetrics-core  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-graphite  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-healthchecks  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jetty9  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jmx  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-json  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jvm  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-servlets  \n4.1.1  \nio.netty  \nnetty-all  \n4.1.51.Final  \nio.prometheus  \nsimpleclient  \n0.7.0  \nio.prometheus  \nsimpleclient_common  \n0.7.0  \nio.prometheus  \nsimpleclient_dropwizard  \n0.7.0  \nio.prometheus  \nsimpleclient_pushgateway  \n0.7.0  \nio.prometheus  \nsimpleclient_servlet  \n0.7.0  \nio.prometheus.jmx  \ncollector  \n0.12.0  \njakarta.annotation  \njakarta.annotation-api  \n1.3.5  \njakarta.validation  \njakarta.validation-api  \n2.0.2  \njakarta.ws.rs  \njakarta.ws.rs-api  \n2.1.6  \njavax.activation  \nactivation  \n1.1.1  \njavax.el  \njavax.el-api  \n2.2.4  \njavax.jdo  \njdo-api  \n3.0.1  \njavax.servlet  \njavax.servlet-api  \n3.1.0  \njavax.servlet.jsp  \njsp-api  \n2.1  \njavax.transaction  \njta  \n1.1  \njavax.transaction  \ntransaction-api  \n1.1  \njavax.xml.bind  \njaxb-api"
    },
    {
        "id": 26,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "jsp-api  \n2.1  \njavax.transaction  \njta  \n1.1  \njavax.transaction  \ntransaction-api  \n1.1  \njavax.xml.bind  \njaxb-api  \n2.2.2  \njavax.xml.stream  \nstax-api  \n1.0-2  \njavolution  \njavolution  \n5.5.1  \njline  \njline  \n2.14.6  \njoda-time  \njoda-time  \n2.10.5  \nlog4j  \napache-log4j-extras  \n1.2.17  \nlog4j  \nlog4j  \n1.2.17  \nmaven-trees  \nhive-2.3__hadoop-2.7  \nliball_deps_2.12  \nnet.razorvine  \npyrolite  \n4.30  \nnet.sf.jpam  \njpam  \n1.1  \nnet.sf.opencsv  \nopencsv  \n2.3  \nnet.sf.supercsv  \nsuper-csv  \n2.2.0  \nnet.snowflake  \nsnowflake-ingest-sdk  \n0.9.6  \nnet.snowflake  \nsnowflake-jdbc  \n3.13.3  \nnet.snowflake  \nspark-snowflake_2.12  \n2.9.0-spark_3.1  \nnet.sourceforge.f2j  \narpack_combined_all  \n0.1  \norg.acplt.remotetea  \nremotetea-oncrpc  \n1.1.2  \norg.antlr  \nST4  \n4.0.4  \norg.antlr  \nantlr-runtime  \n3.5.2  \norg.antlr  \nantlr4-runtime  \n4.8-1  \norg.antlr  \nstringtemplate  \n3.2.1  \norg.apache.ant  \nant  \n1.9.2  \norg.apache.ant  \nant-jsch  \n1.9.2  \norg.apache.ant  \nant-launcher  \n1.9.2  \norg.apache.arrow  \narrow-format  \n2.0.0  \norg.apache.arrow  \narrow-memory-core  \n2.0.0  \norg.apache.arrow  \narrow-memory-netty  \n2.0.0  \norg.apache.arrow  \narrow-vector  \n2.0.0  \norg.apache.avro  \navro  \n1.8.2  \norg.apache.avro  \navro-ipc  \n1.8.2  \norg.apache.avro  \navro-mapred-hadoop2  \n1.8.2"
    },
    {
        "id": 27,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "avro  \n1.8.2  \norg.apache.avro  \navro-ipc  \n1.8.2  \norg.apache.avro  \navro-mapred-hadoop2  \n1.8.2  \norg.apache.commons  \ncommons-compress  \n1.20  \norg.apache.commons  \ncommons-crypto  \n1.1.0  \norg.apache.commons  \ncommons-lang3  \n3.10  \norg.apache.commons  \ncommons-math3  \n3.4.1  \norg.apache.commons  \ncommons-text  \n1.6  \norg.apache.curator  \ncurator-client  \n2.7.1  \norg.apache.curator  \ncurator-framework  \n2.7.1  \norg.apache.curator  \ncurator-recipes  \n2.7.1  \norg.apache.derby  \nderby  \n10.12.1.1  \norg.apache.directory.api  \napi-asn1-api  \n1.0.0-M20  \norg.apache.directory.api  \napi-util  \n1.0.0-M20  \norg.apache.directory.server  \napacheds-i18n  \n2.0.0-M15  \norg.apache.directory.server  \napacheds-kerberos-codec  \n2.0.0-M15  \norg.apache.hadoop  \nhadoop-annotations  \n2.7.4  \norg.apache.hadoop  \nhadoop-auth  \n2.7.4  \norg.apache.hadoop  \nhadoop-client  \n2.7.4  \norg.apache.hadoop  \nhadoop-common  \n2.7.4  \norg.apache.hadoop  \nhadoop-hdfs  \n2.7.4  \norg.apache.hadoop  \nhadoop-mapreduce-client-app  \n2.7.4  \norg.apache.hadoop  \nhadoop-mapreduce-client-common  \n2.7.4  \norg.apache.hadoop  \nhadoop-mapreduce-client-core  \n2.7.4  \norg.apache.hadoop  \nhadoop-mapreduce-client-jobclient  \n2.7.4  \norg.apache.hadoop  \nhadoop-mapreduce-client-shuffle  \n2.7.4  \norg.apache.hadoop  \nhadoop-yarn-api  \n2.7.4  \norg.apache.hadoop  \nhadoop-yarn-client"
    },
    {
        "id": 28,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "2.7.4  \norg.apache.hadoop  \nhadoop-yarn-api  \n2.7.4  \norg.apache.hadoop  \nhadoop-yarn-client  \n2.7.4  \norg.apache.hadoop  \nhadoop-yarn-common  \n2.7.4  \norg.apache.hadoop  \nhadoop-yarn-server-common  \n2.7.4  \norg.apache.hive  \nhive-beeline  \n2.3.7  \norg.apache.hive  \nhive-cli  \n2.3.7  \norg.apache.hive  \nhive-common  \n2.3.7  \norg.apache.hive  \nhive-exec-core  \n2.3.7  \norg.apache.hive  \nhive-jdbc  \n2.3.7  \norg.apache.hive  \nhive-llap-client  \n2.3.7  \norg.apache.hive  \nhive-llap-common  \n2.3.7  \norg.apache.hive  \nhive-metastore  \n2.3.7  \norg.apache.hive  \nhive-serde  \n2.3.7  \norg.apache.hive  \nhive-shims  \n2.3.7  \norg.apache.hive  \nhive-storage-api  \n2.7.2  \norg.apache.hive  \nhive-vector-code-gen  \n2.3.7  \norg.apache.hive.shims  \nhive-shims-0.23  \n2.3.7  \norg.apache.hive.shims  \nhive-shims-common  \n2.3.7  \norg.apache.hive.shims  \nhive-shims-scheduler  \n2.3.7  \norg.apache.htrace  \nhtrace-core  \n3.1.0-incubating  \norg.apache.httpcomponents  \nhttpclient  \n4.5.6  \norg.apache.httpcomponents  \nhttpcore  \n4.4.12  \norg.apache.ivy  \nivy  \n2.4.0  \norg.apache.mesos  \nmesos-shaded-protobuf  \n1.4.0  \norg.apache.orc  \norc-core  \n1.5.12  \norg.apache.orc  \norc-mapreduce  \n1.5.12  \norg.apache.orc  \norc-shims  \n1.5.12  \norg.apache.parquet  \nparquet-column  \n1.10.1-databricks9  \norg.apache.parquet  \nparquet-common  \n1.10.1-databricks9  \norg.apache.parquet  \nparquet-encoding"
    },
    {
        "id": 29,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "parquet-column  \n1.10.1-databricks9  \norg.apache.parquet  \nparquet-common  \n1.10.1-databricks9  \norg.apache.parquet  \nparquet-encoding  \n1.10.1-databricks9  \norg.apache.parquet  \nparquet-format  \n2.4.0  \norg.apache.parquet  \nparquet-hadoop  \n1.10.1-databricks9  \norg.apache.parquet  \nparquet-jackson  \n1.10.1-databricks9  \norg.apache.thrift  \nlibfb303  \n0.9.3  \norg.apache.thrift  \nlibthrift  \n0.12.0  \norg.apache.velocity  \nvelocity  \n1.5  \norg.apache.xbean  \nxbean-asm7-shaded  \n4.15  \norg.apache.yetus  \naudience-annotations  \n0.5.0  \norg.apache.zookeeper  \nzookeeper  \n3.4.14  \norg.codehaus.jackson  \njackson-core-asl  \n1.9.13  \norg.codehaus.jackson  \njackson-jaxrs  \n1.9.13  \norg.codehaus.jackson  \njackson-mapper-asl  \n1.9.13  \norg.codehaus.jackson  \njackson-xc  \n1.9.13  \norg.codehaus.janino  \ncommons-compiler  \n3.0.16  \norg.codehaus.janino  \njanino  \n3.0.16  \norg.datanucleus  \ndatanucleus-api-jdo  \n4.2.4  \norg.datanucleus  \ndatanucleus-core  \n4.1.17  \norg.datanucleus  \ndatanucleus-rdbms  \n4.1.19  \norg.datanucleus  \njavax.jdo  \n3.2.0-m3  \norg.eclipse.jetty  \njetty-client  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-continuation  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-http  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-io  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-jndi  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-plus"
    },
    {
        "id": 30,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "org.eclipse.jetty  \njetty-io  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-jndi  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-plus  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-proxy  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-security  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-server  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-servlet  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-servlets  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-util  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-util-ajax  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-webapp  \n9.4.36.v20210114  \norg.eclipse.jetty  \njetty-xml  \n9.4.36.v20210114  \norg.fusesource.leveldbjni  \nleveldbjni-all  \n1.8  \norg.glassfish.hk2  \nhk2-api  \n2.6.1  \norg.glassfish.hk2  \nhk2-locator  \n2.6.1  \norg.glassfish.hk2  \nhk2-utils  \n2.6.1  \norg.glassfish.hk2  \nosgi-resource-locator  \n1.0.3  \norg.glassfish.hk2.external  \naopalliance-repackaged  \n2.6.1  \norg.glassfish.hk2.external  \njakarta.inject  \n2.6.1  \norg.glassfish.jersey.containers  \njersey-container-servlet  \n2.30  \norg.glassfish.jersey.containers  \njersey-container-servlet-core  \n2.30  \norg.glassfish.jersey.core  \njersey-client  \n2.30  \norg.glassfish.jersey.core  \njersey-common  \n2.30  \norg.glassfish.jersey.core  \njersey-server  \n2.30  \norg.glassfish.jersey.inject  \njersey-hk2  \n2.30  \norg.glassfish.jersey.media  \njersey-media-jaxb"
    },
    {
        "id": 31,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "jersey-common  \n2.30  \norg.glassfish.jersey.core  \njersey-server  \n2.30  \norg.glassfish.jersey.inject  \njersey-hk2  \n2.30  \norg.glassfish.jersey.media  \njersey-media-jaxb  \n2.30  \norg.hibernate.validator  \nhibernate-validator  \n6.1.0.Final  \norg.javassist  \njavassist  \n3.25.0-GA  \norg.jboss.logging  \njboss-logging  \n3.3.2.Final  \norg.jdbi  \njdbi  \n2.63.1  \norg.joda  \njoda-convert  \n1.7  \norg.jodd  \njodd-core  \n3.5.2  \norg.json4s  \njson4s-ast_2.12  \n3.7.0-M5  \norg.json4s  \njson4s-core_2.12  \n3.7.0-M5  \norg.json4s  \njson4s-jackson_2.12  \n3.7.0-M5  \norg.json4s  \njson4s-scalap_2.12  \n3.7.0-M5  \norg.lz4  \nlz4-java  \n1.7.1  \norg.mariadb.jdbc  \nmariadb-java-client  \n2.2.5  \norg.objenesis  \nobjenesis  \n2.5.1  \norg.postgresql  \npostgresql  \n42.1.4  \norg.roaringbitmap  \nRoaringBitmap  \n0.9.0  \norg.roaringbitmap  \nshims  \n0.9.0  \norg.rocksdb  \nrocksdbjni  \n6.2.2  \norg.rosuda.REngine  \nREngine  \n2.1.0  \norg.scala-lang  \nscala-compiler_2.12  \n2.12.10  \norg.scala-lang  \nscala-library_2.12  \n2.12.10  \norg.scala-lang  \nscala-reflect_2.12  \n2.12.10  \norg.scala-lang.modules  \nscala-collection-compat_2.12  \n2.1.1  \norg.scala-lang.modules  \nscala-parser-combinators_2.12  \n1.1.2  \norg.scala-lang.modules  \nscala-xml_2.12"
    },
    {
        "id": 32,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "2.1.1  \norg.scala-lang.modules  \nscala-parser-combinators_2.12  \n1.1.2  \norg.scala-lang.modules  \nscala-xml_2.12  \n1.2.0  \norg.scala-sbt  \ntest-interface  \n1.0  \norg.scalacheck  \nscalacheck_2.12  \n1.14.2  \norg.scalactic  \nscalactic_2.12  \n3.0.8  \norg.scalanlp  \nbreeze-macros_2.12  \n1.0  \norg.scalanlp  \nbreeze_2.12  \n1.0  \norg.scalatest  \nscalatest_2.12  \n3.0.8  \norg.slf4j  \njcl-over-slf4j  \n1.7.30  \norg.slf4j  \njul-to-slf4j  \n1.7.30  \norg.slf4j  \nslf4j-api  \n1.7.30  \norg.slf4j  \nslf4j-log4j12  \n1.7.30  \norg.spark-project.spark  \nunused  \n1.0.0  \norg.springframework  \nspring-core  \n4.1.4.RELEASE  \norg.springframework  \nspring-test  \n4.1.4.RELEASE  \norg.threeten  \nthreeten-extra  \n1.5.0  \norg.tukaani  \nxz  \n1.5  \norg.typelevel  \nalgebra_2.12  \n2.0.0-M2  \norg.typelevel  \ncats-kernel_2.12  \n2.0.0-M4  \norg.typelevel  \nmachinist_2.12  \n0.6.8  \norg.typelevel  \nmacro-compat_2.12  \n1.1.1  \norg.typelevel  \nspire-macros_2.12  \n0.17.0-M1  \norg.typelevel  \nspire-platform_2.12  \n0.17.0-M1  \norg.typelevel  \nspire-util_2.12  \n0.17.0-M1  \norg.typelevel  \nspire_2.12  \n0.17.0-M1  \norg.wildfly.openssl  \nwildfly-openssl  \n1.0.7.Final  \norg.xerial  \nsqlite-jdbc  \n3.8.11.2"
    },
    {
        "id": 33,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/8.3.html",
        "content": "0.17.0-M1  \norg.wildfly.openssl  \nwildfly-openssl  \n1.0.7.Final  \norg.xerial  \nsqlite-jdbc  \n3.8.11.2  \norg.xerial.snappy  \nsnappy-java  \n1.1.8.2  \norg.yaml  \nsnakeyaml  \n1.24  \noro  \noro  \n2.0.8  \npl.edu.icm  \nJLargeArrays  \n1.5  \nsoftware.amazon.ion  \nion-java  \n1.0.2  \nstax  \nstax-api  \n1.0.1  \nxmlenc  \nxmlenc  \n0.52"
    },
    {
        "id": 34,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "Databricks Runtime 7.5 (EoS)  \nNote  \nSupport for this Databricks Runtime version has ended. For the end-of-support date, see End-of-support history. For all supported Databricks Runtime versions, see Databricks Runtime release notes versions and compatibility.  \nDatabricks released this image in December 2020.  \nThe following release notes provide information about Databricks Runtime 7.5, powered by Apache Spark 3.0.  \nNew features"
    },
    {
        "id": 35,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "New features\nDelta Lake features and improvements  \nThis release provides the following Delta Lake features and improvements:  \nAsynchronous checkpointing eliminates streaming micro-batch duration spikes  \nLatest table information is now propagated to catalogs  \nMERGE INTO operation now supports schema evolution of nested columns  \nMERGE INTO operation now automatically uses Optimized Writes  \nMERGE INTO and UPDATE operations now resolve nested struct columns by name  \nCLONE is GA and you can now override table properties  \nRESTORE is GA  \nScala implicits simplify Spark read and write APIs  \nImprove streaming throughput with fetchParallelism connector option  \nTrack stream progress with new Auto Loader metrics  \nAsynchronous checkpointing eliminates streaming micro-batch duration spikes  \nDelta Lake performs checkpoints every 10 commits by default. In streaming workloads, checkpointing can cause minor spikes in the micro-batch duration every 10 commits. Asynchronous checkpointing eliminates these spikes.  \nAsynchronous checkpointing is enabled by default for all non-Trigger Once streaming workloads. With asynchronous checkpointing, streaming writers also write enhanced checkpoints so you don\u2019t need to explicitly opt-in to enhanced checkpoints.  \nTo disable asynchronous checkpointing, set the SQL configuration spark.databricks.delta.checkpoint.async.enabled false.  \nBatch workloads continue to write checkpoints synchronously and we recommend against enabling asynchronous checkpointing for batch workloads.  \nLatest table information is now propagated to catalogs  \nDelta Lake now propagates the latest table information, such as schema and table properties, to catalogs such as the Hive metastore, on a best effort basis. Such information can help you leverage cataloging tools to understand the schema of your Delta tables.  \nDelta Lake does not internally depend on this information, but attempts to keep it up-to-date. In addition, Delta Lake stores table properties delta.lastUpdateVersion and delta.lastCommitTimestamp to denote the version and timestamp of the commit that last pushed a change to the metastore. The metastore is updated only when the schema of the table or the properties in the table do not match. Therefore, the aforementioned table properties do not necessarily reflect the latest version of the table.  \nIf your table information and the metastore information are out of sync, you can run a command such as DESCRIBE <table> to update the metastore.  \nNote"
    },
    {
        "id": 36,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "If your table information and the metastore information are out of sync, you can run a command such as DESCRIBE <table> to update the metastore.  \nNote  \nBecause Hive doesn\u2019t allow the changing partition columns, but Delta tables can change their partitioning, Delta tables do not show up as partitioned in Hive.  \nTable information cannot be used by external Delta Lake readers, such as the manifest format tables.  \nMERGE INTO operation now supports schema evolution of nested columns  \nSchema evolution of nested columns now have the same semantics as that of top-level columns. For example, new nested columns can be automatically added to a StructType column. See Automatic schema evolution in Merge for details.  \nMERGE INTO operation now automatically uses Optimized Writes  \nOptimized Writes can provide much better performance for MERGE INTO out-of-the-box, especially for MERGE INTO on partitioned tables. To disable Optimized Writes, set the Spark configuration spark.databricks.delta.optimizeWrite.enabled to false.  \nMERGE INTO and UPDATE operations now resolve nested struct columns by name  \nUpdate operations UPDATE and MERGE INTO commands now resolve nested struct columns by name. That is, when comparing or assigning columns of type StructType, the order of the nested columns does not matter (exactly in the same way as the order of top-level columns). To revert to resolving by position, set the Spark configuration spark.databricks.delta.resolveMergeUpdateStructsByName.enabled to false.  \nCLONE is GA and you can now override table properties  \nThe CLONE command is now generally available. See CREATE TABLE CLONE. In addition, you can now specify table property overrides when using CLONE. For examples, see Clone a table on Databricks.  \nRESTORE is GA  \nThe RESTORE command is now generally available. See RESTORE.  \nScala implicits simplify Spark read and write APIs  \nYou can import io.delta.implicits._ to use the delta method with Spark read and write APIs.  \nimport io.delta.implicits._ spark.read.delta(\"/my/table/path\") df.write.delta(\"/my/table/path\") spark.readStream.delta(\"/my/table/path\") df.writeStream.delta(\"/my/table/path\")"
    },
    {
        "id": 37,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "Improve streaming throughput with fetchParallelism connector option  \nThe SQS connector now supports a new option fetchParallelism to specify the parallelism to fetch messages from the queue service.  \nTrack stream progress with new Auto Loader metrics  \nAuto Loader now reports how many files exist in the backlog and how large the backlog is after every batch. You can use these metrics to track stream progress. For details on how to view the metrics, see What is Auto Loader?."
    },
    {
        "id": 38,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "Improvements\nWider support for notebook-scoped libraries using %pip and %conda magic commands  \nYou can now install notebook-scoped libraries using %pip and %conda magic commands on High Concurrency clusters with either table ACLs or credential passthrough enabled.  \nAccurate Spark SQL DECIMAL display  \nThe Spark SQL DECIMAL data type is now displayed in tables without loss of precision and scale.  \nADLS Gen 2 storage connector based on Hadoop 3.3 for ABFS  \nAn upgraded version of the ADLS Gen 2 storage connector is based on Hadoop 3.3 for ABFS. The upgraded connector contains many stability improvements (see Hadoop stability) and includes support for SAS token authentication.  \nThe Spark-S3 connection in AWS Redshift connector now supports assumeRole  \nThe Amazon Redshift connector uses S3 as an intermediary storage to read and write data. You can now assume an IAM role in order to authenticate to S3. You configure the IAM role to be assumed using existing configuration parameters, that is, fs.s3a.stsAssumeRole.arn and fs.s3a.credentialsType. See Authenticating to S3 and Redshift."
    },
    {
        "id": 39,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "Other fixes\nOther fixes\nThe Databricks Kafka connector for Spark Streaming has been removed. Use the Apache Spark connector.\n\nLibrary upgrades\nLibrary upgrades\nUpgraded Python libraries:  \nkoalas upgraded from 1.3.0 to 1.4.0.  \nUpgraded several installed R libraries. See Installed R Libraries.  \nOpenJDK 8 build upgraded to Zulu 8.50.0.51-CA-linux64 (build 1.8.0_275-b01).\n\nApache Spark"
    },
    {
        "id": 40,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "Apache Spark\nDatabricks Runtime 7.5 includes Apache Spark 3.0.1. This release includes all Spark fixes and improvements included in Databricks Runtime 7.4 (EoS), as well as the following additional bug fixes and improvements made to Spark:  \n[SPARK-33611] [UI] Avoid encoding twice on the query parameter of rewritten proxy URL  \n[SPARK-33587] [CORE] Kill the executor on nested fatal errors (7.x)  \n[SPARK-33140] [SQL] Revert code that not use passed-in SparkSession to get SQLConf.  \n[SPARK-33472] [SQL] Adjust RemoveRedundantSorts rule order  \n[SPARK-33422] [DOC] Fix the correct display of left menu item  \n[SPARK-27421] [SQL] Fix filter for int column and value class java.lang.String when pruning partition column  \n[SPARK-33483] [INFRA][TESTS] Fix rat exclusion patterns and add a LICENSE  \n[SPARK-33464] [INFRA] Add/remove (un)necessary cache and restructure GitHub Actions yaml  \n[SPARK-33407] [PYTHON][7.X] Simplify the exception message from Python UDFs (disabled by default)  \n[SPARK-33435] [SQL] DSv2: REFRESH TABLE should invalidate caches referencing the table  \n[SPARK-33358] [SQL] Return code when command process failed  \n[SPARK-33183] [DBCONNECT] Add RemoveRedundantSorts.scala to ALLOW_LIST.txt  \n[SPARK-33290] [SQL] REFRESH TABLE should invalidate cache even though the table itself may not be cached  \n[SPARK-33140] [SQL] Fix delta compile  \n[SPARK-33316] [SQL] Add unit tests for from_avro and to_avro with Schema Registry and user provided nullable Avro schema  \n[SPARK-33404] [SQL] Fix incorrect results in date_trunc expression"
    },
    {
        "id": 41,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "[SPARK-33404] [SQL] Fix incorrect results in date_trunc expression  \n[SPARK-33339] [PYTHON] Pyspark application will hang due to non Exception error  \n[SPARK-33412] [SQL] OverwriteByExpression should resolve its delete condition based on the table relation not the input query  \n[SPARK-33391] [SQL] element_at with CreateArray not respect one based index.  \n[SPARK-33316] [SQL] Support user provided nullable Avro schema for non-nullable catalyst schema in Avro writing  \n[SPARK-33372] [SQL] Fix InSet bucket pruning  \n[SPARK-33371] [PYTHON] Update setup.py and tests for Python 3.9  \n[SPARK-33140] [SQL] remove SQLConf and SparkSession in all sub-class of Rule[QueryPlan]  \n[SPARK-33338] [SQL] GROUP BY using literal map should not fail  \n[SPARK-33306] [SQL]Timezone is needed when cast date to string  \n[SPARK-33284] [WEB-UI] In the Storage UI page, clicking any field to sort the table will cause the header content to be lost  \n[SPARK-33362] [SQL] skipSchemaResolution should still require query to be resolved  \n[SPARK-32257] [SQL] Reports explicit errors for invalid usage of SET/RESET command  \n[SPARK-32406] [SQL] Make RESET syntax support single configuration reset  \n[SPARK-32376] [SQL] Make unionByName null-filling behavior work with struct columns  \n[SPARK-32308] [SQL] Move by-name resolution logic of unionByName from API code to analysis phase  \n[SPARK-20044] [UI] Support Spark UI behind front-end reverse proxy using a path prefix Revert proxy url  \n[SPARK-33268] [SQL][PYTHON] Fix bugs for casting data from/to PythonUserDefinedType"
    },
    {
        "id": 42,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "[SPARK-33268] [SQL][PYTHON] Fix bugs for casting data from/to PythonUserDefinedType  \n[SPARK-33292] [SQL] Make Literal ArrayBasedMapData string representation disambiguous  \n[SPARK-33183] [SQL] Fix Optimizer rule EliminateSorts and add a physical rule to remove redundant sorts  \n[SPARK-33272] [SQL] prune the attributes mapping in QueryPlan.transformUpWithNewOutput  \n[SPARK-32090] [SQL] Improve UserDefinedType.equal() to make it be symmetrical  \n[SPARK-33267] [SQL] Fix NPE issue on \u2018In\u2019 filter when one of values contains null  \n[SPARK-33208] [SQL] Update the document of SparkSession#sql  \n[SPARK-33260] [SQL] Fix incorrect results from SortExec when sortOrder is Stream  \n[SPARK-33230] [SQL] Hadoop committers to get unique job ID in \u201cspark.sql.sources.writeJobUUID\u201d  \n[SPARK-33197] [SQL] Make changes to spark.sql.analyzer.maxIterations take effect at runtime  \n[SPARK-33228] [SQL] Don\u2019t uncache data when replacing a view having the same logical plan  \n[SPARK-32557] [CORE] Logging and swallowing the exception per entry in History server  \n[SPARK-32436] [CORE] Initialize numNonEmptyBlocks in HighlyCompressedMapStatus.readExternal  \n[SPARK-33131] [SQL] Fix grouping sets with having clause can not resolve qualified col name  \n[SPARK-32761] [SQL] Allow aggregating multiple foldable distinct expressions  \n[SPARK-33094] [SQL] Make ORC format propagate Hadoop config from DS options to underlying HDFS file system"
    },
    {
        "id": 43,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "System environment\nOperating System: Ubuntu 18.04.5 LTS  \nJava: Zulu 8.50.0.51-CA-linux64 (build 1.8.0_275-b01)  \nScala: 2.12.10  \nPython: 3.7.5  \nR: R version 3.6.3 (2020-02-29)  \nDelta Lake 0.7.0  \nInstalled Python libraries  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nasn1crypto  \n1.3.0  \nbackcall  \n0.1.0  \nboto3  \n1.12.0  \nbotocore  \n1.15.0  \ncertifi  \n2020.6.20  \ncffi  \n1.14.0  \nchardet  \n3.0.4  \ncryptography  \n2.8  \ncycler  \n0.10.0  \nCython  \n0.29.15  \ndecorator  \n4.4.1  \ndocutils  \n0.15.2  \nentrypoints  \n0.3  \nidna  \n2.8  \nipykernel  \n5.1.4  \nipython  \n7.12.0  \nipython-genutils  \n0.2.0  \njedi  \n0.17.2  \njmespath  \n0.10.0  \njoblib  \n0.14.1  \njupyter-client  \n5.3.4  \njupyter-core  \n4.6.1  \nkiwisolver  \n1.1.0  \nkoalas  \n1.4.0  \nmatplotlib  \n3.1.3  \nnumpy  \n1.18.1  \npandas  \n1.0.1  \nparso  \n0.7.0  \npatsy  \n0.5.1  \npexpect  \n4.8.0  \npickleshare  \n0.7.5  \npip  \n20.0.2  \nprompt-toolkit  \n3.0.3  \npsycopg2  \n2.8.4  \nptyprocess  \n0.6.0  \npyarrow  \n1.0.1  \npycparser  \n2.19  \nPygments  \n2.5.2  \nPyGObject  \n3.26.1  \npyOpenSSL  \n19.1.0  \npyparsing  \n2.4.6  \nPySocks  \n1.7.1  \npython-apt  \n1.6.5+ubuntu0.3  \npython-dateutil  \n2.8.1  \npytz  \n2019.3  \npyzmq  \n18.1.1  \nrequests  \n2.22.0  \ns3transfer  \n0.3.3  \nscikit-learn  \n0.22.1  \nscipy  \n1.4.1  \nseaborn  \n0.10.0"
    },
    {
        "id": 44,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "2019.3  \npyzmq  \n18.1.1  \nrequests  \n2.22.0  \ns3transfer  \n0.3.3  \nscikit-learn  \n0.22.1  \nscipy  \n1.4.1  \nseaborn  \n0.10.0  \nsetuptools  \n45.2.0  \nsix  \n1.14.0  \nssh-import-id  \n5.7  \nstatsmodels  \n0.11.0  \ntornado  \n6.0.3  \ntraitlets  \n4.3.3  \nunattended-upgrades  \n0.1  \nurllib3  \n1.25.8  \nvirtualenv  \n16.7.10  \nwcwidth  \n0.1.8  \nwheel  \n0.34.2  \nInstalled R libraries  \nR libraries are installed from the Microsoft CRAN snapshot on 2020-11-02.  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \naskpass  \n1.1  \nassertthat  \n0.2.1  \nbackports  \n1.1.10  \nbase  \n3.6.3  \nbase64enc  \n0.1-3  \nBH  \n1.72.0-3  \nbit  \n4.0.4  \nbit64  \n4.0.5  \nblob  \n1.2.1  \nboot  \n1.3-25  \nbrew  \n1.0-6  \nbrio  \n1.1.0  \nbroom  \n0.7.2  \ncallr  \n3.5.1  \ncaret  \n6.0-86  \ncellranger  \n1.1.0  \nchron  \n2.3-56  \nclass  \n7.3-17  \ncli  \n2.1.0  \nclipr  \n0.7.1  \ncluster  \n2.1.0  \ncodetools  \n0.2-18  \ncolorspace  \n1.4-1  \ncommonmark  \n1.7  \ncompiler  \n3.6.3  \nconfig  \n0.3  \ncovr  \n3.5.1  \ncpp11  \n0.2.3  \ncrayon  \n1.3.4  \ncrosstalk  \n1.1.0.1  \ncurl  \n4.3  \ndata.table  \n1.13.2  \ndatasets  \n3.6.3  \nDBI  \n1.1.0  \ndbplyr  \n1.4.4  \ndesc  \n1.2.0  \ndevtools  \n2.3.2  \ndiffobj  \n0.3.2  \ndigest  \n0.6.27  \ndplyr  \n0.8.5  \nDT  \n0.16  \nellipsis  \n0.3.1  \nevaluate  \n0.14  \nfansi  \n0.4.1  \nfarver  \n2.0.3  \nfastmap  \n1.0.1  \nforcats  \n0.5.0  \nforeach  \n1.5.1  \nforeign  \n0.8-76  \nforge"
    },
    {
        "id": 45,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "evaluate  \n0.14  \nfansi  \n0.4.1  \nfarver  \n2.0.3  \nfastmap  \n1.0.1  \nforcats  \n0.5.0  \nforeach  \n1.5.1  \nforeign  \n0.8-76  \nforge  \n0.2.0  \nfs  \n1.5.0  \ngenerics  \n0.1.0  \nggplot2  \n3.3.2  \ngh  \n1.1.0  \ngit2r  \n0.27.1  \nglmnet  \n3.0-2  \nglobals  \n0.13.1  \nglue  \n1.4.2  \ngower  \n0.2.2  \ngraphics  \n3.6.3  \ngrDevices  \n3.6.3  \ngrid  \n3.6.3  \ngridExtra  \n2.3  \ngsubfn  \n0.7  \ngtable  \n0.3.0  \nhaven  \n2.3.1  \nhighr  \n0.8  \nhms  \n0.5.3  \nhtmltools  \n0.5.0  \nhtmlwidgets  \n1.5.2  \nhttpuv  \n1.5.4  \nhttr  \n1.4.2  \nhwriter  \n1.3.2  \nhwriterPlus  \n1.0-3  \nini  \n0.3.1  \nipred  \n0.9-9  \nisoband  \n0.2.2  \niterators  \n1.0.13  \njsonlite  \n1.7.1  \nKernSmooth  \n2.23-18  \nknitr  \n1.30  \nlabeling  \n0.4.2  \nlater  \n1.1.0.1  \nlattice  \n0.20-41  \nlava  \n1.6.8  \nlazyeval  \n0.2.2  \nlifecycle  \n0.2.0  \nlubridate  \n1.7.9  \nmagrittr  \n1.5  \nmarkdown  \n1.1  \nMASS  \n7.3-53  \nMatrix  \n1.2-18  \nmemoise  \n1.1.0  \nmethods  \n3.6.3  \nmgcv  \n1.8-33  \nmime  \n0.9  \nModelMetrics  \n1.2.2.2  \nmodelr  \n0.1.8  \nmunsell  \n0.5.0  \nnlme  \n3.1-150  \nnnet  \n7.3-14  \nnumDeriv  \n2016.8-1.1  \nopenssl  \n1.4.3  \nparallel  \n3.6.3  \npillar  \n1.4.6  \npkgbuild  \n1.1.0  \npkgconfig  \n2.0.3  \npkgload  \n1.1.0  \nplogr  \n0.2.0  \nplyr  \n1.8.6  \npraise  \n1.0.0  \nprettyunits"
    },
    {
        "id": 46,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "1.1.0  \npkgconfig  \n2.0.3  \npkgload  \n1.1.0  \nplogr  \n0.2.0  \nplyr  \n1.8.6  \npraise  \n1.0.0  \nprettyunits  \n1.1.1  \npROC  \n1.16.2  \nprocessx  \n3.4.4  \nprodlim  \n2019.11.13  \nprogress  \n1.2.2  \npromises  \n1.1.1  \nproto  \n1.0.0  \nps  \n1.4.0  \npurrr  \n0.3.4  \nr2d3  \n0.2.3  \nR6  \n2.5.0  \nrandomForest  \n4.6-14  \nrappdirs  \n0.3.1  \nrcmdcheck  \n1.3.3  \nRColorBrewer  \n1.1-2  \nRcpp  \n1.0.5  \nreadr  \n1.4.0  \nreadxl  \n1.3.1  \nrecipes  \n0.1.14  \nrematch  \n1.0.1  \nrematch2  \n2.1.2  \nremotes  \n2.2.0  \nreprex  \n0.3.0  \nreshape2  \n1.4.4  \nrex  \n1.2.0  \nrjson  \n0.2.20  \nrlang  \n0.4.8  \nrmarkdown  \n2.5  \nRODBC  \n1.3-16  \nroxygen2  \n7.1.1  \nrpart  \n4.1-15  \nrprojroot  \n1.3-2  \nRserve  \n1.8-7  \nRSQLite  \n2.2.1  \nrstudioapi  \n0.11  \nrversions  \n2.0.2  \nrvest  \n0.3.6  \nscales  \n1.1.1  \nselectr  \n0.4-2  \nsessioninfo  \n1.1.1  \nshape  \n1.4.4  \nshiny  \n1.5.0  \nsourcetools  \n0.1.7  \nsparklyr  \n1.4.0  \nSparkR  \n3.0.0  \nspatial  \n7.3-11  \nsplines  \n3.6.3  \nsqldf  \n0.4-11  \nSQUAREM  \n2020.5  \nstats  \n3.6.3  \nstats4  \n3.6.3  \nstringi  \n1.5.3  \nstringr  \n1.4.0  \nsurvival  \n3.2-7  \nsys  \n3.4  \ntcltk  \n3.6.3  \nTeachingDemos  \n2.10  \ntestthat  \n3.0.0  \ntibble  \n3.0.4  \ntidyr  \n1.1.2  \ntidyselect  \n1.1.0  \ntidyverse"
    },
    {
        "id": 47,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "tcltk  \n3.6.3  \nTeachingDemos  \n2.10  \ntestthat  \n3.0.0  \ntibble  \n3.0.4  \ntidyr  \n1.1.2  \ntidyselect  \n1.1.0  \ntidyverse  \n1.3.0  \ntimeDate  \n3043.102  \ntinytex  \n0.27  \ntools  \n3.6.3  \nusethis  \n1.6.3  \nutf8  \n1.1.4  \nutils  \n3.6.3  \nuuid  \n0.1-4  \nvctrs  \n0.3.4  \nviridisLite  \n0.3.0  \nwaldo  \n0.2.2  \nwhisker  \n0.4  \nwithr  \n2.3.0  \nxfun  \n0.19  \nxml2  \n1.3.2  \nxopen  \n1.0.0  \nxtable  \n1.8-4  \nyaml  \n2.2.1  \nInstalled Java and Scala libraries (Scala 2.12 cluster version)  \nGroup ID  \nArtifact ID  \nVersion  \nantlr  \nantlr  \n2.7.7  \ncom.amazonaws  \namazon-kinesis-client  \n1.12.0  \ncom.amazonaws  \naws-java-sdk-autoscaling  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudformation  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudfront  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudhsm  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudsearch  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudtrail  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudwatch  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudwatchmetrics  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-codedeploy  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cognitoidentity  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cognitosync  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-config"
    },
    {
        "id": 48,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "1.11.655  \ncom.amazonaws  \naws-java-sdk-cognitosync  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-config  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-core  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-datapipeline  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-directconnect  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-directory  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-dynamodb  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-ec2  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-ecs  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-efs  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-elasticache  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-elasticbeanstalk  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-elasticloadbalancing  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-elastictranscoder  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-emr  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-glacier  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-iam  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-importexport  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-kinesis  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-kms  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-lambda  \n1.11.655"
    },
    {
        "id": 49,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "com.amazonaws  \naws-java-sdk-kms  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-lambda  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-logs  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-machinelearning  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-opsworks  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-rds  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-redshift  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-route53  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-s3  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-ses  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-simpledb  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-simpleworkflow  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-sns  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-sqs  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-ssm  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-storagegateway  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-sts  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-support  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-swf-libraries  \n1.11.22  \ncom.amazonaws  \naws-java-sdk-workspaces  \n1.11.655  \ncom.amazonaws  \njmespath-java  \n1.11.655  \ncom.chuusai  \nshapeless_2.12"
    },
    {
        "id": 50,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "aws-java-sdk-workspaces  \n1.11.655  \ncom.amazonaws  \njmespath-java  \n1.11.655  \ncom.chuusai  \nshapeless_2.12  \n2.3.3  \ncom.clearspring.analytics  \nstream  \n2.9.6  \ncom.databricks  \nRserve  \n1.8-3  \ncom.databricks  \njets3t  \n0.7.1-0  \ncom.databricks.scalapb  \ncompilerplugin_2.12  \n0.4.15-10  \ncom.databricks.scalapb  \nscalapb-runtime_2.12  \n0.4.15-10  \ncom.esotericsoftware  \nkryo-shaded  \n4.0.2  \ncom.esotericsoftware  \nminlog  \n1.3.0  \ncom.fasterxml  \nclassmate  \n1.3.4  \ncom.fasterxml.jackson.core  \njackson-annotations  \n2.10.0  \ncom.fasterxml.jackson.core  \njackson-core  \n2.10.0  \ncom.fasterxml.jackson.core  \njackson-databind  \n2.10.0  \ncom.fasterxml.jackson.dataformat  \njackson-dataformat-cbor  \n2.10.0  \ncom.fasterxml.jackson.datatype  \njackson-datatype-joda  \n2.10.0  \ncom.fasterxml.jackson.module  \njackson-module-paranamer  \n2.10.0  \ncom.fasterxml.jackson.module  \njackson-module-scala_2.12  \n2.10.0  \ncom.github.ben-manes.caffeine  \ncaffeine  \n2.3.4  \ncom.github.fommil  \njniloader  \n1.1  \ncom.github.fommil.netlib  \ncore  \n1.1.2  \ncom.github.fommil.netlib  \nnative_ref-java  \n1.1  \ncom.github.fommil.netlib  \nnative_ref-java-natives  \n1.1  \ncom.github.fommil.netlib  \nnative_system-java  \n1.1  \ncom.github.fommil.netlib  \nnative_system-java-natives  \n1.1  \ncom.github.fommil.netlib"
    },
    {
        "id": 51,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "native_system-java  \n1.1  \ncom.github.fommil.netlib  \nnative_system-java-natives  \n1.1  \ncom.github.fommil.netlib  \nnetlib-native_ref-linux-x86_64-natives  \n1.1  \ncom.github.fommil.netlib  \nnetlib-native_system-linux-x86_64-natives  \n1.1  \ncom.github.joshelser  \ndropwizard-metrics-hadoop-metrics2-reporter  \n0.1.2  \ncom.github.luben  \nzstd-jni  \n1.4.4-3  \ncom.github.wendykierp  \nJTransforms  \n3.1  \ncom.google.code.findbugs  \njsr305  \n3.0.0  \ncom.google.code.gson  \ngson  \n2.2.4  \ncom.google.flatbuffers  \nflatbuffers-java  \n1.9.0  \ncom.google.guava  \nguava  \n15.0  \ncom.google.protobuf  \nprotobuf-java  \n2.6.1  \ncom.h2database  \nh2  \n1.4.195  \ncom.helger  \nprofiler  \n1.1.1  \ncom.jcraft  \njsch  \n0.1.50  \ncom.jolbox  \nbonecp  \n0.8.0.RELEASE  \ncom.lihaoyi  \nsourcecode_2.12  \n0.1.9  \ncom.microsoft.sqlserver  \nmssql-jdbc  \n8.2.1.jre8  \ncom.microsoft.azure  \nazure-data-lake-store-sdk  \n2.3.8  \ncom.ning  \ncompress-lzf  \n1.0.3  \ncom.sun.mail  \njavax.mail  \n1.5.2  \ncom.tdunning  \njson  \n1.8  \ncom.thoughtworks.paranamer  \nparanamer  \n2.8  \ncom.trueaccord.lenses  \nlenses_2.12  \n0.4.12  \ncom.twitter  \nchill-java  \n0.9.5  \ncom.twitter  \nchill_2.12  \n0.9.5  \ncom.twitter  \nutil-app_2.12  \n7.1.0  \ncom.twitter  \nutil-core_2.12  \n7.1.0  \ncom.twitter  \nutil-function_2.12"
    },
    {
        "id": 52,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "0.9.5  \ncom.twitter  \nutil-app_2.12  \n7.1.0  \ncom.twitter  \nutil-core_2.12  \n7.1.0  \ncom.twitter  \nutil-function_2.12  \n7.1.0  \ncom.twitter  \nutil-jvm_2.12  \n7.1.0  \ncom.twitter  \nutil-lint_2.12  \n7.1.0  \ncom.twitter  \nutil-registry_2.12  \n7.1.0  \ncom.twitter  \nutil-stats_2.12  \n7.1.0  \ncom.typesafe  \nconfig  \n1.2.1  \ncom.typesafe.scala-logging  \nscala-logging_2.12  \n3.7.2  \ncom.univocity  \nunivocity-parsers  \n2.9.0  \ncom.zaxxer  \nHikariCP  \n3.1.0  \ncommons-beanutils  \ncommons-beanutils  \n1.9.4  \ncommons-cli  \ncommons-cli  \n1.2  \ncommons-codec  \ncommons-codec  \n1.10  \ncommons-collections  \ncommons-collections  \n3.2.2  \ncommons-configuration  \ncommons-configuration  \n1.6  \ncommons-dbcp  \ncommons-dbcp  \n1.4  \ncommons-digester  \ncommons-digester  \n1.8  \ncommons-fileupload  \ncommons-fileupload  \n1.3.3  \ncommons-httpclient  \ncommons-httpclient  \n3.1  \ncommons-io  \ncommons-io  \n2.4  \ncommons-lang  \ncommons-lang  \n2.6  \ncommons-logging  \ncommons-logging  \n1.1.3  \ncommons-net  \ncommons-net  \n3.1  \ncommons-pool  \ncommons-pool  \n1.5.4  \ninfo.ganglia.gmetric4j  \ngmetric4j  \n1.0.10  \nio.airlift  \naircompressor  \n0.10  \nio.dropwizard.metrics  \nmetrics-core  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-graphite  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-healthchecks  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jetty9  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jmx  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-json  \n4.1.1  \nio.dropwizard.metrics"
    },
    {
        "id": 53,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "io.dropwizard.metrics  \nmetrics-jmx  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-json  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jvm  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-servlets  \n4.1.1  \nio.netty  \nnetty-all  \n4.1.47.Final  \nio.prometheus  \nsimpleclient  \n0.7.0  \nio.prometheus  \nsimpleclient_common  \n0.7.0  \nio.prometheus  \nsimpleclient_dropwizard  \n0.7.0  \nio.prometheus  \nsimpleclient_pushgateway  \n0.7.0  \nio.prometheus  \nsimpleclient_servlet  \n0.7.0  \nio.prometheus.jmx  \ncollector  \n0.12.0  \njakarta.annotation  \njakarta.annotation-api  \n1.3.5  \njakarta.validation  \njakarta.validation-api  \n2.0.2  \njakarta.ws.rs  \njakarta.ws.rs-api  \n2.1.6  \njavax.activation  \nactivation  \n1.1.1  \njavax.el  \njavax.el-api  \n2.2.4  \njavax.jdo  \njdo-api  \n3.0.1  \njavax.servlet  \njavax.servlet-api  \n3.1.0  \njavax.servlet.jsp  \njsp-api  \n2.1  \njavax.transaction  \njta  \n1.1  \njavax.transaction  \ntransaction-api  \n1.1  \njavax.xml.bind  \njaxb-api  \n2.2.2  \njavax.xml.stream  \nstax-api  \n1.0-2  \njavolution  \njavolution  \n5.5.1  \njline  \njline  \n2.14.6  \njoda-time  \njoda-time  \n2.10.5  \nlog4j  \napache-log4j-extras  \n1.2.17  \nlog4j  \nlog4j  \n1.2.17  \nnet.razorvine  \npyrolite  \n4.30  \nnet.sf.jpam  \njpam  \n1.1  \nnet.sf.opencsv  \nopencsv  \n2.3  \nnet.sf.supercsv  \nsuper-csv  \n2.2.0  \nnet.snowflake  \nsnowflake-ingest-sdk"
    },
    {
        "id": 54,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "jpam  \n1.1  \nnet.sf.opencsv  \nopencsv  \n2.3  \nnet.sf.supercsv  \nsuper-csv  \n2.2.0  \nnet.snowflake  \nsnowflake-ingest-sdk  \n0.9.6  \nnet.snowflake  \nsnowflake-jdbc  \n3.12.8  \nnet.snowflake  \nspark-snowflake_2.12  \n2.8.1-spark_3.0  \nnet.sourceforge.f2j  \narpack_combined_all  \n0.1  \norg.acplt.remotetea  \nremotetea-oncrpc  \n1.1.2  \norg.antlr  \nST4  \n4.0.4  \norg.antlr  \nantlr-runtime  \n3.5.2  \norg.antlr  \nantlr4-runtime  \n4.7.1  \norg.antlr  \nstringtemplate  \n3.2.1  \norg.apache.ant  \nant  \n1.9.2  \norg.apache.ant  \nant-jsch  \n1.9.2  \norg.apache.ant  \nant-launcher  \n1.9.2  \norg.apache.arrow  \narrow-format  \n0.15.1  \norg.apache.arrow  \narrow-memory  \n0.15.1  \norg.apache.arrow  \narrow-vector  \n0.15.1  \norg.apache.avro  \navro  \n1.8.2  \norg.apache.avro  \navro-ipc  \n1.8.2  \norg.apache.avro  \navro-mapred-hadoop2  \n1.8.2  \norg.apache.commons  \ncommons-compress  \n1.8.1  \norg.apache.commons  \ncommons-crypto  \n1.0.0  \norg.apache.commons  \ncommons-lang3  \n3.9  \norg.apache.commons  \ncommons-math3  \n3.4.1  \norg.apache.commons  \ncommons-text  \n1.6  \norg.apache.curator  \ncurator-client  \n2.7.1  \norg.apache.curator  \ncurator-framework  \n2.7.1  \norg.apache.curator  \ncurator-recipes  \n2.7.1  \norg.apache.derby  \nderby  \n10.12.1.1  \norg.apache.directory.api  \napi-asn1-api  \n1.0.0-M20  \norg.apache.directory.api  \napi-util  \n1.0.0-M20  \norg.apache.directory.server  \napacheds-i18n  \n2.0.0-M15"
    },
    {
        "id": 55,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "org.apache.directory.api  \napi-util  \n1.0.0-M20  \norg.apache.directory.server  \napacheds-i18n  \n2.0.0-M15  \norg.apache.directory.server  \napacheds-kerberos-codec  \n2.0.0-M15  \norg.apache.hadoop  \nhadoop-annotations  \n2.7.4  \norg.apache.hadoop  \nhadoop-auth  \n2.7.4  \norg.apache.hadoop  \nhadoop-client  \n2.7.4  \norg.apache.hadoop  \nhadoop-common  \n2.7.4  \norg.apache.hadoop  \nhadoop-hdfs  \n2.7.4  \norg.apache.hadoop  \nhadoop-mapreduce-client-app  \n2.7.4  \norg.apache.hadoop  \nhadoop-mapreduce-client-common  \n2.7.4  \norg.apache.hadoop  \nhadoop-mapreduce-client-core  \n2.7.4  \norg.apache.hadoop  \nhadoop-mapreduce-client-jobclient  \n2.7.4  \norg.apache.hadoop  \nhadoop-mapreduce-client-shuffle  \n2.7.4  \norg.apache.hadoop  \nhadoop-yarn-api  \n2.7.4  \norg.apache.hadoop  \nhadoop-yarn-client  \n2.7.4  \norg.apache.hadoop  \nhadoop-yarn-common  \n2.7.4  \norg.apache.hadoop  \nhadoop-yarn-server-common  \n2.7.4  \norg.apache.hive  \nhive-beeline  \n2.3.7  \norg.apache.hive  \nhive-cli  \n2.3.7  \norg.apache.hive  \nhive-common  \n2.3.7  \norg.apache.hive  \nhive-exec-core  \n2.3.7  \norg.apache.hive  \nhive-jdbc  \n2.3.7  \norg.apache.hive  \nhive-llap-client  \n2.3.7  \norg.apache.hive  \nhive-llap-common  \n2.3.7  \norg.apache.hive  \nhive-metastore  \n2.3.7  \norg.apache.hive  \nhive-serde  \n2.3.7  \norg.apache.hive  \nhive-shims"
    },
    {
        "id": 56,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "hive-llap-common  \n2.3.7  \norg.apache.hive  \nhive-metastore  \n2.3.7  \norg.apache.hive  \nhive-serde  \n2.3.7  \norg.apache.hive  \nhive-shims  \n2.3.7  \norg.apache.hive  \nhive-storage-api  \n2.7.1  \norg.apache.hive  \nhive-vector-code-gen  \n2.3.7  \norg.apache.hive.shims  \nhive-shims-0.23  \n2.3.7  \norg.apache.hive.shims  \nhive-shims-common  \n2.3.7  \norg.apache.hive.shims  \nhive-shims-scheduler  \n2.3.7  \norg.apache.htrace  \nhtrace-core  \n3.1.0-incubating  \norg.apache.httpcomponents  \nhttpclient  \n4.5.6  \norg.apache.httpcomponents  \nhttpcore  \n4.4.12  \norg.apache.ivy  \nivy  \n2.4.0  \norg.apache.mesos  \nmesos-shaded-protobuf  \n1.4.0  \norg.apache.orc  \norc-core  \n1.5.10  \norg.apache.orc  \norc-mapreduce  \n1.5.10  \norg.apache.orc  \norc-shims  \n1.5.10  \norg.apache.parquet  \nparquet-column  \n1.10.1-databricks6  \norg.apache.parquet  \nparquet-common  \n1.10.1-databricks6  \norg.apache.parquet  \nparquet-encoding  \n1.10.1-databricks6  \norg.apache.parquet  \nparquet-format  \n2.4.0  \norg.apache.parquet  \nparquet-hadoop  \n1.10.1-databricks6  \norg.apache.parquet  \nparquet-jackson  \n1.10.1-databricks6  \norg.apache.thrift  \nlibfb303  \n0.9.3  \norg.apache.thrift  \nlibthrift  \n0.12.0  \norg.apache.velocity  \nvelocity  \n1.5  \norg.apache.xbean  \nxbean-asm7-shaded  \n4.15  \norg.apache.yetus  \naudience-annotations  \n0.5.0  \norg.apache.zookeeper  \nzookeeper  \n3.4.14  \norg.codehaus.jackson  \njackson-core-asl  \n1.9.13  \norg.codehaus.jackson  \njackson-jaxrs"
    },
    {
        "id": 57,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "0.5.0  \norg.apache.zookeeper  \nzookeeper  \n3.4.14  \norg.codehaus.jackson  \njackson-core-asl  \n1.9.13  \norg.codehaus.jackson  \njackson-jaxrs  \n1.9.13  \norg.codehaus.jackson  \njackson-mapper-asl  \n1.9.13  \norg.codehaus.jackson  \njackson-xc  \n1.9.13  \norg.codehaus.janino  \ncommons-compiler  \n3.0.16  \norg.codehaus.janino  \njanino  \n3.0.16  \norg.datanucleus  \ndatanucleus-api-jdo  \n4.2.4  \norg.datanucleus  \ndatanucleus-core  \n4.1.17  \norg.datanucleus  \ndatanucleus-rdbms  \n4.1.19  \norg.datanucleus  \njavax.jdo  \n3.2.0-m3  \norg.eclipse.jetty  \njetty-client  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-continuation  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-http  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-io  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-jndi  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-plus  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-proxy  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-security  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-server  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-servlet  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-servlets  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-util  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-webapp  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-xml  \n9.4.18.v20190429  \norg.fusesource.leveldbjni  \nleveldbjni-all"
    },
    {
        "id": 58,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "org.eclipse.jetty  \njetty-xml  \n9.4.18.v20190429  \norg.fusesource.leveldbjni  \nleveldbjni-all  \n1.8  \norg.glassfish.hk2  \nhk2-api  \n2.6.1  \norg.glassfish.hk2  \nhk2-locator  \n2.6.1  \norg.glassfish.hk2  \nhk2-utils  \n2.6.1  \norg.glassfish.hk2  \nosgi-resource-locator  \n1.0.3  \norg.glassfish.hk2.external  \naopalliance-repackaged  \n2.6.1  \norg.glassfish.hk2.external  \njakarta.inject  \n2.6.1  \norg.glassfish.jersey.containers  \njersey-container-servlet  \n2.30  \norg.glassfish.jersey.containers  \njersey-container-servlet-core  \n2.30  \norg.glassfish.jersey.core  \njersey-client  \n2.30  \norg.glassfish.jersey.core  \njersey-common  \n2.30  \norg.glassfish.jersey.core  \njersey-server  \n2.30  \norg.glassfish.jersey.inject  \njersey-hk2  \n2.30  \norg.glassfish.jersey.media  \njersey-media-jaxb  \n2.30  \norg.hibernate.validator  \nhibernate-validator  \n6.1.0.Final  \norg.javassist  \njavassist  \n3.25.0-GA  \norg.jboss.logging  \njboss-logging  \n3.3.2.Final  \norg.jdbi  \njdbi  \n2.63.1  \norg.joda  \njoda-convert  \n1.7  \norg.jodd  \njodd-core  \n3.5.2  \norg.json4s  \njson4s-ast_2.12  \n3.6.6  \norg.json4s  \njson4s-core_2.12  \n3.6.6  \norg.json4s  \njson4s-jackson_2.12  \n3.6.6  \norg.json4s  \njson4s-scalap_2.12  \n3.6.6  \norg.lz4  \nlz4-java  \n1.7.1  \norg.mariadb.jdbc  \nmariadb-java-client  \n2.1.2  \norg.objenesis  \nobjenesis  \n2.5.1"
    },
    {
        "id": 59,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "org.lz4  \nlz4-java  \n1.7.1  \norg.mariadb.jdbc  \nmariadb-java-client  \n2.1.2  \norg.objenesis  \nobjenesis  \n2.5.1  \norg.postgresql  \npostgresql  \n42.1.4  \norg.roaringbitmap  \nRoaringBitmap  \n0.7.45  \norg.roaringbitmap  \nshims  \n0.7.45  \norg.rocksdb  \nrocksdbjni  \n6.2.2  \norg.rosuda.REngine  \nREngine  \n2.1.0  \norg.scala-lang  \nscala-compiler_2.12  \n2.12.10  \norg.scala-lang  \nscala-library_2.12  \n2.12.10  \norg.scala-lang  \nscala-reflect_2.12  \n2.12.10  \norg.scala-lang.modules  \nscala-collection-compat_2.12  \n2.1.1  \norg.scala-lang.modules  \nscala-parser-combinators_2.12  \n1.1.2  \norg.scala-lang.modules  \nscala-xml_2.12  \n1.2.0  \norg.scala-sbt  \ntest-interface  \n1.0  \norg.scalacheck  \nscalacheck_2.12  \n1.14.2  \norg.scalactic  \nscalactic_2.12  \n3.0.8  \norg.scalanlp  \nbreeze-macros_2.12  \n1.0  \norg.scalanlp  \nbreeze_2.12  \n1.0  \norg.scalatest  \nscalatest_2.12  \n3.0.8  \norg.slf4j  \njcl-over-slf4j  \n1.7.30  \norg.slf4j  \njul-to-slf4j  \n1.7.30  \norg.slf4j  \nslf4j-api  \n1.7.30  \norg.slf4j  \nslf4j-log4j12  \n1.7.30  \norg.spark-project.spark  \nunused  \n1.0.0  \norg.springframework  \nspring-core  \n4.1.4.RELEASE  \norg.springframework  \nspring-test  \n4.1.4.RELEASE  \norg.threeten  \nthreeten-extra  \n1.5.0  \norg.tukaani  \nxz  \n1.5"
    },
    {
        "id": 60,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.5.html",
        "content": "spring-core  \n4.1.4.RELEASE  \norg.springframework  \nspring-test  \n4.1.4.RELEASE  \norg.threeten  \nthreeten-extra  \n1.5.0  \norg.tukaani  \nxz  \n1.5  \norg.typelevel  \nalgebra_2.12  \n2.0.0-M2  \norg.typelevel  \ncats-kernel_2.12  \n2.0.0-M4  \norg.typelevel  \nmachinist_2.12  \n0.6.8  \norg.typelevel  \nmacro-compat_2.12  \n1.1.1  \norg.typelevel  \nspire-macros_2.12  \n0.17.0-M1  \norg.typelevel  \nspire-platform_2.12  \n0.17.0-M1  \norg.typelevel  \nspire-util_2.12  \n0.17.0-M1  \norg.typelevel  \nspire_2.12  \n0.17.0-M1  \norg.wildfly.openssl  \nwildfly-openssl  \n1.0.7.Final  \norg.xerial  \nsqlite-jdbc  \n3.8.11.2  \norg.xerial.snappy  \nsnappy-java  \n1.1.7.5  \norg.yaml  \nsnakeyaml  \n1.24  \noro  \noro  \n2.0.8  \npl.edu.icm  \nJLargeArrays  \n1.5  \nsoftware.amazon.ion  \nion-java  \n1.0.2  \nstax  \nstax-api  \n1.0.1  \nxmlenc  \nxmlenc  \n0.52"
    },
    {
        "id": 61,
        "url": "https://docs.databricks.com/en/archive/dev-tools/dbx/dbx-migrate.html",
        "content": "Migrate from dbx to bundles  \nImportant  \nDatabricks recommends that you use Databricks Asset Bundles instead of dbx by Databricks Labs. Related articles about dbx have been retired and might not be updated.  \nThis article describes how to migrate projects for dbx by Databricks Labs over to Databricks Asset Bundles. See Introduction to dbx by Databricks Labs and What are Databricks Asset Bundles?.  \nBefore you migrate, note the following limitations and feature comparisons between dbx by Databricks Labs and Databricks Asset Bundles.  \nLimitations\nLimitations\nThe following functionality supported in dbx by Databricks Labs is limited, does not exist, or requires workarounds in Databricks Asset Bundles.  \nBuilding JAR artifacts is not supported in bundles.  \nFUSE notation for workspace paths is not supported in bundles (for example, /Workspace/<path>/<filename>). However, you can instruct bundles to generate FUSE-style workspace paths during deployments by using notation such as /Workspace/${bundle.file_path}/<filename>.\n\nFeature comparisons"
    },
    {
        "id": 62,
        "url": "https://docs.databricks.com/en/archive/dev-tools/dbx/dbx-migrate.html",
        "content": "Feature comparisons\nBefore you migrate, note how the following features for dbx by Databricks Labs are implemented in Databricks Asset Bundles.  \nTemplates and projects  \ndbx provide support for Jinja templating. You can include Jinja templates in the deployment configuration and pass environment variables either inline or through a variables file. Although not recommended, dbx also provides experimental support for custom user functions.  \nBundles provide support for Go templates for configuration reuse. Users can create bundles based on prebuilt templates. There is almost full parity for templating, except for custom user functions.  \nBuild management  \ndbx provides build support through pip wheel, Poetry, and Flit. Users can specify the build option in the build section of a project\u2019s deployment.yml file.  \nBundles enable users to build, deploy, and run Python wheel files. Users can leverage the built-in whl entry in a bundle\u2019s databricks.yml file.  \nSync, deploy, and run code  \ndbx enables uploading code separately from generating workspace resources such as Databricks jobs.  \nBundles always upload code and create or update workspace resources at the same time. This simplifies deployments and avoids blocking conditions for jobs that are already in progress."
    },
    {
        "id": 63,
        "url": "https://docs.databricks.com/en/archive/dev-tools/dbx/dbx-migrate.html",
        "content": "Migrate a dbx project to a bundle\nAfter you note the preceding limitations and feature comparisons between dbx by Databricks Labs and Databricks Asset Bundles, you are ready to migrate from dbx to bundles.  \nDatabricks recommends that to begin a dbx project migration, you keep your dbx project in its original folder and that you have a separate, blank folder into which you copy your original dbx project\u2019s contents. This separate folder will be your new bundle. You could encounter unexpected issues if you begin converting your dbx project in its original folder to a bundle and then make some mistakes or want to start over from the beginning,  \nStep 1: Install and set up the Databricks CLI  \nDatabricks Asset Bundles are generally available in Databricks CLI version 0.218.0 and above. If you have already installed and set up Databricks CLI version 0.218.0 or above, skip ahead to Step 2.  \nNote  \nBundles are not compatible with Databricks CLI versions 0.18 and below.  \nInstall or update to Databricks CLI version 0.218.0 or above. See Install or update the Databricks CLI.  \nSet up the Databricks CLI for authentication with your target Databricks workspaces, for example by using Databricks personal access token authentication. For other Databricks authentication types, see Authentication for the Databricks CLI.  \nStep 2: Create the bundle configuration file  \nIf you are using an IDE such as Visual Studio Code, PyCharm Professional or IntelliJ IDEA Ultimate that provides support for YAML files and JSON schema files, you can use your IDE not only to create the bundle configuration file but to check the file\u2019s syntax and formatting and provide code completion hints, as follows.  \nAdd YAML language server support to Visual Studio Code, for example by installing the YAML extension from the Visual Studio Code Marketplace.  \nGenerate the Databricks Asset Bundle configuration JSON schema file by using the Databricks CLI to run the bundle schema command and redirect the output to a JSON file. For example, generate a file named bundle_config_schema.json within the current directory, as follows:  \ndatabricks bundle schema > bundle_config_schema.json"
    },
    {
        "id": 64,
        "url": "https://docs.databricks.com/en/archive/dev-tools/dbx/dbx-migrate.html",
        "content": "databricks bundle schema > bundle_config_schema.json  \nUse Visual Studio Code to create or open a bundle configuration file within the current directory. By convention, this file is named databricks.yml.  \nAdd the following comment to the beginning of your bundle configuration file:  \n# yaml-language-server: $schema=bundle_config_schema.json  \nNote  \nIn the preceding comment, if your Databricks Asset Bundle configuration JSON schema file is in a different path, replace bundle_config_schema.json with the full path to your schema file.  \nUse the YAML language server features that you added earlier. For more information, see your YAML language server\u2019s documentation.  \nGenerate the Databricks Asset Bundle configuration JSON schema file by using the Databricks CLI to run the bundle schema command and redirect the output to a JSON file. For example, generate a file named bundle_config_schema.json within the current directory, as follows:  \ndatabricks bundle schema > bundle_config_schema.json  \nConfigure PyCharm to recognize the bundle configuration JSON schema file, and then complete the JSON schema mapping, by following the instructions in Configure a custom JSON schema.  \nUse PyCharm to create or open a bundle configuration file. By convention, this file is named databricks.yml. As you type, PyCharm checks for JSON schema syntax and formatting and provides code completion hints.  \nGenerate the Databricks Asset Bundle configuration JSON schema file by using the Databricks CLI to run the bundle schema command and redirect the output to a JSON file. For example, generate a file named bundle_config_schema.json within the current directory, as follows:  \ndatabricks bundle schema > bundle_config_schema.json  \nConfigure IntelliJ IDEA to recognize the bundle configuration JSON schema file, and then complete the JSON schema mapping, by following the instructions in Configure a custom JSON schema.  \nUse IntelliJ IDEA to create or open a bundle configuration file. By convention, this file is named databricks.yml. As you type, IntelliJ IDEA checks for JSON schema syntax and formatting and provides code completion hints."
    },
    {
        "id": 65,
        "url": "https://docs.databricks.com/en/archive/dev-tools/dbx/dbx-migrate.html",
        "content": "Step 3: Convert dbx project settings to databricks.yml  \nConvert the settings in your dbx project\u2019s .dbx/project.json file to the equivalent settings in your bundle\u2019s databricks.yml file. For details, see Converting dbx project settings to databricks.yml.  \nStep 4: Convert dbx deployment settings to databricks.yml  \nConvert the settings in your dbx project\u2019s conf folder to the equivalent settings in your bundle\u2019s databricks.yml file. For details, see Converting dbx deployment settings to databricks.yml.  \nStep 5: Validate the bundle  \nBefore you deploy artifacts or run a Databricks job, a Delta Live Tables pipeline, or an MLOps pipeline, you should make sure that your bundle configuration file is syntactically correct. To do this, run the bundle validate command from the bundle root:  \ndatabricks bundle validate  \nFor information about bundle validate, see Validate a bundle.  \nStep 6: Deploy the bundle  \nTo deploy any specified local artifacts to the remote workspace, run the bundle deploy command from the bundle root. If no command options are specified, the default target declared in the bundle configuration file is used:  \ndatabricks bundle deploy  \nTo deploy the artifacts within the context of a specific target, specify the -t (or --target) option along with the target\u2019s name as declared within the bundle configuration file. For example, for a target declared with the name development:  \ndatabricks bundle deploy -t development  \nFor information about bundle deploy, see Deploy a bundle.  \nTip  \nYou can link bundle-defined jobs and pipelines to existing jobs and pipelines in the Databricks workspace to keep them in sync. See Bind bundle resources.  \nStep 7: Run the bundle  \nTo run a specific job or pipeline, run the bundle run command from the bundle root. You must specify the job or pipeline declared within the bundle configuration file. If the -t option is not specified, the default target as declared within the bundle configuration file is used. For example, to run a job named hello_job within the context of the default target:  \ndatabricks bundle run hello_job  \nTo run a job named hello_job within the context of a target declared with the name development:  \ndatabricks bundle run -t development hello_job"
    },
    {
        "id": 66,
        "url": "https://docs.databricks.com/en/archive/dev-tools/dbx/dbx-migrate.html",
        "content": "databricks bundle run hello_job  \nTo run a job named hello_job within the context of a target declared with the name development:  \ndatabricks bundle run -t development hello_job  \nFor information about bundle run, see Run a bundle.  \n(Optional) Step 8: Configure the bundle for CI/CD with GitHub  \nIf you use GitHub for CI/CD, you can use GitHub Actions to run the databricks bundle deploy and databricks bundle run commands automatically, based on specific GitHub workflow events and other criteria. See Run a CI/CD workflow with a Databricks Asset Bundle and GitHub Actions."
    },
    {
        "id": 67,
        "url": "https://docs.databricks.com/en/archive/dev-tools/dbx/dbx-migrate.html",
        "content": "Converting dbx project settings to databricks.yml\nFor dbx, project settings are by default in a file named project.json in the project\u2019s .dbx folder. See Project file reference.  \nFor bundles, bundle configurations are by default in a file named databricks.yml within the bundle\u2019s root folder. See Databricks Asset Bundle configurations.  \nFor a conf/project.json file with the following example content:  \n{ \"environments\": { \"default\": { \"profile\": \"charming-aurora\", \"storage_type\": \"mlflow\", \"properties\": { \"workspace_directory\": \"/Shared/dbx/charming_aurora\", \"artifact_location\": \"/Shared/dbx/projects/charming_aurora\" } } }, \"inplace_jinja_support\": true }  \nThe corresponding databricks.yml file is as follows:  \nbundle: name: <some-unique-bundle-name> targets: default: workspace: profile: charming-aurora root_path: /Shared/dbx/charming_aurora artifact_path: /Shared/dbx/projects/charming_aurora resources: # See an example \"resources\" mapping in the following section.  \nThe following objects in this example\u2019s preceding conf/project.json file are not supported in databricks.yml files and have no workarounds:  \ninplace_jinja_support  \nstorage_type  \nThe following additional allowed objects in conf/project.json files are not supported in databricks.yml files and have no workarounds:  \nenable-context-based-upload-for-execute  \nenable-failsafe-cluster-reuse-with-assets"
    },
    {
        "id": 68,
        "url": "https://docs.databricks.com/en/archive/dev-tools/dbx/dbx-migrate.html",
        "content": "Converting dbx deployment settings to databricks.yml\nFor dbx, deployment settings are by default in a file within the project\u2019s conf folder. See Deployment file reference. The deployment settings file by default has one of the following file names:  \ndeployment.yml  \ndeployment.yaml  \ndeployment.json  \ndeployment.yml.j2  \ndeployment.yaml.j2  \ndeployment.json.j2  \nFor bundles, deployment settings are by default in a file named databricks.yml within the bundle\u2019s root folder. See Databricks Asset Bundle configurations.  \nFor a conf/deployment.yml file with the following example content:  \nbuild: python: \"pip\" environments: default: workflows: - name: \"workflow1\" tasks: - task_key: \"task1\" python_wheel_task: package_name: \"some-pkg\" entry_point: \"some-ep\"  \nThe corresponding databricks.yml file is as follows:  \nbundle: name: <some-unique-bundle-name> targets: default: workspace: # See an example \"workspace\" mapping in the preceding section. resources: jobs: workflow1: tasks: - task_key: task1 python_wheel_task: package_name: some-pkg entry_point: some-ep  \nThe following object in this example\u2019s preceding conf/deployment.yml file are not supported in databricks.yml files and have no workarounds:  \nbuild (although see Develop a Python wheel file using Databricks Asset Bundles)  \nThe following additional allowed objects and functionality in conf/deployment.yml files are not supported in databricks.yml files and have no workarounds unless otherwise stated:  \naccess_control_list  \ncustom (use standard YAML anchors instead)  \ndeployment_config  \nDatabricks Jobs 2.0 format (use Jobs 2.1 format instead)  \ndbx Jinja features  \nName-based properties"
    },
    {
        "id": 69,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "Create an IAM role for workspace deployment  \nThis article describes how to:  \nCreate and configure a cross-account IAM role for Databricks workspace deployment. This role gives Databricks limited access to your AWS account for the purposes of creating and managing compute and VPC resources.  \nUse the Databricks account console to create a credential configuration that references the IAM role.  \nRequirements\nRequirements\nYou need to be a Databricks account admin.\n\nAutomate IAM role creation\nAutomate IAM role creation\nYou can automate the IAM role creation by using one of the following automation options:  \nThe AWS Quick Start (CloudFormation) to deploy your workspace. This is the recommended workspace deployment method.  \nThe Databricks Terraform provider. See Create Databricks workspaces using Terraform.\n\nManual IAM role creation\nManual IAM role creation\nThe following steps apply to a custom AWS workspace deployment. You only need to follow these steps if you are deploying a workspace using the Custom AWS configuration option.  \nStep 1: Create a cross-account IAM role  \nStep 2: Create an access policy  \nStep 3: Create a credential configuration for the role in Databricks\n\nStep 1: Create a cross-account IAM role"
    },
    {
        "id": 70,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "Step 1: Create a cross-account IAM role\nGet your Databricks account ID. See Locate your account ID.  \nLog into your AWS Console as a user with administrator privileges and go to the IAM console.  \nClick the Roles tab in the sidebar.  \nClick Create role.  \nIn Select type of trusted entity, click the AWS account tile.  \nSelect the Another AWS account checkbox.  \nIn the Account ID field, enter the Databricks account ID 414351767826. This is not the Account ID you copied from the Databricks account console. If you are are using Databricks on AWS GovCloud use the Databricks account ID 044793339203.  \nSelect the Require external ID checkbox.  \nIn the External ID field, enter your Databricks account ID, which you copied from the Databricks account console.  \nClick the Next button.  \nIn the Add Permissions page, click the Next button. You should now be on the Name, review, and create page.  \nIn the Role name field, enter a role name.  \nClick Create role. The list of roles appears.\n\nStep 2: Create an access policy"
    },
    {
        "id": 71,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "Step 2: Create an access policy\nThe access policy you add to the role depends on your Amazon VPC (Virtual Private Cloud) deployment type. For information about how Databricks uses each permission, see IAM permissions for Databricks-managed VPCs. Use the policy instructions that describe your deployment:  \nOption 1: Default. A single VPC that Databricks creates and configures in your AWS account. This is the default configuration.  \nOption 2: Customer-managed VPC with default restrictions. Create your Databricks workspaces in your own VPC, using a feature known as customer-managed VPC.  \nOption 3: Customer-managed VPC with custom restrictions. Create your Databricks workspaces in your own VPC with custom restrictions for account ID, VPC ID, AWS Region, and security group.  \nOption 1: Default deployment policy  \nIn the Roles section of the IAM console, click the IAM role you created in Step 1.  \nClick the Add permissions drop-down and select Create inline policy.  \nIn the policy editor, click the JSON tab.  \nCopy and paste the following access policy:"
    },
    {
        "id": 72,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1403287045000\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:AllocateAddress\", \"ec2:AssignPrivateIpAddresses\", \"ec2:AssociateDhcpOptions\", \"ec2:AssociateIamInstanceProfile\", \"ec2:AssociateRouteTable\", \"ec2:AttachInternetGateway\", \"ec2:AttachVolume\", \"ec2:AuthorizeSecurityGroupEgress\", \"ec2:AuthorizeSecurityGroupIngress\", \"ec2:CancelSpotInstanceRequests\", \"ec2:CreateDhcpOptions\", \"ec2:CreateFleet\", \"ec2:CreateInternetGateway\", \"ec2:CreateLaunchTemplate\", \"ec2:CreateLaunchTemplateVersion\", \"ec2:CreateNatGateway\", \"ec2:CreateRoute\", \"ec2:CreateRouteTable\", \"ec2:CreateSecurityGroup\", \"ec2:CreateSubnet\", \"ec2:CreateTags\", \"ec2:CreateVolume\", \"ec2:CreateVpc\", \"ec2:CreateVpcEndpoint\", \"ec2:DeleteDhcpOptions\", \"ec2:DeleteFleets\", \"ec2:DeleteInternetGateway\", \"ec2:DeleteLaunchTemplate\", \"ec2:DeleteLaunchTemplateVersions\", \"ec2:DeleteNatGateway\", \"ec2:DeleteRoute\", \"ec2:DeleteRouteTable\", \"ec2:DeleteSecurityGroup\", \"ec2:DeleteSubnet\", \"ec2:DeleteTags\", \"ec2:DeleteVolume\", \"ec2:DeleteVpc\", \"ec2:DeleteVpcEndpoints\","
    },
    {
        "id": 73,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "\"ec2:DeleteTags\", \"ec2:DeleteVolume\", \"ec2:DeleteVpc\", \"ec2:DeleteVpcEndpoints\", \"ec2:DescribeAvailabilityZones\", \"ec2:DescribeFleetHistory\", \"ec2:DescribeFleetInstances\", \"ec2:DescribeFleets\", \"ec2:DescribeIamInstanceProfileAssociations\", \"ec2:DescribeInstanceStatus\", \"ec2:DescribeInstances\", \"ec2:DescribeInternetGateways\", \"ec2:DescribeLaunchTemplates\", \"ec2:DescribeLaunchTemplateVersions\", \"ec2:DescribeNatGateways\", \"ec2:DescribePrefixLists\", \"ec2:DescribeReservedInstancesOfferings\", \"ec2:DescribeRouteTables\", \"ec2:DescribeSecurityGroups\", \"ec2:DescribeSpotInstanceRequests\", \"ec2:DescribeSpotPriceHistory\", \"ec2:DescribeSubnets\", \"ec2:DescribeVolumes\", \"ec2:DescribeVpcs\", \"ec2:DetachInternetGateway\", \"ec2:DisassociateIamInstanceProfile\", \"ec2:DisassociateRouteTable\", \"ec2:GetLaunchTemplateData\", \"ec2:GetSpotPlacementScores\", \"ec2:ModifyFleet\", \"ec2:ModifyLaunchTemplate\", \"ec2:ModifyVpcAttribute\", \"ec2:ReleaseAddress\", \"ec2:ReplaceIamInstanceProfileAssociation\", \"ec2:RequestSpotInstances\", \"ec2:RevokeSecurityGroupEgress\", \"ec2:RevokeSecurityGroupIngress\", \"ec2:RunInstances\", \"ec2:TerminateInstances\" ], \"Resource\": [ \"*\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"iam:CreateServiceLinkedRole\","
    },
    {
        "id": 74,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "\"ec2:TerminateInstances\" ], \"Resource\": [ \"*\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"iam:CreateServiceLinkedRole\", \"iam:PutRolePolicy\" ], \"Resource\": \"arn:aws:iam::*:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot\", \"Condition\": { \"StringLike\": { \"iam:AWSServiceName\": \"spot.amazonaws.com\" } } } ] }"
    },
    {
        "id": 75,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "Click Review policy.  \nIn the Name field, enter a policy name.  \nClick Create policy.  \n(Optional) If you use Service Control Policies to deny certain actions at the AWS account level, ensure that sts:AssumeRole is allowlisted so Databricks can assume the cross-account role.  \nIn the role summary, copy the Role ARN to add to Databricks.  \nOption 2: Customer-managed VPC with default restrictions policy  \nLog into your AWS Console as a user with administrator privileges and go to the IAM console.  \nClick the Roles tab in the sidebar.  \nIn the list of roles, click the cross-account IAM role that you created in Step 1.  \nClick the Add permissions drop-down and select Create inline policy.  \nIn the policy editor, click the JSON tab.  \nCopy and paste the following access policy."
    },
    {
        "id": 76,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1403287045000\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:AssociateIamInstanceProfile\", \"ec2:AttachVolume\", \"ec2:AuthorizeSecurityGroupEgress\", \"ec2:AuthorizeSecurityGroupIngress\", \"ec2:CancelSpotInstanceRequests\", \"ec2:CreateTags\", \"ec2:CreateVolume\", \"ec2:DeleteTags\", \"ec2:DeleteVolume\", \"ec2:DescribeAvailabilityZones\", \"ec2:DescribeIamInstanceProfileAssociations\", \"ec2:DescribeInstanceStatus\", \"ec2:DescribeInstances\", \"ec2:DescribeInternetGateways\", \"ec2:DescribeNatGateways\", \"ec2:DescribeNetworkAcls\", \"ec2:DescribePrefixLists\", \"ec2:DescribeReservedInstancesOfferings\", \"ec2:DescribeRouteTables\", \"ec2:DescribeSecurityGroups\", \"ec2:DescribeSpotInstanceRequests\", \"ec2:DescribeSpotPriceHistory\", \"ec2:DescribeSubnets\", \"ec2:DescribeVolumes\", \"ec2:DescribeVpcAttribute\", \"ec2:DescribeVpcs\", \"ec2:DetachVolume\", \"ec2:DisassociateIamInstanceProfile\", \"ec2:ReplaceIamInstanceProfileAssociation\", \"ec2:RequestSpotInstances\", \"ec2:RevokeSecurityGroupEgress\", \"ec2:RevokeSecurityGroupIngress\", \"ec2:RunInstances\", \"ec2:TerminateInstances\", \"ec2:DescribeFleetHistory\", \"ec2:ModifyFleet\", \"ec2:DeleteFleets\", \"ec2:DescribeFleetInstances\","
    },
    {
        "id": 77,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "\"ec2:DescribeFleetHistory\", \"ec2:ModifyFleet\", \"ec2:DeleteFleets\", \"ec2:DescribeFleetInstances\", \"ec2:DescribeFleets\", \"ec2:CreateFleet\", \"ec2:DeleteLaunchTemplate\", \"ec2:GetLaunchTemplateData\", \"ec2:CreateLaunchTemplate\", \"ec2:DescribeLaunchTemplates\", \"ec2:DescribeLaunchTemplateVersions\", \"ec2:ModifyLaunchTemplate\", \"ec2:DeleteLaunchTemplateVersions\", \"ec2:CreateLaunchTemplateVersion\", \"ec2:AssignPrivateIpAddresses\", \"ec2:GetSpotPlacementScores\" ], \"Resource\": [ \"*\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"iam:CreateServiceLinkedRole\", \"iam:PutRolePolicy\" ], \"Resource\": \"arn:aws:iam::*:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot\", \"Condition\": { \"StringLike\": { \"iam:AWSServiceName\": \"spot.amazonaws.com\" } } } ] }"
    },
    {
        "id": 78,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "Click Review policy.  \nIn the Name field, enter a policy name.  \nClick Create policy.  \n(Optional) If you use Service Control Policies to deny certain actions at the AWS account level, ensure that sts:AssumeRole is allowlisted so Databricks can assume the cross-account role.  \nIn the role summary, copy the Role ARN.  \nOption 3: Customer-managed VPC with custom policy restrictions  \nNote  \nThe Databricks production AWS account from which Amazon Machine Images (AMI) are sourced is 601306020600. You can use this account ID to create custom access policies that restrict which AMIs can be used within your AWS account. For more information, contact your Databricks account team.  \nLog into your AWS Console as a user with administrator privileges and go to the IAM console.  \nClick the Roles tab in the sidebar.  \nIn the list of roles, click the cross-account IAM role that you created for in Step 1.  \nClick the Add permissions dropdown then Create inline policy.  \nIn the policy editor, click the JSON tab.  \nCopy and paste the following access policy.  \nReplace the following values in the policy with your own configuration values:  \nACCOUNTID \u2014 Your AWS account ID, which is a number.  \nVPCID \u2014 ID of the AWS VPC where you want to launch workspaces.  \nREGION \u2014 AWS Region name for your VPC deployment, for example us-west-2.  \nSECURITYGROUPID \u2014 ID of your AWS security group. When you add a security group restriction, you cannot reuse the cross-account IAM role or reference a credentials ID (credentials_id) for any other workspaces. For those other workspaces, you must create separate roles, policies, and credentials objects.  \nNote  \nIf you have custom requirements configured for security groups with your customer-managed vpc, contact your Databricks account team for assistance with IAM policy customizations."
    },
    {
        "id": 79,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"NonResourceBasedPermissions\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:AssignPrivateIpAddresses\", \"ec2:CancelSpotInstanceRequests\", \"ec2:DescribeAvailabilityZones\", \"ec2:DescribeIamInstanceProfileAssociations\", \"ec2:DescribeInstanceStatus\", \"ec2:DescribeInstances\", \"ec2:DescribeInternetGateways\", \"ec2:DescribeNatGateways\", \"ec2:DescribeNetworkAcls\", \"ec2:DescribePrefixLists\", \"ec2:DescribeReservedInstancesOfferings\", \"ec2:DescribeRouteTables\", \"ec2:DescribeSecurityGroups\", \"ec2:DescribeSpotInstanceRequests\", \"ec2:DescribeSpotPriceHistory\", \"ec2:DescribeSubnets\", \"ec2:DescribeVolumes\", \"ec2:DescribeVpcAttribute\", \"ec2:DescribeVpcs\", \"ec2:CreateTags\", \"ec2:DeleteTags\", \"ec2:GetSpotPlacementScores\", \"ec2:RequestSpotInstances\", \"ec2:DescribeFleetHistory\", \"ec2:ModifyFleet\", \"ec2:DeleteFleets\", \"ec2:DescribeFleetInstances\", \"ec2:DescribeFleets\", \"ec2:CreateFleet\", \"ec2:DeleteLaunchTemplate\", \"ec2:GetLaunchTemplateData\", \"ec2:CreateLaunchTemplate\", \"ec2:DescribeLaunchTemplates\", \"ec2:DescribeLaunchTemplateVersions\", \"ec2:ModifyLaunchTemplate\", \"ec2:DeleteLaunchTemplateVersions\", \"ec2:CreateLaunchTemplateVersion\" ], \"Resource\": [ \"*\" ] }, { \"Sid\":"
    },
    {
        "id": 80,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "\"ec2:DeleteLaunchTemplateVersions\", \"ec2:CreateLaunchTemplateVersion\" ], \"Resource\": [ \"*\" ] }, { \"Sid\": \"InstancePoolsSupport\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:AssociateIamInstanceProfile\", \"ec2:DisassociateIamInstanceProfile\", \"ec2:ReplaceIamInstanceProfileAssociation\" ], \"Resource\": \"arn:aws:ec2:REGION:ACCOUNTID:instance/*\", \"Condition\": { \"StringEquals\": { \"ec2:ResourceTag/Vendor\": \"Databricks\" } } }, { \"Sid\": \"AllowEc2RunInstancePerTag\", \"Effect\": \"Allow\", \"Action\": \"ec2:RunInstances\", \"Resource\": [ \"arn:aws:ec2:REGION:ACCOUNTID:volume/*\", \"arn:aws:ec2:REGION:ACCOUNTID:instance/*\" ], \"Condition\": { \"StringEquals\": { \"aws:RequestTag/Vendor\": \"Databricks\" } } }, { \"Sid\": \"AllowEc2RunInstanceImagePerTag\", \"Effect\": \"Allow\", \"Action\": \"ec2:RunInstances\", \"Resource\": [ \"arn:aws:ec2:REGION:ACCOUNTID:image/*\" ], \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/Vendor\": \"Databricks\" } } }, { \"Sid\": \"AllowEc2RunInstancePerVPCid\", \"Effect\": \"Allow\", \"Action\": \"ec2:RunInstances\", \"Resource\": [ \"arn:aws:ec2:REGION:ACCOUNTID:network-interface/*\", \"arn:aws:ec2:REGION:ACCOUNTID:subnet/*\", \"arn:aws:ec2:REGION:ACCOUNTID:security-group/*\" ], \"Condition\": { \"StringEquals\": {"
    },
    {
        "id": 81,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "\"arn:aws:ec2:REGION:ACCOUNTID:security-group/*\" ], \"Condition\": { \"StringEquals\": { \"ec2:vpc\": \"arn:aws:ec2:REGION:ACCOUNTID:vpc/VPCID\" } } }, { \"Sid\": \"AllowEc2RunInstanceOtherResources\", \"Effect\": \"Allow\", \"Action\": \"ec2:RunInstances\", \"NotResource\": [ \"arn:aws:ec2:REGION:ACCOUNTID:image/*\", \"arn:aws:ec2:REGION:ACCOUNTID:network-interface/*\", \"arn:aws:ec2:REGION:ACCOUNTID:subnet/*\", \"arn:aws:ec2:REGION:ACCOUNTID:security-group/*\", \"arn:aws:ec2:REGION:ACCOUNTID:volume/*\", \"arn:aws:ec2:REGION:ACCOUNTID:instance/*\" ] }, { \"Sid\": \"EC2TerminateInstancesTag\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:TerminateInstances\" ], \"Resource\": [ \"arn:aws:ec2:REGION:ACCOUNTID:instance/*\" ], \"Condition\": { \"StringEquals\": { \"ec2:ResourceTag/Vendor\": \"Databricks\" } } }, { \"Sid\": \"EC2AttachDetachVolumeTag\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:AttachVolume\", \"ec2:DetachVolume\" ], \"Resource\": [ \"arn:aws:ec2:REGION:ACCOUNTID:instance/*\", \"arn:aws:ec2:REGION:ACCOUNTID:volume/*\" ], \"Condition\": { \"StringEquals\": { \"ec2:ResourceTag/Vendor\": \"Databricks\" } } }, { \"Sid\": \"EC2CreateVolumeByTag\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:CreateVolume\" ], \"Resource\": ["
    },
    {
        "id": 82,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "} } }, { \"Sid\": \"EC2CreateVolumeByTag\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:CreateVolume\" ], \"Resource\": [ \"arn:aws:ec2:REGION:ACCOUNTID:volume/*\" ], \"Condition\": { \"StringEquals\": { \"aws:RequestTag/Vendor\": \"Databricks\" } } }, { \"Sid\": \"EC2DeleteVolumeByTag\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:DeleteVolume\" ], \"Resource\": [ \"arn:aws:ec2:REGION:ACCOUNTID:volume/*\" ], \"Condition\": { \"StringEquals\": { \"ec2:ResourceTag/Vendor\": \"Databricks\" } } }, { \"Effect\": \"Allow\", \"Action\": [ \"iam:CreateServiceLinkedRole\", \"iam:PutRolePolicy\" ], \"Resource\": \"arn:aws:iam::*:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot\", \"Condition\": { \"StringLike\": { \"iam:AWSServiceName\": \"spot.amazonaws.com\" } } }, { \"Sid\": \"VpcNonresourceSpecificActions\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:AuthorizeSecurityGroupEgress\", \"ec2:AuthorizeSecurityGroupIngress\", \"ec2:RevokeSecurityGroupEgress\", \"ec2:RevokeSecurityGroupIngress\" ], \"Resource\": \"arn:aws:ec2:REGION:ACCOUNTID:security-group/SECURITYGROUPID\", \"Condition\": { \"StringEquals\": { \"ec2:vpc\": \"arn:aws:ec2:REGION:ACCOUNTID:vpc/VPCID\" } } } ] }"
    },
    {
        "id": 83,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "Click Review policy.  \nIn the Name field, enter a policy name.  \nClick Create policy.  \n(Optional) If you use Service Control Policies to deny certain actions at the AWS account level, ensure that sts:AssumeRole is allowlisted so Databricks can assume the cross-account role.  \nIn the role summary, copy the Role ARN."
    },
    {
        "id": 84,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "Step 3: Create a credential configuration in Databricks\nStep 3: Create a credential configuration in Databricks\nWhen you have created the IAM role, you can tell Databricks about it by creating a credential configuration that uses that role\u2019s ID.  \nTo create a credential configuration:  \nIn the account console, click Cloud resources.  \nClick Credential configuration.  \nClick Add credential configuration.  \nIn the Credential configuration name field, enter a human-readable name for your new credential configuration.  \nIn the Role ARN field, enter your role\u2019s ARN.  \nClick Add.  \nValidation is not run during credential configuration creation. Some errors are detected only when you use the configuration to create a new workspace. These errors can include an invalid ARN or incorrect permissions for the role, among others.\n\nDelete a credential configuration"
    },
    {
        "id": 85,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/credentials.html",
        "content": "Delete a credential configuration\nCredential configurations cannot be edited after creation. If the configuration has incorrect data or if you no longer need it, delete the credential configuration:  \nIn the account console, click Cloud resources.  \nClick Credential configuration.  \nOn the credential configuration row, click the Actions menu icon, and select Delete.  \nYou can also click the credential configuration name and click Delete on the pop-up dialog.  \nOn the confirmation dialog, click Confirm Delete."
    },
    {
        "id": 86,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/single-sign-on/one-login.html",
        "content": "SSO to Databricks with OneLogin  \nThis article shows how to configure OneLogin as the identity provider for single sign-on (SSO) in your Databricks account. OneLogin supports both OpenID Connect (OIDC) and SAML 2.0. To sync users and groups from OneLogin, see Sync users and groups from your identity provider.  \nWarning  \nTo prevent getting locked out of Databricks during single sign-on testing, Databricks recommends keeping the account console open in a different browser window. You can also configure emergency access with security keys to prevent lock out. See Emergency access for SSO.  \nEnable account single sign-on authentication using OIDC"
    },
    {
        "id": 87,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/single-sign-on/one-login.html",
        "content": "Enable account single sign-on authentication using OIDC\nAs an account owner or account admin, log in to the account console and click the Settings icon in the sidebar.  \nClick the Single sign-on tab.  \nNext to Authentication, click Manage.  \nChoose Single sign-on with my identity provider.  \nClick Continue.  \nUnder Identity protocol, select OpenID Connect.  \nOn the Single sign-on tab, make note of the Databricks Redirect URI value.  \nIn a new browser tab, log in to OneLogin.  \nClick Administration.  \nClick Applications.  \nClick Add App.  \nSearch for OpenId Connect and select the OpenId Connect (OIDC) app.  \nEnter a name and click Save.  \nIn the Configuration tab, Databricks Redirect URI from step 4. You can choose to configure the other settings or you can leave them to their default values.  \nIn the SSO tab, copy the copy the client ID, client secret, and issuer URL values.  \nClient ID is the unique identifier for the Databricks application you created in OneLogin.  \nClient secret is a secret or password generated for the Databricks application that you created. It is used to authorize Databricks with your identity provider.  \nOpenID issuer URL is the URL at which OneLogin\u2019s OpenID Configuration Document can be found. That OpenID Configuration Document must found be in {issuer-url}/.well-known/openid-configuration.  \nReturn to the Databricks account console Single sign-on tab and enter values you copied from the identity provider application to the Client ID, Client secret, and OpenID issuer URL fields.  \nOptionally, enter a Username claim if you want to use a claim other than email as users\u2019 Databricks usernames. See your identity provider\u2019s documentation for specific information on claim values.  \nClick Save.  \nClick Test SSO to validate that your SSO configuration is working properly.  \nClick Enable SSO to enable single sign-on for your account.  \nTest account console login with SSO."
    },
    {
        "id": 88,
        "url": "https://docs.databricks.com/en/admin/account-settings-e2/single-sign-on/one-login.html",
        "content": "Enable account single sign-on authentication using SAML\nFollow these steps to create a OneLogin SAML application for use with Databricks account console.  \nAs an account owner or account admin, log in to the account console and click the Settings icon in the sidebar.  \nClick the Single sign-on tab.  \nNext to Authentication, click Manage.  \nChoose Single sign-on with my identity provider.  \nClick Continue.  \nUnder Identity protocol, select SAML 2.0.  \nOn the Single sign-on tab, make note of the Databricks Redirect URI value.  \nIn a new browser tab, log in to OneLogin.  \nClick Administration.  \nClick Applications.  \nClick Add App.  \nSearch for SAML Custom Connector (Advanced) and click the result by OneLogin, Inc.  \nSet Display Name to Databricks.  \nClick Save. The application\u2019s Info tab loads.  \nClick Configuration.  \nIn Gather required information, set each of the following fields to the Databricks SAML URL:  \nAudience  \nRecipient  \nACS (Consumer) URL Validator  \nACS (Consumer) URL  \nSingle Logout URL  \nLogin URL  \nSet SAML signature element to Both.  \nClick Parameters.  \nSet Credentials are to Configured by admins and shared by all users.  \nClick Email. Set the value to email and enable Include in SAML Assertion.  \nClick the SSO tab.  \nCopy the following values:  \nx.509 certificate  \nIssuer URL  \nSAML 2.0 endpoint (HTTP)  \nVerify that SAML signature element is set to Response or Both.  \nVerify that Encrypt assertion is disabled.  \nConfigure Databricks in the Databricks account console SSO page.  \nSet the SSO type drop-down to SAML 2.0.  \nSet Single Sign-On URL to the OneLogin SAML 2.0 endpoint.  \nSet Identity Provider Entity ID to the OneLogin Issuer URL.  \nSet x.509 Certificate to the OneLogin x.509 certificate, including the markers for the beginning and ending of the certificate.  \nClick Save.  \nClick Test SSO to validate that your SSO configuration is working properly.  \nClick Enable SSO to enable single sign-on for your account.  \nTest account console login with SSO."
    },
    {
        "id": 89,
        "url": "https://docs.databricks.com/en/archive/init-scripts/legacy-cluster-named.html",
        "content": "Cluster-named init scripts (legacy)  \nImportant  \nThis documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See What are init scripts?.  \nWarning  \nLegacy global init scripts and cluster-named init scripts are end-of-life and can no longer be used.  \nLegacy cluster-named init scripts run on a cluster with the same name as the script. Cluster-named init scripts are best-effort (silently ignore failures), and attempt to continue the cluster launch process.  \nDatabricks recommends migrating all existing cluster-named init scripts to cluster-scoped init scripts. See Use cluster-scoped init scripts. Cluster-named init scripts are not visible in the UI but can be found at dbfs:/databricks/init/<cluster-name>/, where <cluster-name> is the name of the folder that contains the init scripts. They must be deleted or moved from that folder to complete a migration.  \nDisable legacy cluster-named init scripts for a workspace"
    },
    {
        "id": 90,
        "url": "https://docs.databricks.com/en/archive/init-scripts/legacy-cluster-named.html",
        "content": "Disable legacy cluster-named init scripts for a workspace\nUse the workspace-conf API to disable legacy cluster-named init scripts for a workspace. See Enable/disable features.  \nIn a JSON request body, specify enableDeprecatedClusterNamedInitScripts to false, as in the following example:  \ncurl -X PATCH -n \\ https://<databricks-instance>/api/2.0/workspace-conf \\ -d '{ \"enableDeprecatedClusterNamedInitScripts\": \"false\" }'  \nExample response:  \n{ \"enableDeprecatedClusterNamedInitScripts\": \"false\" }"
    },
    {
        "id": 91,
        "url": "https://docs.databricks.com/en/archive/security/hipaa-legacy-cluster.html",
        "content": "Create and verify a cluster for legacy HIPAA support  \nImportant  \nThis article applies to the legacy HIPAA compliance features. For an overview of HIPAA compliance features on the E2 platform, instead see the article HIPAA compliance features.  \nIf your workspace uses the legacy HIPAA support, use the following instructions to create and verify a cluster for HIPAA compliance features to process PHI data.  \nCreate a cluster  \nFollow the instructions in Compute configuration reference. As part of the configuration step you must choose a Databricks runtime version.  \nWarning  \nDatabricks Runtime for Machine Learning includes high-performance distributed machine learning packages that use MPI (Message Passing Interface) and other low-level communication protocols. Because these protocols do not natively support encryption over the wire, these ML packages can potentially send unencrypted sensitive data across the network. These packages do not change data encryption over the wire if your workflow does not depend on them.  \nMessages sent across the network by these ML packages are typically either ML model parameters or summary statistics about training data. It is therefore not typically expected that sensitive data, such as protected health information, would be sent over the wire unencrypted. However, it is possible that certain configurations or uses of these packages (such as specific model designs) could result in messages being sent across the network that contain such information.  \nThe following packages are affected:  \nXGBoost  \nHorovod, HorovodEstimator, and HorovodRunner  \nDistributed TensorFlow  \nConfigure the cluster with an EBS volume (Legacy HIPAA support)  \nProvision an EBS volume, as Databricks EBS volumes are encrypted while the default local storage is not.  \nVerify that encryption is enabled.  \nCreate a notebook in the workspace and attach the notebook to the cluster that was created in the previous step.  \nRun the following command in the notebook:  \n%scala spark.conf.get(\"spark.ssl.enabled\")  \nIf the returned value is true, you have successfully created a cluster with encryption turned on. If not, contact help@databricks.com.  \nImportant  \nspark-submit is not supported on HIPAA-compliant clusters."
    },
    {
        "id": 92,
        "url": "https://docs.databricks.com/en/admin/system-tables/index.html",
        "content": "Monitor usage with system tables  \nPreview  \nThis feature is in Public Preview. There are currently no charges to use this feature. In the future, some of this usage might incur a charge.  \nThis article explains the concept of system tables in Databricks and highlights resources you can use to get the most out of your system tables data.  \nWhat are system tables?\nWhat are system tables?\nSystem tables are a Databricks-hosted analytical store of your account\u2019s operational data found in the system catalog. System tables can be used for historical observability across your account.  \nNote  \nFor documentation on system.information_schema, see Information schema.\n\nRequirements\nRequirements\nTo access system tables, your workspace must be enabled for Unity Catalog. For more information, see Enable system table schemas.  \nSystem tables are not available in AWS GovCloud regions.\n\nWhich system tables are available?"
    },
    {
        "id": 93,
        "url": "https://docs.databricks.com/en/admin/system-tables/index.html",
        "content": "Which system tables are available?\nCurrently, Databricks hosts the following system tables:  \nTable  \nDescription  \nLocation  \nSupports streaming  \nRetention  \nInclude global or regional data  \nAudit logs  \nIncludes records for all audit events from workspaces in your region. For a list of available audit events, see Audit log reference.  \nsystem.access.audit  \nYes  \n365 days  \nRegional for workspace-level events. Global for account-level events.  \nTable lineage  \nIncludes a record for each read or write event on a Unity Catalog table or path.  \nsystem.access.table_lineage  \nYes  \n365 days  \nRegional  \nColumn lineage  \nIncludes a record for each read or write event on a Unity Catalog column (but does not include events that do not have a source).  \nsystem.access.column_lineage  \nYes  \n365 days  \nRegional  \nBillable usage  \nIncludes records for all billable usage across your account. Each usage record is an hourly aggregate of a resource\u2019s billable usage.  \nsystem.billing.usage  \nYes  \n365 days  \nGlobal  \nPricing  \nA historical log of SKU pricing. A record gets added each time there is a change to a SKU price.  \nsystem.billing.list_prices  \nNo  \nN/A  \nGlobal  \nClusters  \nA slow-changing dimension table that contains the full history of compute configurations over time for any cluster.  \nsystem.compute.clusters  \nYes  \nNone  \nRegional  \nNode timeline  \nCaptures the utilization metrics of your all-purpose and jobs compute resources.  \nsystem.compute.node_timeline  \nYes  \n30 days  \nRegional  \nNode types  \nCaptures the currently available node types with their basic hardware information.  \nsystem.compute.node_types  \nNo  \nN/A  \nRegional  \nSQL warehouse events  \nCaptures events related to SQL warehouses. For example, starting, stopping, running, scaling up and down.  \nsystem.compute.warehouse_events  \nYes  \n365 days  \nRegional  \nJobs  \nTracks all jobs created in the account.  \nsystem.lakeflow.jobs  \nYes  \n365 days  \nRegional  \nJob tasks  \nTracks all job tasks that run in the account.  \nsystem.lakeflow.job_tasks  \nYes  \n365 days  \nRegional  \nJob run timeline  \nTracks the start and end times of job runs.  \nsystem.lakeflow.  \njob_run_timeline  \nYes  \n365 days  \nRegional  \nJob task timeline  \nTracks the start and end times and compute resources used for job task runs.  \nsystem.lakeflow.  \njob_task_run_timeline  \nYes  \n365 days  \nRegional  \nMarketplace funnel events  \nIncludes consumer impression and funnel data for your listings."
    },
    {
        "id": 94,
        "url": "https://docs.databricks.com/en/admin/system-tables/index.html",
        "content": "Tracks the start and end times and compute resources used for job task runs.  \nsystem.lakeflow.  \njob_task_run_timeline  \nYes  \n365 days  \nRegional  \nMarketplace funnel events  \nIncludes consumer impression and funnel data for your listings.  \nsystem.marketplace.listing_  \nfunnel_events  \nYes  \n365 days  \nRegional  \nMarketplace listing access  \nIncludes consumer info for completed request data or get data events on your listings.  \nsystem.marketplace.listing_  \naccess_events  \nYes  \n365 days  \nRegional  \nPredictive optimization  \nTracks the operation history of the predictive optimization feature.  \nsystem.storage.predictive_  \noptimization_operations_history  \nNo  \n180 days  \nRegional  \nDatabricks Assistant events  \nTracks user messages sent to the Databricks Assistant.  \nsystem.access.assistant_events  \nNo  \n365 days  \nRegional  \nQuery history  \nCaptures records for all queries run on SQL warehouses.  \nsystem.query.history  \nYes  \n90 days  \nRegional  \nClean room events  \nCaptures events related to clean rooms.  \nsystem.access.clean_room_events  \nYes  \n365 days  \nRegional  \nNote  \nYou may see other system tables in your account besides the ones listed above. Those tables are in Private Preview currently and are empty by default. If you are interested in using any of these tables, please reach out to your Databricks account team."
    },
    {
        "id": 95,
        "url": "https://docs.databricks.com/en/admin/system-tables/index.html",
        "content": "Enable system table schemas\nSince system tables are governed by Unity Catalog, you need to have at least one Unity Catalog-enabled workspace in your account to enable and access system tables. System tables include data from all workspaces in your account but they can only be accessed from a Unity Catalog-enabled workspace.  \nSystem tables are enabled at the schema level. If you enable a system schema, you enable all the tables within that schema. When new schemas are released, an account admin needs to manually enable the schema.  \nSystem tables must be enabled by an account admin. You can enable system tables using the SystemSchemas API.  \nNote  \nThe billing schema is enabled by default. Other schemas must be enabled manually.  \nList available system schemas  \nUse the following curl command to list available system schemas:  \ncurl -v -X GET -H \"Authorization: Bearer <PAT Token>\" \"https://<workspace>.cloud.databricks.com/api/2.0/unity-catalog/metastores/<metastore-id>/systemschemas\"  \nThe following is an example output of the GET command:  \n{\"schemas\":[{\"schema\":\"access\",\"state\":\"<AVAILABLE OR EnableCompleted>\"},{\"schema\":\"billing\",\"state\":\"<AVAILABLE OR EnableCompleted>\"},{\"schema\":\"information_schema\",\"state\":\"<AVAILABLE OR EnableCompleted>\"}]}  \nstate: AVAILABLE: The system schema is available but has not yet been enabled.  \nstate: EnableCompleted: You have enabled the system schema and it is visible in Catalog Explorer.  \nEnable a system schema  \nUse the following curl command to enable a system schema:  \ncurl -v -X PUT -H \"Authorization: Bearer <PAT Token>\" \"https://<workspace>.databricks.com/api/2.0/unity-catalog/metastores/<metastore-id>/systemschemas/<SCHEMA_NAME>\"  \nIf the system schema is enabled successfully, result code 200 is returned."
    },
    {
        "id": 96,
        "url": "https://docs.databricks.com/en/admin/system-tables/index.html",
        "content": "If the system schema is enabled successfully, result code 200 is returned.  \nIf you attempt to re-enable a system schema, the following is returned: \"error_code\":\"SCHEMA_ALREADY_EXISTS\",\"message\":\"Schema <schema-name> already exists\".  \nDisable a system schema  \nUse the following curl command to disable a system schema:  \ncurl -v -X DELETE -H \"Authorization: Bearer <PAT Token>\" \"https://<workspace>.databricks.com/api/2.0/unity-catalog/metastores/<metastore-id>/systemschemas/<SCHEMA_NAME>\""
    },
    {
        "id": 97,
        "url": "https://docs.databricks.com/en/admin/system-tables/index.html",
        "content": "Grant access to system tables\nGrant access to system tables\nSystem table access is governed by Unity Catalog. By default, no users have access to system tables. To grant access, a metastore admin or other privileged user must grant USE and SELECT permissions on the system schemas. See Manage privileges in Unity Catalog.  \nSystem tables are read-only and cannot be modified.  \nNote  \nIf your account was created after November 8, 2023, you might not have a metastore admin by default. For more information, see Set up and manage Unity Catalog.\n\nDo system tables contain data for all workspaces in your account?\nDo system tables contain data for all workspaces in your account?\nThe audit log and lineage tables contain operational data for all workspaces in your account deployed within the same cloud region. The billing system table (system.billing.usage) contains data for all workspaces in your account, no matter what region they are deployed in.  \nEven though system tables can only be accessed through a Unity Catalog workspace, the tables also include operational data for non-Unity Catalog workspaces in your account.\n\nWhere are the system tables located?"
    },
    {
        "id": 98,
        "url": "https://docs.databricks.com/en/admin/system-tables/index.html",
        "content": "Where are the system tables located?\nThe system tables in your account are located in a catalog called system, which is included in every Unity Catalog metastore. In the system catalog you\u2019ll see schemas such as access and billing that contain the system tables.  \nNote  \nDuring the system tables Public Preview, Databricks will retain all your system tables data.\n\nConsiderations for streaming system tables"
    },
    {
        "id": 99,
        "url": "https://docs.databricks.com/en/admin/system-tables/index.html",
        "content": "Considerations for streaming system tables\nDatabricks uses Delta Sharing to share system table data with customers. Be aware of the following considerations when streaming with Delta Sharing:  \nIf you are using streaming with system tables, set the skipChangeCommits option to true. This ensures the streaming job is not disrupted from deletes in the system tables. See Ignore updates and deletes.  \nTrigger.AvailableNow is not supported with Delta Sharing streaming. It will be converted to Trigger.Once.  \nIf you use a trigger in your streaming job and find it isn\u2019t catching up to the latest system table version, Databricks recommends increasing the scheduled frequency of the job.  \nRead incremental changes from streaming system tables  \nspark.readStream.option(\"skipChangeCommits\", \"true\").table(\"system.billing.usage\")\n\nKnown issues"
    },
    {
        "id": 100,
        "url": "https://docs.databricks.com/en/admin/system-tables/index.html",
        "content": "Known issues\nCurrently no support for real-time monitoring. Data is updated throughout the day. If you don\u2019t see a log for a recent event, check back later.  \nIf your workspace uses a customer-managed VPC, you might be denied access to the S3 bucket where the logs are stored. If so, you need to update your VPC endpoint policy to allow access to the S3 bucket where your region\u2019s system tables data is stored. For a list of regional bucket names, see the System tables bucket column in Storage bucket addresses table.  \nThe system schemas system.operational_data and system.lineage are deprecated and will contain empty tables."
    },
    {
        "id": 101,
        "url": "https://docs.databricks.com/en/archive/machine-learning/ai-generate-text-example.html",
        "content": "Analyze customer reviews with ai_generate_text() and OpenAI  \nPreview  \nThis feature is in Public Preview.  \nWarning  \nThe AI function, ai_generate_text() is deprecated. Databricks recommends using ai_query with external models.  \nThis article illustrates how to use the built-in Databricks SQL function, ai_generate_text() to examine customer reviews and determine if a response needs to be generated. See AI Functions on Databricks for more detail about the function.  \nThe dataset and commands in this guide are from the Databricks demo Action Customer Reviews at Scale with Databricks SQL AI Functions. The demo uses fake data generated by OpenAI that mimics customer reviews for grocery products submitted to an e-commerce website.  \nThis example steps you through:  \nBreaking down free-form customer review text into its constituent entities.  \nFor each entity, determining sentiment and whether a response is required back to the customer.  \nGenerating a response mentioning alternative products that may satisfy the customer.  \nPrerequisites"
    },
    {
        "id": 102,
        "url": "https://docs.databricks.com/en/archive/machine-learning/ai-generate-text-example.html",
        "content": "Prerequisites\nIf you want to run the commands in this guide, you need a dataset to run it on. You can create a fake dataset in the Generate Fake data with AI Functions notebook.  \nThe SQL commands in this guide must be run in the Databricks SQL query editor. They cannot be run directly in a Databricks notebook using interactive clusters.  \nThe ai_generate_text() function is only available in public preview on pro or serverless SQL warehouses.  \nTo enroll in the Public Preview, please populate and submit the AI Functions Public Preview enrollment form.  \nAn Azure OpenAI key.  \nStore the key in Databricks secrets. In this example you store the API key in scope tokens and secret azure-openai. See Set up and considerations for ai_generate_text().\n\nPrompt design"
    },
    {
        "id": 103,
        "url": "https://docs.databricks.com/en/archive/machine-learning/ai-generate-text-example.html",
        "content": "Prompt design\nThe keys to getting useful results back from a GPT model are:  \nAsking it a well-formed question.  \nBeing specific about the type of answer that you are expecting.  \nIn order to get results in a form that you can easily store in a table, you can ask the model to return the result in a string that reflects JSON representation, and specify the expected schema.  \nThe following is the example prompt for this scenario:  \nA customer left a review. Follow up with anyone who appears unhappy.  \nExtract all entities mentioned. For each entity:  \nClassify sentiment as [\u201cPOSITIVE\u201d,\u201dNEUTRAL\u201d,\u201dNEGATIVE\u201d]  \nWhether customer requires a follow-up: Y or N  \nReason for requiring followup  \nReturn JSON ONLY. No other text outside the JSON.  \nJSON format:  \n{ \"entities\": [{ \"entity_name\": \"entity_name\", \"entity_type\": \"entity_type\", \"entity_sentiment\": \"entity_sentiment\", \"followup\": \"Y or N for follow up\", \"followup_reason\": \"reason for followup\" }] }  \nReview:  \n<\u2019insertreview_text_here\u2019>_"
    },
    {
        "id": 104,
        "url": "https://docs.databricks.com/en/archive/machine-learning/ai-generate-text-example.html",
        "content": "Create SQL functions\nDatabricks recommends decomposing your questions into granular SQL functions so that they can be reused for other scenarios within your organization.  \nThroughout this section, you create SQL functions in order to abstract away the details of the ai_generate_text() call from the end users, and use those functions as your interface for interacting with Azure OpenAI.  \nHandle calls to Azure OpenAI  \nThe following wrapper function, prompt_handler(), handles all your calls to Azure OpenAI. The Azure OpenAI API key is stored in a Databricks secret, and you can reference it with the secret() function. You can also pass it the Azure OpenAI resource name (resourceName) and the model\u2019s deployment name (deploymentName).  \nCREATE OR REPLACE FUNCTION PROMPT_HANDLER(prompt STRING) RETURNS STRING RETURN AI_GENERATE_TEXT(prompt, \"azure_openai/gpt-35-turbo\", \"apiKey\", SECRET(\"tokens\", \"azure-openai\"), \"temperature\", CAST(0.0 AS DOUBLE), \"deploymentName\", \"llmbricks\", \"apiVersion\", \"2023-03-15-preview\", \"resourceName\", \"llmbricks\" );  \nAnalyze customer review data  \nThe annotate_review() function annotates your review with entities, entity sentiments, and whether a follow-up is required and why. Notice the prompt returns a well-formed json representation, so you can instruct the function to return a struct type for easier querying downstream, such as inserting it into a Delta table."
    },
    {
        "id": 105,
        "url": "https://docs.databricks.com/en/archive/machine-learning/ai-generate-text-example.html",
        "content": "CREATE OR REPLACE FUNCTION ANNOTATE_REVIEW(review STRING) RETURNS STRUCT<entities: ARRAY<STRUCT<entity_name: STRING, entity_type: STRING, entity_sentiment: STRING, followup: STRING, followup_reason: STRING>>> RETURN FROM_JSON( PROMPT_HANDLER(CONCAT( 'A customer left a review. Follow up with anyone who appears unhappy. Extract all entities mentioned. For each entity: - classify sentiment as [\"POSITIVE\",\"NEUTRAL\",\"NEGATIVE\"] - whether customer requires a follow-up: Y or N - reason for requiring followup Return JSON ONLY. No other text outside the JSON. JSON format: { entities: [{ \"entity_name\": <entity name>, \"entity_type\": <entity type>, \"entity_sentiment\": <entity sentiment>, \"followup\": <Y or N for follow up>, \"followup_reason\": <reason for followup> }] } Review: ', review)), \"STRUCT<entities: ARRAY<STRUCT<entity_name: STRING, entity_type: STRING, entity_sentiment: STRING, followup: STRING, followup_reason: STRING>>>\" );  \nYou can pass in data from the customer reviews dataset to see how the annotate_review() function classifies freeform customer reviews.  \nSELECT review_body, ANNOTATE_REVIEW(review_body) AS review_annotated FROM dbdemos.openai_demo.fake_reviews WHERE product_category = \"Grocery\" LIMIT 3;  \nGenerate responses with recommendations  \nAfter reviewing the customer responses, you can use the generate_response() function to generate a response to a customer based on their complaint and include recommendations for alternative products to try.  \nCREATE OR REPLACE FUNCTION GENERATE_RESPONSE(product STRING, entity STRING, reason STRING) RETURNS STRING RETURN PROMPT_HANDLER( CONCAT(\"What alternative products can you recommend for \", product, \" when a customer had a complaint about \", entity, \" because \", reason, \"Give me a response in the tone of an empathetic message back to the customer; only provide the body\") );  \nThe following generates a sample message response for a customer\u2019s review on Country Choice Snacking Cookies."
    },
    {
        "id": 106,
        "url": "https://docs.databricks.com/en/archive/machine-learning/ai-generate-text-example.html",
        "content": "The following generates a sample message response for a customer\u2019s review on Country Choice Snacking Cookies.  \nSELECT GENERATE_RESPONSE(\"Country Choice Snacking Cookies\", \"cookies\", \"Quality issue\") AS customer_response"
    },
    {
        "id": 107,
        "url": "https://docs.databricks.com/en/archive/machine-learning/ai-generate-text-example.html",
        "content": "Adhoc queries\nAdhoc queries\nYou can also create ad-hoc queries using your newly created prompt_handler() function.  \nFor example, you might be interested in understanding whether a review discusses beverages.  \nSELECT review_id, PROMPT_HANDLER( CONCAT( \"Does this review discuss beverages? Answer Y or N only, no explanations or notes. Review: \", review_body) ) AS discusses_beverages, review_body FROM dbdemos.openai_demo.fake_reviews WHERE review_id IN (\"R9LEFDWWXPDEY\", \"R27UON10EV9FSV\", \"R299ZTEFIAHRQD\") ORDER BY discusses_beverages DESC;"
    },
    {
        "id": 108,
        "url": "https://docs.databricks.com/en/archive/admin-guide/account.html",
        "content": "Manage your subscription (legacy)  \nImportant  \nThis documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. To view current admin documentation, see Manage your Databricks account.  \nNote  \nThis article describes how to manage your subscription using the legacy account console, which differs from the new account console for E2 accounts. Legacy workspaces will be retired on December 31st, 2023. For more information, see End-of-life for legacy workspaces.  \nThe Databricks account owner is the only user who can manage your Databricks subscription.  \nManage billing\nManage billing\nAs the Databricks account owner, log in to the account console.  \nClick the Billing Details tab to view and configure billing details for your account.\n\nUpgrade your subscription"
    },
    {
        "id": 109,
        "url": "https://docs.databricks.com/en/archive/admin-guide/account.html",
        "content": "Upgrade your subscription\nDatabricks has multiple pricing plans. If your current plan is no longer meeting all of your organization\u2019s needs, you can upgrade to a higher-tier plan.  \nNote  \nTo upgrade from Databricks Community Edition to the full Databricks platform, sign up for the Databricks Free Trial or contact a Databricks representative. For more information, see Get started: Account and workspace setup.  \nTo upgrade to another plan:  \nAs the Databricks account owner, log in to the account console.  \nClick the Databricks Plans tab.  \nYour current plan is indicated on the page. To upgrade to a new plan, click the tile for that plan. If you are upgrading from Standard to Premium, click the Change Plan button and follow the prompts to confirm your selection. If you are upgrading to an Enterprise or Dedicated plan, click the Contact Us button; a Databricks representative will be in touch with you shortly.  \nFor more information, see the plans and pricing page.\n\nCancel your Databricks subscription"
    },
    {
        "id": 110,
        "url": "https://docs.databricks.com/en/archive/admin-guide/account.html",
        "content": "Cancel your Databricks subscription\nCanceling a Databricks subscription deletes all notebooks, libraries, users, and other objects from your Databricks workspace.  \nTo cancel your Databricks subscription:  \nAs the Databricks account owner, log in to the account console.  \nClick the Deploy Databricks tab.  \nClick the Cancel Databricks Subscription button.  \nFill in at least one item in the Exit Survey.  \nClick Cancel Subscription.\n\nCancel your Community Edition subscription\nCancel your Community Edition subscription\nCanceling a Community Edition subscription deletes all notebooks, libraries, users, and other objects from your Databricks Community Edition workspace.  \nTo cancel your Community Edition subscription:  \nAs the Databricks account owner, log in to the account console.  \nClick the Delete Account tab.  \nClick the Cancel Community Edition button.  \nClick Confirm.\n\nDelete your Databricks account"
    },
    {
        "id": 111,
        "url": "https://docs.databricks.com/en/archive/admin-guide/account.html",
        "content": "Delete your Databricks account\nDeleting an account deletes the account owner and all history associated with the management of your organization\u2019s account.  \nBefore you delete a Databricks account, you must first cancel your Databricks and Community Edition subscriptions.  \nAs the Databricks account owner, log in to the account console.  \nClick the Delete Account tab. This tab is grayed out if you have an active Databricks subscription.  \nClick the Delete Databricks Account button.  \nClick Delete."
    },
    {
        "id": 112,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-federation.html",
        "content": "Access S3 with IAM credential passthrough with SAML 2.0 federation (legacy)  \nImportant  \nThis documentation has been retired and might not be updated.  \nCredential passthrough is deprecated starting with Databricks Runtime 15.0 and will be removed in future Databricks Runtime versions. Databricks recommends that you upgrade to Unity Catalog. Unity Catalog simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See What is Unity Catalog?.  \nAWS supports SAML 2.0 identity federation to allow for single-sign on to AWS Management Console and AWS APIs. Databricks workspaces that are configured with single sign-on can use AWS IAM federation to maintain the mapping of users to IAM roles within their identity provider (IdP) rather than within Databricks using SCIM. This allows you to centralize data access within your IdP and have those entitlements pass directly to Databricks clusters.  \nNote  \nIAM credential passthrough with SAML 2.0 federation can only be configured when unified login is disabled. Databricks recommends that you upgrade to Unity Catalog, see What is Unity Catalog?. If your account was created after June 21, 2023 and you require IAM credential passthrough with SAML 2.0 federation, contact your Databricks account team.  \nThe following diagram illustrates the federation workflow:  \nConfigure a trust relationship between your IdP and AWS accounts in order for the IdP to control which roles users can assume.  \nUsers login to Databricks via SAML SSO, the entitlement to the roles are passed by the IdP.  \nDatabricks calls the AWS Security Token Service (STS) and assumes the roles for the user by passing the SAML response and getting temporary tokens.  \nWhen a user accesses S3 from a Databricks cluster, Databricks runtime uses the temporary tokens for the user to perform the access automatically and securely.  \nNote  \nFederation for IAM credential passthrough always maps roles to users in SAML when the Allow IAM role entitlement auto sync is enabled. It will overwrite any previous roles set via the SCIM API.  \nRequirements"
    },
    {
        "id": 113,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-federation.html",
        "content": "Requirements\nPremium plan or above.  \nSAML single sign-on configured in your Databricks workspace.  \nAWS administrator access to:  \nIAM roles and policies in the AWS account of the Databricks deployment.  \nAWS account of the S3 bucket.  \nIdentity provider (IdP) administrator to configure your IdP to pass AWS roles to Databricks.  \nA Databricks workspace admin to include AWS roles in the SAML assertion.\n\nStep 1: Get the Databricks SAML URL\nStep 1: Get the Databricks SAML URL\nGo to the settings page.  \nClick the Single Sign-On tab.  \nCopy the Databricks SAML URL.\n\nStep 2: Download identity provider metadata\nStep 2: Download identity provider metadata\nNote  \nThe steps within the identity provider console vary slightly for each identity provider. See Integrating Third-Party SAML Solution Providers with AWS for examples with your identity provider.  \nIn your identity provider admin console, find your Databricks application for single sign-on.  \nDownload the SAML metadata.\n\nStep 3: Configure the identity provider"
    },
    {
        "id": 114,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-federation.html",
        "content": "Step 3: Configure the identity provider\nIn the AWS console, go to the IAM service.  \nClick the Identity Providers tab in the sidebar.  \nClick Create Provider.  \nIn Provider Type, select SAML.  \nIn Provider Name, enter a name.  \nIn Metadata Document, click Choose File and navigate to the file containing the metadata document you downloaded above.  \nClick Next Step and then Create.\n\nStep 4: Configure the IAM role for federation"
    },
    {
        "id": 115,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-federation.html",
        "content": "Step 4: Configure the IAM role for federation\nNote  \nOnly roles used for data access should be used for federation with Databricks. We do not recommend allowing roles normally used for AWS console access as they may have more privileges than necessary.  \nIn the AWS console, go to the IAM service.  \nClick the Roles tab in the sidebar.  \nClick Create role.  \nUnder Select type of trusted entity, select SAML 2.0 federation.  \nIn SAML provider, select the name created in Step 3.  \nSelect Allow programmatic access only.  \nIn Attribute, select SAML:aud.  \nIn Value, paste the Databricks SAML URL you copied in Step 1.  \nClick Next: Permissions, Next: Tags, and Next: Review.  \nIn the Role Name field, type a role name.  \nClick Create role. The list of roles displays.  \nAdd an inline policy to the role. This policy grants access to the S3 bucket.  \nIn the Permissions tab, click .  \nClick the JSON tab. Copy this policy and set <s3-bucket-name> to the name of your bucket.  \n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\" ], \"Resource\": [ \"arn:aws:s3:::<s3-bucket-name>\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", \"s3:GetObject\", \"s3:DeleteObject\", \"s3:PutObjectAcl\" ], \"Resource\": [ \"arn:aws:s3:::<s3-bucket-name>/*\" ] } ] }  \nClick Review policy.  \nIn the Name field, type a policy name.  \nClick Create policy.  \nIn the Trusted Relationships tab, you should be able to see something similar to:  \nClick the Edit trust relationship button. The IAM resulting trust policy document should be similar to the following:"
    },
    {
        "id": 116,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-federation.html",
        "content": "In the Name field, type a policy name.  \nClick Create policy.  \nIn the Trusted Relationships tab, you should be able to see something similar to:  \nClick the Edit trust relationship button. The IAM resulting trust policy document should be similar to the following:  \n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::<accountID>:saml-provider/<IdP-name>\" }, \"Action\": \"sts:AssumeRoleWithSAML\", \"Condition\": { \"StringEquals\": { \"SAML:aud\": \"https://xxxxxx.cloud.databricks.com/saml/consume\" } } } ] }"
    },
    {
        "id": 117,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-federation.html",
        "content": "Step 5: Configure the identity provider to pass attributes to Databricks\nThe following attributes must be passed to Databricks in the SAML response via SSO in order for Databricks to pass roles to clusters:  \nhttps://aws.amazon.com/SAML/Attributes/Role  \nhttps://aws.amazon.com/SAML/Attributes/RoleSessionName  \nThese attributes are the list of role ARNs and the username matching the single sign-on login. Role mappings are refreshed when a user logs in to the Databricks workspace.  \nNote  \nIf user entitlement to the IAM roles is based on AD/LDAP group membership, you must configure that group to role mapping per your IdP.  \nEach identity provider differs in how you add attributes to pass through SAML. The following section shows one example with Okta. See Integrating Third-Party SAML Solution Providers with AWS for examples with your identity provider.  \nOkta example  \nIn the Okta Admin Console under Applications, select your Single Sign-On to Databricks application.  \nClick Edit under SAML Settings and click Next to the Configure SAML tab.  \nIn Attribute Statements add the following attributes:  \nName: https://aws.amazon.com/SAML/Attributes/RoleSessionName, Name format: URI Reference, Value: user.login  \nTo manage the roles easily using groups, create groups corresponding to your IAM roles, for example GroupA and GroupB, and add the users to those groups.  \nYou can use Okta Expressions to match groups and roles in the following way:  \nName: https://aws.amazon.com/SAML/Attributes/Role, Name format: URI Reference, Value:  \nArrays.flatten(isMemberOfGroupName(\"GroupA\") ? \"arn:aws:iam::xxx:role/role-a,arn:aws:iam::xxx:saml-provider/okta-databricks\" : {}, isMemberOfGroupName(\"GroupB\") ? \"arn:aws:iam::xxx:role/role-b,arn:aws:iam::xxx:saml-provider/okta-databricks\" : {})  \nIt should look like:"
    },
    {
        "id": 118,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-federation.html",
        "content": "It should look like:  \nOnly users in a certain group would have permission to use the corresponding IAM role.  \nUse Manage People to add users to the group.  \nUse Manage Apps to assign the group to the SSO application to allow users to log in to Databricks.  \nTo add additional roles follow the steps above, mapping an Okta group to a federated role. To have roles in different AWS accounts, add the SSO application as a new IAM identity provider to each additional AWS account that will have federated roles for Databricks."
    },
    {
        "id": 119,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-federation.html",
        "content": "Step 6: Optionally configure Databricks to synchronize role mappings from SAML to SCIM\nStep 6: Optionally configure Databricks to synchronize role mappings from SAML to SCIM\nDo this step if you want to use IAM credential passthrough for jobs or JDBC. Otherwise, you must set IAM role mappings using the SCIM API.  \nGo to the settings page.  \nClick the Single Sign-On tab.  \nSelect Allow IAM role entitlement auto sync.\n\nBest practices\nBest practices\nFor the best experience we recommend setting the IAM role maximum session duration between 4 to 8 hours. This is to avoid users having to repeatedly re-authenticate themselves in order to fetch new tokens or long queries failing due to expired tokens. To set the duration:  \nIn the AWS console, click the IAM role you configured in Step 4: Configure the IAM role for federation.  \nIn the Maximum CLI/API session duration property, click Edit.  \nSelect the duration and click Save changes.\n\nUse IAM credential passthrough with federation"
    },
    {
        "id": 120,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-federation.html",
        "content": "Use IAM credential passthrough with federation\nFollow the instructions in Launch an IAM credential passthrough cluster and do not add an instance profile. To use IAM passthrough with federation for jobs or JDBC connections, follow the instructions in Set up a meta instance profile.\n\nSecurity\nSecurity\nIt is safe to share high concurrency IAM credential passthrough clusters with other users. You will be isolated from each other and will not be able to read or use each other\u2019s credentials.\n\nTroubleshooting"
    },
    {
        "id": 121,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-federation.html",
        "content": "Troubleshooting\nMisconfigurations are a common source of errors when setting up credential passthrough. The X-Databricks-PassThrough-Error header is returned with the sign-on response headers to help identify the source of these errors. The possible values are:  \nValidationError: The configuration of roles in the identity provider fails to satisfy the constraints specified by the AWS service. A common cause of this error is that role name and identity provider name are in the wrong order.  \nInvalidIdentityToken: The identity provider submitted is invalid. A common cause of this error is that the identity provider\u2019s metadata is not set correctly in the AWS IAM service.  \nAccessDenied: Validation of the role has failed. A common cause of this error is that the identity provider has not been added to the role\u2019s Trusted Relationships in the AWS IAM service.  \nMalformed role name attribute: The configuration of the role in the identity provider is in the wrong format.  \nConsult your web browser documentation for instructions on accessing the response headers.\n\nKnown limitations"
    },
    {
        "id": 122,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-federation.html",
        "content": "Known limitations\nThe following features are not supported with IAM federation:  \n%fs (use the equivalent dbutils.fs command instead).  \nTable access control.  \nThe following methods on SparkContext (sc) and SparkSession (spark) objects:  \nDeprecated methods.  \nMethods such as addFile() and addJar() that would allow non-admin users to call Scala code.  \nAny method that accesses a filesystem other than S3.  \nOld Hadoop APIs (hadoopFile() and hadoopRDD()).  \nStreaming APIs, since the passed-through credentials would expire while the stream was still running.  \nDBFS mounts (/dbfs) are available only in Databricks Runtime 7.3 LTS and above. Mount points with credential passthrough configured are not supported through this path.  \nCluster-wide libraries that require a cluster instance profile\u2019s permission to download. Only libraries with DBFS paths are supported.  \nDatabricks Connect on High Concurrency clusters is available only in Databricks Runtime 7.3 LTS and above.  \nMLflow"
    },
    {
        "id": 123,
        "url": "https://docs.databricks.com/en/archive/storage/delta-storage-credentials.html",
        "content": "Configure Delta storage credentials  \nNote  \nTo configure Delta storage credentials, see Configure access to cloud object storage for Databricks. Databricks no longer recommends passing storage credentials through DataFrame options as described in this article.  \nDatabricks stores data for Delta Lake tables in cloud object storage. Configuring access to cloud object storage requires permissions within the cloud account that contains your storage account.  \nPass storage credentials as DataFrame options"
    },
    {
        "id": 124,
        "url": "https://docs.databricks.com/en/archive/storage/delta-storage-credentials.html",
        "content": "Pass storage credentials as DataFrame options\nDelta Lake supports specifying storage credentials as options for DataFrameReader and DataFrameWriter. You might use this if you need to interact with data in several storage accounts governed by different access keys.  \nNote  \nThis feature is available in Databricks Runtime 10.4 LTS and above.  \nFor example, you can pass your storage credentials through DataFrame options:  \ndf1 = (spark.read .option(\"fs.s3a.access.key\", \"<access-key-1>\") .option(\"fs.s3a.secret.key\", \"<secret-key-1>\") .read(\"...\") ) df2 = (spark.read .option(\"fs.s3a.access.key\", \"<access-key-1>\") .option(\"fs.s3a.secret.key\", \"<secret-key-2>\") .read(\"...\") ) (df1.union(df2).write .mode(\"overwrite\") .option(\"fs.s3a.access.key\", \"<access-key-3>\") .option(\"fs.s3a.secret.key\", \"<secret-key-3>\") .save(\"...\") )"
    },
    {
        "id": 125,
        "url": "https://docs.databricks.com/en/archive/storage/delta-storage-credentials.html",
        "content": "val df1 = spark.read .option(\"fs.s3a.access.key\", \"<access-key-1>\") .option(\"fs.s3a.secret.key\", \"<secret-key-1>\") .read(\"...\") val df2 = spark.read .option(\"fs.s3a.access.key\", \"<access-key-2>\") .option(\"fs.s3a.secret.key\", \"<secret-key-2>\") .read(\"...\") df1.union(df2).write .mode(\"overwrite\") .option(\"fs.s3a.access.key\", \"<access-key-3>\") .option(\"fs.s3a.secret.key\", \"<secret-key-3>\") .save(\"...\")"
    },
    {
        "id": 126,
        "url": "https://docs.databricks.com/en/admin/users-groups/scim/index.html",
        "content": "Sync users and groups from your identity provider  \nThis article describes how to configure your identity provider (IdP) and Databricks to provision users and groups to Databricks using SCIM, or System for Cross-domain Identity Management, an open standard that allows you to automate user provisioning.  \nAbout SCIM provisioning in Databricks"
    },
    {
        "id": 127,
        "url": "https://docs.databricks.com/en/admin/users-groups/scim/index.html",
        "content": "About SCIM provisioning in Databricks\nSCIM lets you use an identity provider (IdP) to create users in Databricks, give them the proper level of access, and remove access (deprovision them) when they leave your organization or no longer need access to Databricks.  \nYou can use a SCIM provisioning connector in your IdP or invoke the Identity and Access Management SCIM APIs to manage provisioning. You can also use these APIs to manage identities in Databricks directly, without an IdP.  \nAccount-level and workspace-level SCIM provisioning  \nYou can either configure one SCIM provisioning connector from your identity provider to your Databricks account, using account-level SCIM provisioning, or configure separate SCIM provisioning connectors to each workspace, using workspace-level SCIM provisioning.  \nAccount-level SCIM provisioning: Databricks recommends that you use account-level SCIM provisioning to create, update, and delete all users from the account. You manage the assignment of users and groups to workspaces within Databricks. Your workspaces must be enabled for identity federation to manage users\u2019 workspace assignments.  \nWorkspace-level SCIM provisioning (legacy and Public Preview): For workspaces that are not enabled for identity federation, you must manage account-level and workspace-level SCIM provisioning in parallel. You don\u2019t need workspace-level SCIM provisioning for any workspaces that are enabled for identity federation.  \nIf you already have workspace-level SCIM provisioning set up for a workspace, Databricks recommends that you enable the workspace for identity federation, set up account-level SCIM provisioning, and turn off the workspace-level SCIM provisioner. See Migrate workspace-level SCIM provisioning to the account level."
    },
    {
        "id": 128,
        "url": "https://docs.databricks.com/en/admin/users-groups/scim/index.html",
        "content": "Requirements\nTo provision users and groups to Databricks using SCIM:  \nYour Databricks account must have the Premium plan or above.  \nTo provision users to your Databricks account using SCIM (including the SCIM REST APIs), you must be a Databricks account admin.  \nTo provision users to a Databricks workspace using SCIM (including the SCIM REST APIs), you must be a Databricks workspace admin.  \nDatabricks recommends that you configure single sign-on for users to log in to Databricks using your IdP. See Configure SSO in Databricks.  \nFor more information about admin privileges, see Manage users, service principals, and groups.  \nYou can have a maximum of 10,000 combined users and service principals and 5000 groups in an account. Each workspace can have a maximum of 10,000 combined users and service principals and 5000 groups.  \nNote  \nWhen you use SCIM provisioning, user and group attributes stored in your identity provider can override changes you make using the Databricks admin settings page, account console, or SCIM (Groups) API.  \nFor example, if a user is assigned the Allow Cluster Creation entitlement in your identity provider and you remove that entitlement using the Databricks admin settings, the user is re-granted that entitlement the next time the IdP syncs with Databricks, if the IdP is configured to provision that entitlement. The same behavior applies to groups."
    },
    {
        "id": 129,
        "url": "https://docs.databricks.com/en/admin/users-groups/scim/index.html",
        "content": "Provision identities to your Databricks account\nYou can use SCIM to provision users and groups from your identity provider to your Databricks account using a SCIM provisioning connector or directly using the SCIM APIs.  \nAdd users and groups to your Databricks account using an IdP provisioning connector  \nYou can sync users and groups from your IdP to your Databricks account using a SCIM provisioning connector.  \nImportant  \nIf you already have SCIM connectors that sync identities directly to your workspaces, you must disable those SCIM connectors when the account-level SCIM connector is enabled. See Migrate workspace-level SCIM provisioning to the account level.  \nTo configure a SCIM connector to provision users and groups to your account:  \nAs an account admin, log in to the Databricks account console.  \nIn the sidebar, click Settings.  \nClick User Provisioning.  \nClick Enable user provisioning.  \nCopy the SCIM token and the Account SCIM URL. You will use these to configure your IdP.  \nLog in to your IdP as a user who can configure a SCIM connector to provision users.  \nEnter the following values in your IdP\u2019s SCIM connector:  \nFor the SAML provisioning URL, enter the SCIM URL you copied from Databricks.  \nFor the provisioning API token, enter the SCIM token you copied from Databricks.  \nYou can also follow these IdP-specific instructions for your IdP:  \nConfigure SCIM provisioning using Microsoft Entra ID (Azure Active Directory)  \nConfigure SCIM provisioning for Okta  \nConfigure SCIM provisioning for OneLogin  \nNote  \nWhen you remove a user from the account-level SCIM connector, that user is deactivated from the account and all of their workspaces, regardless of whether or not identity federation has been enabled. When you remove a group from the account-level SCIM connector, all users in that group are deactivated from the account and from any workspaces they had access to, (unless they are members of another group or have been directly granted access to the account-level SCIM connector).  \nAdd users, service principals, and groups to your account using the SCIM API"
    },
    {
        "id": 130,
        "url": "https://docs.databricks.com/en/admin/users-groups/scim/index.html",
        "content": "Add users, service principals, and groups to your account using the SCIM API  \nAccount admins can add users, service principals, and groups to the Databricks account using the Account SCIM APIs. Account admins can use either a SCIM token or OAuth to authenticate. See the Account SCIM v2.1 API reference,  \nNote  \nThe SCIM token is restricted to the Account SCIM APIs and cannot be used to authenticate to other Databricks REST APIs.  \nTo get the SCIM token, do the following:  \nAs an account admin, log in to the account console.  \nIn the sidebar, click Settings.  \nClick User Provisioning.  \nIf provisioning isn\u2019t enabled, click Set up user provisioning and copy the token.  \nIf provisioning is already enabled, click Regenerate token and copy the token.  \nTo use OAuth to authenticate, see Use a service principal to authenticate with Databricks (OAuth M2M).  \nWorkspace admins can add users and service principals using the same API. Workspace admins call the API on the workspace domain {workspace-domain}/api/2.0/account/scim/v2/.  \nRotate the account-level SCIM token  \nIf the account-level SCIM token is compromised or if you have business requirements to rotate authentication tokens periodically, you can rotate the SCIM token.  \nAs a Databricks account admin, log in to the account console.  \nIn the sidebar, click Settings.  \nClick User Provisioning.  \nClick Regenerate token. Make a note of the new token. The previous token will continue to work for 24 hours.  \nWithin 24 hours, update your SCIM application to use the new SCIM token."
    },
    {
        "id": 131,
        "url": "https://docs.databricks.com/en/admin/users-groups/scim/index.html",
        "content": "Migrate workspace-level SCIM provisioning to the account level\nIf you are enabling account-level SCIM provisioning and you already have workspace-level SCIM provisioning set up for some workspaces, Databricks recommends that you turn off the workspace-level SCIM provisioner and instead sync users and group to the account level.  \nCreate a group in your identity provider that includes all of the users and groups that you are currently provisioning to Databricks using your workspace-level SCIM connectors.  \nDatabricks recommends that this group include all users in all workspaces in your account.  \nConfigure a new SCIM provisioning connector to provision users and groups to your account, using the instructions in Provision identities to your Databricks account.  \nUse the group or groups that you created in step 1. If you add a user that shares a username (email address) with an existing account user, those users are merged. Existing groups in the account are not affected.  \nConfirm that the new SCIM provisioning connector is successfully provisioning users and groups to your account.  \nShut down the old workspace-level SCIM connectors that were provisioning users and groups to your workspaces.  \nDo not remove users and groups from the workspace-level SCIM connectors before shutting them down. Revoking access from a SCIM connector deactivates the user in the Databricks workspace. For more information, see Deactivate a user in your Databricks workspace.  \nMigrate workspace-local groups to account groups.  \nIf you have legacy groups in your workspaces, they are known as workspace-local groups. You cannot manage workspace-local groups using account-level interfaces. Databricks recommends that you convert them to account groups. See Migrate workspace-local groups to account groups"
    },
    {
        "id": 132,
        "url": "https://docs.databricks.com/en/admin/users-groups/scim/index.html",
        "content": "Provision identities to a Databricks workspace (legacy)\nPreview  \nThis feature is in Public Preview.  \nIf you want to use an IdP connector to provision users and groups and you have a workspace that is not identity federated, you must configure SCIM provisioning at the workspace level.  \nNote  \nWorkspace-level SCIM does not recognize account groups that are assigned to your identity federated workspace and workspace-level SCIM API calls will fail if they involve account groups. If your workspace is enabled for identity federation, Databricks recommends that you use the account-level SCIM API instead of the workspace-level SCIM API and that you set up account-level SCIM provisioning and turn off the workspace-level SCIM provisioner. For detailed instructions, see Migrate workspace-level SCIM provisioning to the account level.  \nAdd users and groups to your workspace using an IdP provisioning connector  \nFollow the instructions in the appropriate IdP-specific article:  \nConfigure SCIM provisioning using Microsoft Entra ID (Azure Active Directory)  \nConfigure SCIM provisioning for Okta  \nConfigure SCIM provisioning for OneLogin  \nAdd users, groups, and service principals to your workspace using the SCIM API  \nWorkspace admins can add users, groups, and service principals to the Databricks account using workspace-level SCIM APIs. See Workspace Users API, Workspace Groups API, and Workspace Service Principals API"
    },
    {
        "id": 133,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.3ml.html",
        "content": "Databricks Runtime 10.3 for ML (EoS)  \nNote  \nSupport for this Databricks Runtime version has ended. For the end-of-support date, see End-of-support history. For all supported Databricks Runtime versions, see Databricks Runtime release notes versions and compatibility.  \nDatabricks Runtime 10.3 for Machine Learning provides a ready-to-go environment for machine learning and data science based on Databricks Runtime 10.3 (EoS). Databricks Runtime ML contains many popular machine learning libraries, including TensorFlow, PyTorch, and XGBoost. Databricks Runtime ML includes AutoML, a tool to automatically train machine learning pipelines. Databricks Runtime ML also supports distributed deep learning training using Horovod.  \nFor more information, including instructions for creating a Databricks Runtime ML cluster, see AI and Machine Learning on Databricks.  \nNew features and improvements"
    },
    {
        "id": 134,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.3ml.html",
        "content": "New features and improvements\nDatabricks Runtime 10.3 ML is built on top of Databricks Runtime 10.3. For information on what\u2019s new in Databricks Runtime 10.3, including Apache Spark MLlib and SparkR, see the Databricks Runtime 10.3 (EoS) release notes.  \nEnhancements to Databricks AutoML  \nThe following enhancements have been made to Databricks AutoML.  \nAutoML now supports ARIMA model for forecasting  \nIn addition to Prophet, AutoML now creates and evaluates ARIMA models for forecasting problems.  \nExclude columns from dataset  \nWhen you use the AutoML API, you can specify columns that AutoML should ignore during its calculations. This is available only for classification and regression problems. See Databricks AutoML Python API reference for details.  \nExclude algorithm frameworks from an AutoML run  \nYou can specify algorithm frameworks, such as scikit-learn, that AutoML should not consider as it develops models. See Advanced configurations and Databricks AutoML Python API reference for details.  \nmax_trials deprecated  \nThe max_trials parameter is deprecated and will be removed in the next major Databricks Runtime ML release. Use timeout_minutes to control the duration of an AutoML run. Also, in Databricks Runtime 10.1 ML and above, AutoML incorporates early stopping; it will stop training and tuning models if the validation metric is no longer improving.  \nEnhancements to Databricks Feature Store  \nYou can now apply point-in-time lookups to time series feature tables. See Point-in-time support using time series feature tables for details.  \nDatabricks Autologging (GA)  \nDatabricks Autologging is now generally available in Databricks Runtime 10.3 ML. Databricks Autologging is a no-code solution that provides automatic experiment tracking for machine learning training sessions on Databricks. With Databricks Autologging, model parameters, metrics, files, and lineage information are automatically captured when you train models from a variety of popular machine learning libraries. Training sessions are recorded as MLflow Tracking Runs. Model files are also tracked so you can easily log them to the MLflow Model Registry and deploy them for real-time scoring with MLflow Model Serving."
    },
    {
        "id": 135,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.3ml.html",
        "content": "System environment\nSystem environment\nThe system environment in Databricks Runtime 10.3 ML differs from Databricks Runtime 10.3 as follows:  \nDBUtils: Databricks Runtime ML does not include Library utility (dbutils.library) (legacy). Use %pip commands instead. See Notebook-scoped Python libraries.  \nFor GPU clusters, Databricks Runtime ML includes the following NVIDIA GPU libraries:  \nCUDA 11.0  \ncuDNN 8.0.5.39  \nNCCL 2.10.3  \nTensorRT 7.2.2\n\nLibraries"
    },
    {
        "id": 136,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.3ml.html",
        "content": "Libraries\nThe following sections list the libraries included in Databricks Runtime 10.3 ML that differ from those included in Databricks Runtime 10.3.  \nIn this section:  \nTop-tier libraries  \nPython libraries  \nR libraries  \nJava and Scala libraries (Scala 2.12 cluster)  \nTop-tier libraries  \nDatabricks Runtime 10.3 ML includes the following top-tier libraries:  \nGraphFrames  \nHorovod and HorovodRunner  \nMLflow  \nPyTorch  \nspark-tensorflow-connector  \nTensorFlow  \nTensorBoard  \nPython libraries  \nDatabricks Runtime 10.3 ML uses Virtualenv for Python package management and includes many popular ML packages.  \nIn addition to the packages specified in the in the following sections, Databricks Runtime 10.3 ML also includes the following packages:  \nhyperopt 0.2.7.db1  \nsparkdl 2.2.0-db5  \nfeature_store 0.3.7  \nautoml 1.6.0  \nPython libraries on CPU clusters  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nabsl-py  \n0.11.0  \nAntergos Linux  \n2015.10 (ISO-Rolling)  \nappdirs  \n1.4.4  \nargon2-cffi  \n20.1.0  \nastor  \n0.8.1  \nastunparse  \n1.6.3  \nasync-generator  \n1.10  \nattrs  \n20.3.0  \nbackcall  \n0.2.0  \nbcrypt  \n3.2.0  \nbidict  \n0.21.4  \nbleach  \n3.3.0  \nblis  \n0.7.4  \nboto3  \n1.16.7  \nbotocore  \n1.19.7  \ncachetools  \n4.2.4  \ncatalogue  \n2.0.6  \ncertifi  \n2020.12.5  \ncffi  \n1.14.5  \nchardet  \n4.0.0  \nclick  \n7.1.2  \ncloudpickle  \n1.6.0  \ncmdstanpy  \n0.9.68  \nconfigparser  \n5.0.1  \nconvertdate  \n2.3.2  \ncryptography  \n3.4.7  \ncycler  \n0.10.0  \ncymem  \n2.0.5  \nCython  \n0.29.23  \ndatabricks-automl-runtime  \n0.2.5  \ndatabricks-cli  \n0.16.2  \ndbl-tempo  \n0.1.2  \ndbus-python  \n1.2.16  \ndecorator  \n5.0.6"
    },
    {
        "id": 137,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.3ml.html",
        "content": "databricks-automl-runtime  \n0.2.5  \ndatabricks-cli  \n0.16.2  \ndbl-tempo  \n0.1.2  \ndbus-python  \n1.2.16  \ndecorator  \n5.0.6  \ndefusedxml  \n0.7.1  \ndill  \n0.3.2  \ndiskcache  \n5.2.1  \ndistlib  \n0.3.4  \ndistro-info  \n0.23ubuntu1  \nentrypoints  \n0.3  \nephem  \n4.1.3  \nfacets-overview  \n1.0.0  \nfasttext  \n0.9.2  \nfilelock  \n3.0.12  \nFlask  \n1.1.2  \nflatbuffers  \n2.0  \nfsspec  \n0.9.0  \nfuture  \n0.18.2  \ngast  \n0.4.0  \ngitdb  \n4.0.7  \nGitPython  \n3.1.12  \ngoogle-auth  \n1.22.1  \ngoogle-auth-oauthlib  \n0.4.2  \ngoogle-pasta  \n0.2.0  \ngrpcio  \n1.39.0  \ngunicorn  \n20.0.4  \ngviz-api  \n1.10.0  \nh5py  \n3.1.0  \nhijri-converter  \n2.2.2  \nholidays  \n0.12  \nhorovod  \n0.23.0  \nhtmlmin  \n0.1.12  \nhuggingface-hub  \n0.1.2  \nidna  \n2.10  \nImageHash  \n4.2.1  \nimbalanced-learn  \n0.8.1  \nimportlib-metadata  \n3.10.0  \nipykernel  \n5.3.4  \nipython  \n7.22.0  \nipython-genutils  \n0.2.0  \nipywidgets  \n7.6.3  \nisodate  \n0.6.0  \nitsdangerous  \n1.1.0  \njedi  \n0.17.2  \nJinja2  \n2.11.3  \njmespath  \n0.10.0  \njoblib  \n1.0.1  \njoblibspark  \n0.3.0  \njsonschema  \n3.2.0  \njupyter-client  \n6.1.12  \njupyter-core  \n4.7.1  \njupyterlab-pygments  \n0.1.2  \njupyterlab-widgets  \n1.0.0  \nkeras  \n2.7.0  \nKeras-Preprocessing  \n1.1.2  \nkiwisolver  \n1.3.1  \nkoalas  \n1.8.2"
    },
    {
        "id": 138,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.3ml.html",
        "content": "0.1.2  \njupyterlab-widgets  \n1.0.0  \nkeras  \n2.7.0  \nKeras-Preprocessing  \n1.1.2  \nkiwisolver  \n1.3.1  \nkoalas  \n1.8.2  \nkorean-lunar-calendar  \n0.2.1  \nlangcodes  \n3.3.0  \nlibclang  \n12.0.0  \nlightgbm  \n3.3.1  \nllvmlite  \n0.38.0  \nLunarCalendar  \n0.0.9  \nMako  \n1.1.3  \nMarkdown  \n3.3.3  \nMarkupSafe  \n2.0.1  \nmatplotlib  \n3.4.2  \nmissingno  \n0.5.0  \nmistune  \n0.8.4  \nmleap  \n0.18.1  \nmlflow-skinny  \n1.23.0  \nmultimethod  \n1.6  \nmurmurhash  \n1.0.5  \nnbclient  \n0.5.3  \nnbconvert  \n6.0.7  \nnbformat  \n5.1.3  \nnest-asyncio  \n1.5.1  \nnetworkx  \n2.5  \nnltk  \n3.6.1  \nnotebook  \n6.3.0  \nnumba  \n0.55.0  \nnumpy  \n1.20.1  \noauthlib  \n3.1.0  \nopt-einsum  \n3.3.0  \npackaging  \n21.3  \npandas  \n1.2.4  \npandas-profiling  \n3.1.0  \npandocfilters  \n1.4.3  \nparamiko  \n2.7.2  \nparso  \n0.7.0  \npathy  \n0.6.0  \npatsy  \n0.5.1  \npetastorm  \n0.11.3  \npexpect  \n4.8.0  \nphik  \n0.12.0  \npickleshare  \n0.7.5  \nPillow  \n8.2.0  \npip  \n21.0.1  \nplotly  \n5.5.0  \npmdarima  \n1.8.4  \npreshed  \n3.0.5  \nprometheus-client  \n0.10.1  \nprompt-toolkit  \n3.0.17  \nprophet  \n1.0.1  \nprotobuf  \n3.17.2  \npsutil  \n5.8.0  \npsycopg2  \n2.8.5  \nptyprocess  \n0.7.0  \npyarrow  \n4.0.0  \npyasn1  \n0.4.8  \npyasn1-modules  \n0.2.8  \npybind11  \n2.9.0  \npycparser  \n2.20  \npydantic  \n1.8.2  \nPygments  \n2.8.1"
    },
    {
        "id": 139,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.3ml.html",
        "content": "0.4.8  \npyasn1-modules  \n0.2.8  \npybind11  \n2.9.0  \npycparser  \n2.20  \npydantic  \n1.8.2  \nPygments  \n2.8.1  \nPyGObject  \n3.36.0  \nPyMeeus  \n0.5.11  \nPyNaCl  \n1.4.0  \npyodbc  \n4.0.30  \npyparsing  \n2.4.7  \npyrsistent  \n0.17.3  \npystan  \n2.19.1.1  \npython-apt  \n2.0.0+ubuntu0.20.4.6  \npython-dateutil  \n2.8.1  \npython-editor  \n1.0.4  \npython-engineio  \n4.3.0  \npython-socketio  \n5.4.1  \npytz  \n2020.5  \nPyWavelets  \n1.1.1  \nPyYAML  \n5.4.1  \npyzmq  \n20.0.0  \nregex  \n2021.4.4  \nrequests  \n2.25.1  \nrequests-oauthlib  \n1.3.0  \nrequests-unixsocket  \n0.2.0  \nrsa  \n4.7.2  \ns3transfer  \n0.3.7  \nsacremoses  \n0.0.46  \nscikit-learn  \n0.24.1  \nscipy  \n1.6.2  \nseaborn  \n0.11.1  \nSend2Trash  \n1.5.0  \nsetuptools  \n52.0.0  \nsetuptools-git  \n1.2  \nshap  \n0.40.0  \nsimplejson  \n3.17.2  \nsix  \n1.15.0  \nslicer  \n0.0.7  \nsmart-open  \n5.2.0  \nsmmap  \n3.0.5  \nspacy  \n3.2.1  \nspacy-legacy  \n3.0.8  \nspacy-loggers  \n1.0.1  \nspark-tensorflow-distributor  \n1.0.0  \nsqlparse  \n0.4.1  \nsrsly  \n2.4.1  \nssh-import-id  \n5.10  \nstatsmodels  \n0.12.2  \ntabulate  \n0.8.7  \ntangled-up-in-unicode  \n0.1.0  \ntenacity  \n6.2.0  \ntensorboard  \n2.7.0  \ntensorboard-data-server  \n0.6.1  \ntensorboard-plugin-profile  \n2.5.0  \ntensorboard-plugin-wit  \n1.8.1"
    },
    {
        "id": 140,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.3ml.html",
        "content": "tensorboard  \n2.7.0  \ntensorboard-data-server  \n0.6.1  \ntensorboard-plugin-profile  \n2.5.0  \ntensorboard-plugin-wit  \n1.8.1  \ntensorflow-cpu  \n2.7.0  \ntensorflow-estimator  \n2.7.0  \ntensorflow-io-gcs-filesystem  \n0.23.1  \ntermcolor  \n1.1.0  \nterminado  \n0.9.4  \ntestpath  \n0.4.4  \nthinc  \n8.0.12  \nthreadpoolctl  \n2.1.0  \ntokenizers  \n0.10.3  \ntorch  \n1.10.1+cpu  \ntorchvision  \n0.11.2+cpu  \ntornado  \n6.1  \ntqdm  \n4.59.0  \ntraitlets  \n5.0.5  \ntransformers  \n4.15.0  \ntyper  \n0.3.2  \ntyping-extensions  \n3.7.4.3  \nujson  \n4.0.2  \nunattended-upgrades  \n0.1  \nurllib3  \n1.25.11  \nvirtualenv  \n20.4.1  \nvisions  \n0.7.4  \nwasabi  \n0.8.2  \nwcwidth  \n0.2.5  \nwebencodings  \n0.5.1  \nwebsocket-client  \n0.57.0  \nWerkzeug  \n1.0.1  \nwheel  \n0.36.2  \nwidgetsnbextension  \n3.5.1  \nwrapt  \n1.12.1  \nxgboost  \n1.5.1  \nzipp  \n3.4.1  \nPython libraries on GPU clusters  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nabsl-py  \n0.11.0  \nAntergos Linux  \n2015.10 (ISO-Rolling)  \nappdirs  \n1.4.4  \nargon2-cffi  \n20.1.0  \nastor  \n0.8.1  \nastunparse  \n1.6.3  \nasync-generator  \n1.10  \nattrs  \n20.3.0  \nbackcall  \n0.2.0  \nbcrypt  \n3.2.0  \nbidict  \n0.21.4  \nbleach  \n3.3.0  \nblis  \n0.7.4  \nboto3  \n1.16.7  \nbotocore  \n1.19.7  \ncachetools  \n4.2.4  \ncatalogue  \n2.0.6  \ncertifi  \n2020.12.5  \ncffi  \n1.14.5  \nchardet  \n4.0.0  \nclick  \n7.1.2"
    },
    {
        "id": 141,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.3ml.html",
        "content": "1.19.7  \ncachetools  \n4.2.4  \ncatalogue  \n2.0.6  \ncertifi  \n2020.12.5  \ncffi  \n1.14.5  \nchardet  \n4.0.0  \nclick  \n7.1.2  \ncloudpickle  \n1.6.0  \ncmdstanpy  \n0.9.68  \nconfigparser  \n5.0.1  \nconvertdate  \n2.3.2  \ncryptography  \n3.4.7  \ncycler  \n0.10.0  \ncymem  \n2.0.5  \nCython  \n0.29.23  \ndatabricks-automl-runtime  \n0.2.5  \ndatabricks-cli  \n0.16.2  \ndbl-tempo  \n0.1.2  \ndbus-python  \n1.2.16  \ndecorator  \n5.0.6  \ndefusedxml  \n0.7.1  \ndill  \n0.3.2  \ndiskcache  \n5.2.1  \ndistlib  \n0.3.4  \ndistro-info  \n0.23ubuntu1  \nentrypoints  \n0.3  \nephem  \n4.1.3  \nfacets-overview  \n1.0.0  \nfasttext  \n0.9.2  \nfilelock  \n3.0.12  \nFlask  \n1.1.2  \nflatbuffers  \n2.0  \nfsspec  \n0.9.0  \nfuture  \n0.18.2  \ngast  \n0.4.0  \ngitdb  \n4.0.7  \nGitPython  \n3.1.12  \ngoogle-auth  \n1.22.1  \ngoogle-auth-oauthlib  \n0.4.2  \ngoogle-pasta  \n0.2.0  \ngrpcio  \n1.39.0  \ngunicorn  \n20.0.4  \ngviz-api  \n1.10.0  \nh5py  \n3.1.0  \nhijri-converter  \n2.2.2  \nholidays  \n0.12  \nhorovod  \n0.23.0  \nhtmlmin  \n0.1.12  \nhuggingface-hub  \n0.1.2  \nidna  \n2.10  \nImageHash  \n4.2.1  \nimbalanced-learn  \n0.8.1  \nimportlib-metadata  \n3.10.0  \nipykernel  \n5.3.4  \nipython  \n7.22.0  \nipython-genutils  \n0.2.0  \nipywidgets  \n7.6.3  \nisodate  \n0.6.0  \nitsdangerous  \n1.1.0  \njedi  \n0.17.2  \nJinja2  \n2.11.3"
    },
    {
        "id": 142,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.3ml.html",
        "content": "ipython-genutils  \n0.2.0  \nipywidgets  \n7.6.3  \nisodate  \n0.6.0  \nitsdangerous  \n1.1.0  \njedi  \n0.17.2  \nJinja2  \n2.11.3  \njmespath  \n0.10.0  \njoblib  \n1.0.1  \njoblibspark  \n0.3.0  \njsonschema  \n3.2.0  \njupyter-client  \n6.1.12  \njupyter-core  \n4.7.1  \njupyterlab-pygments  \n0.1.2  \njupyterlab-widgets  \n1.0.0  \nkeras  \n2.7.0  \nKeras-Preprocessing  \n1.1.2  \nkiwisolver  \n1.3.1  \nkoalas  \n1.8.2  \nkorean-lunar-calendar  \n0.2.1  \nlangcodes  \n3.3.0  \nlibclang  \n12.0.0  \nlightgbm  \n3.3.1  \nllvmlite  \n0.38.0  \nLunarCalendar  \n0.0.9  \nMako  \n1.1.3  \nMarkdown  \n3.3.3  \nMarkupSafe  \n2.0.1  \nmatplotlib  \n3.4.2  \nmissingno  \n0.5.0  \nmistune  \n0.8.4  \nmleap  \n0.18.1  \nmlflow-skinny  \n1.23.0  \nmultimethod  \n1.6  \nmurmurhash  \n1.0.5  \nnbclient  \n0.5.3  \nnbconvert  \n6.0.7  \nnbformat  \n5.1.3  \nnest-asyncio  \n1.5.1  \nnetworkx  \n2.5  \nnltk  \n3.6.1  \nnotebook  \n6.3.0  \nnumba  \n0.55.0  \nnumpy  \n1.20.1  \noauthlib  \n3.1.0  \nopt-einsum  \n3.3.0  \npackaging  \n21.3  \npandas  \n1.2.4  \npandas-profiling  \n3.1.0  \npandocfilters  \n1.4.3  \nparamiko  \n2.7.2  \nparso  \n0.7.0  \npathy  \n0.6.0  \npatsy  \n0.5.1  \npetastorm  \n0.11.3  \npexpect  \n4.8.0  \nphik  \n0.12.0  \npickleshare  \n0.7.5  \nPillow  \n8.2.0  \npip  \n21.0.1  \nplotly  \n5.5.0  \npmdarima  \n1.8.4  \npreshed  \n3.0.5  \nprompt-toolkit"
    },
    {
        "id": 143,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.3ml.html",
        "content": "pickleshare  \n0.7.5  \nPillow  \n8.2.0  \npip  \n21.0.1  \nplotly  \n5.5.0  \npmdarima  \n1.8.4  \npreshed  \n3.0.5  \nprompt-toolkit  \n3.0.17  \nprophet  \n1.0.1  \nprotobuf  \n3.17.2  \npsutil  \n5.8.0  \npsycopg2  \n2.8.5  \nptyprocess  \n0.7.0  \npyarrow  \n4.0.0  \npyasn1  \n0.4.8  \npyasn1-modules  \n0.2.8  \npybind11  \n2.9.0  \npycparser  \n2.20  \npydantic  \n1.8.2  \nPygments  \n2.8.1  \nPyGObject  \n3.36.0  \nPyMeeus  \n0.5.11  \nPyNaCl  \n1.4.0  \npyodbc  \n4.0.30  \npyparsing  \n2.4.7  \npyrsistent  \n0.17.3  \npystan  \n2.19.1.1  \npython-apt  \n2.0.0+ubuntu0.20.4.6  \npython-dateutil  \n2.8.1  \npython-editor  \n1.0.4  \npython-engineio  \n4.3.0  \npython-socketio  \n5.4.1  \npytz  \n2020.5  \nPyWavelets  \n1.1.1  \nPyYAML  \n5.4.1  \npyzmq  \n20.0.0  \nregex  \n2021.4.4  \nrequests  \n2.25.1  \nrequests-oauthlib  \n1.3.0  \nrequests-unixsocket  \n0.2.0  \nrsa  \n4.7.2  \ns3transfer  \n0.3.7  \nsacremoses  \n0.0.46  \nscikit-learn  \n0.24.1  \nscipy  \n1.6.2  \nseaborn  \n0.11.1  \nSend2Trash  \n1.5.0  \nsetuptools  \n52.0.0  \nsetuptools-git  \n1.2  \nshap  \n0.40.0  \nsimplejson  \n3.17.2  \nsix  \n1.15.0  \nslicer  \n0.0.7  \nsmart-open  \n5.2.0  \nsmmap  \n3.0.5  \nspacy  \n3.2.1  \nspacy-legacy  \n3.0.8  \nspacy-loggers  \n1.0.1  \nspark-tensorflow-distributor  \n1.0.0  \nsqlparse"
    },
    {
        "id": 144,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.3ml.html",
        "content": "3.0.5  \nspacy  \n3.2.1  \nspacy-legacy  \n3.0.8  \nspacy-loggers  \n1.0.1  \nspark-tensorflow-distributor  \n1.0.0  \nsqlparse  \n0.4.1  \nsrsly  \n2.4.1  \nssh-import-id  \n5.10  \nstatsmodels  \n0.12.2  \ntabulate  \n0.8.7  \ntangled-up-in-unicode  \n0.1.0  \ntenacity  \n6.2.0  \ntensorboard  \n2.7.0  \ntensorboard-data-server  \n0.6.1  \ntensorboard-plugin-profile  \n2.5.0  \ntensorboard-plugin-wit  \n1.8.1  \ntensorflow  \n2.7.0  \ntensorflow-estimator  \n2.7.0  \ntensorflow-io-gcs-filesystem  \n0.23.1  \ntermcolor  \n1.1.0  \nterminado  \n0.9.4  \ntestpath  \n0.4.4  \nthinc  \n8.0.12  \nthreadpoolctl  \n2.1.0  \ntokenizers  \n0.10.3  \ntorch  \n1.10.1+cu111  \ntorchvision  \n0.11.2+cu111  \ntornado  \n6.1  \ntqdm  \n4.59.0  \ntraitlets  \n5.0.5  \ntransformers  \n4.15.0  \ntyper  \n0.3.2  \ntyping-extensions  \n3.7.4.3  \nujson  \n4.0.2  \nunattended-upgrades  \n0.1  \nurllib3  \n1.25.11  \nvirtualenv  \n20.4.1  \nvisions  \n0.7.4  \nwasabi  \n0.8.2  \nwcwidth  \n0.2.5  \nwebencodings  \n0.5.1  \nwebsocket-client  \n0.57.0  \nWerkzeug  \n1.0.1  \nwheel  \n0.36.2  \nwidgetsnbextension  \n3.5.1  \nwrapt  \n1.12.1  \nxgboost  \n1.5.1  \nzipp  \n3.4.1  \nSpark packages containing Python modules  \nSpark Package  \nPython Module  \nVersion  \ngraphframes  \ngraphframes  \n0.8.2-db1-spark3.2  \nR libraries  \nThe R libraries are identical to the R Libraries in Databricks Runtime 10.3.  \nJava and Scala libraries (Scala 2.12 cluster)"
    },
    {
        "id": 145,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.3ml.html",
        "content": "0.8.2-db1-spark3.2  \nR libraries  \nThe R libraries are identical to the R Libraries in Databricks Runtime 10.3.  \nJava and Scala libraries (Scala 2.12 cluster)  \nIn addition to Java and Scala libraries in Databricks Runtime 10.3, Databricks Runtime 10.3 ML contains the following JARs:  \nCPU clusters  \nGroup ID  \nArtifact ID  \nVersion  \ncom.typesafe.akka  \nakka-actor_2.12  \n2.5.23  \nml.combust.mleap  \nmleap-databricks-runtime_2.12  \n0.18.1-23eb1ef  \nml.dmlc  \nxgboost4j-spark_2.12  \n1.5.1  \nml.dmlc  \nxgboost4j_2.12  \n1.5.1  \norg.graphframes  \ngraphframes_2.12  \n0.8.2-db1-spark3.2  \norg.mlflow  \nmlflow-client  \n1.23.0  \norg.mlflow  \nmlflow-spark  \n1.23.0  \norg.scala-lang.modules  \nscala-java8-compat_2.12  \n0.8.0  \norg.tensorflow  \nspark-tensorflow-connector_2.12  \n1.15.0  \nGPU clusters  \nGroup ID  \nArtifact ID  \nVersion  \ncom.typesafe.akka  \nakka-actor_2.12  \n2.5.23  \nml.combust.mleap  \nmleap-databricks-runtime_2.12  \n0.18.1-23eb1ef  \nml.dmlc  \nxgboost4j-spark_2.12  \n1.5.1  \nml.dmlc  \nxgboost4j_2.12  \n1.5.1  \norg.graphframes  \ngraphframes_2.12  \n0.8.2-db1-spark3.2  \norg.mlflow  \nmlflow-client  \n1.23.0  \norg.mlflow  \nmlflow-spark  \n1.23.0  \norg.scala-lang.modules  \nscala-java8-compat_2.12  \n0.8.0  \norg.tensorflow  \nspark-tensorflow-connector_2.12  \n1.15.0"
    },
    {
        "id": 146,
        "url": "https://docs.databricks.com/en/archive/unity-catalog/20220825.html",
        "content": "Unity Catalog GA release note  \nImportant  \nThis documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See What is Unity Catalog?.  \nAugust 25, 2022  \nUnity Catalog is now generally available on Databricks.  \nThis article describes Unity Catalog as of the date of its GA release. It focuses primarily on the features and updates added to Unity Catalog since the Public Preview. For current information about Unity Catalog, see What is Unity Catalog?. For release notes that describe updates to Unity Catalog since GA, see Databricks platform release notes and Databricks Runtime release notes versions and compatibility.  \nMetastore limits and resource quotas\nMetastore limits and resource quotas\nAs of August 25, 2022  \nYour Databricks account can have only one metastore per region  \nA metastore can have up to 1000 catalogs.  \nA catalog can have up to 10,000 schemas.  \nA schema can have up to 10,000 tables.  \nFor current Unity Catalog quotas, see Resource quotas.\n\nSupported storage formats at GA"
    },
    {
        "id": 147,
        "url": "https://docs.databricks.com/en/archive/unity-catalog/20220825.html",
        "content": "Supported storage formats at GA\nAs of August 25, 2022:  \nAll managed Unity Catalog tables store data with Delta Lake  \nExternal Unity Catalog tables and external locations support Delta Lake, JSON, CSV, Avro, Parquet, ORC, and text data.  \nFor current Unity Catalog supported table formats, see File format support.\n\nManage Unity Catalog resources from the account console\nManage Unity Catalog resources from the account console\nUse the Databricks account console UI to:  \nManage the metastore lifecycle (create, update, delete, and view Unity Catalog-managed metastores)  \nAssign and remove metastores for workspaces\n\nSupported cluster types and Databricks Runtime versions"
    },
    {
        "id": 148,
        "url": "https://docs.databricks.com/en/archive/unity-catalog/20220825.html",
        "content": "Supported cluster types and Databricks Runtime versions\nUnity Catalog requires clusters that run Databricks Runtime 11.1 or above. Unity Catalog is supported by default on all SQL warehouse compute versions.  \nEarlier versions of Databricks Runtime supported preview versions of Unity Catalog. Clusters running on earlier versions of Databricks Runtime do not provide support for all Unity Catalog GA features and functionality.  \nUnity Catalog requires one of the following access modes when you create a new cluster:  \nShared  \nLanguages: SQL or Python  \nA secure cluster that can be shared by multiple users. Cluster users are fully isolated so that they cannot see each other\u2019s data and credentials.  \nSingle user  \nLanguages: SQL, Scala, Python, R  \nA secure cluster that can be used exclusively by a specified single user.  \nFor more information about cluster access modes, see Access modes.  \nFor information about updated Unity Catalog functionality in later Databricks Runtime versions, see the release notes for those versions.\n\nSystem tables"
    },
    {
        "id": 149,
        "url": "https://docs.databricks.com/en/archive/unity-catalog/20220825.html",
        "content": "System tables\ninformation_schema is fully supported for Unity Catalog data assets. Each metastore includes a catalog referred to as system that includes a metastore scoped information_schema. See Information schema. You can use information_schema to answer questions like the following:  \n\u201cCount the number of tables per catalog\u201d  \nSELECT table_catalog, count(table_name) FROM system.information_schema.tables GROUP BY 1 ORDER by 2 DESC  \n\u201cShow me all of the tables that have been altered in the last 24 hours\u201c  \nSELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog FROM system.information_schema.tables WHERE datediff(now(), last_altered) < 1\n\nStructured Streaming support\nStructured Streaming support\nStructured Streaming workloads are now supported with Unity Catalog. For details and limitations, see Limitations.  \nSee also Using Unity Catalog with Structured Streaming.\n\nSQL functions\nSQL functions\nUser-defined SQL functions are now fully supported on Unity Catalog. For information about how to create and use SQL UDFs, see CREATE FUNCTION (SQL and Python)."
    },
    {
        "id": 150,
        "url": "https://docs.databricks.com/en/archive/unity-catalog/20220825.html",
        "content": "SQL syntax for external locations in Unity Catalog\nSQL syntax for external locations in Unity Catalog\nStandard data definition and data definition language commands are now supported in Spark SQL for external locations, including the following:  \nCREATE | DROP | ALTER | DESCRIBE | SHOW EXTERNAL LOCATION  \nYou can also manage and view permissions with GRANT, REVOKE, and SHOW for external locations with SQL. See External locations.  \nExample Syntax:  \nCREATE EXTERNAL LOCATION <your-location-name> URL `<your-location-path>' WITH (CREDENTIAL <your-credential-name>); GRANT READ FILES, WRITE FILES, CREATE EXTERNAL TABLE ON EXTERNAL LOCATION `<your-location-name>` TO `finance`;\n\nUnity Catalog limitations at GA"
    },
    {
        "id": 151,
        "url": "https://docs.databricks.com/en/archive/unity-catalog/20220825.html",
        "content": "Unity Catalog limitations at GA\nAs of August 25, 2022, Unity Catalog had the following limitations. For current limitations, see Limitations.  \nScala, R, and workloads using the Machine Learning Runtime are supported only on clusters using the single user access mode. Workloads in these languages do not support the use of dynamic views for row-level or column-level security.  \nShallow clones are not supported when using Unity Catalog as the source or target of the clone.  \nBucketing is not supported for Unity Catalog tables. If you run commands that try to create a bucketed table in Unity Catalog, it will throw an exception.  \nWriting to the same path or Delta Lake table from workspaces in multiple regions can lead to unreliable performance if some clusters access Unity Catalog and others do not.  \nOverwrite mode for DataFrame write operations into Unity Catalog is supported only for Delta tables, not for other file formats. The user must have the CREATE privilege on the parent schema and must be the owner of the existing object.  \nStreaming currently has the following limitations:  \nIt is not supported in clusters using shared access mode. For streaming workloads, you must use single user access mode.  \nAsynchronous checkpointing is not yet supported.  \nOn Databricks Runtime version 11.2 and below, streaming queries that last more than 30 days on all-purpose or jobs clusters will throw an exception. For long-running streaming queries, configure automatic job retries or use Databricks Runtime 11.3 and above.  \nReferencing Unity Catalog tables from Delta Live Tables pipelines is currently not supported.  \nGroups previously created in a workspace cannot be used in Unity Catalog GRANT statements. This is to ensure a consistent view of groups that can span across workspaces. To use groups in GRANT statements, create your groups in the account console and update any automation for principal or group management (such as SCIM, Okta and Microsoft Entra ID connectors, and Terraform) to reference account endpoints instead of workspace endpoints.  \nUnity Catalog requires the E2 version of the Databricks platform. All new Databricks accounts and most existing accounts are on E2."
    },
    {
        "id": 152,
        "url": "https://docs.databricks.com/en/archive/unity-catalog/20220825.html",
        "content": "Unity Catalog availability regions at GA\nUnity Catalog availability regions at GA\nAs of August 25, 2022, Unity Catalog was available in the following regions. For the list of currently supported regions, see Databricks clouds and regions.  \nus-east-1  \nus-east-2  \nus-west-2  \nap-northeast-1  \nap-northeast-2  \nap-south-1  \nap-southeast-1  \nap-southeast-2  \nca-central-1  \neu-central-1  \neu-west-1  \neu-west-2"
    },
    {
        "id": 153,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "Databricks Runtime 6.4 for ML (EoS)  \nNote  \nSupport for this Databricks Runtime version has ended. For the end-of-support date, see End-of-support history. For all supported Databricks Runtime versions, see Databricks Runtime release notes versions and compatibility.  \nDatabricks released this image in February 2020.  \nDatabricks Runtime 6.4 for Machine Learning provides a ready-to-go environment for machine learning and data science based on Databricks Runtime 6.4 (EoS). Databricks Runtime ML contains many popular machine learning libraries, including TensorFlow, PyTorch, Keras, and XGBoost. It also supports distributed deep learning training using Horovod.  \nFor more information, including instructions for creating a Databricks Runtime ML cluster, see AI and Machine Learning on Databricks.  \nNew features\nNew features\nDatabricks Runtime 6.4 ML is built on top of Databricks Runtime 6.4. For information on what\u2019s new in Databricks Runtime 6.4, see the Databricks Runtime 6.4 (EoS) release notes.\n\nImprovements"
    },
    {
        "id": 154,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "Improvements\nDeprecations  \nThe standalone keras package is deprecated and will be removed in an upcoming major release of Databricks Runtime for ML. Databricks recommends you instead use tensorflow.keras.  \nThe pymongo package is deprecated and will be removed in an upcoming major release of Databricks Runtime for ML.  \nUpgraded machine learning libraries  \nPyTorch: 1.3.1 to 1.4.0  \nHorovod: 0.18.2 to 1.19.0\n\nSystem environment\nSystem environment\nThe system environment in Databricks Runtime 6.4 ML differs from Databricks Runtime 6.4 as follows:  \nDBUtils: Does not contain Library utility (dbutils.library) (legacy).  \nFor GPU clusters, the following NVIDIA GPU libraries:  \nCUDA 10.0  \ncuDNN 7.6.4  \nNCCL 2.4.8\n\nLibraries"
    },
    {
        "id": 155,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "Libraries\nThe following sections list the libraries included in Databricks Runtime 6.4 ML that differ from those included in Databricks Runtime 6.4.  \nIn this section:  \nTop-tier libraries  \nPython libraries  \nR libraries  \nJava and Scala libraries (Scala 2.11 cluster)  \nTop-tier libraries  \nDatabricks Runtime 6.4 ML includes the following top-tier libraries:  \nGraphFrames  \nHorovod and HorovodRunner  \nMLflow  \nPyTorch  \nspark-tensorflow-connector  \nTensorFlow  \nTensorBoard  \nPython libraries  \nDatabricks Runtime 6.4 ML uses Conda for Python package management and includes many popular ML packages. The following section describes the Conda environment for Databricks Runtime 6.4 ML.  \nPython on CPU clusters"
    },
    {
        "id": 156,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "name: databricks-ml channels: - Databricks - pytorch - defaults dependencies: - _libgcc_mutex=0.1=main - _py-xgboost-mutex=2.0=cpu_0 - _tflow_select=2.3.0=mkl - absl-py=0.9.0=py37_0 - asn1crypto=0.24.0=py37_0 - astor=0.8.0=py37_0 - backcall=0.1.0=py37_0 - backports=1.0=py_2 - bcrypt=3.1.7=py37h7b6447c_0 - blas=1.0=mkl - boto=2.49.0=py37_0 - boto3=1.9.162=py_0 - botocore=1.12.163=py_0 - c-ares=1.15.0=h7b6447c_1001 - ca-certificates=2019.1.23=0 - certifi=2019.3.9=py37_0 - cffi=1.12.2=py37h2e261b9_1 - chardet=3.0.4=py37_1003 - click=7.0=py_0 - cloudpickle=0.8.0=py37_0 - colorama=0.4.1=py_0 - configparser=3.7.4=py37_0 - cpuonly=1.0=0 - cryptography=2.6.1=py37h1ba5d50_0 - cycler=0.10.0=py37_0 - cython=0.29.6=py37he6710b0_0 - decorator=4.4.0=py37_1 - docutils=0.14=py37_0 - entrypoints=0.3=py37_0 - et_xmlfile=1.0.1=py37_0 - flask=1.0.2=py37_1 -"
    },
    {
        "id": 157,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "- entrypoints=0.3=py37_0 - et_xmlfile=1.0.1=py37_0 - flask=1.0.2=py37_1 - freetype=2.9.1=h8a8886c_1 - future=0.17.1=py37_0 - gast=0.2.2=py37_0 - gitdb2=2.0.6=py_0 - gitpython=2.1.11=py37_0 - google-pasta=0.1.8=py_0 - grpcio=1.16.1=py37hf8bcb03_1 - gunicorn=19.9.0=py37_0 - h5py=2.9.0=py37h7918eee_0 - hdf5=1.10.4=hb1b8bf9_0 - html5lib=1.0.1=py_0 - icu=58.2=h9c2bf20_1 - idna=2.8=py37_0 - intel-openmp=2019.3=199 - ipykernel=5.1.0=py37h39e3cac_0 - ipython=7.4.0=py37h39e3cac_0 - ipython_genutils=0.2.0=py37_0 - itsdangerous=1.1.0=py_0 - jdcal=1.4=py37_0 - jedi=0.13.3=py37_0 - jinja2=2.10=py37_0 - jmespath=0.9.4=py_0 - jpeg=9b=h024ee3a_2 - jupyter_client=5.2.4=py37_0 - jupyter_core=4.4.0=py37_0 - keras-applications=1.0.8=py_0 - keras-preprocessing=1.1.0=py_1 -"
    },
    {
        "id": 158,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "- jupyter_core=4.4.0=py37_0 - keras-applications=1.0.8=py_0 - keras-preprocessing=1.1.0=py_1 - kiwisolver=1.0.1=py37hf484d3e_0 - krb5=1.16.1=h173b8e3_7 - libedit=3.1.20181209=hc058e9b_0 - libffi=3.2.1=hd88cf55_4 - libgcc-ng=8.2.0=hdf63c60_1 - libgfortran-ng=7.3.0=hdf63c60_0 - libpng=1.6.36=hbc83047_0 - libpq=11.2=h20c2e04_0 - libprotobuf=3.11.4=hd408876_0 - libsodium=1.0.16=h1bed415_0 - libstdcxx-ng=8.2.0=hdf63c60_1 - libtiff=4.0.10=h2733197_2 - libxgboost=0.90=he6710b0_1 - libxml2=2.9.9=hea5a465_1 - libxslt=1.1.33=h7d1a2b0_0 - llvmlite=0.28.0=py37hd408876_0 - lxml=4.3.2=py37hefd8a0e_0 - mako=1.0.10=py_0 - markdown=3.1.1=py37_0 - markupsafe=1.1.1=py37h7b6447c_0 - mkl=2019.3=199 -"
    },
    {
        "id": 159,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "- markdown=3.1.1=py37_0 - markupsafe=1.1.1=py37h7b6447c_0 - mkl=2019.3=199 - mkl_fft=1.0.10=py37ha843d7b_0 - mkl_random=1.0.2=py37hd81dba3_0 - ncurses=6.1=he6710b0_1 - networkx=2.2=py37_1 - ninja=1.9.0=py37hfd86e86_0 - nose=1.3.7=py37_2 - numba=0.43.1=py37h962f231_0 - numpy=1.16.2=py37h7e9f1db_0 - numpy-base=1.16.2=py37hde5b4d6_0 - olefile=0.46=py_0 - openpyxl=2.6.1=py37_1 - openssl=1.1.1b=h7b6447c_1 - opt_einsum=3.1.0=py_0 - pandas=0.24.2=py37he6710b0_0 - paramiko=2.4.2=py37_0 - parso=0.3.4=py37_0 - pathlib2=2.3.3=py37_0 - patsy=0.5.1=py37_0 - pexpect=4.6.0=py37_0 - pickleshare=0.7.5=py37_0 - pillow=5.4.1=py37h34e0f95_0 - pip=19.0.3=py37_0 - ply=3.11=py37_0 - prompt_toolkit=2.0.9=py37_0 - protobuf=3.11.4=py37he6710b0_0 -"
    },
    {
        "id": 160,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "- ply=3.11=py37_0 - prompt_toolkit=2.0.9=py37_0 - protobuf=3.11.4=py37he6710b0_0 - psutil=5.6.1=py37h7b6447c_0 - psycopg2=2.7.6.1=py37h1ba5d50_0 - ptyprocess=0.6.0=py37_0 - py-xgboost=0.90=py37he6710b0_1 - py-xgboost-cpu=0.90=py37_1 - pyasn1=0.4.8=py_0 - pycparser=2.19=py_0 - pygments=2.3.1=py37_0 - pymongo=3.8.0=py37he6710b0_1 - pynacl=1.3.0=py37h7b6447c_0 - pyopenssl=19.0.0=py37_0 - pyparsing=2.3.1=py37_0 - pysocks=1.6.8=py37_0 - python=3.7.3=h0371630_0 - python-dateutil=2.8.0=py37_0 - python-editor=1.0.4=py_0 - pytorch=1.4.0=py3.7_cpu_0 - pytz=2018.9=py37_0 - pyyaml=5.1=py37h7b6447c_0 - pyzmq=18.0.0=py37he6710b0_0 - readline=7.0=h7b6447c_5 - requests=2.21.0=py37_0 - s3transfer=0.2.1=py37_0 - scikit-learn=0.20.3=py37hd81dba3_0 -"
    },
    {
        "id": 161,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "- s3transfer=0.2.1=py37_0 - scikit-learn=0.20.3=py37hd81dba3_0 - scipy=1.2.1=py37h7c811a0_0 - setuptools=40.8.0=py37_0 - simplejson=3.16.0=py37h14c3975_0 - singledispatch=3.4.0.3=py37_0 - six=1.12.0=py37_0 - smmap2=2.0.5=py_0 - sqlite=3.27.2=h7b6447c_0 - sqlparse=0.3.0=py_0 - statsmodels=0.9.0=py37h035aef0_0 - tabulate=0.8.3=py37_0 - tensorboard=1.15.0+db2=pyhb230dea_0 - tensorflow=1.15.0+db2=mkl_py37hc5fbf04_0 - tensorflow-base=1.15.0+db2=mkl_py37h2ae1e84_0 - tensorflow-estimator=1.15.1+db2=pyh2649769_0 - tensorflow-mkl=1.15.0+db2=h4fcabd2_0 - termcolor=1.1.0=py37_1 - tk=8.6.8=hbc83047_0 - torchvision=0.5.0=py37_cpu - tornado=6.0.2=py37h7b6447c_0 - tqdm=4.31.1=py37_1 - traitlets=4.3.2=py37_0 - urllib3=1.24.1=py37_0 - virtualenv=16.0.0=py37_0 -"
    },
    {
        "id": 162,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "- traitlets=4.3.2=py37_0 - urllib3=1.24.1=py37_0 - virtualenv=16.0.0=py37_0 - wcwidth=0.1.7=py37_0 - webencodings=0.5.1=py37_1 - websocket-client=0.56.0=py37_0 - werkzeug=0.14.1=py37_0 - wheel=0.33.1=py37_0 - wrapt=1.11.1=py37h7b6447c_0 - xz=5.2.4=h14c3975_4 - yaml=0.1.7=had09818_2 - zeromq=4.3.1=he6710b0_3 - zlib=1.2.11=h7b6447c_3 - zstd=1.3.7=h0b5b093_0 - pip: - argparse==1.4.0 - databricks-cli==0.9.1 - deprecated==1.2.7 - docker==4.2.0 - fusepy==2.0.4 - gorilla==0.3.0 - horovod==0.19.0 - hyperopt==0.2.2.db1 - keras==2.2.5 - matplotlib==3.0.3 - mleap==0.8.1 - mlflow==1.5.0 - nose-exclude==0.5.0 - pyarrow==0.13.0 - querystring-parser==1.2.4 - seaborn==0.9.0 - tensorboardx==1.9 prefix: /databricks/conda/envs/databricks-ml"
    },
    {
        "id": 163,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "name: databricks-ml-gpu channels: - Databricks - pytorch - defaults dependencies: - _libgcc_mutex=0.1=main - _py-xgboost-mutex=1.0=gpu_0 - _tflow_select=2.1.0=gpu - absl-py=0.9.0=py37_0 - asn1crypto=0.24.0=py37_0 - astor=0.8.0=py37_0 - backcall=0.1.0=py37_0 - backports=1.0=py_2 - bcrypt=3.1.7=py37h7b6447c_0 - blas=1.0=mkl - boto=2.49.0=py37_0 - boto3=1.9.162=py_0 - botocore=1.12.163=py_0 - c-ares=1.15.0=h7b6447c_1001 - ca-certificates=2019.1.23=0 - certifi=2019.3.9=py37_0 - cffi=1.12.2=py37h2e261b9_1 - chardet=3.0.4=py37_1003 - click=7.0=py_0 - cloudpickle=0.8.0=py37_0 - colorama=0.4.1=py_0 - configparser=3.7.4=py37_0 - cryptography=2.6.1=py37h1ba5d50_0 - cudatoolkit=10.0.130=0 - cudnn=7.6.4=cuda10.0_0 - cupti=10.0.130=0 - cycler=0.10.0=py37_0 - cython=0.29.6=py37he6710b0_0 - decorator=4.4.0=py37_1 - docutils=0.14=py37_0 -"
    },
    {
        "id": 164,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "- cython=0.29.6=py37he6710b0_0 - decorator=4.4.0=py37_1 - docutils=0.14=py37_0 - entrypoints=0.3=py37_0 - et_xmlfile=1.0.1=py37_0 - flask=1.0.2=py37_1 - freetype=2.9.1=h8a8886c_1 - future=0.17.1=py37_0 - gast=0.2.2=py37_0 - gitdb2=2.0.6=py_0 - gitpython=2.1.11=py37_0 - google-pasta=0.1.8=py_0 - grpcio=1.16.1=py37hf8bcb03_1 - gunicorn=19.9.0=py37_0 - h5py=2.9.0=py37h7918eee_0 - hdf5=1.10.4=hb1b8bf9_0 - html5lib=1.0.1=py_0 - icu=58.2=h9c2bf20_1 - idna=2.8=py37_0 - intel-openmp=2019.3=199 - ipykernel=5.1.0=py37h39e3cac_0 - ipython=7.4.0=py37h39e3cac_0 - ipython_genutils=0.2.0=py37_0 - itsdangerous=1.1.0=py_0 - jdcal=1.4=py37_0 - jedi=0.13.3=py37_0 - jinja2=2.10=py37_0 - jmespath=0.9.4=py_0 - jpeg=9b=h024ee3a_2 - jupyter_client=5.2.4=py37_0 -"
    },
    {
        "id": 165,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "- jmespath=0.9.4=py_0 - jpeg=9b=h024ee3a_2 - jupyter_client=5.2.4=py37_0 - jupyter_core=4.4.0=py37_0 - keras-applications=1.0.8=py_0 - keras-preprocessing=1.1.0=py_1 - kiwisolver=1.0.1=py37hf484d3e_0 - krb5=1.16.1=h173b8e3_7 - libedit=3.1.20181209=hc058e9b_0 - libffi=3.2.1=hd88cf55_4 - libgcc-ng=8.2.0=hdf63c60_1 - libgfortran-ng=7.3.0=hdf63c60_0 - libpng=1.6.36=hbc83047_0 - libpq=11.2=h20c2e04_0 - libprotobuf=3.11.4=hd408876_0 - libsodium=1.0.16=h1bed415_0 - libstdcxx-ng=8.2.0=hdf63c60_1 - libtiff=4.0.10=h2733197_2 - libxgboost=0.90=h688424c_0 - libxml2=2.9.9=hea5a465_1 - libxslt=1.1.33=h7d1a2b0_0 - llvmlite=0.28.0=py37hd408876_0 - lxml=4.3.2=py37hefd8a0e_0 - mako=1.0.10=py_0 - markdown=3.1.1=py37_0 -"
    },
    {
        "id": 166,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "- lxml=4.3.2=py37hefd8a0e_0 - mako=1.0.10=py_0 - markdown=3.1.1=py37_0 - markupsafe=1.1.1=py37h7b6447c_0 - mkl=2019.3=199 - mkl_fft=1.0.10=py37ha843d7b_0 - mkl_random=1.0.2=py37hd81dba3_0 - ncurses=6.1=he6710b0_1 - networkx=2.2=py37_1 - ninja=1.9.0=py37hfd86e86_0 - nose=1.3.7=py37_2 - numba=0.43.1=py37h962f231_0 - numpy=1.16.2=py37h7e9f1db_0 - numpy-base=1.16.2=py37hde5b4d6_0 - olefile=0.46=py_0 - openpyxl=2.6.1=py37_1 - openssl=1.1.1b=h7b6447c_1 - opt_einsum=3.1.0=py_0 - pandas=0.24.2=py37he6710b0_0 - paramiko=2.4.2=py37_0 - parso=0.3.4=py37_0 - pathlib2=2.3.3=py37_0 - patsy=0.5.1=py37_0 - pexpect=4.6.0=py37_0 - pickleshare=0.7.5=py37_0 - pillow=5.4.1=py37h34e0f95_0 - pip=19.0.3=py37_0 - ply=3.11=py37_0 - prompt_toolkit=2.0.9=py37_0 -"
    },
    {
        "id": 167,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "- pip=19.0.3=py37_0 - ply=3.11=py37_0 - prompt_toolkit=2.0.9=py37_0 - protobuf=3.11.4=py37he6710b0_0 - psutil=5.6.1=py37h7b6447c_0 - psycopg2=2.7.6.1=py37h1ba5d50_0 - ptyprocess=0.6.0=py37_0 - py-xgboost=0.90=py37h688424c_0 - py-xgboost-gpu=0.90=py37h28bbb66_0 - pyasn1=0.4.8=py_0 - pycparser=2.19=py_0 - pygments=2.3.1=py37_0 - pymongo=3.8.0=py37he6710b0_1 - pynacl=1.3.0=py37h7b6447c_0 - pyopenssl=19.0.0=py37_0 - pyparsing=2.3.1=py37_0 - pysocks=1.6.8=py37_0 - python=3.7.3=h0371630_0 - python-dateutil=2.8.0=py37_0 - python-editor=1.0.4=py_0 - pytorch=1.4.0=py3.7_cuda10.0.130_cudnn7.6.3_0 - pytz=2018.9=py37_0 - pyyaml=5.1=py37h7b6447c_0 - pyzmq=18.0.0=py37he6710b0_0 - readline=7.0=h7b6447c_5 - requests=2.21.0=py37_0 - s3transfer=0.2.1=py37_0 -"
    },
    {
        "id": 168,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "- readline=7.0=h7b6447c_5 - requests=2.21.0=py37_0 - s3transfer=0.2.1=py37_0 - scikit-learn=0.20.3=py37hd81dba3_0 - scipy=1.2.1=py37h7c811a0_0 - setuptools=40.8.0=py37_0 - simplejson=3.16.0=py37h14c3975_0 - singledispatch=3.4.0.3=py37_0 - six=1.12.0=py37_0 - smmap2=2.0.5=py_0 - sqlite=3.27.2=h7b6447c_0 - sqlparse=0.3.0=py_0 - statsmodels=0.9.0=py37h035aef0_0 - tabulate=0.8.3=py37_0 - tensorboard=1.15.0+db2=pyhb230dea_0 - tensorflow=1.15.0+db2=gpu_py37h9fd0ff8_0 - tensorflow-base=1.15.0+db2=gpu_py37hd56f5dd_0 - tensorflow-estimator=1.15.1+db2=pyh2649769_0 - tensorflow-gpu=1.15.0+db2=h0d30ee6_0 - termcolor=1.1.0=py37_1 - tk=8.6.8=hbc83047_0 - torchvision=0.5.0=py37_cu100 - tornado=6.0.2=py37h7b6447c_0 - tqdm=4.31.1=py37_1 - traitlets=4.3.2=py37_0 -"
    },
    {
        "id": 169,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "- tqdm=4.31.1=py37_1 - traitlets=4.3.2=py37_0 - urllib3=1.24.1=py37_0 - virtualenv=16.0.0=py37_0 - wcwidth=0.1.7=py37_0 - webencodings=0.5.1=py37_1 - websocket-client=0.56.0=py37_0 - werkzeug=0.14.1=py37_0 - wheel=0.33.1=py37_0 - wrapt=1.11.1=py37h7b6447c_0 - xz=5.2.4=h14c3975_4 - yaml=0.1.7=had09818_2 - zeromq=4.3.1=he6710b0_3 - zlib=1.2.11=h7b6447c_3 - zstd=1.3.7=h0b5b093_0 - pip: - argparse==1.4.0 - databricks-cli==0.9.1 - deprecated==1.2.7 - docker==4.2.0 - fusepy==2.0.4 - gorilla==0.3.0 - horovod==0.19.0 - hyperopt==0.2.2.db1 - keras==2.2.5 - matplotlib==3.0.3 - mleap==0.8.1 - mlflow==1.5.0 - nose-exclude==0.5.0 - pyarrow==0.13.0 - querystring-parser==1.2.4 - seaborn==0.9.0 - tensorboardx==1.9 prefix: /databricks/conda/envs/databricks-ml-gpu"
    },
    {
        "id": 170,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.4ml.html",
        "content": "Spark packages containing Python modules  \nSpark Package  \nPython Module  \nVersion  \ngraphframes  \ngraphframes  \n0.7.0-db1-spark2.4  \nspark-deep-learning  \nsparkdl  \n1.5.0-db12-spark2.4  \ntensorframes  \ntensorframes  \n0.8.2-s_2.11  \nR libraries  \nThe R libraries are identical to the R Libraries in Databricks Runtime 6.4.  \nJava and Scala libraries (Scala 2.11 cluster)  \nIn addition to Java and Scala libraries in Databricks Runtime 6.4, Databricks Runtime 6.4 ML contains the following JARs:  \nGroup ID  \nArtifact ID  \nVersion  \ncom.databricks  \nspark-deep-learning  \n1.5.0-db12-spark2.4  \ncom.typesafe.akka  \nakka-actor_2.11  \n2.3.11  \nml.combust.mleap  \nmleap-databricks-runtime_2.11  \n0.15.0  \nml.dmlc  \nxgboost4j  \n0.90  \nml.dmlc  \nxgboost4j-spark  \n0.90  \norg.graphframes  \ngraphframes_2.11  \n0.7.0-db1-spark2.4  \norg.mlflow  \nmlflow-client  \n1.4.0  \norg.tensorflow  \nlibtensorflow  \n1.15.0  \norg.tensorflow  \nlibtensorflow_jni  \n1.15.0  \norg.tensorflow  \nspark-tensorflow-connector_2.11  \n1.15.0  \norg.tensorflow  \ntensorflow  \n1.15.0  \norg.tensorframes  \ntensorframes  \n0.8.2-s_2.11"
    },
    {
        "id": 171,
        "url": "https://docs.databricks.com/en/admin/workspace-settings/notification-destinations.html",
        "content": "Manage notification destinations  \nPreview  \nThis feature is in Public Preview.  \nThis article teaches you how to create and configure notification destinations for your workspace.  \nSystem notifications are messages that tell you when your workflow experiences a run event (start, success, and failure). By default, notifications are sent to user email addresses, but admins can configure alternate notification destinations using webhooks. This allows you to build event-driven integrations with Databricks.  \nYou must be a Databricks workspace admin to manage notification destinations. After a destination is configured, it is available to all users.  \nCreate a new notification destination\nCreate a new notification destination\nTo configure a new notification destination  \nClick your username in the top bar of the workspace and select Settings from the dropdown.  \nIn the Workspace admin section, click the Notifications tab.  \nClick the Manage button.  \nClick +Add destination.  \nSelect a destination type. The following destinations are currently supported:  \nEmail  \nSlack  \nWebhook  \nMS Teams  \nPagerDuty  \nConfigure the destination based on the type.  \nClick Create.\n\nUse different credentials for each destination"
    },
    {
        "id": 172,
        "url": "https://docs.databricks.com/en/admin/workspace-settings/notification-destinations.html",
        "content": "Use different credentials for each destination\nThe configuration of a destination is securely stored encrypted in your Databricks workspace. To improve security if the third-party endpoint is compromised, Databricks recommends using different credentials for each configured destination. These include:  \nSlack: the URL to which the notification is sent.  \nMS Teams: the URL to which the notification is sent.  \nPagerDuty: integration key used to uniquely route notifications to a PagerDuty service.  \nWebhook: username and password used to authenticate to a third party endpoint using HTTP Basic authentication when delivering notifications.  \nUsing different secrets for each configured destination allows you to individually revoke access of individual notification destinations without impacting the operation of all other destinations in your Databricks workspace.\n\nSlack destination\nSlack destination\nTo set up a Slack destination, follow the instructions in Incoming webhooks for Slack. Paste the generated URL into your Databricks notification destination.\n\nAdd a webhook to a job"
    },
    {
        "id": 173,
        "url": "https://docs.databricks.com/en/admin/workspace-settings/notification-destinations.html",
        "content": "Add a webhook to a job\nAfter configuring a destination, you can add the notification destination to a job. For more information, see Add email and system notifications for job events.  \nUsers can configure up to three system destinations for each event type per job.\n\nLimitations\nLimitations\nNotification destinations currently has the following limitations:  \nYou can only configure notifications for Databricks SQL and Jobs.  \nEmail notification destinations have a 1,300-character limit on the recipient address length."
    },
    {
        "id": 174,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-passthrough.html",
        "content": "Access S3 with IAM credential passthrough with SCIM (legacy)  \nImportant  \nThis documentation has been retired and might not be updated.  \nCredential passthrough is deprecated starting with Databricks Runtime 15.0 and will be removed in future Databricks Runtime versions. Databricks recommends that you upgrade to Unity Catalog. Unity Catalog simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See What is Unity Catalog?.  \nIAM credential passthrough allows you to authenticate automatically to S3 buckets from Databricks clusters using the identity that you use to log in to Databricks. When you enable IAM credential passthrough for your cluster, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using instance profiles:  \nIAM credential passthrough allows multiple users with different data access policies to share one Databricks cluster to access data in S3 while always maintaining data security. An instance profile can be associated with only one IAM role. This requires all users on a Databricks cluster to share that role and the data access policies of that role.  \nIAM credential passthrough associates a user with an identity. This in turn enables S3 object logging via CloudTrail. All S3 access is tied directly to the user via the ARN in CloudTrail logs.  \nRequirements"
    },
    {
        "id": 175,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-passthrough.html",
        "content": "Requirements\nPremium plan or above.  \nAWS administrator access to:  \nIAM roles and policies in the AWS account of the Databricks deployment.  \nAWS account of the S3 bucket.  \nDatabricks administrator access to configure instance profiles.\n\nSet up a meta instance profile"
    },
    {
        "id": 176,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-passthrough.html",
        "content": "Set up a meta instance profile\nIn order to use IAM credential passthrough, you must first set up at least one meta instance profile to assume the IAM roles that you assign to your users.  \nAn IAM role is an AWS identity with policies that determine what the identity can and cannot do in AWS. An instance profile is a container for an IAM role that you can use to pass the role information to an EC2 instance when the instance starts. Instance profiles allow you to access data from Databricks clusters without having to embed your AWS keys in notebooks.  \nWhile instance profiles make configuring roles on clusters very simple, an instance profile can be associated with only one IAM role. This requires all users on a Databricks cluster to share that role and the data access policies of that role. However, IAM roles can be used to assume other IAM roles or to access data directly themselves. Using the credentials for one role to assume a different role is called role chaining.  \nIAM credential passthrough allows admins to split the IAM role the instance profile is using and the roles users use to access data. In Databricks, we call the instance role the meta IAM role and the data access role the data IAM role. Similar to the instance profile, a meta instance profile is a container for a meta IAM role.  \nUsers are granted access to data IAM roles using the SCIM API. If you are mapping roles with your identity provider, then those roles will sync to the Databricks SCIM API. When you use a cluster with credential passthrough and a meta instance profile, you can assume only the data IAM roles that you can access. This allows multiple users with different data access policies to share one Databricks cluster while keeping data secure.  \nThis section describes how to set up the meta instance profile required to enable IAM credential passthrough.  \nStep 1: Configure roles for IAM credential passthrough  \nIn this section:  \nCreate a data IAM role  \nConfigure a meta IAM role  \nConfigure the data IAM role to trust the meta IAM role  \nCreate a data IAM role  \nUse an existing data IAM role or optionally follow Tutorial: Configure S3 access with an instance profile to create a data IAM role that can access S3 buckets.  \nConfigure a meta IAM role  \nConfigure your meta IAM role to assume the data IAM role."
    },
    {
        "id": 177,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-passthrough.html",
        "content": "Configure a meta IAM role  \nConfigure your meta IAM role to assume the data IAM role.  \nIn the AWS console, go to the IAM service.  \nClick the Roles tab in the sidebar.  \nClick Create role.  \nUnder Select type of trusted entity, select AWS service.  \nClick the EC2 service.  \nClick Next Permissions.  \nClick Create Policy. A new window opens.  \nClick the JSON tab.  \nCopy the following policy and set <account-id> to your AWS Account ID and <data-iam-role> to the name of your data IAM role from the preceding section.  \n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AssumeDataRoles\", \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": [ \"arn:aws:iam::<account-id>:role/<data-iam-role>\" ] } ] }  \nClick Review Policy.  \nIn the Name field, type a policy name and click Create policy.  \nReturn to the role window and refresh it.  \nSearch for the policy name and select the checkbox next to the policy name.  \nClick Next Tags and Next Review.  \nIn the Role name file, type a name for the meta IAM role.  \nClick Create role.  \nIn the role summary, copy the Instance Profile ARN.  \nConfigure the data IAM role to trust the meta IAM role  \nTo make the meta IAM role able to assume the data IAM role, you make the meta role trusted by the data role.  \nIn the AWS console, go to the IAM service.  \nClick the Roles tab in the sidebar.  \nFind the data role created in the previous step and click it to go to the role detail page.  \nClick the Trust relationships tab and add the following statement if not set:  \n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::<account-id>:role/<meta-iam-role>\" }, \"Action\": \"sts:AssumeRole\" } ] }  \nStep 2: Configure a meta instance profile in Databricks  \nThis section describes how to configure a meta instance profile in Databricks.  \nIn this section:"
    },
    {
        "id": 178,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-passthrough.html",
        "content": "Step 2: Configure a meta instance profile in Databricks  \nThis section describes how to configure a meta instance profile in Databricks.  \nIn this section:  \nDetermine the IAM role used for Databricks deployment  \nModify policy in the IAM role used for Databricks deployment  \nAdd the meta instance profile to Databricks  \nDetermine the IAM role used for Databricks deployment  \nGo to the account console.  \nClick the Workspaces icon.  \nClick on the name of your workspace.  \nNote the role name at the end of the ARN key in the credentials section, in the image below it\u2019s`testco-role`.  \nModify policy in the IAM role used for Databricks deployment  \nIn the AWS console, go to the IAM service.  \nClick the Roles tab in the sidebar.  \nEdit the role you noted in the preceding section.  \nClick the policy attached to the role.  \nModify the policy to allow the EC2 instances for the Spark clusters within Databricks to use the meta instance profile you created in Configure a meta IAM role. For an example, see Step 5: Add the S3 IAM role to the EC2 policy.  \nClick Review policy and Save Changes.  \nAdd the meta instance profile to Databricks  \nGo to the settings page.  \nSelect the Instance Profiles tab.  \nClick the Add Instance Profile button. A dialog appears.  \nPaste in the Instance Profile ARN for the meta IAM role from Configure a meta IAM role.  \nCheck the Meta Instance Profile checkbox and click Add.  \nOptionally identify users who can launch clusters with the meta instance profile.  \nStep 3: Attach IAM role permissions to Databricks users  \nThere are two ways to maintain the mapping of users to IAM roles:  \nWithin Databricks using the SCIM Users API or SCIM Groups API.  \nWithin your identity provider. This allows you to centralize data access and pass those entitlements directly to Databricks clusters via SAML 2.0 identity federation.  \nUse the following chart to help you decide which mapping method is better for your workspace:  \nRequirement  \nSCIM  \nIdentity Provider  \nSingle sign-on to Databricks  \nNo  \nYes  \nConfigure AWS identity provider  \nNo  \nYes  \nConfigure meta instance profile  \nYes  \nYes  \nDatabricks workspace admin  \nYes  \nYes  \nAWS admin  \nYes  \nYes  \nIdentity provider admin  \nNo  \nYes"
    },
    {
        "id": 179,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-passthrough.html",
        "content": "Requirement  \nSCIM  \nIdentity Provider  \nSingle sign-on to Databricks  \nNo  \nYes  \nConfigure AWS identity provider  \nNo  \nYes  \nConfigure meta instance profile  \nYes  \nYes  \nDatabricks workspace admin  \nYes  \nYes  \nAWS admin  \nYes  \nYes  \nIdentity provider admin  \nNo  \nYes  \nWhen you start a cluster with a meta instance profile, the cluster will pass through your identity and only assume the data IAM roles that you can access. An admin must grant users permissions on the data IAM roles using SCIM API methods to set permissions on roles.  \nNote  \nIf you are mapping roles within your IdP, those roles will overwrite any roles mapped within SCIM and you should not map users to roles directly. See Step 6: Optionally configure Databricks to synchronize role mappings from SAML to SCIM.  \nYou can also attach an instance profile to a user or group with Databricks Terraform provider and databricks_user_role or databricks_group_instance_profile."
    },
    {
        "id": 180,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-passthrough.html",
        "content": "Launch an IAM credential passthrough cluster\nThe process to launch a cluster with credential passthrough differs according to the cluster mode.  \nEnable credential passthrough for a High Concurrency cluster  \nHigh Concurrency clusters can be shared by multiple users. They support only Python and SQL with passthrough.  \nWhen you create a cluster, set Cluster Mode to High Concurrency.  \nChoose a Databricks Runtime Version 6.1 or above.  \nUnder Advanced Options, select Enable credential passthrough for user-level data access and only allow Python and SQL commands.  \nClick the Instances tab. In the Instance Profile drop-down, choose the meta instance profile you created in Add the meta instance profile to Databricks.  \nEnable IAM credential passthrough for a Standard cluster  \nStandard clusters with credential passthrough are supported and are limited to a single user. Standard clusters support Python, SQL, Scala, and R. On Databricks Runtime 10.4 LTS and above, sparklyr is also supported.  \nYou must assign a user at cluster creation, but the cluster can be edited by a user with CAN MANAGE permissions at any time to replace the original user.  \nImportant  \nThe user assigned to the cluster must have at least CAN ATTACH TO permission for the cluster in order to run commands on the cluster. Workspace admins and the cluster creator have CAN MANAGE permissions, but cannot run commands on the cluster unless they are the designated cluster user.  \nWhen you create a cluster, set Cluster Mode to Standard.  \nChoose a Databricks Runtime Version 6.1 or above.  \nUnder Advanced Options, select Enable credential passthrough for user-level data access.  \nSelect the user name from the Single User Access drop-down.  \nClick the Instances tab. In the Instance Profile drop-down, select the meta instance profile you created in Add the meta instance profile to Databricks."
    },
    {
        "id": 181,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-passthrough.html",
        "content": "Access S3 using IAM credential passthrough\nYou can access S3 using credential passthrough either by assuming a role and accessing S3 directly or by using the role to mount the S3 bucket and accessing the data through the mount.  \nRead and write S3 data using credential passthrough  \nRead and write data to/from S3:  \ndbutils.credentials.assumeRole(\"arn:aws:iam::xxxxxxxx:role/<data-iam-role>\") spark.read.format(\"csv\").load(\"s3a://prod-foobar/sampledata.csv\") spark.range(1000).write.mode(\"overwrite\").save(\"s3a://prod-foobar/sampledata.parquet\")  \ndbutils.credentials.assumeRole(\"arn:aws:iam::xxxxxxxx:role/<data-iam-role>\") # SparkR library(SparkR) sparkR.session() read.df(\"s3a://prod-foobar/sampledata.csv\", source = \"csv\") write.df(as.DataFrame(data.frame(1:1000)), path=\"s3a://prod-foobar/sampledata.parquet\", source = \"parquet\", mode = \"overwrite\") # sparklyr library(sparklyr) sc <- spark_connect(method = \"databricks\") sc %>% spark_read_csv(\"s3a://prod-foobar/sampledata.csv\") sc %>% sdf_len(1000) %>% spark_write_parquet(\"s3a://prod-foobar/sampledata.parquet\", mode = \"overwrite\")  \nUse dbutils with a role:  \ndbutils.credentials.assumeRole(\"arn:aws:iam::xxxxxxxx:role/<data-iam-role>\") dbutils.fs.ls(\"s3a://bucketA/\")"
    },
    {
        "id": 182,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-passthrough.html",
        "content": "dbutils.credentials.assumeRole(\"arn:aws:iam::xxxxxxxx:role/<data-iam-role>\") dbutils.fs.ls(\"s3a://bucketA/\")  \nFor other dbutils.credentials methods, see Credentials utility (dbutils.credentials).  \nMount an S3 bucket to DBFS using IAM credential passthrough  \nFor more advanced scenarios where different buckets or prefixes require different roles, it\u2019s more convenient to use Databricks bucket mounts to specify the role to use when accessing a specific bucket path.  \nWhen you mount data using a cluster enabled with IAM credential passthrough, any read or write to the mount point uses your credentials to authenticate to the mount point. This mount point will be visible to other users, but the only users that will have read and write access are those who:  \nHave access to the underlying S3 storage account via IAM data roles  \nAre using a cluster enabled for IAM credential passthrough  \ndbutils.fs.mount( \"s3a://<s3-bucket>/data/confidential\", \"/mnt/confidential-data\", extra_configs = { \"fs.s3a.credentialsType\": \"Custom\", \"fs.s3a.credentialsType.customClass\": \"com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider\", \"fs.s3a.stsAssumeRole.arn\": \"arn:aws:iam::xxxxxxxx:role/<confidential-data-role>\" })"
    },
    {
        "id": 183,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-passthrough.html",
        "content": "Access S3 data in a job using IAM credential passthrough\nAccess S3 data in a job using IAM credential passthrough\nTo access S3 data using credential passthrough in a job, configure the cluster according to Launch an IAM credential passthrough cluster when you select a new or existing cluster.  \nThe cluster will assume only the roles that the job owner has been granted permission to assume, and therefore can access only the S3 data that the role has permission to access.\n\nAccess S3 data from a JDBC or ODBC client using IAM credential passthrough"
    },
    {
        "id": 184,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-passthrough.html",
        "content": "Access S3 data from a JDBC or ODBC client using IAM credential passthrough\nTo access S3 data using IAM credential passthrough using a JDBC or ODBC client, configure the cluster according to Launch an IAM credential passthrough cluster and connect to this cluster in your client. The cluster will assume only the roles that the user connecting to it has been granted permission to access, and therefore can only access the S3 data that the user has permission to access.  \nTo specify a role in your SQL query, do the following:  \nSET spark.databricks.credentials.assumed.role=arn:aws:iam::XXXX:role/<data-iam-role>; -- Access the bucket which <my-role> has permission to access SELECT count(*) from csv.`s3://my-bucket/test.csv`;\n\nKnown limitations"
    },
    {
        "id": 185,
        "url": "https://docs.databricks.com/en/archive/credential-passthrough/iam-passthrough.html",
        "content": "Known limitations\nThe following features are not supported with IAM credential passthrough:  \n%fs (use the equivalent dbutils.fs command instead).  \nTable access control.  \nThe following methods on SparkContext (sc) and SparkSession (spark) objects:  \nDeprecated methods.  \nMethods such as addFile() and addJar() that would allow non-admin users to call Scala code.  \nAny method that accesses a filesystem other than S3.  \nOld Hadoop APIs (hadoopFile() and hadoopRDD()).  \nStreaming APIs, since the passed-through credentials would expire while the stream was still running.  \nDBFS mounts (/dbfs) are available only in Databricks Runtime 7.3 LTS and above. Mount points with credential passthrough configured are not supported through this path.  \nCluster-wide libraries that require a cluster instance profile\u2019s permission to download. Only libraries with DBFS paths are supported.  \nDatabricks Connect on High Concurrency clusters is available only in Databricks Runtime 7.3 LTS and above.  \nMLflow"
    },
    {
        "id": 186,
        "url": "https://docs.databricks.com/en/admin/workspace/delete-workspace.html",
        "content": "Delete a workspace  \nThis article shows you how to delete a workspace in your Databricks account.  \nNote  \nOnce a workspace is deleted, Databricks is not responsible for cleaning up the resources attached to that workspace. Databricks recommends terminating all clusters and instance pools associated with a workspace before you delete it. Additionally, terminate any associated resources in your cloud provider account.  \nTo delete a workspace:  \nTerminate all clusters and instance pools associated with the workspace.  \nFrom the the account console, click the Workspaces icon.  \nOn the row with your workspace, click the kebab menu then select Delete. Alternatively, click the workspace name, click , then select Delete Workspace.  \nIn the confirmation dialog, type the workspace name and click Confirm Delete."
    },
    {
        "id": 187,
        "url": "https://docs.databricks.com/en/archive/legacy/skew-join.html",
        "content": "Skew join optimization using skew hints  \nImportant  \nThis documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.  \nSkew join hints are not required. Databricks handles skew by default by using adaptive query execution (AQE). See Adaptive query execution.  \nNote  \nspark.sql.adaptive.skewJoin.enabled must be True, which is the default setting on Databricks.  \nWhat is data skew?"
    },
    {
        "id": 188,
        "url": "https://docs.databricks.com/en/archive/legacy/skew-join.html",
        "content": "What is data skew?\nData skew is a condition in which a table\u2019s data is unevenly distributed among partitions in the cluster. Data skew can severely downgrade performance of queries, especially those with joins. Joins between big tables require shuffling data and the skew can lead to an extreme imbalance of work in the cluster. It\u2019s likely that data skew is affecting a query if a query appears to be stuck finishing very few tasks (for example, the last 3 tasks out of 200). To verify that data skew is affecting a query:  \nClick the stage that is stuck and verify that it is doing a join.  \nAfter the query finishes, find the stage that does a join and check the task duration distribution.  \nSort the tasks by decreasing duration and check the first few tasks. If one task took much longer to complete than the other tasks, there is skew.  \nTo ameliorate skew, Delta Lake on Databricks SQL accepts skew hints in queries. With the information from a skew hint, Databricks Runtime can construct a better query plan, one that does not suffer from data skew.\n\nConfigure skew hint with relation name"
    },
    {
        "id": 189,
        "url": "https://docs.databricks.com/en/archive/legacy/skew-join.html",
        "content": "Configure skew hint with relation name\nA skew hint must contain at least the name of the relation with skew. A relation is a table, view, or a subquery. All joins with this relation then use skew join optimization.  \n-- table with skew SELECT /*+ SKEW('orders') */ * FROM orders, customers WHERE c_custId = o_custId -- subquery with skew SELECT /*+ SKEW('C1') */ * FROM (SELECT * FROM customers WHERE c_custId < 100) C1, orders WHERE C1.c_custId = o_custId\n\nConfigure skew hint with relation name and column names"
    },
    {
        "id": 190,
        "url": "https://docs.databricks.com/en/archive/legacy/skew-join.html",
        "content": "Configure skew hint with relation name and column names\nThere might be multiple joins on a relation and only some of them will suffer from skew. Skew join optimization has some overhead so it is better to use it only when needed. For this purpose, the skew hint accepts column names. Only joins with these columns use skew join optimization.  \n-- single column SELECT /*+ SKEW('orders', 'o_custId') */ * FROM orders, customers WHERE o_custId = c_custId -- multiple columns SELECT /*+ SKEW('orders', ('o_custId', 'o_storeRegionId')) */ * FROM orders, customers WHERE o_custId = c_custId AND o_storeRegionId = c_regionId\n\nConfigure skew hint with relation name, column names, and skew values"
    },
    {
        "id": 191,
        "url": "https://docs.databricks.com/en/archive/legacy/skew-join.html",
        "content": "Configure skew hint with relation name, column names, and skew values\nYou can also specify skew values in the hint. Depending on the query and data, the skew values might be known (for example, because they never change) or might be easy to find out. Doing this reduces the overhead of skew join optimization. Otherwise, Delta Lake detects them automatically.  \n-- single column, single skew value SELECT /*+ SKEW('orders', 'o_custId', 0) */ * FROM orders, customers WHERE o_custId = c_custId -- single column, multiple skew values SELECT /*+ SKEW('orders', 'o_custId', (0, 1, 2)) */ * FROM orders, customers WHERE o_custId = c_custId -- multiple columns, multiple skew values SELECT /*+ SKEW('orders', ('o_custId', 'o_storeRegionId'), ((0, 1001), (1, 1002))) */ * FROM orders, customers WHERE o_custId = c_custId AND o_storeRegionId = c_regionId"
    },
    {
        "id": 192,
        "url": "https://docs.databricks.com/en/admin/system-tables/audit-logs.html",
        "content": "Audit log system table reference  \nPreview  \nThis feature is in Public Preview. The schema must be enabled to be visible in your system catalog. For more information, see Enable system table schemas  \nThis article outlines the audit log table schema and provides you with sample queries you can use with the audit log system table to answer common account usage questions. For information on audit log events, see Audit log reference.  \nThe audit log system table is located at system.access.audit.  \nAudit log considerations\nAudit log considerations\nMost audit logs are only available in the region of the workspace.  \nOnly Unity Catalog account-level logs are available in all regions.  \nAccount-level audit logs record workspace_id as 0.\n\nAudit log system table schema"
    },
    {
        "id": 193,
        "url": "https://docs.databricks.com/en/admin/system-tables/audit-logs.html",
        "content": "Audit log system table schema\nThe audit log system table uses the following schema:  \nColumn name  \nData type  \nDescription  \nExample  \nversion  \nstring  \nAudit log schema version  \n2.0  \nevent_time  \ntimestamp  \nTimestamp of the event. Timezone information is recorded at the end of the value with +00:00 representing UTC timezone.  \n2023-01-01T01:01:01.123+00:00  \nevent_date  \ndate  \nCalendar date the action took place  \n2023-01-01  \nworkspace_id  \nlong  \nID of the workspace  \n1234567890123456  \nsource_ip_address  \nstring  \nIP address where the request originated  \n10.30.0.242  \nuser_agent  \nstring  \nOrigination of request  \nApache-HttpClient/4.5.13 (Java/1.8.0_345)  \nsession_id  \nstring  \nID of the session where the request came from  \n123456789  \nuser_identity  \nstring  \nIdentity of user initiating request  \n{\"email\": \"user@domain.com\", \"subjectName\": null}  \nservice_name  \nstring  \nService name initiating request  \nunityCatalog  \naction_name  \nstring  \nCategory of the event captured in audit log  \ngetTable  \nrequest_id  \nstring  \nID of request  \nServiceMain-4529754264  \nrequest_params  \nmap  \nMap of key values containing all the request parameters. Depends on request type  \n[[\"full_name_arg\", \"user.chat.messages\"], [\"workspace_id\", \"123456789\"], [\"metastore_id\", \"123456789\"]]  \nresponse  \nstruct  \nStruct of response return values  \n{\"statusCode\": 200, \"errorMessage\": null, \"result\": null}  \naudit_level  \nstring  \nWorkspace or account level event  \nACCOUNT_LEVEL  \naccount_id  \nstring  \nID of the account  \n23e22ba4-87b9-4cc2-9770-d10b894bxx  \nevent_id  \nstring  \nID of the event  \n34ac703c772f3549dcc8671f654950f0"
    },
    {
        "id": 194,
        "url": "https://docs.databricks.com/en/admin/system-tables/audit-logs.html",
        "content": "Sample queries\nThe following sections include sample queries you can use to gain insights into your audit logs system table. For these queries to work, replace the values within curly brackets {{}} with your own parameters.  \nNote  \nSome of these examples include verbose audit log events, which are not enabled by default. To enable verbose audit logs in a workspace, see Enable verbose audit logs.  \nThis article includes the following example queries:  \nWho can access this table?  \nWhich users accessed a table within the last day?  \nWhich tables did a user access?  \nView permissions changes for all securable objects  \nView the most recently run notebook commands  \nWho can access this table?  \nThis query uses the information_schema to find out which users have permissions on a table.  \nSELECT DISTINCT(grantee) AS `ACCESSIBLE BY` FROM system.information_schema.table_privileges WHERE table_schema = '{{schema_name}}' AND table_name = '{{table_name}}' UNION SELECT table_owner FROM system.information_schema.tables WHERE table_schema = '{{schema_name}}' AND table_name = '{{table}}' UNION SELECT DISTINCT(grantee) FROM system.information_schema.schema_privileges WHERE schema_name = '{{schema_name}}'  \nWhich users accessed a table within the last day?  \nNote  \nFull names are not captured in the log for DML operations. Include the schema and simple name to capture all.  \nSELECT user_identity.email as `User`, IFNULL(request_params.full_name_arg, request_params.name) AS `Table`, action_name AS `Type of Access`, event_time AS `Time of Access` FROM system.access.audit WHERE (request_params.full_name_arg = '{{catalog.schema.table}}' OR (request_params.name = '{{table_name}}' AND request_params.schema_name = '{{schema_name}}')) AND action_name IN ('createTable','getTable','deleteTable') AND event_date > now() - interval '1 day' ORDER BY event_date DESC"
    },
    {
        "id": 195,
        "url": "https://docs.databricks.com/en/admin/system-tables/audit-logs.html",
        "content": "Which tables did a user access?  \nNote  \nTo filter by date range, uncomment out the date clause at the bottom of the query.  \nSELECT action_name as `EVENT`, event_time as `WHEN`, IFNULL(request_params.full_name_arg, 'Non-specific') AS `TABLE ACCESSED`, IFNULL(request_params.commandText,'GET table') AS `QUERY TEXT` FROM system.access.audit WHERE user_identity.email = '{{User}}' AND action_name IN ('createTable', 'commandSubmit','getTable','deleteTable') -- AND datediff(now(), event_date) < 1 -- ORDER BY event_date DESC  \nExample result  \nEVENT  \nWHEN  \nTABLE ACCESSED  \nQUERY TEXT  \ngetTable  \n2023-05-31  \nsystem.access.audit  \nGET table  \ngetTable  \n2023-05-31  \nsystem.access.table_lineage  \nGET table  \ncommandSubmit  \n2023-05-31  \nNon-specific  \nshow functions;  \ncommandSubmit  \n2023-05-31  \nNon-specific  \nSELECT  \nrequest_params  \nFROM  \nsystem.access.audit  \nWHERE  \nservice_name = \"notebook\"  \nAND action_name = \"moveFolder\"  \nLIMIT  \n5  \nView permissions changes for all securable objects  \nThis query will return an event for every permission change that has occurred in your account. The query will return the user who made the change, the securable object type and name, and the specific changes that were made.  \nSELECT event_time, user_identity.email, request_params.securable_type, request_params.securable_full_name, request_params.changes FROM system.access.audit WHERE service_name = 'unityCatalog' AND action_name = 'updatePermissions' ORDER BY 1 DESC  \nView the most recently run notebook commands  \nThis query returns the most recently run notebook commands along with the user who ran the command.  \nNote  \nThe runCommand action is only emitted when verbose audit logs are enabled. To enable verbose audit logs, see Enable verbose audit logs.  \nSELECT event_time, user_identity.email, request_params.commandText FROM system.access.audit WHERE action_name = `runCommand` ORDER BY event_time DESC LIMIT 100"
    },
    {
        "id": 196,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.5x.html",
        "content": "Databricks Runtime 5.5 Extended Support (EoS)  \nNote  \nSupport for this Databricks Runtime version has ended. For the end-of-support date, see End-of-support history. For all supported Databricks Runtime versions, see Databricks Runtime release notes versions and compatibility.  \nDatabricks released this image in July 2021. It is supported through December 2021.  \nDatabricks Runtime 5.5 Extended Support is intended for customers who are on the soon-to-be-unsupported Databricks Runtime 5.5 LTS but are unable to migrate to Databricks Runtime 7.x or 8.x. Databricks Runtime 5.5 Extended Support uses Ubuntu 18.04.5 LTS instead of the deprecated Ubuntu 16.04.6 LTS distribution used in the original Databricks Runtime 5.5. Ubuntu 16.04 reached end of life on April 1, 2021.  \nDatabricks continues to recommend that you migrate your workloads to Databricks Runtime 7.x or 8.x as soon as you can to get the benefits of Apache Spark 3.x and the many new features and improvements built into these newer runtimes.  \nFor migration information, see Databricks Runtime 7.x migration guide (EoS).  \nThese release notes list the differences between Databricks Runtime 5.5 Extended Support and the original Databricks Runtime 5.5. For all other details about this release, see the release note for the original Databricks Runtime 5.5 LTS (EoS).  \nDifferences between Databricks Runtime 5.5 Extended Support and Databricks Runtime 5.5"
    },
    {
        "id": 197,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.5x.html",
        "content": "Differences between Databricks Runtime 5.5 Extended Support and Databricks Runtime 5.5\nThe following environment details and libraries are different from those in Databricks Runtime 5.5.  \nSystem environment  \nOperating System: Ubuntu 18.04.5 LTS  \nJava: Zulu 8.54.0.21-CA-linux64 (build 1.8.0_292-b10)  \nPython2: Python 2.7.17  \nPython3: Python 3.6.9  \nPython2 libraries  \ncffi upgraded from 1.7.0 to 1.12.2  \ncryptography upgraded from 1.5 to 2.6.1  \ndocutils upgraded from 0.14 to 0.17.1  \nfutures upgraded from 3.2.0 to 3.3.0  \nllvmlite upgraded from 0.13.0 to 0.28.0  \nnumba upgraded from 0.28.1 to 0.43.1  \npsycopg2 upgraded from 2.6.2 to 2.7.5  \nPyGObject upgraded from 3.20.0 to 3.26.1  \nPyOpenSSL upgraded from 16.0.0 to 19.0.0  \nsetuptools upgraded from 41.0.1 to 44.1.1  \nwheel upgraded from 0.33.4 to 0.36.2  \nasn1crypto 0.24.0  \nkeyring 10.6.0  \nkeyrings.alt 3.0  \npycairo 1.16.2  \npycrypto 2.6.1  \npyxdg 0.25  \nSecretStorage 2.3.1  \nPython3 libraries  \ncffi upgraded from 1.7.0 to 1.12.2  \ncryptography upgraded from 1.5 to 2.6.1  \ndocutils upgraded from 0.14 to 0.17.1  \nllvmlite upgraded from 0.13.0 to 0.28.0  \nnumba upgraded from 0.28.1 to 0.43.1  \npsycopg2 upgraded from 2.6.2 to 2.7.5  \nPyGObject upgraded from 3.20.0 to 3.26.1  \nPyOpenSSL upgraded from 16.0.0 to 19.0.0  \npython-apt upgraded from 1.1.0 to 1.6.5"
    },
    {
        "id": 198,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.5x.html",
        "content": "PyGObject upgraded from 3.20.0 to 3.26.1  \nPyOpenSSL upgraded from 16.0.0 to 19.0.0  \npython-apt upgraded from 1.1.0 to 1.6.5  \nssh-import-id upgraded from 5.5 to 5.7  \nwheel upgraded from 0.33.4 to 0.36.2  \nasn1crypto 1.4.0  \nenum34 removed  \npycurl removed  \nR libraries  \nforeign upgraded from 0.8-70 to 0.8-71  \nRserve upgraded from 1.8-6 to 1.8-8"
    },
    {
        "id": 199,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "Databricks Runtime 6.1 for ML (EoS)  \nNote  \nSupport for this Databricks Runtime version has ended. For the end-of-support date, see End-of-support history. For all supported Databricks Runtime versions, see Databricks Runtime release notes versions and compatibility.  \nDatabricks released this image in October 2019.  \nDatabricks Runtime 6.1 for Machine Learning provides a ready-to-go environment for machine learning and data science based on Databricks Runtime 6.1 (EoS). Databricks Runtime ML contains many popular machine learning libraries, including TensorFlow, PyTorch, Keras, and XGBoost. It also supports distributed deep learning training using Horovod.  \nFor more information, including instructions for creating a Databricks Runtime ML cluster, see AI and Machine Learning on Databricks.  \nNew features\nNew features\nDatabricks Runtime 6.1 ML is built on top of Databricks Runtime 6.1. For information on what\u2019s new in Databricks Runtime 6.1, see the Databricks Runtime 6.1 (EoS) release notes.\n\nImprovements"
    },
    {
        "id": 200,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "Improvements\nUpgraded machine learning libraries:  \nTensorFlow: 1.13.1 to 1.14.0  \nPyTorch: 1.1.0 to 1.2.0  \nTorchvision: 0.3.0 to 0.4.0  \nMLflow: 1.2.0 to 1.3.0\n\nSystem environment\nSystem environment\nThe system environment in Databricks Runtime 6.1 ML differs from Databricks Runtime 6.1 as follows:  \nDBUtils: Does not contain Library utility (dbutils.library) (legacy).  \nFor GPU clusters, the following NVIDIA GPU libraries:  \nNVIDIA driver 418.40  \nCUDA 10.0  \nCUDNN 7.6.0\n\nLibraries"
    },
    {
        "id": 201,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "Libraries\nThe following sections list the libraries included in Databricks Runtime 6.1 ML that differ from those included in Databricks Runtime 6.1.  \nTop-tier libraries  \nDatabricks Runtime 6.1 ML includes the following top-tier libraries:  \nGraphFrames  \nHorovod and HorovodRunner  \nMLflow  \nPyTorch  \nspark-tensorflow-connector  \nTensorFlow  \nTensorBoard  \nPython libraries  \nDatabricks Runtime 6.1 ML uses Conda for Python package management and includes many popular ML packages. The following section describes the Conda environment for Databricks Runtime 6.1 ML.  \nPython on CPU clusters"
    },
    {
        "id": 202,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "name: databricks-ml channels: - Databricks - pytorch - defaults dependencies: - _libgcc_mutex=0.1=main - _py-xgboost-mutex=2.0=cpu_0 - _tflow_select=2.3.0=mkl - absl-py=0.8.0=py37_0 - asn1crypto=0.24.0=py37_0 - astor=0.8.0=py37_0 - backcall=0.1.0=py37_0 - backports=1.0=py_2 - bcrypt=3.1.7=py37h7b6447c_0 - blas=1.0=mkl - boto=2.49.0=py37_0 - boto3=1.9.162=py_0 - botocore=1.12.163=py_0 - c-ares=1.15.0=h7b6447c_1001 - ca-certificates=2019.1.23=0 - certifi=2019.3.9=py37_0 - cffi=1.12.2=py37h2e261b9_1 - chardet=3.0.4=py37_1003 - click=7.0=py37_0 - cloudpickle=0.8.0=py37_0 - colorama=0.4.1=py37_0 - configparser=3.7.4=py37_0 - cpuonly=1.0=0 - cryptography=2.6.1=py37h1ba5d50_0 - cycler=0.10.0=py37_0 - cython=0.29.6=py37he6710b0_0 - decorator=4.4.0=py37_1 - docutils=0.14=py37_0 - entrypoints=0.3=py37_0 - et_xmlfile=1.0.1=py37_0 -"
    },
    {
        "id": 203,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "- docutils=0.14=py37_0 - entrypoints=0.3=py37_0 - et_xmlfile=1.0.1=py37_0 - flask=1.0.2=py37_1 - freetype=2.9.1=h8a8886c_1 - future=0.17.1=py37_0 - gast=0.3.2=py_0 - gitdb2=2.0.5=py37_0 - gitpython=2.1.11=py37_0 - google-pasta=0.1.7=py_0 - grpcio=1.16.1=py37hf8bcb03_1 - gunicorn=19.9.0=py37_0 - h5py=2.9.0=py37h7918eee_0 - hdf5=1.10.4=hb1b8bf9_0 - html5lib=1.0.1=py_0 - icu=58.2=h9c2bf20_1 - idna=2.8=py37_0 - intel-openmp=2019.3=199 - ipython=7.4.0=py37h39e3cac_0 - ipython_genutils=0.2.0=py37_0 - itsdangerous=1.1.0=py37_0 - jdcal=1.4=py37_0 - jedi=0.13.3=py37_0 - jinja2=2.10=py37_0 - jmespath=0.9.4=py_0 - jpeg=9b=h024ee3a_2 - keras=2.2.4=0 - keras-applications=1.0.8=py_0 - keras-base=2.2.4=py37_0 - keras-preprocessing=1.1.0=py_1 - kiwisolver=1.0.1=py37hf484d3e_0 -"
    },
    {
        "id": 204,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "- keras-preprocessing=1.1.0=py_1 - kiwisolver=1.0.1=py37hf484d3e_0 - krb5=1.16.1=h173b8e3_7 - libedit=3.1.20181209=hc058e9b_0 - libffi=3.2.1=hd88cf55_4 - libgcc-ng=8.2.0=hdf63c60_1 - libgfortran-ng=7.3.0=hdf63c60_0 - libpng=1.6.36=hbc83047_0 - libpq=11.2=h20c2e04_0 - libprotobuf=3.9.2=hd408876_0 - libsodium=1.0.16=h1bed415_0 - libstdcxx-ng=8.2.0=hdf63c60_1 - libtiff=4.0.10=h2733197_2 - libxgboost=0.90=he6710b0_0 - libxml2=2.9.9=hea5a465_1 - libxslt=1.1.33=h7d1a2b0_0 - llvmlite=0.28.0=py37hd408876_0 - lxml=4.3.2=py37hefd8a0e_0 - mako=1.0.10=py_0 - markdown=3.1.1=py37_0 - markupsafe=1.1.1=py37h7b6447c_0 - mkl=2019.3=199 - mkl_fft=1.0.10=py37ha843d7b_0 - mkl_random=1.0.2=py37hd81dba3_0 -"
    },
    {
        "id": 205,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "- mkl_fft=1.0.10=py37ha843d7b_0 - mkl_random=1.0.2=py37hd81dba3_0 - ncurses=6.1=he6710b0_1 - networkx=2.2=py37_1 - ninja=1.9.0=py37hfd86e86_0 - nose=1.3.7=py37_2 - numba=0.43.1=py37h962f231_0 - numpy=1.16.2=py37h7e9f1db_0 - numpy-base=1.16.2=py37hde5b4d6_0 - olefile=0.46=py37_0 - openpyxl=2.6.1=py37_1 - openssl=1.1.1b=h7b6447c_1 - pandas=0.24.2=py37he6710b0_0 - paramiko=2.4.2=py37_0 - parso=0.3.4=py37_0 - pathlib2=2.3.3=py37_0 - patsy=0.5.1=py37_0 - pexpect=4.6.0=py37_0 - pickleshare=0.7.5=py37_0 - pillow=5.4.1=py37h34e0f95_0 - pip=19.0.3=py37_0 - ply=3.11=py37_0 - prompt_toolkit=2.0.9=py37_0 - protobuf=3.9.2=py37he6710b0_0 - psutil=5.6.1=py37h7b6447c_0 - psycopg2=2.7.6.1=py37h1ba5d50_0 - ptyprocess=0.6.0=py37_0 -"
    },
    {
        "id": 206,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "- psycopg2=2.7.6.1=py37h1ba5d50_0 - ptyprocess=0.6.0=py37_0 - py-xgboost=0.90=py37he6710b0_0 - py-xgboost-cpu=0.90=py37_0 - pyasn1=0.4.7=py_0 - pycparser=2.19=py37_0 - pygments=2.3.1=py37_0 - pymongo=3.8.0=py37he6710b0_1 - pynacl=1.3.0=py37h7b6447c_0 - pyopenssl=19.0.0=py37_0 - pyparsing=2.3.1=py37_0 - pysocks=1.6.8=py37_0 - python=3.7.3=h0371630_0 - python-dateutil=2.8.0=py37_0 - python-editor=1.0.4=py_0 - pytorch=1.2.0=py3.7_cpu_0 - pytz=2018.9=py37_0 - pyyaml=5.1=py37h7b6447c_0 - readline=7.0=h7b6447c_5 - requests=2.21.0=py37_0 - s3transfer=0.2.1=py37_0 - scikit-learn=0.20.3=py37hd81dba3_0 - scipy=1.2.1=py37h7c811a0_0 - setuptools=40.8.0=py37_0 - simplejson=3.16.0=py37h14c3975_0 - singledispatch=3.4.0.3=py37_0 - six=1.12.0=py37_0 - smmap2=2.0.5=py37_0 -"
    },
    {
        "id": 207,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "- singledispatch=3.4.0.3=py37_0 - six=1.12.0=py37_0 - smmap2=2.0.5=py37_0 - sqlite=3.27.2=h7b6447c_0 - sqlparse=0.3.0=py_0 - statsmodels=0.9.0=py37h035aef0_0 - tabulate=0.8.3=py37_0 - tensorboard=1.14.0=py37hf484d3e_0 - tensorflow=1.14.0+db1=mkl_py37h0f35a5d_0 - tensorflow-base=1.14.0+db1=mkl_py37h7ce6ba3_0 - tensorflow-estimator=1.14.0+db1=py_0 - tensorflow-mkl=1.14.0+db1=h4fcabd2_0 - termcolor=1.1.0=py37_1 - tk=8.6.8=hbc83047_0 - torchvision=0.4.0=py37_cpu - tqdm=4.31.1=py37_1 - traitlets=4.3.2=py37_0 - urllib3=1.24.1=py37_0 - virtualenv=16.0.0=py37_0 - wcwidth=0.1.7=py37_0 - webencodings=0.5.1=py37_1 - websocket-client=0.56.0=py37_0 - werkzeug=0.14.1=py37_0 - wheel=0.33.1=py37_0 - wrapt=1.11.1=py37h7b6447c_0 - xz=5.2.4=h14c3975_4 -"
    },
    {
        "id": 208,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "- wrapt=1.11.1=py37h7b6447c_0 - xz=5.2.4=h14c3975_4 - yaml=0.1.7=had09818_2 - zlib=1.2.11=h7b6447c_3 - zstd=1.3.7=h0b5b093_0 - pip: - argparse==1.4.0 - databricks-cli==0.9.0 - docker==4.1.0 - fusepy==2.0.4 - gorilla==0.3.0 - horovod==0.18.1 - hyperopt==0.1.2.db8 - matplotlib==3.0.3 - mleap==0.8.1 - mlflow==1.3.0 - nose-exclude==0.5.0 - pyarrow==0.13.0 - querystring-parser==1.2.4 - seaborn==0.9.0 - tensorboardx==1.8+db1 prefix: /databricks/conda/envs/databricks-ml"
    },
    {
        "id": 209,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "name: databricks-ml-gpu channels: - Databricks - pytorch - defaults dependencies: - _libgcc_mutex=0.1=main - _py-xgboost-mutex=1.0=gpu_0 - _tflow_select=2.1.0=gpu - absl-py=0.8.0=py37_0 - asn1crypto=0.24.0=py37_0 - astor=0.8.0=py37_0 - backcall=0.1.0=py37_0 - backports=1.0=py_2 - bcrypt=3.1.7=py37h7b6447c_0 - blas=1.0=mkl - boto=2.49.0=py37_0 - boto3=1.9.162=py_0 - botocore=1.12.163=py_0 - c-ares=1.15.0=h7b6447c_1001 - ca-certificates=2019.1.23=0 - certifi=2019.3.9=py37_0 - cffi=1.12.2=py37h2e261b9_1 - chardet=3.0.4=py37_1003 - click=7.0=py37_0 - cloudpickle=0.8.0=py37_0 - colorama=0.4.1=py37_0 - configparser=3.7.4=py37_0 - cryptography=2.6.1=py37h1ba5d50_0 - cudatoolkit=10.0.130=0 - cudnn=7.6.0=cuda10.0_0 - cupti=10.0.130=0 - cycler=0.10.0=py37_0 - cython=0.29.6=py37he6710b0_0 - decorator=4.4.0=py37_1 - docutils=0.14=py37_0 -"
    },
    {
        "id": 210,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "- cython=0.29.6=py37he6710b0_0 - decorator=4.4.0=py37_1 - docutils=0.14=py37_0 - entrypoints=0.3=py37_0 - et_xmlfile=1.0.1=py37_0 - flask=1.0.2=py37_1 - freetype=2.9.1=h8a8886c_1 - future=0.17.1=py37_0 - gast=0.3.2=py_0 - gitdb2=2.0.5=py37_0 - gitpython=2.1.11=py37_0 - google-pasta=0.1.7=py_0 - grpcio=1.16.1=py37hf8bcb03_1 - gunicorn=19.9.0=py37_0 - h5py=2.9.0=py37h7918eee_0 - hdf5=1.10.4=hb1b8bf9_0 - html5lib=1.0.1=py_0 - icu=58.2=h9c2bf20_1 - idna=2.8=py37_0 - intel-openmp=2019.3=199 - ipython=7.4.0=py37h39e3cac_0 - ipython_genutils=0.2.0=py37_0 - itsdangerous=1.1.0=py37_0 - jdcal=1.4=py37_0 - jedi=0.13.3=py37_0 - jinja2=2.10=py37_0 - jmespath=0.9.4=py_0 - jpeg=9b=h024ee3a_2 - keras=2.2.4=0 - keras-applications=1.0.8=py_0 - keras-base=2.2.4=py37_0 -"
    },
    {
        "id": 211,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "- keras=2.2.4=0 - keras-applications=1.0.8=py_0 - keras-base=2.2.4=py37_0 - keras-preprocessing=1.1.0=py_1 - kiwisolver=1.0.1=py37hf484d3e_0 - krb5=1.16.1=h173b8e3_7 - libedit=3.1.20181209=hc058e9b_0 - libffi=3.2.1=hd88cf55_4 - libgcc-ng=8.2.0=hdf63c60_1 - libgfortran-ng=7.3.0=hdf63c60_0 - libpng=1.6.36=hbc83047_0 - libpq=11.2=h20c2e04_0 - libprotobuf=3.9.2=hd408876_0 - libsodium=1.0.16=h1bed415_0 - libstdcxx-ng=8.2.0=hdf63c60_1 - libtiff=4.0.10=h2733197_2 - libxgboost=0.90=h688424c_0 - libxml2=2.9.9=hea5a465_1 - libxslt=1.1.33=h7d1a2b0_0 - llvmlite=0.28.0=py37hd408876_0 - lxml=4.3.2=py37hefd8a0e_0 - mako=1.0.10=py_0 - markdown=3.1.1=py37_0 - markupsafe=1.1.1=py37h7b6447c_0 - mkl=2019.3=199 -"
    },
    {
        "id": 212,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "- markdown=3.1.1=py37_0 - markupsafe=1.1.1=py37h7b6447c_0 - mkl=2019.3=199 - mkl_fft=1.0.10=py37ha843d7b_0 - mkl_random=1.0.2=py37hd81dba3_0 - ncurses=6.1=he6710b0_1 - networkx=2.2=py37_1 - ninja=1.9.0=py37hfd86e86_0 - nose=1.3.7=py37_2 - numba=0.43.1=py37h962f231_0 - numpy=1.16.2=py37h7e9f1db_0 - numpy-base=1.16.2=py37hde5b4d6_0 - olefile=0.46=py37_0 - openpyxl=2.6.1=py37_1 - openssl=1.1.1b=h7b6447c_1 - pandas=0.24.2=py37he6710b0_0 - paramiko=2.4.2=py37_0 - parso=0.3.4=py37_0 - pathlib2=2.3.3=py37_0 - patsy=0.5.1=py37_0 - pexpect=4.6.0=py37_0 - pickleshare=0.7.5=py37_0 - pillow=5.4.1=py37h34e0f95_0 - pip=19.0.3=py37_0 - ply=3.11=py37_0 - prompt_toolkit=2.0.9=py37_0 - protobuf=3.9.2=py37he6710b0_0 - psutil=5.6.1=py37h7b6447c_0 -"
    },
    {
        "id": 213,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "- protobuf=3.9.2=py37he6710b0_0 - psutil=5.6.1=py37h7b6447c_0 - psycopg2=2.7.6.1=py37h1ba5d50_0 - ptyprocess=0.6.0=py37_0 - py-xgboost=0.90=py37h688424c_0 - py-xgboost-gpu=0.90=py37h28bbb66_0 - pyasn1=0.4.7=py_0 - pycparser=2.19=py37_0 - pygments=2.3.1=py37_0 - pymongo=3.8.0=py37he6710b0_1 - pynacl=1.3.0=py37h7b6447c_0 - pyopenssl=19.0.0=py37_0 - pyparsing=2.3.1=py37_0 - pysocks=1.6.8=py37_0 - python=3.7.3=h0371630_0 - python-dateutil=2.8.0=py37_0 - python-editor=1.0.4=py_0 - pytorch=1.2.0=py3.7_cuda10.0.130_cudnn7.6.2_0 - pytz=2018.9=py37_0 - pyyaml=5.1=py37h7b6447c_0 - readline=7.0=h7b6447c_5 - requests=2.21.0=py37_0 - s3transfer=0.2.1=py37_0 - scikit-learn=0.20.3=py37hd81dba3_0 - scipy=1.2.1=py37h7c811a0_0 - setuptools=40.8.0=py37_0 -"
    },
    {
        "id": 214,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "- scipy=1.2.1=py37h7c811a0_0 - setuptools=40.8.0=py37_0 - simplejson=3.16.0=py37h14c3975_0 - singledispatch=3.4.0.3=py37_0 - six=1.12.0=py37_0 - smmap2=2.0.5=py37_0 - sqlite=3.27.2=h7b6447c_0 - sqlparse=0.3.0=py_0 - statsmodels=0.9.0=py37h035aef0_0 - tabulate=0.8.3=py37_0 - tensorboard=1.14.0=py37hf484d3e_0 - tensorflow=1.14.0+db1=gpu_py37h517d0a7_0 - tensorflow-base=1.14.0+db1=gpu_py37he292aa2_0 - tensorflow-estimator=1.14.0+db1=py_0 - tensorflow-gpu=1.14.0+db1=h0d30ee6_0 - termcolor=1.1.0=py37_1 - tk=8.6.8=hbc83047_0 - torchvision=0.4.0=py37_cu100 - tqdm=4.31.1=py37_1 - traitlets=4.3.2=py37_0 - urllib3=1.24.1=py37_0 - virtualenv=16.0.0=py37_0 - wcwidth=0.1.7=py37_0 - webencodings=0.5.1=py37_1 - websocket-client=0.56.0=py37_0 - werkzeug=0.14.1=py37_0 -"
    },
    {
        "id": 215,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "- webencodings=0.5.1=py37_1 - websocket-client=0.56.0=py37_0 - werkzeug=0.14.1=py37_0 - wheel=0.33.1=py37_0 - wrapt=1.11.1=py37h7b6447c_0 - xz=5.2.4=h14c3975_4 - yaml=0.1.7=had09818_2 - zlib=1.2.11=h7b6447c_3 - zstd=1.3.7=h0b5b093_0 - pip: - argparse==1.4.0 - databricks-cli==0.9.0 - docker==4.1.0 - fusepy==2.0.4 - gorilla==0.3.0 - horovod==0.18.1 - hyperopt==0.1.2.db8 - matplotlib==3.0.3 - mleap==0.8.1 - mlflow==1.3.0 - nose-exclude==0.5.0 - pyarrow==0.13.0 - querystring-parser==1.2.4 - seaborn==0.9.0 - tensorboardx==1.8+db1 prefix: /databricks/conda/envs/databricks-ml-gpu"
    },
    {
        "id": 216,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.1ml.html",
        "content": "Spark packages containing Python modules  \nSpark Package  \nPython Module  \nVersion  \ngraphframes  \ngraphframes  \n0.7.0-db1-spark2.4  \nspark-deep-learning  \nsparkdl  \n1.5.0-db5-spark2.4  \ntensorframes  \ntensorframes  \n0.8.1-s_2.11  \nR libraries  \nThe R libraries are identical to the R Libraries in Databricks Runtime 6.1.  \nJava and Scala libraries (Scala 2.11 cluster)  \nIn addition to Java and Scala libraries in Databricks Runtime 6.1, Databricks Runtime 6.1 ML contains the following JARs:  \nGroup ID  \nArtifact ID  \nVersion  \ncom.databricks  \nspark-deep-learning  \n1.5.0-db5-spark2.4  \ncom.typesafe.akka  \nakka-actor_2.11  \n2.3.11  \nml.combust.mleap  \nmleap-databricks-runtime_2.11  \n0.14.0  \nml.dmlc  \nxgboost4j  \n0.90  \nml.dmlc  \nxgboost4j-spark  \n0.90  \norg.graphframes  \ngraphframes_2.11  \n0.7.0-db1-spark2.4  \norg.mlflow  \nmlflow-client  \n1.3.0  \norg.tensorflow  \nlibtensorflow  \n1.14.0  \norg.tensorflow  \nlibtensorflow_jni  \n1.14.0  \norg.tensorflow  \nspark-tensorflow-connector_2.11  \n1.14.0  \norg.tensorflow  \ntensorflow  \n1.14.0  \norg.tensorframes  \ntensorframes  \n0.8.1-s_2.11"
    },
    {
        "id": 217,
        "url": "https://docs.databricks.com/en/admin/account-settings/standard-tier.html",
        "content": "End of life for Standard tier workspaces on Databricks on AWS  \nDatabricks has announced the end of life for Standard tier workspaces on Databricks on AWS.  \nCustomers have until October 1, 2025, to upgrade existing workspaces to the Premium or Enterprise tier. Any remaining Standard tier workspaces on October 1, 2025, will be automatically upgraded to the Premium tier. Refer to the pricing page to assess how this upgrade might impact your bill.  \nStarting immediately, all new subscriptions must be created on the Premium or Enterprise tier. Existing Standard tier customers can continue to create new Standard workspaces until Oct 1, 2024, and they have until Oct 1, 2025, to upgrade any new workspace to the Premium or Enterprise tier.  \nWhat features do I get when upgrading from the Standard tier?"
    },
    {
        "id": 218,
        "url": "https://docs.databricks.com/en/admin/account-settings/standard-tier.html",
        "content": "What features do I get when upgrading from the Standard tier?\nStandard  \nPremium  \nEnterprise  \nDatabricks workspace with managed Spark  \n\u2714  \n\u2714  \n\u2714  \nUnity Catalog  \n\u2718  \n\u2714  \n\u2714  \nServerless offerings  \n\u2718  \n\u2714  \n\u2714  \nSQL  \n\u2718  \n\u2714  \n\u2714  \nMosaic/AI  \n\u2718  \n\u2714  \n\u2714  \nAssistant / Databricks Intelligence  \n\u2718  \n\u2714  \n\u2714  \nLakehouse Monitoring / Predictive Optimization  \n\u2718  \n\u2714  \n\u2714  \nGovernance and Manageability  \n\u2718  \n\u2714  \n\u2714  \nAdvanced Enterprise Security  \n\u2718  \n\u25d1  \n\u2714\n\nWill I lose access to any features?\nWill I lose access to any features?\nNo, you will not lose access to any capabilities. Higher tiers include all the features that are currently available in the Standard tier.\n\nHow do I upgrade my workspace to a higher tier?\nHow do I upgrade my workspace to a higher tier?\nRefer to our documentation for details on how to upgrade your account to a higher tier. See Manage your subscription.\n\nHow do I know the tier for my workspace?\nHow do I know the tier for my workspace?\nYou can view a workspace\u2019s current tier in the Workspaces page your account console."
    },
    {
        "id": 219,
        "url": "https://docs.databricks.com/en/admin/account-settings/index.html",
        "content": "Manage your Databricks account  \nDatabricks account-level configurations are managed by account admins. This article includes various settings the account admin can manage through the account console. The other articles in this section cover additional tasks performed by account admins.  \nAs an account admin, you can also manage your Databricks account using the Account API.  \nManage account console settings\nManage account console settings\nThe following are account console settings available to account admins.\n\nLocate your account ID\nLocate your account ID\nTo retrieve your account ID, go to the account console and click the down arrow next to your username in the upper right corner. In the drop-down menu you can view and copy your Account ID.  \nYou must be in the account console to retrieve the account ID, the ID will not display inside a workspace.\n\nAdd an account nickname"
    },
    {
        "id": 220,
        "url": "https://docs.databricks.com/en/admin/account-settings/index.html",
        "content": "Add an account nickname\nTo help easily identify your Databricks account in the Databricks UI, give your account a human-readable nickname. This nickname displays at the top of the account console and in the dropdown menu next to your account ID. Account nicknames are especially useful if you have more than one Databricks account.  \nTo add an account nickname:  \nIn the account console, click Settings.  \nClick the Account settings tab.  \nUnder Account name, enter your new account nickname then click Save.  \nYou can update account nicknames at any time.\n\nChange the account console language settings\nChange the account console language settings\nThe account console is available in multiple languages. To change the account console language, select Settings then go to the Preferences tab.\n\nManage email preferences"
    },
    {
        "id": 221,
        "url": "https://docs.databricks.com/en/admin/account-settings/index.html",
        "content": "Manage email preferences\nDatabricks can occasionally send emails with personalized product and feature recommendations based on your use of Databricks. These messages may include information to help users get started with Databricks or learn about new features and previews.  \nYou can manage whether you receive these emails in the account console:  \nLog in to the account console and click the Settings icon in the sidebar.  \nIn the My preferences section, click the Instructional product and feature emails toggle.  \nYou can also manage your promotional email communications by clicking Manage under Promotional email communications or by going to the Marketing preference center. Non-admin users can update this setting by clicking the My preferences link next to their workspace in the account console."
    },
    {
        "id": 222,
        "url": "https://docs.databricks.com/en/admin/clusters/automatic-cluster-update.html",
        "content": "Automatic cluster update  \nAutomatic cluster update ensures that all the clusters in a workspace are periodically updated to the latest host OS image and security updates. Account admins can schedule the maintenance window frequency, start date, and start time.  \nIf the compliance security profile is enabled, automatic cluster update is automatically enabled, but it can be enabled independently. See Automatic cluster update. If there are no updates for images for running compute resources, by default they are not restarted but you can configure the feature to force restart during the maintenance window.  \nThis applies to all compute resources that run in the classic compute plane: clusters, pools, classic SQL warehouses, and legacy Model Serving. It does not apply to serverless compute resources.  \nEnabling this feature on a workspace requires that you add the Enhanced Security and Compliance add-on as described on the pricing page. This feature also requires the Enterprise pricing tier.  \nBy default, automatic cluster update is scheduled for the first Sunday of every month at 1:00 AM UTC. Account admins can use the Databricks account console to change the maintenance window frequency, start date, and start time. See Configure automatic cluster update.  \nEnable automatic cluster update on a workspace"
    },
    {
        "id": 223,
        "url": "https://docs.databricks.com/en/admin/clusters/automatic-cluster-update.html",
        "content": "Enable automatic cluster update on a workspace\nImportant  \nYou must be an account admin to configure automatic cluster update. Although this setting is configured for each workspace, the controls for this feature are part of the account console UI, not the workspace admin console.  \nIf you enable the compliance security profile, this feature is automatically enabled. To separately enable automatic cluster update on a workspace, see Enable enhanced security and compliance features on an existing workspace.  \nYou can also set an account-level default for new workspaces to enable automatic cluster update initially. Alternatively, you can set an account-level default to enable the compliance security profile, which automatically enables automatic cluster update. See Set account-level defaults for all new workspaces.  \nUpdates may take up to six hours to propagate to all environments and to downstream systems like billing. Workloads that are actively running continue with the settings that were active at the time of starting the cluster or other compute resource, and new settings will start applying the next time these workloads are started. This means that if a change is made late in the day or on the last day of the month, you might still see usage reported with the old settings early the next day or month.  \nRestart any compute resources to ensure they immediately get the latest updates."
    },
    {
        "id": 224,
        "url": "https://docs.databricks.com/en/admin/clusters/automatic-cluster-update.html",
        "content": "Set default for new workspaces to enable automatic cluster update\nSet default for new workspaces to enable automatic cluster update\nAccount admins can set account-level defaults for new workspaces for the compliance security profile with optional compliance standards. See Set account-level defaults for all new workspaces.\n\nConfigure automatic cluster update\nConfigure automatic cluster update\nGo to the Databricks account console.  \nNavigate directly to https://accounts.cloud.databricks.com.  \nIn the sidebar, click Workspaces.  \nClick the name of your workspace.  \nClick Security.  \nNext to Automatic cluster update, click the Configure button.  \nSet the maintenance frequency.  \nFor monthly schedules, choose which numbered week such as 1st or 3rd.  \nFor twice-monthly schedules, choose 1st and 3rd or 2nd and 4th.  \nSet the day and time for your maintenance window.  \nBy default, compute resources only restart if updates are available. To force clusters and other compute resources to restart during this maintenance window regardless of the availability of a new update, select Always restart.  \nClick Save."
    },
    {
        "id": 225,
        "url": "https://docs.databricks.com/en/archive/single-sign-on/ping-identity.html",
        "content": "Configure SSO with Ping Identity for your workspace  \nWarning  \nWorkspace-level SSO is a legacy configuration. It can only be configured when unified login is disabled. When unified login is enabled, your workspace uses the same SSO configuration as your account.  \nIf your account was created after June 21, 2023, unified login is enabled on your account by default for all workspaces, new and existing, and it cannot be disabled.  \nDatabricks recommends enabling unified login on all workspaces. See Enable unified login.  \nThis documentation has been retired and might not be updated.  \nThis article shows how to configure Ping Identity as the identity provider for a Databricks workspace. To configure SSO in your Databricks account, see Configure SSO in Databricks.  \nGather required information\nGather required information\nAs a workspace admin, log in to the Databricks workspace.  \nClick your username in the top bar of the Databricks workspace and select Settings.  \nClick on the Identity and access tab.  \nNext to SSO settings, click Manage.  \nCopy the Databricks SAML URL.  \nDo not close this browser tab.\n\nConfigure Ping Identity"
    },
    {
        "id": 226,
        "url": "https://docs.databricks.com/en/archive/single-sign-on/ping-identity.html",
        "content": "Configure Ping Identity\nIn a new browser tab, log in to Ping Identity as an administrator.  \nInside the PingOne admin portal, click the Connections icon. It looks like a flow chart connector.  \nClick +Add Application.  \nClick Advanced Configuration.  \nNext to SAML, click Configure.  \nSet Application Name to Databricks, then click Next.  \nFor Provide App Metadata, click Manually Enter.  \nEnter the Databricks SAML URL from Gather required information into the following fields:  \nACS URL  \nEntity ID  \nSLO Endpoint  \nSLO Response Endpoint  \nTarget Application URL  \nUnder Signing Key, select Sign Response or Sign Assertion and Response.  \nImportant  \nDo not select Enable Encryption or Enforce Signed Authn Request.  \nSet Assertion Validity to a value between 30 and 180 seconds. For more details, see Accounting for Time Drift Between SAML Endpoints in the Ping Identity knowledge base.  \nClick Save and Continue.  \nUnder SAML Attributes, set PINGONE USER ATTRIBUTE to Email Address.  \nClick Save and Close. The SAML application appears.  \nClick Configuration.  \nClick Download Metadata.  \nOpen the downloaded XML file in a text editor.\n\nConfigure Databricks"
    },
    {
        "id": 227,
        "url": "https://docs.databricks.com/en/archive/single-sign-on/ping-identity.html",
        "content": "Configure Databricks\nGo back to the browser tab for Databricks.  \nClick your username in the top bar of the Databricks workspace and select Settings.  \nClick on the Identity and access tab.  \nNext to SSO settings, click Manage.  \nSet both Single Sign-On URL and Identity Provider Entity ID to the value of the Location attribute of the <SingleSignOnService> tag in the XML file you downloaded from Ping Identity.  \nSet x.509 Certificate to the value of the <ds:X509Certificate> tag in the XML file you downloaded from Ping Identity.  \nClick Enable SSO.  \nOptionally, click Allow auto user creation.\n\nTest the configuration\nTest the configuration\nIn an incognito browser window, go to your Databricks workspace.  \nClick Single Sign On. You are redirected to Ping Identity.  \nLog in to Ping Identity. If SSO is configured correctly, you are redirected to Databricks.  \nIf the test fails, review Troubleshooting."
    },
    {
        "id": 228,
        "url": "https://docs.databricks.com/en/archive/connectors/spark-xml-library.html",
        "content": "Read and write XML data using the spark-xml library  \nImportant  \nThis documentation has been retired and might not be updated. The products, services, or technologies mentioned int his content are not officially endorsed or tested by Databricks.  \nNative XML file format support is available as a Public Preview. See Read and write XML files.  \nThis article describes how to read and write an XML file as an Apache Spark data source.  \nRequirements\nRequirements\nCreate the spark-xml library as a Maven library. For the Maven coordinate, specify:  \nDatabricks Runtime 7.x and above: com.databricks:spark-xml_2.12:<release>  \nSee spark-xml Releases for the latest version of <release>.  \nInstall the library on a cluster.\n\nExample"
    },
    {
        "id": 229,
        "url": "https://docs.databricks.com/en/archive/connectors/spark-xml-library.html",
        "content": "Example\nThe example in this section uses the books XML file.  \nRetrieve the books XML file:  \n$ wget https://github.com/databricks/spark-xml/raw/master/src/test/resources/books.xml  \nUpload the file to DBFS.  \nRead and write XML data  \n/*Infer schema*/ CREATE TABLE books USING xml OPTIONS (path \"dbfs:/books.xml\", rowTag \"book\") /*Specify column names and types*/ CREATE TABLE books (author string, description string, genre string, _id string, price double, publish_date string, title string) USING xml OPTIONS (path \"dbfs:/books.xml\", rowTag \"book\")"
    },
    {
        "id": 230,
        "url": "https://docs.databricks.com/en/archive/connectors/spark-xml-library.html",
        "content": "// Infer schema import com.databricks.spark.xml._ // Add the DataFrame.read.xml() method val df = spark.read .option(\"rowTag\", \"book\") .xml(\"dbfs:/books.xml\") val selectedData = df.select(\"author\", \"_id\") selectedData.write .option(\"rootTag\", \"books\") .option(\"rowTag\", \"book\") .xml(\"dbfs:/newbooks.xml\") // Specify schema import org.apache.spark.sql.types.{StructType, StructField, StringType, DoubleType} val customSchema = StructType(Array( StructField(\"_id\", StringType, nullable = true), StructField(\"author\", StringType, nullable = true), StructField(\"description\", StringType, nullable = true), StructField(\"genre\", StringType, nullable = true), StructField(\"price\", DoubleType, nullable = true), StructField(\"publish_date\", StringType, nullable = true), StructField(\"title\", StringType, nullable = true))) val df = spark.read .option(\"rowTag\", \"book\") .schema(customSchema) .xml(\"books.xml\") val selectedData = df.select(\"author\", \"_id\") selectedData.write .option(\"rootTag\", \"books\") .option(\"rowTag\", \"book\") .xml(\"dbfs:/newbooks.xml\")"
    },
    {
        "id": 231,
        "url": "https://docs.databricks.com/en/archive/connectors/spark-xml-library.html",
        "content": "# Infer schema library(SparkR) sparkR.session(\"local[4]\", sparkPackages = c(\"com.databricks:spark-xml_2.12:<release>\")) df <- read.df(\"dbfs:/books.xml\", source = \"xml\", rowTag = \"book\") # Default `rootTag` and `rowTag` write.df(df, \"dbfs:/newbooks.xml\", \"xml\") # Specify schema customSchema <- structType( structField(\"_id\", \"string\"), structField(\"author\", \"string\"), structField(\"description\", \"string\"), structField(\"genre\", \"string\"), structField(\"price\", \"double\"), structField(\"publish_date\", \"string\"), structField(\"title\", \"string\")) df <- read.df(\"dbfs:/books.xml\", source = \"xml\", schema = customSchema, rowTag = \"book\") # In this case, `rootTag` is set to \"ROWS\" and `rowTag` is set to \"ROW\". write.df(df, \"dbfs:/newbooks.xml\", \"xml\", \"overwrite\")"
    },
    {
        "id": 232,
        "url": "https://docs.databricks.com/en/archive/connectors/spark-xml-library.html",
        "content": "Options\nRead  \npath: Location of XML files. Accepts standard Hadoop globbing expressions.  \nrowTag: The row tag to treat as a row. For example, in this XML <books><book><book>...</books>, the value would be book. Default is ROW.  \nsamplingRatio: Sampling ratio for inferring schema (0.0 ~ 1). Default is 1. Possible types are StructType, ArrayType, StringType, LongType, DoubleType, BooleanType, TimestampType and NullType, unless you provide a schema.  \nexcludeAttribute: Whether to exclude attributes in elements. Default is false.  \nnullValue: The value to treat as a null value. Default is \"\".  \nmode: The mode for dealing with corrupt records. Default is PERMISSIVE.  \nPERMISSIVE:  \nWhen it encounters a corrupted record, sets all fields to null and puts the malformed string into a new field configured by columnNameOfCorruptRecord.  \nWhen it encounters a field of the wrong data type, sets the offending field to null.  \nDROPMALFORMED: ignores corrupted records.  \nFAILFAST: throws an exception when it detects corrupted records.  \ninferSchema: if true, attempts to infer an appropriate type for each resulting DataFrame column, like a boolean, numeric or date type. If false, all resulting columns are of string type. Default is true.  \ncolumnNameOfCorruptRecord: The name of new field where malformed strings are stored. Default is _corrupt_record.  \nattributePrefix: The prefix for attributes so that to differentiate attributes and elements. This is the prefix for field names. Default is _.  \nvalueTag: The tag used for the value when there are attributes in an element that has no child elements. Default is _VALUE.  \ncharset: Defaults to UTF-8 but can be set to other valid charset names.  \nignoreSurroundingSpaces: Whether or not whitespaces surrounding values should be skipped. Default is false."
    },
    {
        "id": 233,
        "url": "https://docs.databricks.com/en/archive/connectors/spark-xml-library.html",
        "content": "charset: Defaults to UTF-8 but can be set to other valid charset names.  \nignoreSurroundingSpaces: Whether or not whitespaces surrounding values should be skipped. Default is false.  \nrowValidationXSDPath: Path to an XSD file that is used to validate the XML for each row. Rows that fail to validate are treated like parse errors as above. The XSD does not otherwise affect the schema provided or inferred. If the same local path is not already also visible on the executors in the cluster, then the XSD and any others it depends on should be added to the Spark executors with SparkContext.addFile. In this case, to use local XSD /foo/bar.xsd, call addFile(\"/foo/bar.xsd\") and pass \"bar.xsd\" as rowValidationXSDPath.  \nWrite  \npath: Location to write files.  \nrowTag: The row tag to treat as a row. For example, in this XML <books><book><book>...</books>, the value would be book. Default is ROW.  \nrootTag: The root tag to treat as the root. For example, in this XML <books><book><book>...</books>, the value would be books. Default is ROWS.  \nnullValue: The value to write null value. Default is the string \"null\". When \"null\", it does not write attributes and elements for fields.  \nattributePrefix: The prefix for attributes to differentiate attributes and elements. This is the prefix for field names. Default is _.  \nvalueTag: The tag used for the value when there are attributes in an element that has no child elements. Default is _VALUE.  \ncompression: Compression codec to use when saving to file. Should be the fully qualified name of a class implementing org.apache.hadoop.io.compress.CompressionCodec or one of case-insensitive short names (bzip2, gzip, lz4, and snappy). Default is no compression.  \nSupports the shortened name usage; You can use xml instead of com.databricks.spark.xml."
    },
    {
        "id": 234,
        "url": "https://docs.databricks.com/en/archive/connectors/spark-xml-library.html",
        "content": "XSD support\nXSD support\nYou can validate individual rows against an XSD schema using rowValidationXSDPath.  \nYou use the utility com.databricks.spark.xml.util.XSDToSchema to extract a Spark DataFrame schema from some XSD files. It supports only simple, complex and sequence types, only basic XSD functionality, and is experimental.  \nimport com.databricks.spark.xml.util.XSDToSchema import java.nio.file.Paths val schema = XSDToSchema.read(Paths.get(\"/path/to/your.xsd\")) val df = spark.read.schema(schema)....xml(...)\n\nParse nested XML"
    },
    {
        "id": 235,
        "url": "https://docs.databricks.com/en/archive/connectors/spark-xml-library.html",
        "content": "Parse nested XML\nAlthough primarily used to convert an XML file into a DataFrame, you can also use the from_xml method to parse XML in a string-valued column in an existing DataFrame and add it as a new column with parsed results as a struct with:  \nimport com.databricks.spark.xml.functions.from_xml import com.databricks.spark.xml.schema_of_xml import spark.implicits._ val df = ... /// DataFrame with XML in column 'payload' val payloadSchema = schema_of_xml(df.select(\"payload\").as[String]) val parsed = df.withColumn(\"parsed\", from_xml($\"payload\", payloadSchema))  \nNote  \nmode:  \nIf set to PERMISSIVE, the default, the parse mode instead defaults to DROPMALFORMED. If you include a column in the schema for from_xml that matches the columnNameOfCorruptRecord, then PERMISSIVE mode outputs malformed records to that column in the resulting struct.  \nIf set to DROPMALFORMED, XML values that do not parse correctly result in a null value for the column. No rows are dropped.  \nfrom_xml converts arrays of strings containing XML to arrays of parsed structs. Use schema_of_xml_array instead.  \nfrom_xml_string is an alternative for use in UDFs that operates on a String directly instead of a column."
    },
    {
        "id": 236,
        "url": "https://docs.databricks.com/en/archive/connectors/spark-xml-library.html",
        "content": "Conversion rules\nDue to structural differences between DataFrames and XML, there are some conversion rules from XML data to DataFrame and from DataFrame to XML data. You can disable handling attributes with the option excludeAttribute.  \nConvert XML to DataFrame  \nAttributes: Attributes are converted as fields with the prefix specified in the attributePrefix option. If attributePrefix is _, the document  \n<one myOneAttrib=\"AAAA\"> <two>two</two> <three>three</three> </one>  \nproduces the schema:  \nroot |-- _myOneAttrib: string (nullable = true) |-- two: string (nullable = true) |-- three: string (nullable = true)  \nIf an element has attributes but no child elements, the attribute value is put in a separate field specified in the valueTag option. If valueTag is _VALUE, the document  \n<one> <two myTwoAttrib=\"BBBBB\">two</two> <three>three</three> </one>  \nproduces the schema:  \nroot |-- two: struct (nullable = true) | |-- _VALUE: string (nullable = true) | |-- _myTwoAttrib: string (nullable = true) |-- three: string (nullable = true)  \nConvert DataFrame to XML  \nWriting an XML file from DataFrame having a field ArrayType with its element as ArrayType would have an additional nested field for the element. This would not happen in reading and writing XML data but in writing a DataFrame read from other sources. Therefore, roundtrip in reading and writing XML files has the same structure but writing a DataFrame read from other sources is possible to have a different structure.  \nA DataFrame with the schema:  \n|-- a: array (nullable = true) | |-- element: array (containsNull = true) | | |-- element: string (containsNull = true)  \nand data:"
    },
    {
        "id": 237,
        "url": "https://docs.databricks.com/en/archive/connectors/spark-xml-library.html",
        "content": "|-- a: array (nullable = true) | |-- element: array (containsNull = true) | | |-- element: string (containsNull = true)  \nand data:  \n+------------------------------------+ | a| +------------------------------------+ |[WrappedArray(aa), WrappedArray(bb)]| +------------------------------------+  \nproduces the XML file:  \n<a> <item>aa</item> </a> <a> <item>bb</item> </a>"
    },
    {
        "id": 238,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Disaster recovery  \nA clear disaster recovery pattern is critical for a cloud-native data analytics platform such as Databricks. It\u2019s critical that your data teams can use the Databricks platform even in the rare case of a regional service-wide cloud-service provider outage, whether caused by a regional disaster like a hurricane or earthquake, or other source.  \nDatabricks is often a core part of an overall data ecosystem that includes many services, including upstream data ingestion services (batch/streaming), cloud native storage such as Amazon S3, downstream tools and services such as business intelligence apps, and orchestration tooling. Some of your use cases might be particularly sensitive to a regional service-wide outage.  \nThis article describes concepts and best practices for a successful interregional disaster recovery solution for the Databricks platform.  \nDisaster recovery overview"
    },
    {
        "id": 239,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Disaster recovery overview\nDisaster recovery involves a set of policies, tools, and procedures that enable the recovery or continuation of vital technology infrastructure and systems following a natural or human-induced disaster. A large cloud service like AWS serves many customers and has built-in guards against a single failure. For example, a region is a group of buildings connected to different power sources to guarantee that a single power loss will not shut down a region. However, cloud region failures can happen, and the degree of disruption and its impact on your organization can vary.  \nBefore implementing a disaster recovery plan, it\u2019s important to understand the difference between disaster recovery (DR) and high availability (HA).  \nHigh availability is a resiliency characteristic of a system. High availability ensures a minimum level of operational performance that is usually defined in terms of consistent uptime or percentage of uptime. High availability is implemented in place (in the same region as your primary system) by designing it as a feature of the primary system. For example, cloud services like AWS have high-availability services such as Amazon S3. High availability does not require significant explicit preparation from the Databricks customer.  \nIn contrast, a disaster recovery plan requires decisions and solutions that work for your specific organization to handle a larger regional outage for critical systems. This article discusses common disaster recovery terminology, common solutions, and some best practices for disaster recovery plans with Databricks."
    },
    {
        "id": 240,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Terminology\nRegion terminology  \nThis article uses the following definitions for regions:  \nPrimary region: The geographic region in which users run typical daily interactive and automated data analytics workloads.  \nSecondary region: The geographic region in which IT teams move data analytics workloads temporarily during an outage in the primary region.  \nGeo-redundant storage: AWS has geo-redundant storage across regions for persisted buckets using an asynchronous storage replication process.  \nImportant  \nFor disaster recovery processes, Databricks recommends that you do not rely on geo-redundant storage for cross-region duplication of data such as your root S3 bucket. In general, use Deep Clone for Delta Tables and convert data to Delta format to use Deep Clone if possible for other data formats.  \nDeployment status terminology  \nThis article uses the following definitions of deployment status:  \nActive deployment: Users can connect to an active deployment of a Databricks workspace and run workloads. Jobs are scheduled periodically using Databricks scheduler or other mechanism. Data streams can be executed on this deployment as well. Some documents might refer to an active deployment as a hot deployment.  \nPassive deployment: Processes do not run on a passive deployment. IT teams can setup automated procedures to deploy code, configuration, and other Databricks objects to the passive deployment. A deployment becomes active only if a current active deployment is down. Some documents might refer to a passive deployment as a cold deployment.  \nImportant  \nA project can optionally include multiple passive deployments in different regions to provide additional options for resolving regional outages.  \nGenerally speaking, a team has only one active deployment at a time, in what is called an active-passive disaster recovery strategy. There is a less common disaster recovery solution strategy called active-active, in which there are two simultaneous active deployments.  \nDisaster recovery industry terminology  \nThere are two important industry terms that you must understand and define for your team:"
    },
    {
        "id": 241,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Disaster recovery industry terminology  \nThere are two important industry terms that you must understand and define for your team:  \nRecovery point objective: A recovery point objective (RPO) is the maximum targeted period in which data (transactions) might be lost from an IT service due to a major incident. Your Databricks deployment does not store your main customer data. That is stored in separate systems such as Amazon S3 or other data sources under your control. The Databricks control plane stores some objects in part or in full, such as jobs and notebooks. For Databricks, the RPO is defined as the maximum targeted period in which objects such as job and notebook changes can be lost. Additionally, you are responsible for defining the RPO for your own customer data in Amazon S3 or other data sources under your control.  \nRecovery time objective: The recovery time objective (RTO) is the targeted duration of time and a service level within which a business process must be restored after a disaster.  \nDisaster recovery and data corruption  \nA disaster recovery solution does not mitigate data corruption. Corrupted data in the primary region is replicated from the primary region to a secondary region and is corrupted in both regions. There are other ways to mitigate this kind of failure, for example Delta time travel."
    },
    {
        "id": 242,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Typical recovery workflow\nA Databricks disaster recovery scenario typically plays out in the following way:  \nA failure occurs in a critical service you use in your primary region. This can be a data source service or a network that impacts the Databricks deployment.  \nYou investigate the situation with the cloud provider.  \nIf you conclude that your company cannot wait for the problem to be remediated in the primary region, you may decide you need failover to a secondary region.  \nVerify that the same problem does not also impact your secondary region.  \nFail over to a secondary region.  \nStop all activities in the workspace. Users stop workloads. Users or administrators are instructed to make a backup of the recent changes if possible. Jobs are shut down if they haven\u2019t already failed due to the outage.  \nStart the recovery procedure in the secondary region. The recovery procedure updates routing and renaming of the connections and network traffic to the secondary region.  \nAfter testing, declare the secondary region operational. Production workloads can now resume. Users can log in to the now active deployment. You can retrigger scheduled or delayed jobs.  \nFor detailed steps in a Databricks context, see Test failover.  \nAt some point, the problem in the primary region is mitigated and you confirm this fact.  \nRestore (fail back) to your primary region.  \nStop all work on the secondary region.  \nStart the recovery procedure in the primary region. The recovery procedure handles routing and renaming of the connection and network traffic back to the primary region.  \nReplicate data back to the primary region as needed. To reduce complexity, perhaps minimize how much data needs to be replicated. For example, if some jobs are read-only when run in the secondary deployment, you may not need to replicate that data back to your primary deployment in the primary region. However, you may have one production job that needs to run and may need data replication back to the primary region.  \nTest the deployment in the primary region.  \nDeclare your primary region operational and that it is your active deployment. Resume production workloads.  \nFor more information about restoring to your primary region, see Test restore (failback).  \nImportant  \nDuring these steps, some data loss might happen. Your organization must define how much data loss is acceptable and what you can do to mitigate this loss."
    },
    {
        "id": 243,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Step 1: Understand your business needs\nYour first step is to define and understand your business needs. Define which data services are critical and what is their expected RPO and RTO.  \nResearch the real-world tolerance of each system, and remember that disaster recovery failover and failback can be costly and carries other risks. Other risks might include data corruption, data duplicated if you write to the wrong storage location, and users who log in and make changes in the wrong places.  \nMap all of the Databricks integration points that affect your business:  \nDoes your disaster recovery solution need to accommodate interactive processes, automated processes, or both?  \nWhich data services do you use? Some may be on-premises.  \nHow does input data get to the cloud?  \nWho consumes this data? What processes consume it downstream?  \nAre there third-party integrations that need to be aware of disaster recovery changes?  \nDetermine the tools or communication strategies that can support your disaster recovery plan:  \nWhat tools will you use to modify network configurations quickly?  \nCan you predefine your configuration and make it modular to accommodate disaster recovery solutions in a natural and maintainable way?  \nWhich communication tools and channels will notify internal teams and third-parties (integrations, downstream consumers) of disaster recovery failover and failback changes? And how will you confirm their acknowledgement?  \nWhat tools or special support will be needed?  \nWhat services if any will be shut down until complete recovery is in place?"
    },
    {
        "id": 244,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Step 2: Choose a process that meets your business needs\nYour solution must replicate the correct data in both control plane, compute plane, and data sources. Redundant workspaces for disaster recovery must map to different control planes in different regions. You must keep that data in sync periodically using a script-based solution, either a synchronization tool or a CI/CD workflow. There is no need to synchronize data from within the compute plane network itself, such as from Databricks Runtime workers.  \nIf you use the customer-managed VPC feature (not available with all subscription and deployment types), you can consistently deploy these networks in both regions using template-based tooling such as Terraform.  \nAdditionally, you need to ensure that your data sources are replicated as needed across regions.  \nGeneral best practices  \nGeneral best practices for a successful disaster recovery plan include:  \nUnderstand which processes are critical to the business and have to run in disaster recovery.  \nClearly identify which services are involved, which data is being processed, what the data flow is and where it is stored  \nIsolate the services and data as much as possible. For example, create a special cloud storage container for the data for disaster recovery or move Databricks objects that are needed during a disaster to a separate workspace.  \nIt is your responsibility to maintain integrity between primary and secondary deployments for other objects that are not stored in the Databricks Control Plane.  \nWarning  \nIt is a best practice not to store data in the root Amazon S3 bucket that is used for DBFS root access for the workspace. That DBFS root storage is unsupported for production customer data. Databricks also recommends not to store libraries, configuration files, or init scripts in this location.  \nFor data sources, where possible, it is recommended that you use native AWS tools for replication and redundancy to replicate data to the disaster recovery regions.  \nChoose a recovery solution strategy  \nTypical disaster recovery solutions involve two (or possibly more) workspaces. There are several strategies you can choose. Consider the potential length of the disruption (hours or maybe even a day), the effort to ensure that the workspace is fully operational, and the effort to restore (fail back) to the primary region.  \nActive-passive solution strategy"
    },
    {
        "id": 245,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Active-passive solution strategy  \nAn active-passive solution is the most common and the easiest solution, and this type of solution is the focus of this article. An active-passive solution synchronizes data and object changes from your active deployment to your passive deployment. If you prefer, you could have multiple passive deployments in different regions, but this article focuses on the single passive deployment approach. During a disaster recovery event, the passive deployment in the secondary region becomes your active deployment.  \nThere are two main variants of this strategy:  \nUnified (enterprise-wise) solution: Exactly one set of active and passive deployments that support the entire organization.  \nSolution by department or project: Each department or project domain maintains a separate disaster recovery solution. Some organizations want to decouple disaster recovery details between departments and use different primary and secondary regions for each team based on the unique needs of each team.  \nThere are other variants, such as using a passive deployment for read-only use cases. If you have workloads that are read-only, for example user queries, they can run on a passive solution at any time if they do not modify data or Databricks objects such as notebooks or jobs.  \nActive-active solution strategy  \nIn an active-active solution, you run all data processes in both regions at all times in parallel. Your operations team must ensure that a data process such as a job is marked as complete only when it finishes successfully on both regions. Objects cannot be changed in production and must follow a strict CI/CD promotion from development/staging to production.  \nAn active-active solution is the most complex strategy, and because jobs run in both regions, there is additional financial cost.  \nJust as with the active-passive strategy, you can implement this as a unified organization solution or by department.  \nYou may not need an equivalent workspace in the secondary system for all workspaces, depending on your workflow. For example, perhaps a development or staging workspace may not need a duplicate. With a well-designed development pipeline, you may be able to reconstruct those workspaces easily if needed.  \nChoose your tooling  \nThere are two main approaches for tools to keep data as similar as possible between workspaces in your primary and secondary regions:  \nSynchronization client that copies from primary to secondary:\u00a0A sync client pushes production data and assets from the primary region to the secondary region. Typically this runs on a scheduled basis."
    },
    {
        "id": 246,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Synchronization client that copies from primary to secondary:\u00a0A sync client pushes production data and assets from the primary region to the secondary region. Typically this runs on a scheduled basis.  \nCI/CD tooling for parallel deployment: For production code and assets, use CI/CD tooling that pushes changes to production systems simultaneously to both regions. For example, when pushing code and assets from staging/development to production, a CI/CD system makes it available in both regions at the same time. The core idea is to treat all artifacts in a Databricks workspace as infrastructure-as-code. Most artifacts could be co-deployed to both primary and secondary workspaces, while some artifacts may need to be deployed only after a disaster recovery event. For tools, see Automation scripts, samples, and prototypes.  \nThe following diagram contrasts these two approaches.  \nDepending on your needs, you could combine the approaches. For example, use CI/CD for notebook source code but use synchronization for configuration like pools and access controls.  \nThe following table describes how to handle different types of data with each tooling option.  \nDescription  \nHow to handle with CI/CD tooling  \nHow to handle with sync tool  \nSource code: notebook source exports and source code for packaged libraries  \nCo-deploy both to primary and secondary.  \nSynchronize source code from primary to secondary.  \nUsers and groups  \nManage metadata as config in Git. Alternatively, use the same identity provider (IdP) for both workspaces. Co-deploy user and group data to primary and secondary deployments.  \nUse SCIM or other automation for both regions. Manual creation is not recommended, but if used must be done for both at the same time. If you use a manual setup, create a scheduled automated process to compare the list of users and group between the two deployments.  \nPool configurations  \nCan be templates in Git. Co-deploy to primary and secondary. However, min_idle_instances in secondary must be zero until the disaster recovery event.  \nPools created with any min_idle_instances when they are synced to secondary workspace using the API or CLI.  \nJob configurations  \nCan be templates in Git. For primary deployment, deploy the job definition as is. For secondary deployment, deploy the job and set the concurrencies to zero. This disables the job in this deployment and prevents extra runs. Change the concurrencies value after the secondary deployment becomes active."
    },
    {
        "id": 247,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "If the jobs run on existing <interactive> clusters for some reason, then the sync client needs to map to the corresponding cluster_id in the secondary workspace.  \nAccess control lists (ACLs)  \nCan be templates in Git. Co-deploy to primary and secondary deployments for notebooks, folders, and clusters. However, hold the data for jobs until the disaster recovery event.  \nThe Permissions API can set access controls for clusters, jobs, pools, notebooks, and folders. A sync client needs to map to corresponding object IDs for each object in the secondary workspace. Databricks recommends creating a map of object IDs from primary to secondary workspace while syncing those objects before replicating the access controls.  \nLibraries  \nInclude in source code and cluster/job templates.  \nSync custom libraries from centralized repositories, DBFS, or cloud storage (can be mounted).  \nCluster init scripts  \nInclude in source code if you prefer.  \nFor simpler synchronization, store init scripts in the primary workspace in a common folder or in a small set of folders if possible.  \nMount points  \nInclude in source code if created only through notebook-based jobs or Command API.  \nUse jobs. Note that the storage endpoints might change, given that workspaces would be in different regions. This depends a lot on your data disaster recovery strategy as well.  \nTable metadata  \nInclude with source code if created only through notebook-based jobs or Command API. This applies to both internal Databricks metastore or external configured metastore.  \nCompare the metadata definitions between the metastores using Spark Catalog API or Show Create Table via a notebook or scripts. Note that the tables for underlying storage can be region-based and will be different between metastore instances.  \nSecrets  \nInclude in source code if created only through Command API. Note that some secrets content might need to change between the primary and secondary.  \nSecrets are created in both workspaces via the API. Note that some secrets content might need to change between the primary and secondary.  \nCluster configurations  \nCan be templates in Git. Co-deploy to primary and secondary deployments, although the ones in secondary deployment should be terminated until the disaster recovery event.  \nClusters are created after they are synced to the secondary workspace using the API or CLI. Those can be explicitly terminated if you want, depending on auto-termination settings.  \nNotebook, job, and folder permissions"
    },
    {
        "id": 248,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Clusters are created after they are synced to the secondary workspace using the API or CLI. Those can be explicitly terminated if you want, depending on auto-termination settings.  \nNotebook, job, and folder permissions  \nCan be templates in Git. Co-deploy to primary and secondary deployments.  \nReplicate using the Permissions API.  \nChoose regions and multiple secondary workspaces  \nYou need full control of your disaster recovery trigger. You may decide to trigger this at any time or for any reason. You must take responsibility for disaster recovery stabilization before you can restart your operation failback (normal production) mode. Typically this means that you need to create multiple Databricks workspaces to serve your production and disaster recovery needs, and choose your secondary failover region.  \nIn AWS, you can have full control of the chosen secondary region. Ensure that all of your resources and products are available there, such as EC2. Some Databricks services are available only in some regions."
    },
    {
        "id": 249,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Step 3: Prep workspaces and do a one-time copy\nStep 3: Prep workspaces and do a one-time copy\nIf a workspace is already in production, it is typical to run a one-time copy operation to synchronize your passive deployment with your active deployment. This one time copy handles the following:  \nData replication: Replicate using a cloud replication solution or Delta Deep Clone operation.  \nToken generation: Use token generation to automate the replication and future workloads.  \nWorkspace replication: Use workspace replication using the methods described in Step 4: Prepare your data sources.  \nWorkspace validation: - test to make sure that the workspace and the process can execute successfully and provide the expected results.  \nAfter your initial one-time copy operation, subsequent copy and sync actions are faster and any logging from your tools is also a log of what changed and when it changed.\n\nStep 4: Prepare your data sources"
    },
    {
        "id": 250,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Step 4: Prepare your data sources\nDatabricks can process a large variety of data sources using batch processing or data streams.  \nBatch processing from data sources  \nWhen data is processed in batch, it usually resides in a data source that can be replicated easily or delivered into another region.  \nFor example, data might regularly be uploaded to a cloud storage location. In disaster recovery mode for your secondary region, you must ensure that the files will be uploaded to your secondary region storage. Workloads must read the secondary region storage and write to the secondary region storage.  \nData streams  \nProcessing a data stream is a bigger challenge. Streaming data can be ingested from various sources and be processed and sent to a streaming solution:  \nMessage queue such as Kafka  \nDatabase change data capture stream  \nFile-based continuous processing  \nFile-based scheduled processing, also known as trigger once  \nIn all of these cases, you must configure your data sources to handle disaster recovery mode and to use your secondary deployment in your secondary region.  \nA stream writer stores a checkpoint with information about the data that has been processed. This checkpoint can contain a data location (usually cloud storage) that has to be modified to a new location to ensure a successful restart of the stream. For example, the source subfolder under the checkpoint might store the file-based cloud folder.  \nThis checkpoint must be replicated in a timely manner. Consider synchronization of the checkpoint interval with any new cloud replication solution.  \nThe checkpoint update is a function of the writer and therefore applies to data stream ingestion or processing and storing on another streaming source.  \nFor streaming workloads, ensure that checkpoints are configured in customer-managed storage so that they can be replicated to the secondary region for workload resumption from the point of last failure. You might also choose to run the secondary streaming process in parallel to the primary process."
    },
    {
        "id": 251,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Step 5: Implement and test your solution\nPeriodically test your disaster recovery setup to ensure that it functions correctly. There\u2019s no value in maintaining a disaster recovery solution if you cannot use it when you need it. Some companies switch between regions every few months. Switching regions on a regular schedule tests your assumptions and processes and ensures that they meet your recovery needs. This also ensures that your organization is familiar with the policies and procedures for emergencies.  \nImportant  \nRegularly test your disaster recovery solution in real-world conditions.  \nIf you discover that you are missing an object or template and still need to rely on the information stored in your primary workspace, modify your plan to remove these obstacles, replicate this information in the secondary system, or make it available in some other way.  \nTest any required organizational changes to your processes and to configuration in general. Your disaster recovery plan impacts your deployment pipeline, and it is important that your team knows what needs to be kept in sync. After you set up your disaster recovery workspaces, you must ensure that your infrastructure (manual or code), jobs, notebook, libraries, and other workspace objects are available in your secondary region.  \nTalk with your team about how to expand standard work processes and configuration pipelines to deploy changes to all workspaces. Manage user identities in all workspaces. Remember to configure tools such as job automation and monitoring for new workspaces.  \nPlan for and test changes to configuration tooling:  \nIngestion:\u00a0Understand where your data sources are and where those sources get their data. Where possible, parameterize the source and ensure that you have a separate configuration template for working with your secondary deployments and secondary regions. Prepare a plan for failover and test all assumptions.  \nExecution changes:\u00a0If you have a scheduler to trigger jobs or other actions, you may need to configure a separate scheduler that works with the secondary deployment or its data sources. Prepare a plan for failover and test all assumptions.  \nInteractive connectivity:\u00a0Consider how configuration, authentication, and network connections might be affected by regional disruptions for any use of REST APIs, CLI tools, or other services such as JDBC/ODBC. Prepare a plan for failover and test all assumptions.  \nAutomation changes:\u00a0For all automation tools, prepare a plan for failover and test all assumptions."
    },
    {
        "id": 252,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Automation changes:\u00a0For all automation tools, prepare a plan for failover and test all assumptions.  \nOutputs:\u00a0For any tools that generate output data or logs, prepare a plan for failover and test all assumptions.  \nTest failover  \nDisaster recovery can be triggered by many different scenarios. It can be triggered by an unexpected break. Some core functionality may be down, including the cloud network, cloud storage, or another core service. You do not have access to shut down the system gracefully and must try to recover. However, the process could be triggered by a shutdown or planned outage, or even by periodic switching of your active deployments between two regions.  \nWhen you test failover, connect to the system and run a shutdown process. Ensure that all jobs are complete and the clusters are terminated.  \nA sync client (or CI/CD tooling) can replicate relevant Databricks objects and resources to the secondary workspace. To activate your secondary workspace, your process might include some or all of the following:  \nRun tests to confirm that the platform is up to date.  \nDisable pools and clusters on the primary region so that if the failed service returns online, the primary region does not start processing new data.  \nRecovery process:  \nCheck the date of the latest synced data. See Disaster recovery industry terminology. The details of this step vary based on how you synchronize data and your unique business needs.  \nStabilize your data sources and ensure that they are all available. Include all external data sources, such as AWS RDS, as well as your Delta Lake, Parquet, or other files.  \nFind your streaming recovery point. Set up the process to restart from there and have a process ready to identify and eliminate potential duplicates (Delta Lake Lake makes this easier).  \nComplete the data flow process and inform the users.  \nStart relevant pools (or increase the min_idle_instances to relevant number).  \nStart relevant clusters (if not terminated).  \nChange the concurrent run for jobs and run relevant jobs. These could be one-time runs or periodic runs."
    },
    {
        "id": 253,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Start relevant pools (or increase the min_idle_instances to relevant number).  \nStart relevant clusters (if not terminated).  \nChange the concurrent run for jobs and run relevant jobs. These could be one-time runs or periodic runs.  \nFor any outside tool that uses a URL or domain name for your Databricks workspace, update configurations to account for the new control plane. For example, update URLs for REST APIs and JDBC/ODBC connections. The Databricks web application\u2019s customer-facing URL changes when the control plane changes, so notify your organization\u2019s users of the new URL.  \nTest restore (failback)  \nFailback is easier to control and can be done in a maintenance window. This plan can include some or all of the following:  \nGet confirmation that the primary region is restored.  \nDisable pools and clusters on the secondary region so it will not start processing new data.  \nSync any new or modified assets in the secondary workspace back to the primary deployment. Depending on the design of your failover scripts, you might be able to run the same scripts to sync the objects from the secondary (disaster recovery) region to the primary (production) region.  \nSync any new data updates back to the primary deployment. You can use the audit trails of logs and Delta tables to guarantee no loss of data.  \nShut down all workloads in the disaster recovery region.  \nChange the jobs and users URL to the primary region.  \nRun tests to confirm that the platform is up to date.  \nStart relevant pools (or increase the min_idle_instances to a relevant number) .  \nStart relevant clusters (if not terminated).  \nChange the concurrent run for jobs, and run relevant jobs. These could be one-time runs or periodic runs.  \nAs needed, set up your secondary region again for future disaster recovery."
    },
    {
        "id": 254,
        "url": "https://docs.databricks.com/en/admin/disaster-recovery.html",
        "content": "Automation scripts, samples, and prototypes\nAutomation scripts, samples, and prototypes\nAutomation scripts to consider for your disaster recovery projects:  \nDatabricks recommends that you use the Databricks Terraform Provider to help develop your own sync process.  \nSee also Databricks Workspace Migration Tools for sample automation and prototype scripts.  \nThe Databricks Sync (DBSync) project is an object synchronization tool that backs up, restores, and syncs Databricks workspaces."
    },
    {
        "id": 255,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.2ml.html",
        "content": "Databricks Runtime 5.2 ML  \nDatabricks released this image in January 2019.  \nDatabricks Runtime 5.2 ML provides a ready-to-go environment for machine learning and data science based on Databricks Runtime 5.2 (EoS). Databricks Runtime for ML contains many popular machine learning libraries, including TensorFlow, PyTorch, Keras, and XGBoost. It also supports distributed TensorFlow training using Horovod.  \nFor more information, including instructions for creating a Databricks Runtime ML cluster, see AI and Machine Learning on Databricks.  \nNew features"
    },
    {
        "id": 256,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.2ml.html",
        "content": "New features\nDatabricks Runtime 5.2 ML is built on top of Databricks Runtime 5.2. For information on what\u2019s new in Databricks Runtime 5.2, see the Databricks Runtime 5.2 (EoS) release notes. In addition to library updates, Databricks Runtime 5.2 ML introduces the following new features:  \nGraphFrames now supports the Pregel API (Python) with Databricks\u2019s performance optimizations.  \nHorovodRunner adds:  \nOn a GPU cluster, training processes are mapped to GPUs instead of worker nodes to simplify the support of multi-GPU instance types. This built-in support allows you to distribute to all of the GPUs on a multi-GPU machine without custom code.  \nHorovodRunner.run() now returns the return value from the first training process.  \nNote  \nDatabricks Runtime ML releases pick up all maintenance updates to the base Databricks Runtime release. For a list of all maintenance updates, see Maintenance updates for Databricks Runtime (archived).\n\nSystem environment"
    },
    {
        "id": 257,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.2ml.html",
        "content": "System environment\nThe system environment in Databricks Runtime 5.2 ML differs from Databricks Runtime 5.2 as follows:  \nPython: 2.7.15 for Python 2 clusters and 3.6.5 for Python 3 clusters.  \nDBUtils: Databricks Runtime 5.2 ML does not contain Library utility (dbutils.library) (legacy).  \nFor GPU clusters, the following NVIDIA GPU libraries:  \nTesla driver 396.44  \nCUDA 9.2  \nCUDNN 7.2.1\n\nLibraries"
    },
    {
        "id": 258,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.2ml.html",
        "content": "Libraries\nThe following sections list the libraries included in Databricks Runtime 5.2 ML that differ from those included in Databricks Runtime 5.2.  \nPython libraries  \nDatabricks Runtime 5.2 ML uses Conda for Python package management. As a result, there are major differences in pre-installed Python libraries compared to Databricks Runtime. The following is a full list of provided Python packages and versions installed using Conda package manager.  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nabsl-py  \n0.6.1  \nargparse  \n1.4.0  \nasn1crypto  \n0.24.0  \nastor  \n0.7.1  \nbackports-abc  \n0.5  \nbackports.functools-lru-cache  \n1.5  \nbackports.weakref  \n1.0.post1  \nbcrypt  \n3.1.5  \nbleach  \n2.1.3  \nboto  \n2.48.0  \nboto3  \n1.7.62  \nbotocore  \n1.10.62  \ncertifi  \n2018.04.16  \ncffi  \n1.11.5  \nchardet  \n3.0.4  \ncloudpickle  \n0.5.3  \ncolorama  \n0.3.9  \nconfigparser  \n3.5.0  \ncryptography  \n2.2.2  \ncycler  \n0.10.0  \nCython  \n0.28.2  \ndecorator  \n4.3.0  \ndocutils  \n0.14  \nentrypoints  \n0.2.3  \nenum34  \n1.1.6  \net-xmlfile  \n1.0.1  \nfuncsigs  \n1.0.2  \nfunctools32  \n3.2.3-2  \nfusepy  \n2.0.4  \nfutures  \n3.2.0  \ngast  \n0.2.0  \ngrpcio  \n1.12.1  \nh5py  \n2.8.0  \nhorovod  \n0.15.2  \nhtml5lib  \n1.0.1  \nidna  \n2.6  \nipaddress  \n1.0.22  \nipython  \n5.7.0  \nipython_genutils  \n0.2.0  \njdcal  \n1.4  \nJinja2  \n2.10  \njmespath  \n0.9.3  \njsonschema  \n2.6.0  \njupyter-client  \n5.2.3  \njupyter-core  \n4.4.0  \nKeras  \n2.2.4  \nKeras-Applications  \n1.0.6  \nKeras-Preprocessing  \n1.0.5  \nkiwisolver  \n1.0.1"
    },
    {
        "id": 259,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.2ml.html",
        "content": "5.2.3  \njupyter-core  \n4.4.0  \nKeras  \n2.2.4  \nKeras-Applications  \n1.0.6  \nKeras-Preprocessing  \n1.0.5  \nkiwisolver  \n1.0.1  \nlinecache2  \n1.0.0  \nllvmlite  \n0.23.1  \nlxml  \n4.2.1  \nMarkdown  \n3.0.1  \nMarkupSafe  \n1.0  \nmatplotlib  \n2.2.2  \nmistune  \n0.8.3  \nmleap  \n0.8.1  \nmock  \n2.0.0  \nmsgpack  \n0.5.6  \nnbconvert  \n5.3.1  \nnbformat  \n4.4.0  \nnose  \n1.3.7  \nnose-exclude  \n0.5.0  \nnumba  \n0.38.0+0.g2a2b772fc.dirty  \nnumpy  \n1.14.3  \nolefile  \n0.45.1  \nopenpyxl  \n2.5.3  \npandas  \n0.23.0  \npandocfilters  \n1.4.2  \nparamiko  \n2.4.1  \npathlib2  \n2.3.2  \npatsy  \n0.5.0  \npbr  \n5.1.1  \npexpect  \n4.5.0  \npickleshare  \n0.7.4  \nPillow  \n5.1.0  \npip  \n10.0.1  \nply  \n3.11  \nprompt-toolkit  \n1.0.15  \nprotobuf  \n3.6.1  \npsycopg2  \n2.7.5  \nptyprocess  \n0.5.2  \npyarrow  \n0.8.0  \npyasn1  \n0.4.4  \npycparser  \n2.18  \nPygments  \n2.2.0  \nPyNaCl  \n1.3.0  \npyOpenSSL  \n18.0.0  \npyparsing  \n2.2.0  \nPySocks  \n1.6.8  \nPython  \n2.7.15  \npython-dateutil  \n2.7.3  \npytz  \n2018.4  \nPyYAML  \n3.12  \npyzmq  \n17.0.0  \nrequests  \n2.18.4  \ns3transfer  \n0.1.13  \nscandir  \n1.7  \nscikit-learn  \n0.19.1  \nscipy  \n1.1.0  \nseaborn  \n0.8.1  \nsetuptools  \n39.1.0  \nsimplegeneric  \n0.8.1  \nsingledispatch  \n3.4.0.3  \nsix  \n1.11.0  \nstatsmodels  \n0.9.0"
    },
    {
        "id": 260,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.2ml.html",
        "content": "seaborn  \n0.8.1  \nsetuptools  \n39.1.0  \nsimplegeneric  \n0.8.1  \nsingledispatch  \n3.4.0.3  \nsix  \n1.11.0  \nstatsmodels  \n0.9.0  \nsubprocess32  \n3.5.3  \ntensorboard  \n1.12.2  \ntensorboardX  \n1.4  \ntensorflow  \n1.12.0  \ntermcolor  \n1.1.0  \ntestpath  \n0.3.1  \ntorch  \n0.4.1  \ntorchvision  \n0.2.1  \ntornado  \n5.0.2  \ntraceback2  \n1.4.0  \ntraitlets  \n4.3.2  \nunittest2  \n1.1.0  \nurllib3  \n1.22  \nvirtualenv  \n16.0.0  \nwcwidth  \n0.1.7  \nwebencodings  \n0.5.1  \nWerkzeug  \n0.14.1  \nwheel  \n0.31.1  \nwrapt  \n1.10.11  \nwsgiref  \n0.1.2  \nIn addition, the following Spark packages include Python modules:  \nSpark Package  \nPython Module  \nVersion  \ngraphframes  \ngraphframes  \n0.7.0-db1-spark2.4  \nspark-deep-learning  \nsparkdl  \n1.5.0-db1-spark2.4  \ntensorframes  \ntensorframes  \n0.6.0-s_2.11  \nR libraries  \nThe R libraries are identical to the R Libraries in Databricks Runtime 5.2.  \nJava and Scala libraries (Scala 2.11 cluster)  \nIn addition to Java and Scala libraries in Databricks Runtime 5.2, Databricks Runtime 5.2 ML contains the following JARs:  \nGroup ID  \nArtifact ID  \nVersion  \ncom.databricks  \nspark-deep-learning  \n1.5.0-db1-spark2.4  \ncom.typesafe.akka  \nakka-actor_2.11  \n2.3.11  \nml.combust.mleap  \nmleap-databricks-runtime_2.11  \n0.13.0  \nml.dmlc  \nxgboost4j  \n0.81  \nml.dmlc  \nxgboost4j-spark  \n0.81  \norg.graphframes  \ngraphframes_2.11  \n0.7.0-db1-spark2.4  \norg.tensorflow  \nlibtensorflow  \n1.12.0  \norg.tensorflow"
    },
    {
        "id": 261,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.2ml.html",
        "content": "0.81  \norg.graphframes  \ngraphframes_2.11  \n0.7.0-db1-spark2.4  \norg.tensorflow  \nlibtensorflow  \n1.12.0  \norg.tensorflow  \nlibtensorflow_jni  \n1.12.0  \norg.tensorflow  \nspark-tensorflow-connector_2.11  \n1.12.0  \norg.tensorflow  \ntensorflow  \n1.12.0  \norg.tensorframes  \ntensorframes  \n0.6.0-s_2.11"
    },
    {
        "id": 262,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "Databricks Runtime 6.5 (EoS)  \nNote  \nSupport for this Databricks Runtime version has ended. For the end-of-support date, see End-of-support history. For all supported Databricks Runtime versions, see Databricks Runtime release notes versions and compatibility.  \nDatabricks released this image in April 2020.  \nThe following release notes provide information about Databricks Runtime 6.5, powered by Apache Spark.  \nNew features"
    },
    {
        "id": 263,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "New features\nOperation metrics for all writes, updates, and deletes on a Delta table now shown in table history  \nYou can now see operation metrics (for example, number of files and rows changed) for all writes, updates, and deletes on a Delta table in the table\u2019s history. The generation of these metrics is enabled by default. You can see the history by running:  \nDESCRIBE HISTORY deltaTableName  \nRate-limit the data processed in Delta Lake streaming micro-batches  \nWhen you process data from Delta Lake using Structured Streaming, you can now set the option maxBytesPerTrigger to rate-limit how much data gets processed in each micro-batch. This sets a \u201csoft max,\u201d meaning that a batch processes approximately this amount of data and may process more than the limit.  \nIf you use Trigger.Once for your streaming, this option is ignored. In addition, if you use this option in conjunction with maxFilesPerTrigger, the micro-batch processes data until either the maxFilesPerTrigger or maxBytesPerTrigger limit is reached.\n\nImprovements"
    },
    {
        "id": 264,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "Improvements\nSnowflake connector updated to 2.5.9  \nThe updated connector promotes the Snowflake JDBC driver to 3.12.0.  \nJava upgraded to 1.8.0_242  \nThis version introduces new checks on trust anchor certificates to ensure that trust anchors are CA certificates and contain proper extensions. Trust anchor certificates must include a Basic Constraints extension with the ca field set to true. In addition, if they include a Key Usage extension, the keyCertSign bit must be set.  \nIf you use self-signed trust anchor certificates, you may hit the following exceptions introduced by the new checks:  \nTrustAnchor with subject \"...\" is not a CA certificate TrustAnchor with subject \"...\" does not have keyCertSign bit set in KeyUsage extension  \nTo keep using self-signed trust anchor certificates, you have two options to address the above exceptions:  \nSet Java system property jdk.security.allowNonCaAnchor introduced with Java 1.9.0_242 to true to restore the previous behavior. You can use spark.driver.extraJavaOptions and spark.executor.extraJavaOptions to set this system property to Spark driver and executors, respectively.  \nInclude a Basic Constraints extension with the ca field set to true to your trust anchor certificates. If your trust anchor certificates include a Key Usage extension, set keyCertSign bit.  \nIf you use Java Keytool to generate public/private key pairs, set -ext BasicConstraints=ca:true when generating trust anchor certificates. If your trust anchor certificates include a Key Usage extension, set keyCertSign bit -ext KeyUsage=...,keyCertSign when generating trust anchor certificates.  \nS3 Select connector: syntax fix for DECIMAL and CASE WHEN  \nDECIMAL type conversion ignores precision and scale when converting to the DECIMAL type in Amazon S3 Select because of removed support for DECIMAL(precision, scale) in Amazon S3 Select. All internal CASE WHEN references have been replaced with COALESCE and NULLIF functions that are officially supported by Amazon S3 Select.  \nKinesis streaming source launches more efficiently sized tasks  \nThe Kinesis streaming source now automatically coalesces prefetched data blocks to reduce the number of tasks generated to process the blocks.  \nUpgraded R libraries  \nUpgraded several installed R libraries. See Installed R Libraries."
    },
    {
        "id": 265,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "Deprecations\nDeprecations\nThe boto package is deprecated and will be removed in an upcoming major release of Databricks Runtime. Databricks recommends that you use boto3 instead.  \nThe following R packages will be removed in an upcoming major release of Databricks Runtime:  \nabind  \nbitops  \ncar  \ncarData  \ndoMC  \ngbm  \nh2o  \nlittler  \nlme4  \nmapproj  \nmaps  \nmaptools  \nMatrixModels  \nminqa  \nmvtnorm  \nnloptr  \nopenxlsx  \npbkrtest  \npkgKitten  \nquantreg  \nR.methodsS3  \nR.oo  \nR.utils  \nRcppEigen  \nRCurl  \nrio  \nsp  \nSparseM  \nstatmod  \nzip  \nDatabricks recommends using the following code to install and load R packages that are not installed by Databricks Runtime:  \nif (!require(\"package_name\")) { install.packages(\"package_name\") library(\"package_name\") }\n\nApache Spark"
    },
    {
        "id": 266,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "Apache Spark\nDatabricks Runtime 6.5 includes Apache Spark 2.4.5. This release includes all Spark fixes and improvements included in Databricks Runtime 6.4 (EoS), as well as the following additional bug fixes and improvements made to Spark:  \n[SPARK-27099] [SQL] Add \u2018xxhash64\u2019 for hashing arbitrary columns to Long  \n[SPARK-26151] [SQL] Return partial results for bad CSV records  \n[SPARK-30927] [SS] StreamingQueryManager should avoid keeping reference to terminated StreamingQuery  \n[SPARK-22590] Copy sparkContext.localproperties to child thread in BroadcastExchangeExec.executionContext  \n[SPARK-30671] emptyDataFrame should use a LocalRelation  \n[SPARK-30556] [SQL] Reset the status changed in SQLExecution withThreadLocalCaptured  \n[SPARK-30763] [SQL] Fix java.lang.IndexOutOfBoundsException No group 1 for regexp_extract  \n[SPARK-30806] [SQL] Evaluate once per group in UnboundedWindowFunctionFrame  \n[SPARK-30731] Update deprecated Mkdocs option  \n[SPARK-30857] [SQL] Fix truncations of timestamps before the epoch to hours and days  \n[SPARK-30798] Scope Session.active in QueryExecution  \n[SPARK-30793] [SQL] Fix truncations of timestamps before the epoch to minutes and seconds  \n[SPARK-30826] [SQL] Respect reference case in StringStartsWith pushed down to parquet  \n[SPARK-30797] [SQL] Set tradition user/group/other permission to ACL entries when setting up ACLs in truncate table  \n[SPARK-30065] [SQL] DataFrameNaFunctions.drop should handle duplicate columns  \n[SPARK-29890] [SQL] DataFrameNaFunctions.fill should handle duplicate columns  \n[SPARK-30310] [CORE] Resolve missing match case in SparkUncaughtExceptionHandler and added tests  \n[SPARK-30512] Added a dedicated boss event loop group"
    },
    {
        "id": 267,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "System environment\nOperating System: Ubuntu 16.04.6 LTS  \nJava: 1.8.0_252  \nScala: 2.11.12  \nPython: 3.7.3  \nR: R version 3.6.3 (2020-02-29)  \nDelta Lake: 0.5.0  \nNote  \nAlthough Scala 2.12 is available as an experimental feature in Apache Spark 2.4, it is not supported in Databricks Runtime 6.5.  \nIn this section:  \nInstalled Python libraries  \nInstalled R libraries  \nInstalled Java and Scala libraries (Scala 2.11 cluster version)  \nInstalled Python libraries  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nasn1crypto  \n0.24.0  \nbackcall  \n0.1.0  \nboto  \n2.49.0  \nboto3  \n1.9.162  \nbotocore  \n1.12.163  \ncertifi  \n2019.3.9  \ncffi  \n1.12.2  \nchardet  \n3.0.4  \ncryptography  \n2.6.1  \ncycler  \n0.10.0  \nCython  \n0.29.6  \ndecorator  \n4.4.0  \ndocutils  \n0.14  \nidna  \n2.8  \nipykernel  \n5.1.0  \nipython  \n7.4.0  \nipython-genutils  \n0.2.0  \njedi  \n0.13.3  \njmespath  \n0.9.4  \njupyter-client  \n5.2.4  \njupyter-core  \n4.4.0  \nkiwisolver  \n1.1.0  \nmatplotlib  \n3.0.3  \nnumpy  \n1.16.2  \npandas  \n0.24.2  \nparso  \n0.3.4  \npatsy  \n0.5.1  \npexpect  \n4.6.0  \npickleshare  \n0.7.5  \npip  \n19.0.3  \nprompt-toolkit  \n2.0.9  \npsycopg2  \n2.7.6.1  \nptyprocess  \n0.6.0  \npyarrow  \n0.13.0  \npycparser  \n2.19  \npycurl  \n7.43.0  \nPygments  \n2.3.1  \nPyGObject  \n3.20.0  \npyOpenSSL  \n19.0.0  \npyparsing  \n2.4.6  \nPySocks  \n1.6.8  \npython-apt  \n1.1.0.b1+ubuntu0.16.04.8  \npython-dateutil  \n2.8.0  \npytz  \n2018.9  \npyzmq"
    },
    {
        "id": 268,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "PySocks  \n1.6.8  \npython-apt  \n1.1.0.b1+ubuntu0.16.04.8  \npython-dateutil  \n2.8.0  \npytz  \n2018.9  \npyzmq  \n18.0.0  \nrequests  \n2.21.0  \ns3transfer  \n0.2.1  \nscikit-learn  \n0.20.3  \nscipy  \n1.2.1  \nseaborn  \n0.9.0  \nsetuptools  \n40.8.0  \nsix  \n1.12.0  \nssh-import-id  \n5.5  \nstatsmodels  \n0.9.0  \ntornado  \n6.0.2  \ntraitlets  \n4.3.2  \nunattended-upgrades  \n0.1  \nurllib3  \n1.24.1  \nvirtualenv  \n16.4.1  \nwcwidth  \n0.1.7  \nwheel  \n0.33.1  \nInstalled R libraries  \nR libraries are installed from Microsoft CRAN snapshot on 2020-02-26.  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nabind  \n1.4-5  \naskpass  \n1.1  \nassertthat  \n0.2.1  \nbackports  \n1.1.5  \nbase  \n3.6.3  \nbase64enc  \n0.1-3  \nBH  \n1.72.0-3  \nbit  \n1.1-15.2  \nbit64  \n0.9-7  \nbitops  \n1.0-6  \nblob  \n1.2.1  \nboot  \n1.3-23  \nbrew  \n1.0-6  \ncallr  \n3.4.2  \ncar  \n3.0-6  \ncarData  \n3.0-3  \ncaret  \n6.0-85  \ncellranger  \n1.1.0  \nchron  \n2.3-55  \nclass  \n7.3-15  \ncli  \n2.0.1  \nclipr  \n0.7.0  \nclisymbols  \n1.2.0  \ncluster  \n2.1.0  \ncodetools  \n0.2-16  \ncolorspace  \n1.4-1  \ncommonmark  \n1.7  \ncompiler  \n3.6.3  \nconfig  \n0.3  \ncovr  \n3.4.0  \ncrayon  \n1.3.4  \ncrosstalk  \n1.0.0  \ncurl  \n4.3  \ndata.table  \n1.12.8  \ndatasets  \n3.6.3  \nDBI  \n1.1.0  \ndbplyr  \n1.4.2  \ndesc  \n1.2.0  \ndevtools  \n2.2.2  \ndigest  \n0.6.25  \ndoMC  \n1.3.6  \ndplyr  \n0.8.4  \nDT  \n0.12  \nellipsis"
    },
    {
        "id": 269,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "dbplyr  \n1.4.2  \ndesc  \n1.2.0  \ndevtools  \n2.2.2  \ndigest  \n0.6.25  \ndoMC  \n1.3.6  \ndplyr  \n0.8.4  \nDT  \n0.12  \nellipsis  \n0.3.0  \nevaluate  \n0.14  \nfansi  \n0.4.1  \nfarver  \n2.0.3  \nfastmap  \n1.0.1  \nforcats  \n0.4.0  \nforeach  \n1.4.8  \nforeign  \n0.8-75  \nforge  \n0.2.0  \nfs  \n1.3.1  \ngbm  \n2.1.5  \ngenerics  \n0.0.2  \nggplot2  \n3.2.1  \ngh  \n1.1.0  \ngit2r  \n0.26.1  \nglmnet  \n3.0-2  \nglue  \n1.3.1  \ngower  \n0.2.1  \ngraphics  \n3.6.3  \ngrDevices  \n3.6.3  \ngrid  \n3.6.3  \ngridExtra  \n2.3  \ngsubfn  \n0.7  \ngtable  \n0.3.0  \nh2o  \n3.28.0.2  \nhaven  \n2.2.0  \nhms  \n0.5.3  \nhtmltools  \n0.4.0  \nhtmlwidgets  \n1.5.1  \nhttpuv  \n1.5.2  \nhttr  \n1.4.1  \nhwriter  \n1.3.2  \nhwriterPlus  \n1.0-3  \nini  \n0.3.1  \nipred  \n0.9-9  \niterators  \n1.0.12  \njsonlite  \n1.6.1  \nKernSmooth  \n2.23-16  \nlabeling  \n0.3  \nlater  \n1.0.0  \nlattice  \n0.20-40  \nlava  \n1.6.6  \nlazyeval  \n0.2.2  \nlifecycle  \n0.1.0  \nlittler  \n0.3.9  \nlme4  \n1.1-21  \nlubridate  \n1.7.4  \nmagrittr  \n1.5  \nmapproj  \n1.2.7  \nmaps  \n3.3.0  \nmaptools  \n0.9-9  \nMASS  \n7.3-51.5  \nMatrix  \n1.2-18  \nMatrixModels  \n0.4-1  \nmemoise  \n1.1.0  \nmethods  \n3.6.3  \nmgcv  \n1.8-31  \nmime  \n0.9  \nminqa  \n1.2.4  \nModelMetrics  \n1.2.2.1  \nmunsell  \n0.5.0  \nmvtnorm  \n1.1-0  \nnlme"
    },
    {
        "id": 270,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "1.8-31  \nmime  \n0.9  \nminqa  \n1.2.4  \nModelMetrics  \n1.2.2.1  \nmunsell  \n0.5.0  \nmvtnorm  \n1.1-0  \nnlme  \n3.1-144  \nnloptr  \n1.2.1  \nnnet  \n7.3-13  \nnumDeriv  \n2016.8-1.1  \nopenssl  \n1.4.1  \nopenxlsx  \n4.1.4  \nparallel  \n3.6.3  \npbkrtest  \n0.4-8.6  \npillar  \n1.4.3  \npkgbuild  \n1.0.6  \npkgconfig  \n2.0.3  \npkgKitten  \n0.1.5  \npkgload  \n1.0.2  \nplogr  \n0.2.0  \nplyr  \n1.8.5  \npraise  \n1.0.0  \nprettyunits  \n1.1.1  \npROC  \n1.16.1  \nprocessx  \n3.4.2  \nprodlim  \n2019.11.13  \nprogress  \n1.2.2  \npromises  \n1.1.0  \nproto  \n1.0.0  \nps  \n1.3.2  \npurrr  \n0.3.3  \nquantreg  \n5.54  \nR.methodsS3  \n1.8.0  \nR.oo  \n1.23.0  \nR.utils  \n2.9.2  \nr2d3  \n0.2.3  \nR6  \n2.4.1  \nrandomForest  \n4.6-14  \nrappdirs  \n0.3.1  \nrcmdcheck  \n1.3.3  \nRColorBrewer  \n1.1-2  \nRcpp  \n1.0.3  \nRcppEigen  \n0.3.3.7.0  \nRCurl  \n1.98-1.1  \nreadr  \n1.3.1  \nreadxl  \n1.3.1  \nrecipes  \n0.1.9  \nrematch  \n1.0.1  \nremotes  \n2.1.1  \nreshape2  \n1.4.3  \nrex  \n1.1.2  \nrio  \n0.5.16  \nrlang  \n0.4.4  \nRODBC  \n1.3-16  \nroxygen2  \n7.0.2  \nrpart  \n4.1-15  \nrprojroot  \n1.3-2  \nRserve  \n1.8-6  \nRSQLite  \n2.2.0  \nrstudioapi  \n0.11  \nrversions  \n2.0.1  \nscales  \n1.1.0  \nsessioninfo  \n1.1.1  \nshape"
    },
    {
        "id": 271,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "1.3-2  \nRserve  \n1.8-6  \nRSQLite  \n2.2.0  \nrstudioapi  \n0.11  \nrversions  \n2.0.1  \nscales  \n1.1.0  \nsessioninfo  \n1.1.1  \nshape  \n1.4.4  \nshiny  \n1.4.0  \nsourcetools  \n0.1.7  \nsp  \n1.4-0  \nsparklyr  \n1.1.0  \nSparkR  \n2.4.5  \nSparseM  \n1.78  \nspatial  \n7.3-11  \nsplines  \n3.6.3  \nsqldf  \n0.4-11  \nSQUAREM  \n2020.2  \nstatmod  \n1.4.34  \nstats  \n3.6.3  \nstats4  \n3.6.3  \nstringi  \n1.4.6  \nstringr  \n1.4.0  \nsurvival  \n2.44-1.1  \nsys  \n3.3  \ntcltk  \n3.6.3  \nTeachingDemos  \n2.10  \ntestthat  \n2.3.1  \ntibble  \n2.1.3  \ntidyr  \n1.0.2  \ntidyselect  \n1.0.0  \ntimeDate  \n3043.102  \ntools  \n3.6.3  \nusethis  \n1.5.1  \nutf8  \n1.1.4  \nutils  \n3.6.3  \nvctrs  \n0.2.3  \nviridisLite  \n0.3.0  \nwhisker  \n0.4  \nwithr  \n2.1.2  \nxml2  \n1.2.2  \nxopen  \n1.0.0  \nxtable  \n1.8-4  \nyaml  \n2.2.1  \nzip  \n2.0.4  \nInstalled Java and Scala libraries (Scala 2.11 cluster version)  \nGroup ID  \nArtifact ID  \nVersion  \nantlr  \nantlr  \n2.7.7  \ncom.amazonaws  \namazon-kinesis-client  \n1.8.10  \ncom.amazonaws  \naws-java-sdk-autoscaling  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-cloudformation  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-cloudfront  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-cloudhsm  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-cloudsearch  \n1.11.595  \ncom.amazonaws"
    },
    {
        "id": 272,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "aws-java-sdk-cloudhsm  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-cloudsearch  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-cloudtrail  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-cloudwatch  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-cloudwatchmetrics  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-codedeploy  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-cognitoidentity  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-cognitosync  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-config  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-core  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-datapipeline  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-directconnect  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-directory  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-dynamodb  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-ec2  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-ecs  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-efs  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-elasticache  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-elasticbeanstalk  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-elasticloadbalancing  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-elastictranscoder  \n1.11.595"
    },
    {
        "id": 273,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "aws-java-sdk-elasticloadbalancing  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-elastictranscoder  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-emr  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-glacier  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-glue  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-iam  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-importexport  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-kinesis  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-kms  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-lambda  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-logs  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-machinelearning  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-opsworks  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-rds  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-redshift  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-route53  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-s3  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-ses  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-simpledb  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-simpleworkflow  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-sns  \n1.11.595  \ncom.amazonaws"
    },
    {
        "id": 274,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "aws-java-sdk-simpleworkflow  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-sns  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-sqs  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-ssm  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-storagegateway  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-sts  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-support  \n1.11.595  \ncom.amazonaws  \naws-java-sdk-swf-libraries  \n1.11.22  \ncom.amazonaws  \naws-java-sdk-workspaces  \n1.11.595  \ncom.amazonaws  \njmespath-java  \n1.11.595  \ncom.carrotsearch  \nhppc  \n0.7.2  \ncom.chuusai  \nshapeless_2.11  \n2.3.2  \ncom.clearspring.analytics  \nstream  \n2.7.0  \ncom.databricks  \nRserve  \n1.8-3  \ncom.databricks  \ndbml-local_2.11  \n0.5.0-db8-spark2.4  \ncom.databricks  \ndbml-local_2.11-tests  \n0.5.0-db8-spark2.4  \ncom.databricks  \njets3t  \n0.7.1-0  \ncom.databricks.scalapb  \ncompilerplugin_2.11  \n0.4.15-9  \ncom.databricks.scalapb  \nscalapb-runtime_2.11  \n0.4.15-9  \ncom.esotericsoftware  \nkryo-shaded  \n4.0.2  \ncom.esotericsoftware  \nminlog  \n1.3.0  \ncom.fasterxml  \nclassmate  \n1.0.0  \ncom.fasterxml.jackson.core  \njackson-annotations  \n2.6.7  \ncom.fasterxml.jackson.core  \njackson-core  \n2.6.7  \ncom.fasterxml.jackson.core  \njackson-databind  \n2.6.7.3"
    },
    {
        "id": 275,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "jackson-annotations  \n2.6.7  \ncom.fasterxml.jackson.core  \njackson-core  \n2.6.7  \ncom.fasterxml.jackson.core  \njackson-databind  \n2.6.7.3  \ncom.fasterxml.jackson.dataformat  \njackson-dataformat-cbor  \n2.6.7  \ncom.fasterxml.jackson.datatype  \njackson-datatype-joda  \n2.6.7  \ncom.fasterxml.jackson.module  \njackson-module-paranamer  \n2.6.7  \ncom.fasterxml.jackson.module  \njackson-module-scala_2.11  \n2.6.7.1  \ncom.github.fommil  \njniloader  \n1.1  \ncom.github.fommil.netlib  \ncore  \n1.1.2  \ncom.github.fommil.netlib  \nnative_ref-java  \n1.1  \ncom.github.fommil.netlib  \nnative_ref-java-natives  \n1.1  \ncom.github.fommil.netlib  \nnative_system-java  \n1.1  \ncom.github.fommil.netlib  \nnative_system-java-natives  \n1.1  \ncom.github.fommil.netlib  \nnetlib-native_ref-linux-x86_64-natives  \n1.1  \ncom.github.fommil.netlib  \nnetlib-native_system-linux-x86_64-natives  \n1.1  \ncom.github.luben  \nzstd-jni  \n1.3.2-2  \ncom.github.rwl  \njtransforms  \n2.4.0  \ncom.google.code.findbugs  \njsr305  \n2.0.1  \ncom.google.code.gson  \ngson  \n2.2.4  \ncom.google.guava  \nguava  \n15.0  \ncom.google.protobuf  \nprotobuf-java  \n2.6.1  \ncom.googlecode.javaewah  \nJavaEWAH  \n0.3.2  \ncom.h2database  \nh2  \n1.3.174  \ncom.jcraft  \njsch  \n0.1.50  \ncom.jolbox  \nbonecp  \n0.8.0.RELEASE"
    },
    {
        "id": 276,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "JavaEWAH  \n0.3.2  \ncom.h2database  \nh2  \n1.3.174  \ncom.jcraft  \njsch  \n0.1.50  \ncom.jolbox  \nbonecp  \n0.8.0.RELEASE  \ncom.microsoft.azure  \nazure-data-lake-store-sdk  \n2.2.8  \ncom.microsoft.azure  \nazure-storage  \n7.0.0  \ncom.microsoft.sqlserver  \nmssql-jdbc  \n6.2.2.jre8  \ncom.ning  \ncompress-lzf  \n1.0.3  \ncom.sun.mail  \njavax.mail  \n1.5.2  \ncom.thoughtworks.paranamer  \nparanamer  \n2.8  \ncom.trueaccord.lenses  \nlenses_2.11  \n0.3  \ncom.twitter  \nchill-java  \n0.9.3  \ncom.twitter  \nchill_2.11  \n0.9.3  \ncom.twitter  \nparquet-hadoop-bundle  \n1.6.0  \ncom.twitter  \nutil-app_2.11  \n6.23.0  \ncom.twitter  \nutil-core_2.11  \n6.23.0  \ncom.twitter  \nutil-jvm_2.11  \n6.23.0  \ncom.typesafe  \nconfig  \n1.2.1  \ncom.typesafe.scala-logging  \nscala-logging-api_2.11  \n2.1.2  \ncom.typesafe.scala-logging  \nscala-logging-slf4j_2.11  \n2.1.2  \ncom.univocity  \nunivocity-parsers  \n2.7.3  \ncom.vlkan  \nflatbuffers  \n1.2.0-3f79e055  \ncom.zaxxer  \nHikariCP  \n3.1.0  \ncommons-beanutils  \ncommons-beanutils  \n1.9.4  \ncommons-cli  \ncommons-cli  \n1.2  \ncommons-codec  \ncommons-codec  \n1.10  \ncommons-collections  \ncommons-collections  \n3.2.2  \ncommons-configuration  \ncommons-configuration  \n1.6  \ncommons-dbcp  \ncommons-dbcp  \n1.4  \ncommons-digester  \ncommons-digester  \n1.8  \ncommons-fileupload  \ncommons-fileupload  \n1.3.3  \ncommons-httpclient  \ncommons-httpclient  \n3.1  \ncommons-io  \ncommons-io  \n2.4  \ncommons-lang  \ncommons-lang"
    },
    {
        "id": 277,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "1.8  \ncommons-fileupload  \ncommons-fileupload  \n1.3.3  \ncommons-httpclient  \ncommons-httpclient  \n3.1  \ncommons-io  \ncommons-io  \n2.4  \ncommons-lang  \ncommons-lang  \n2.6  \ncommons-logging  \ncommons-logging  \n1.1.3  \ncommons-net  \ncommons-net  \n3.1  \ncommons-pool  \ncommons-pool  \n1.5.4  \ninfo.ganglia.gmetric4j  \ngmetric4j  \n1.0.7  \nio.airlift  \naircompressor  \n0.10  \nio.dropwizard.metrics  \nmetrics-core  \n3.1.5  \nio.dropwizard.metrics  \nmetrics-ganglia  \n3.1.5  \nio.dropwizard.metrics  \nmetrics-graphite  \n3.1.5  \nio.dropwizard.metrics  \nmetrics-healthchecks  \n3.1.5  \nio.dropwizard.metrics  \nmetrics-jetty9  \n3.1.5  \nio.dropwizard.metrics  \nmetrics-json  \n3.1.5  \nio.dropwizard.metrics  \nmetrics-jvm  \n3.1.5  \nio.dropwizard.metrics  \nmetrics-log4j  \n3.1.5  \nio.dropwizard.metrics  \nmetrics-servlets  \n3.1.5  \nio.netty  \nnetty  \n3.9.9.Final  \nio.netty  \nnetty-all  \n4.1.42.Final  \njavax.activation  \nactivation  \n1.1.1  \njavax.annotation  \njavax.annotation-api  \n1.2  \njavax.el  \njavax.el-api  \n2.2.4  \njavax.jdo  \njdo-api  \n3.0.1  \njavax.servlet  \njavax.servlet-api  \n3.1.0  \njavax.servlet.jsp  \njsp-api  \n2.1  \njavax.transaction  \njta  \n1.1  \njavax.validation  \nvalidation-api  \n1.1.0.Final  \njavax.ws.rs  \njavax.ws.rs-api  \n2.0.1  \njavax.xml.bind  \njaxb-api  \n2.2.2  \njavax.xml.stream  \nstax-api"
    },
    {
        "id": 278,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "javax.ws.rs-api  \n2.0.1  \njavax.xml.bind  \njaxb-api  \n2.2.2  \njavax.xml.stream  \nstax-api  \n1.0-2  \njavolution  \njavolution  \n5.5.1  \njline  \njline  \n2.14.6  \njoda-time  \njoda-time  \n2.9.3  \njunit  \njunit  \n4.12  \nlog4j  \napache-log4j-extras  \n1.2.17  \nlog4j  \nlog4j  \n1.2.17  \nnet.hydromatic  \neigenbase-properties  \n1.1.5  \nnet.razorvine  \npyrolite  \n4.13  \nnet.sf.jpam  \njpam  \n1.1  \nnet.sf.opencsv  \nopencsv  \n2.3  \nnet.sf.supercsv  \nsuper-csv  \n2.2.0  \nnet.snowflake  \nsnowflake-ingest-sdk  \n0.9.6  \nnet.snowflake  \nsnowflake-jdbc  \n3.12.0  \nnet.snowflake  \nspark-snowflake_2.11  \n2.5.9-spark_2.4  \nnet.sourceforge.f2j  \narpack_combined_all  \n0.1  \norg.acplt  \noncrpc  \n1.0.7  \norg.antlr  \nST4  \n4.0.4  \norg.antlr  \nantlr-runtime  \n3.4  \norg.antlr  \nantlr4-runtime  \n4.7  \norg.antlr  \nstringtemplate  \n3.2.1  \norg.apache.ant  \nant  \n1.9.2  \norg.apache.ant  \nant-jsch  \n1.9.2  \norg.apache.ant  \nant-launcher  \n1.9.2  \norg.apache.arrow  \narrow-format  \n0.10.0  \norg.apache.arrow  \narrow-memory  \n0.10.0  \norg.apache.arrow  \narrow-vector  \n0.10.0  \norg.apache.avro  \navro  \n1.8.2  \norg.apache.avro  \navro-ipc  \n1.8.2  \norg.apache.avro  \navro-mapred-hadoop2  \n1.8.2  \norg.apache.calcite  \ncalcite-avatica  \n1.2.0-incubating  \norg.apache.calcite  \ncalcite-core  \n1.2.0-incubating  \norg.apache.calcite"
    },
    {
        "id": 279,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "org.apache.calcite  \ncalcite-avatica  \n1.2.0-incubating  \norg.apache.calcite  \ncalcite-core  \n1.2.0-incubating  \norg.apache.calcite  \ncalcite-linq4j  \n1.2.0-incubating  \norg.apache.commons  \ncommons-compress  \n1.8.1  \norg.apache.commons  \ncommons-crypto  \n1.0.0  \norg.apache.commons  \ncommons-lang3  \n3.5  \norg.apache.commons  \ncommons-math3  \n3.4.1  \norg.apache.curator  \ncurator-client  \n2.7.1  \norg.apache.curator  \ncurator-framework  \n2.7.1  \norg.apache.curator  \ncurator-recipes  \n2.7.1  \norg.apache.derby  \nderby  \n10.12.1.1  \norg.apache.directory.api  \napi-asn1-api  \n1.0.0-M20  \norg.apache.directory.api  \napi-util  \n1.0.0-M20  \norg.apache.directory.server  \napacheds-i18n  \n2.0.0-M15  \norg.apache.directory.server  \napacheds-kerberos-codec  \n2.0.0-M15  \norg.apache.hadoop  \nhadoop-annotations  \n2.7.3  \norg.apache.hadoop  \nhadoop-auth  \n2.7.3  \norg.apache.hadoop  \nhadoop-client  \n2.7.3  \norg.apache.hadoop  \nhadoop-common  \n2.7.3  \norg.apache.hadoop  \nhadoop-hdfs  \n2.7.3  \norg.apache.hadoop  \nhadoop-mapreduce-client-app  \n2.7.3  \norg.apache.hadoop  \nhadoop-mapreduce-client-common  \n2.7.3  \norg.apache.hadoop  \nhadoop-mapreduce-client-core  \n2.7.3  \norg.apache.hadoop  \nhadoop-mapreduce-client-jobclient  \n2.7.3  \norg.apache.hadoop  \nhadoop-mapreduce-client-shuffle  \n2.7.3  \norg.apache.hadoop  \nhadoop-yarn-api  \n2.7.3  \norg.apache.hadoop"
    },
    {
        "id": 280,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "hadoop-mapreduce-client-shuffle  \n2.7.3  \norg.apache.hadoop  \nhadoop-yarn-api  \n2.7.3  \norg.apache.hadoop  \nhadoop-yarn-client  \n2.7.3  \norg.apache.hadoop  \nhadoop-yarn-common  \n2.7.3  \norg.apache.hadoop  \nhadoop-yarn-server-common  \n2.7.3  \norg.apache.htrace  \nhtrace-core  \n3.1.0-incubating  \norg.apache.httpcomponents  \nhttpclient  \n4.5.6  \norg.apache.httpcomponents  \nhttpcore  \n4.4.10  \norg.apache.ivy  \nivy  \n2.4.0  \norg.apache.orc  \norc-core-nohive  \n1.5.5  \norg.apache.orc  \norc-mapreduce-nohive  \n1.5.5  \norg.apache.orc  \norc-shims  \n1.5.5  \norg.apache.parquet  \nparquet-column  \n1.10.1.2-databricks4  \norg.apache.parquet  \nparquet-common  \n1.10.1.2-databricks4  \norg.apache.parquet  \nparquet-encoding  \n1.10.1.2-databricks4  \norg.apache.parquet  \nparquet-format  \n2.4.0  \norg.apache.parquet  \nparquet-hadoop  \n1.10.1.2-databricks4  \norg.apache.parquet  \nparquet-jackson  \n1.10.1.2-databricks4  \norg.apache.thrift  \nlibfb303  \n0.9.3  \norg.apache.thrift  \nlibthrift  \n0.9.3  \norg.apache.xbean  \nxbean-asm6-shaded  \n4.8  \norg.apache.zookeeper  \nzookeeper  \n3.4.6  \norg.codehaus.jackson  \njackson-core-asl  \n1.9.13  \norg.codehaus.jackson  \njackson-jaxrs  \n1.9.13  \norg.codehaus.jackson  \njackson-mapper-asl  \n1.9.13  \norg.codehaus.jackson  \njackson-xc  \n1.9.13  \norg.codehaus.janino  \ncommons-compiler  \n3.0.10  \norg.codehaus.janino  \njanino  \n3.0.10"
    },
    {
        "id": 281,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "jackson-xc  \n1.9.13  \norg.codehaus.janino  \ncommons-compiler  \n3.0.10  \norg.codehaus.janino  \njanino  \n3.0.10  \norg.datanucleus  \ndatanucleus-api-jdo  \n3.2.6  \norg.datanucleus  \ndatanucleus-core  \n3.2.10  \norg.datanucleus  \ndatanucleus-rdbms  \n3.2.9  \norg.eclipse.jetty  \njetty-client  \n9.3.27.v20190418  \norg.eclipse.jetty  \njetty-continuation  \n9.3.27.v20190418  \norg.eclipse.jetty  \njetty-http  \n9.3.27.v20190418  \norg.eclipse.jetty  \njetty-io  \n9.3.27.v20190418  \norg.eclipse.jetty  \njetty-jndi  \n9.3.27.v20190418  \norg.eclipse.jetty  \njetty-plus  \n9.3.27.v20190418  \norg.eclipse.jetty  \njetty-proxy  \n9.3.27.v20190418  \norg.eclipse.jetty  \njetty-security  \n9.3.27.v20190418  \norg.eclipse.jetty  \njetty-server  \n9.3.27.v20190418  \norg.eclipse.jetty  \njetty-servlet  \n9.3.27.v20190418  \norg.eclipse.jetty  \njetty-servlets  \n9.3.27.v20190418  \norg.eclipse.jetty  \njetty-util  \n9.3.27.v20190418  \norg.eclipse.jetty  \njetty-webapp  \n9.3.27.v20190418  \norg.eclipse.jetty  \njetty-xml  \n9.3.27.v20190418  \norg.fusesource.leveldbjni  \nleveldbjni-all  \n1.8  \norg.glassfish.hk2  \nhk2-api  \n2.4.0-b34  \norg.glassfish.hk2  \nhk2-locator  \n2.4.0-b34  \norg.glassfish.hk2  \nhk2-utils  \n2.4.0-b34  \norg.glassfish.hk2  \nosgi-resource-locator  \n1.0.1  \norg.glassfish.hk2.external  \naopalliance-repackaged"
    },
    {
        "id": 282,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "hk2-utils  \n2.4.0-b34  \norg.glassfish.hk2  \nosgi-resource-locator  \n1.0.1  \norg.glassfish.hk2.external  \naopalliance-repackaged  \n2.4.0-b34  \norg.glassfish.hk2.external  \njavax.inject  \n2.4.0-b34  \norg.glassfish.jersey.bundles.repackaged  \njersey-guava  \n2.22.2  \norg.glassfish.jersey.containers  \njersey-container-servlet  \n2.22.2  \norg.glassfish.jersey.containers  \njersey-container-servlet-core  \n2.22.2  \norg.glassfish.jersey.core  \njersey-client  \n2.22.2  \norg.glassfish.jersey.core  \njersey-common  \n2.22.2  \norg.glassfish.jersey.core  \njersey-server  \n2.22.2  \norg.glassfish.jersey.media  \njersey-media-jaxb  \n2.22.2  \norg.hamcrest  \nhamcrest-core  \n1.3  \norg.hamcrest  \nhamcrest-library  \n1.3  \norg.hibernate  \nhibernate-validator  \n5.1.1.Final  \norg.iq80.snappy  \nsnappy  \n0.2  \norg.javassist  \njavassist  \n3.18.1-GA  \norg.jboss.logging  \njboss-logging  \n3.1.3.GA  \norg.jdbi  \njdbi  \n2.63.1  \norg.joda  \njoda-convert  \n1.7  \norg.jodd  \njodd-core  \n3.5.2  \norg.json4s  \njson4s-ast_2.11  \n3.5.3  \norg.json4s  \njson4s-core_2.11  \n3.5.3  \norg.json4s  \njson4s-jackson_2.11  \n3.5.3  \norg.json4s  \njson4s-scalap_2.11  \n3.5.3  \norg.lz4  \nlz4-java  \n1.4.0  \norg.mariadb.jdbc  \nmariadb-java-client  \n2.1.2  \norg.mockito  \nmockito-core  \n1.10.19  \norg.objenesis  \nobjenesis  \n2.5.1  \norg.postgresql  \npostgresql  \n42.1.4  \norg.roaringbitmap"
    },
    {
        "id": 283,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "org.mockito  \nmockito-core  \n1.10.19  \norg.objenesis  \nobjenesis  \n2.5.1  \norg.postgresql  \npostgresql  \n42.1.4  \norg.roaringbitmap  \nRoaringBitmap  \n0.7.45  \norg.roaringbitmap  \nshims  \n0.7.45  \norg.rocksdb  \nrocksdbjni  \n6.2.2  \norg.rosuda.REngine  \nREngine  \n2.1.0  \norg.scala-lang  \nscala-compiler_2.11  \n2.11.12  \norg.scala-lang  \nscala-library_2.11  \n2.11.12  \norg.scala-lang  \nscala-reflect_2.11  \n2.11.12  \norg.scala-lang.modules  \nscala-parser-combinators_2.11  \n1.1.0  \norg.scala-lang.modules  \nscala-xml_2.11  \n1.0.5  \norg.scala-sbt  \ntest-interface  \n1.0  \norg.scalacheck  \nscalacheck_2.11  \n1.12.5  \norg.scalactic  \nscalactic_2.11  \n3.0.3  \norg.scalanlp  \nbreeze-macros_2.11  \n0.13.2  \norg.scalanlp  \nbreeze_2.11  \n0.13.2  \norg.scalatest  \nscalatest_2.11  \n3.0.3  \norg.slf4j  \njcl-over-slf4j  \n1.7.16  \norg.slf4j  \njul-to-slf4j  \n1.7.16  \norg.slf4j  \nslf4j-api  \n1.7.16  \norg.slf4j  \nslf4j-log4j12  \n1.7.16  \norg.spark-project.hive  \nhive-beeline  \n1.2.1.spark2  \norg.spark-project.hive  \nhive-cli  \n1.2.1.spark2  \norg.spark-project.hive  \nhive-jdbc  \n1.2.1.spark2  \norg.spark-project.hive  \nhive-metastore  \n1.2.1.spark2  \norg.spark-project.spark  \nunused  \n1.0.0  \norg.spire-math  \nspire-macros_2.11  \n0.13.0  \norg.spire-math"
    },
    {
        "id": 284,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/6.5.html",
        "content": "hive-metastore  \n1.2.1.spark2  \norg.spark-project.spark  \nunused  \n1.0.0  \norg.spire-math  \nspire-macros_2.11  \n0.13.0  \norg.spire-math  \nspire_2.11  \n0.13.0  \norg.springframework  \nspring-core  \n4.1.4.RELEASE  \norg.springframework  \nspring-test  \n4.1.4.RELEASE  \norg.tukaani  \nxz  \n1.5  \norg.typelevel  \nmachinist_2.11  \n0.6.1  \norg.typelevel  \nmacro-compat_2.11  \n1.1.1  \norg.xerial  \nsqlite-jdbc  \n3.8.11.2  \norg.xerial.snappy  \nsnappy-java  \n1.1.7.3  \norg.yaml  \nsnakeyaml  \n1.16  \noro  \noro  \n2.0.8  \nsoftware.amazon.ion  \nion-java  \n1.0.2  \nstax  \nstax-api  \n1.0.1  \nxmlenc  \nxmlenc  \n0.52"
    },
    {
        "id": 285,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "Databricks Runtime 7.6 for Machine Learning (EoS)  \nNote  \nSupport for this Databricks Runtime version has ended. For the end-of-support date, see End-of-support history. For all supported Databricks Runtime versions, see Databricks Runtime release notes versions and compatibility.  \nDatabricks released this image in February 2021.  \nDatabricks Runtime 7.6 for Machine Learning provides a ready-to-go environment for machine learning and data science based on Databricks Runtime 7.6 (EoS). Databricks Runtime ML contains many popular machine learning libraries, including TensorFlow, PyTorch, and XGBoost. It also supports distributed deep learning training using Horovod.  \nFor more information, including instructions for creating a Databricks Runtime ML cluster, see AI and Machine Learning on Databricks.  \nFor help with migration from Databricks Runtime 6.x, see Databricks Runtime 7.x migration guide (EoS).  \nNew features and major changes"
    },
    {
        "id": 286,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "New features and major changes\nDatabricks Runtime 7.6 ML is built on top of Databricks Runtime 7.6. For information on what\u2019s new in Databricks Runtime 7.6, including Apache Spark MLlib and SparkR, see the Databricks Runtime 7.6 (EoS) release notes.  \nDeprecations  \nTensoflow 1.x will not be supported in the upcoming major release of Databricks Runtime  \nThe following CUDA packages are deprecated and will be removed in the upcoming major release of Databricks Runtime:  \ncuda-command-line-tools  \ncuda-compiler  \ncuda-cudart-dev  \ncuda-cufft  \ncuda-cufft-dev  \ncuda-cuobjdump  \ncuda-cupti  \ncuda-curand  \ncuda-curand-dev  \ncuda-cusolver  \ncuda-cusolver-dev  \ncuda-cusparse  \ncuda-cusparse-dev  \ncuda-documentation  \ncuda-driver-dev  \ncuda-gdb  \ncuda-gpu-library-advisor  \ncuda-libraries-dev  \ncuda-license  \ncuda-memcheck  \ncuda-minimal-build  \ncuda-misc-headers  \ncuda-npp  \ncuda-npp-dev  \ncuda-nsight  \ncuda-nvcc  \ncuda-nvdisasm  \ncuda-nvgraph  \ncuda-nvgraph-dev  \ncuda-nvjpeg  \ncuda-nvjpeg-dev  \ncuda-nvml-dev  \ncuda-nvprune  \ncuda-nvrtc-dev  \ncuda-nvvp  \ncuda-samples  \ncuda-sanitizer-api  \ncuda-toolkit  \ncuda-tools  \ncuda-visual-tools  \nfreeglut3  \nlibcublas-dev  \nlibcudnn7-dev  \nlibdrm-dev  \nlibegl1  \nlibegm-mesa0  \nlibgbl1-mesa-dev  \nlibgbm1  \nlibgles1  \nlibgles2  \nlibglu1-mesa  \nlibglu1-mesa-dev  \nlibnccl-dev  \nlibnvinfer-dev  \nlibnvinfer-plugin-dev  \nlibopengl0  \nlibwayland-server0  \nlibx11-xcb-dev"
    },
    {
        "id": 287,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "libnccl-dev  \nlibnvinfer-dev  \nlibnvinfer-plugin-dev  \nlibopengl0  \nlibwayland-server0  \nlibx11-xcb-dev  \nlibxcb-dri2-0-dev  \nlibxcb-dri3-dev  \nlibxcb-glx0-dev  \nlibxcb-present-dev  \nlibxcb-randr0  \nlibxcb-randr0-dev  \nlibxcb-render0-dev  \nlibxcb-shape0-dev  \nlibxcb-sync-dev  \nlibxcb-xfixes0  \nlibxcb-xfixes0-dev  \nlibxdamage-dev  \nlibxext-dev  \nlibxfixes-dev  \nlibxi-dev  \nlibxmu-dev  \nlibxmu-headers  \nlibxshmfence-dev  \nlibxxf86vm-dev  \nmesa-common-dev  \nnsight-compute  \nnsight-systems  \nx11proto-damage-dev  \nx11proto-fixes-dev  \nx11proto-input-dev  \nx11proto-xext-dev  \nx11proto-xf86vidmode-dev  \nMajor changes to Databricks Runtime ML Python environment  \nSee Databricks Runtime 7.6 (EoS) for the major changes to the Databricks Runtime Python environment. For a full list of installed Python packages and their versions, see Python libraries.  \nPython packages upgraded  \ndatabricks-cli 0.14.0 -> 0.14.1  \nkoalas 1.4.0 -> 1.5.0  \nlightgbm 2.3.0 -> 3.1.1  \nmlflow 1.12.1 -> 1.13.1  \nplotly 4.12.0 -> 4.14.1  \npytorch 1.7.0 -> 1.7.1  \ntorchvision 0.8.1 -> 0.8.2  \nxgboost 1.2.1 -> 1.3.1"
    },
    {
        "id": 288,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "Improvements\nImprovements\nPySpark integration of XGBoost (Public Preview)  \nThe XGBoost integration with PySpark has been improved. The package sparkdl 2.1.0-db5 includes two new PySpark ML estimators, XgboostRegressor and XgboostClassifier, which enable users to train XGBoost models in PySpark ML Pipelines.  \nPrior to this version, XGBoost was not integrated with PySpark. Users had to either use xgboost4j-spark in Scala or break the PySpark ML Pipeline, collect the Spark DataFrame on the driver as a pandas DataFrame, and use the Python package xgboost. See sparkdl API documentation and Use XGBoost on Databricks for more details.\n\nSystem environment"
    },
    {
        "id": 289,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "System environment\nThe system environment in Databricks Runtime 7.6 ML differs from Databricks Runtime 7.6 as follows:  \nDBUtils: Databricks Runtime ML does not contain Library utility (dbutils.library) (legacy). You can use %pip and %conda commands instead. See Notebook-scoped Python libraries.  \nFor GPU clusters, Databricks Runtime ML includes the following NVIDIA GPU libraries:  \nCUDA 10.1 Update 2  \ncuDNN 7.6.5  \nNCCL 2.7.3  \nTensorRT 6.0.1\n\nLibraries"
    },
    {
        "id": 290,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "Libraries\nThe following sections list the libraries included in Databricks Runtime 7.6 ML that differ from those included in Databricks Runtime 7.6.  \nIn this section:  \nTop-tier libraries  \nPython libraries  \nR libraries  \nJava and Scala libraries (Scala 2.12 cluster)  \nTop-tier libraries  \nDatabricks Runtime 7.6 ML includes the following top-tier libraries:  \nGraphFrames  \nHorovod and HorovodRunner  \nMLflow  \nPyTorch  \nspark-tensorflow-connector  \nTensorFlow  \nTensorBoard  \nPython libraries  \nDatabricks Runtime 7.6 ML uses Conda for Python package management and includes many popular ML packages.  \nIn addition to the packages specified in the Conda environments in the following sections, Databricks Runtime 7.6 ML also installs the following packages:  \nhyperopt 0.2.5.db1  \nsparkdl 2.1.0-db5  \nPython libraries on CPU clusters"
    },
    {
        "id": 291,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "name: databricks-ml channels: - pytorch - defaults dependencies: - _libgcc_mutex=0.1=main - absl-py=0.9.0=py37_0 - asn1crypto=1.3.0=py37_1 - astor=0.8.0=py37_0 - backcall=0.1.0=py37_0 - backports=1.0=pyhd3eb1b0_2 - bcrypt=3.2.0=py37h7b6447c_0 - blas=1.0=mkl - blinker=1.4=py37_0 - boto3=1.12.0=py_0 - botocore=1.15.0=py_0 - c-ares=1.17.1=h27cfd23_0 - ca-certificates=2021.1.19=h06a4308_1 # (updated from h06a4308_0 in May 26, 2021 maintenance update) - cachetools=4.2.0=pyhd3eb1b0_0 - certifi=2020.12.5=py37h06a4308_0 - cffi=1.14.0=py37he30daa8_1 # (updated from py37h2e261b9_0 in May 26, 2021 maintenance update) - chardet=3.0.4=py37h06a4308_1003 - click=7.0=py37_0 - cloudpickle=1.4.1=py_0 - configparser=3.7.4=py37_0 - cpuonly=1.0=0 - cryptography=2.8=py37h1ba5d50_0 - cycler=0.10.0=py37_0 - cython=0.29.15=py37he6710b0_0 - decorator=4.4.1=py_0 - dill=0.3.1.1=py37_1 -"
    },
    {
        "id": 292,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "- cython=0.29.15=py37he6710b0_0 - decorator=4.4.1=py_0 - dill=0.3.1.1=py37_1 - docutils=0.15.2=py37_0 - entrypoints=0.3=py37_0 - flask=1.1.1=py_1 - freetype=2.9.1=h8a8886c_1 - future=0.18.2=py37_1 - gast=0.3.3=py_0 - gitdb=4.0.5=py_0 - gitpython=3.1.0=py_0 - google-auth=1.11.2=py_0 - google-auth-oauthlib=0.4.1=py_2 - google-pasta=0.2.0=py_0 - grpcio=1.27.2=py37hf8bcb03_0 - gunicorn=20.0.4=py37_0 - h5py=2.10.0=py37h7918eee_0 - hdf5=1.10.4=hb1b8bf9_0 - icu=58.2=he6710b0_3 - idna=2.8=py37_0 - intel-openmp=2020.0=166 - ipykernel=5.1.4=py37h39e3cac_0 - ipython=7.12.0=py37h5ca1d4c_0 - ipython_genutils=0.2.0=pyhd3eb1b0_1 - isodate=0.6.0=py_1 - itsdangerous=1.1.0=py37_0 - jedi=0.17.2=py37h06a4308_1 - jinja2=2.11.1=py_0 - jmespath=0.10.0=py_0 - joblib=0.14.1=py_0 -"
    },
    {
        "id": 293,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "- jinja2=2.11.1=py_0 - jmespath=0.10.0=py_0 - joblib=0.14.1=py_0 - jpeg=9b=h024ee3a_2 - jupyter_client=5.3.4=py37_0 - jupyter_core=4.6.1=py37_0 - kiwisolver=1.1.0=py37he6710b0_0 - krb5=1.17.1=h173b8e3_0 # (updated from 1.16.4 in May 26, 2021 maintenance update) - ld_impl_linux-64=2.33.1=h53a641e_7 - libedit=3.1.20181209=hc058e9b_0 - libffi=3.3=he6710b0_2 # (updated from 3.2.1 in May 26, 2021 maintenance update) - libgcc-ng=9.1.0=hdf63c60_0 - libgfortran-ng=7.3.0=hdf63c60_0 - libpng=1.6.37=hbc83047_0 - libpq=12.2=h20c2e04_0 # (updated from 11.2 in May 26, 2021 maintenance update) - libprotobuf=3.11.4=hd408876_0 - libsodium=1.0.16=h1bed415_0 - libstdcxx-ng=9.1.0=hdf63c60_0 - libtiff=4.1.0=h2733197_0 - libuv=1.40.0=h7b6447c_0 - lightgbm=3.1.1=py37h2531618_0 - lz4-c=1.8.1.2=h14c3975_0 -"
    },
    {
        "id": 294,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "- lightgbm=3.1.1=py37h2531618_0 - lz4-c=1.8.1.2=h14c3975_0 - mako=1.1.2=py_0 - markdown=3.1.1=py37_0 - markupsafe=1.1.1=py37h14c3975_1 - matplotlib-base=3.1.3=py37hef1b27d_0 - mkl=2020.0=166 - mkl-service=2.3.0=py37he8ac12f_0 - mkl_fft=1.0.15=py37ha843d7b_0 - mkl_random=1.1.0=py37hd6b4f25_0 - ncurses=6.2=he6710b0_1 - networkx=2.4=py_1 - ninja=1.10.2=py37hff7bd54_0 - nltk=3.4.5=py37_0 - numpy=1.18.1=py37h4f9e942_0 - numpy-base=1.18.1=py37hde5b4d6_1 - oauthlib=3.1.0=py_0 - olefile=0.46=py37_0 - openssl=1.1.1k=h27cfd23_0 # (updated from 1.1.1i in May 26, 2021 maintenance update) - packaging=20.1=py_0 - pandas=1.0.1=py37h0573a6f_0 - paramiko=2.7.1=py_0 - parso=0.7.0=py_0 - patsy=0.5.1=py37_0 - pexpect=4.8.0=pyhd3eb1b0_3 - pickleshare=0.7.5=pyhd3eb1b0_1003 -"
    },
    {
        "id": 295,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "- pexpect=4.8.0=pyhd3eb1b0_3 - pickleshare=0.7.5=pyhd3eb1b0_1003 - pillow=7.0.0=py37hb39fc2d_0 - pip=20.0.2=py37_3 - plotly=4.14.1=pyhd3eb1b0_0 - prompt_toolkit=3.0.3=py_0 - protobuf=3.11.4=py37he6710b0_0 - psutil=5.6.7=py37h7b6447c_0 - psycopg2=2.8.6=py37h3c74f83_1 # (updated from 2.8.4 in May 26, 2021 maintenance update) - ptyprocess=0.6.0=pyhd3eb1b0_2 - pyasn1=0.4.8=py_0 - pyasn1-modules=0.2.8=py_0 - pycparser=2.19=py37_0 - pygments=2.5.2=py_0 - pyjwt=2.0.1=py37h06a4308_0 - pynacl=1.3.0=py37h7b6447c_0 - pyodbc=4.0.30=py37he6710b0_0 - pyopenssl=19.1.0=pyhd3eb1b0_1 - pyparsing=2.4.6=py_0 - pysocks=1.7.1=py37_1 - python=3.7.10=hdb3f193_0 # (updated from 3.7.6 in May 26, 2021 maintenance update) - python-dateutil=2.8.1=py_0 - python-editor=1.0.4=py_0 - pytorch=1.7.1=py3.7_cpu_0 -"
    },
    {
        "id": 296,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "- python-dateutil=2.8.1=py_0 - python-editor=1.0.4=py_0 - pytorch=1.7.1=py3.7_cpu_0 - pytz=2019.3=py_0 - pyzmq=18.1.1=py37he6710b0_0 - readline=8.1=h27cfd23_0 # (updated from 7.0 in May 26, 2021 maintenance update) - requests=2.22.0=py37_1 - requests-oauthlib=1.3.0=py_0 - retrying=1.3.3=py37_2 - rsa=4.0=py_0 - s3transfer=0.3.4=pyhd3eb1b0_0 - scikit-learn=0.22.1=py37hd81dba3_0 - scipy=1.4.1=py37h0b6359f_0 - setuptools=45.2.0=py37_0 - simplejson=3.17.0=py37h7b6447c_0 - six=1.14.0=py37h06a4308_0 - smmap=3.0.4=py_0 - sqlite=3.35.4=hdfb4753_0 # (updated from 3.31.1 in May 26, 2021 maintenance update) - sqlparse=0.4.1=py_0 - statsmodels=0.11.0=py37h7b6447c_0 - tabulate=0.8.3=py37_0 - tk=8.6.10=hbc83047_0 # (updated from 8.6.8 in May 26, 2021 maintenance update) - torchvision=0.8.2=py37_cpu - tornado=6.0.3=py37h7b6447c_3 - tqdm=4.42.1=py_0 -"
    },
    {
        "id": 297,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "- tornado=6.0.3=py37h7b6447c_3 - tqdm=4.42.1=py_0 - traitlets=4.3.3=py37_0 - typing_extensions=3.7.4.3=py_0 - unixodbc=2.3.7=h14c3975_0 - urllib3=1.25.8=py37_0 - wcwidth=0.1.8=py_0 - websocket-client=0.56.0=py37_0 - werkzeug=1.0.0=py_0 - wheel=0.34.2=py37_0 - wrapt=1.11.2=py37h7b6447c_0 - xz=5.2.5=h7b6447c_0 # (updated from 5.2.4 in May 26, 2021 maintenance update) - zeromq=4.3.1=he6710b0_3 - zlib=1.2.11=h7b6447c_3 - zstd=1.3.7=h0b5b093_0 - pip: - astunparse==1.6.3 - azure-core==1.10.0 - azure-storage-blob==12.7.0 - databricks-cli==0.14.1 - diskcache==5.1.0 - docker==4.4.1 - gorilla==0.3.0 - horovod==0.20.3 - joblibspark==0.3.0 - keras-preprocessing==1.1.2 - koalas==1.5.0 - mleap==0.16.1 - mlflow==1.13.1 - msrest==0.6.19 - opt-einsum==3.3.0 - petastorm==0.9.7 - pyarrow==1.0.1 - pyyaml==5.4 - querystring-parser==1.2.4 -"
    },
    {
        "id": 298,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "- petastorm==0.9.7 - pyarrow==1.0.1 - pyyaml==5.4 - querystring-parser==1.2.4 - seaborn==0.10.0 - spark-tensorflow-distributor==0.1.0 - tensorboard==2.3.0 - tensorboard-plugin-wit==1.8.0 - tensorflow-cpu==2.3.1 - tensorflow-estimator==2.3.0 - termcolor==1.1.0 - xgboost==1.3.1 prefix: /databricks/conda/envs/databricks-ml"
    },
    {
        "id": 299,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "name: databricks-ml-gpu channels: - pytorch - defaults dependencies: - _libgcc_mutex=0.1=main - absl-py=0.9.0=py37_0 - asn1crypto=1.3.0=py37_1 - astor=0.8.0=py37_0 - backcall=0.1.0=py37_0 - backports=1.0=pyhd3eb1b0_2 - bcrypt=3.2.0=py37h7b6447c_0 - blas=1.0=mkl - blinker=1.4=py37_0 - boto3=1.12.0=py_0 - botocore=1.15.0=py_0 - c-ares=1.17.1=h27cfd23_0 - ca-certificates=2021.1.19=h06a4308_1 # (updated from h06a4308_0 in May 26, 2021 maintenance update) - cachetools=4.2.0=pyhd3eb1b0_0 - certifi=2020.12.5=py37h06a4308_0 - cffi=1.14.0=py37he30daa8_1 # (updated from py37h2e261b9_0 in May 26, 2021 maintenance update) - chardet=3.0.4=py37h06a4308_1003 - click=7.0=py37_0 - cloudpickle=1.4.1=py_0 - configparser=3.7.4=py37_0 - cryptography=2.8=py37h1ba5d50_0 - cudatoolkit=10.1.243=h6bb024c_0 - cycler=0.10.0=py37_0 - cython=0.29.15=py37he6710b0_0 - decorator=4.4.1=py_0 -"
    },
    {
        "id": 300,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "- cycler=0.10.0=py37_0 - cython=0.29.15=py37he6710b0_0 - decorator=4.4.1=py_0 - dill=0.3.1.1=py37_1 - docutils=0.15.2=py37_0 - entrypoints=0.3=py37_0 - flask=1.1.1=py_1 - freetype=2.9.1=h8a8886c_1 - future=0.18.2=py37_1 - gast=0.3.3=py_0 - gitdb=4.0.5=py_0 - gitpython=3.1.0=py_0 - google-auth=1.11.2=py_0 - google-auth-oauthlib=0.4.1=py_2 - google-pasta=0.2.0=py_0 - grpcio=1.27.2=py37hf8bcb03_0 - gunicorn=20.0.4=py37_0 - h5py=2.10.0=py37h7918eee_0 - hdf5=1.10.4=hb1b8bf9_0 - icu=58.2=he6710b0_3 - idna=2.8=py37_0 - intel-openmp=2020.0=166 - ipykernel=5.1.4=py37h39e3cac_0 - ipython=7.12.0=py37h5ca1d4c_0 - ipython_genutils=0.2.0=pyhd3eb1b0_1 - isodate=0.6.0=py_1 - itsdangerous=1.1.0=py37_0 - jedi=0.17.2=py37h06a4308_1 - jinja2=2.11.1=py_0 - jmespath=0.10.0=py_0 -"
    },
    {
        "id": 301,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "jedi=0.17.2=py37h06a4308_1 - jinja2=2.11.1=py_0 - jmespath=0.10.0=py_0 - joblib=0.14.1=py_0 - jpeg=9b=h024ee3a_2 - jupyter_client=5.3.4=py37_0 - jupyter_core=4.6.1=py37_0 - kiwisolver=1.1.0=py37he6710b0_0 - krb5=1.17.1=h173b8e3_0 # (updated from 1.16.4 in May 26, 2021 maintenance update) - ld_impl_linux-64=2.33.1=h53a641e_7 - libedit=3.1.20181209=hc058e9b_0 - libffi=3.3=he6710b0_2 # (updated from 3.2.1 in May 26, 2021 maintenance update) - libgcc-ng=9.1.0=hdf63c60_0 - libgfortran-ng=7.3.0=hdf63c60_0 - libpng=1.6.37=hbc83047_0 - libpq=12.2=h20c2e04_0 # (updated from 11.2 in May 26, 2021 maintenance update) - libprotobuf=3.11.4=hd408876_0 - libsodium=1.0.16=h1bed415_0 - libstdcxx-ng=9.1.0=hdf63c60_0 - libtiff=4.1.0=h2733197_0 - libuv=1.40.0=h7b6447c_0 - lightgbm=3.1.1=py37h2531618_0 -"
    },
    {
        "id": 302,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "- libuv=1.40.0=h7b6447c_0 - lightgbm=3.1.1=py37h2531618_0 - lz4-c=1.8.1.2=h14c3975_0 - mako=1.1.2=py_0 - markdown=3.1.1=py37_0 - markupsafe=1.1.1=py37h14c3975_1 - matplotlib-base=3.1.3=py37hef1b27d_0 - mkl=2020.0=166 - mkl-service=2.3.0=py37he8ac12f_0 - mkl_fft=1.0.15=py37ha843d7b_0 - mkl_random=1.1.0=py37hd6b4f25_0 - ncurses=6.2=he6710b0_1 - networkx=2.4=py_1 - ninja=1.10.2=py37hff7bd54_0 - nltk=3.4.5=py37_0 - numpy=1.18.1=py37h4f9e942_0 - numpy-base=1.18.1=py37hde5b4d6_1 - oauthlib=3.1.0=py_0 - olefile=0.46=py37_0 - openssl=1.1.1k=h27cfd23_0 # (updated from 1.1.1i in May 26, 2021 maintenance update) - packaging=20.1=py_0 - pandas=1.0.1=py37h0573a6f_0 - paramiko=2.7.1=py_0 - parso=0.7.0=py_0 - patsy=0.5.1=py37_0 - pexpect=4.8.0=pyhd3eb1b0_3 -"
    },
    {
        "id": 303,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "- parso=0.7.0=py_0 - patsy=0.5.1=py37_0 - pexpect=4.8.0=pyhd3eb1b0_3 - pickleshare=0.7.5=pyhd3eb1b0_1003 - pillow=7.0.0=py37hb39fc2d_0 - pip=20.0.2=py37_3 - plotly=4.14.1=pyhd3eb1b0_0 - prompt_toolkit=3.0.3=py_0 - protobuf=3.11.4=py37he6710b0_0 - psutil=5.6.7=py37h7b6447c_0 - psycopg2=2.8.6=py37h3c74f83_1 # (updated from 2.8.4 in May 26, 2021 maintenance update) - ptyprocess=0.6.0=pyhd3eb1b0_2 - pyasn1=0.4.8=py_0 - pyasn1-modules=0.2.8=py_0 - pycparser=2.19=py37_0 - pygments=2.5.2=py_0 - pyjwt=2.0.1=py37h06a4308_0 - pynacl=1.3.0=py37h7b6447c_0 - pyodbc=4.0.30=py37he6710b0_0 - pyopenssl=19.1.0=pyhd3eb1b0_1 - pyparsing=2.4.6=py_0 - pysocks=1.7.1=py37_1 - python=3.7.10=hdb3f193_0 # (updated from 3.7.6 in May 26, 2021 maintenance update) - python-dateutil=2.8.1=py_0 -"
    },
    {
        "id": 304,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "# (updated from 3.7.6 in May 26, 2021 maintenance update) - python-dateutil=2.8.1=py_0 - python-editor=1.0.4=py_0 - pytorch=1.7.1=py3.7_cuda10.1.243_cudnn7.6.3_0 - pytz=2019.3=py_0 - pyzmq=18.1.1=py37he6710b0_0 - readline=8.1=h27cfd23_0 # (updated from 7.0 in May 26, 2021 maintenance update) - requests=2.22.0=py37_1 - requests-oauthlib=1.3.0=py_0 - retrying=1.3.3=py37_2 - rsa=4.0=py_0 - s3transfer=0.3.4=pyhd3eb1b0_0 - scikit-learn=0.22.1=py37hd81dba3_0 - scipy=1.4.1=py37h0b6359f_0 - setuptools=45.2.0=py37_0 - simplejson=3.17.0=py37h7b6447c_0 - six=1.14.0=py37h06a4308_0 - smmap=3.0.4=py_0 - sqlite=3.35.4=hdfb4753_0 # (updated from 3.31.1 in May 26, 2021 maintenance update) - sqlparse=0.4.1=py_0 - statsmodels=0.11.0=py37h7b6447c_0 - tabulate=0.8.3=py37_0 - tk=8.6.10=hbc83047_0 # (updated from 8.6.8 in May 26, 2021 maintenance update) - torchvision=0.8.2=py37_cu101 -"
    },
    {
        "id": 305,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "# (updated from 8.6.8 in May 26, 2021 maintenance update) - torchvision=0.8.2=py37_cu101 - tornado=6.0.3=py37h7b6447c_3 - tqdm=4.42.1=py_0 - traitlets=4.3.3=py37_0 - typing_extensions=3.7.4.3=py_0 - unixodbc=2.3.7=h14c3975_0 - urllib3=1.25.8=py37_0 - wcwidth=0.1.8=py_0 - websocket-client=0.56.0=py37_0 - werkzeug=1.0.0=py_0 - wheel=0.34.2=py37_0 - wrapt=1.11.2=py37h7b6447c_0 - xz=5.2.5=h7b6447c_0 # (updated from 5.2.4 in May 26, 2021 maintenance update) - zeromq=4.3.1=he6710b0_3 - zlib=1.2.11=h7b6447c_3 - zstd=1.3.7=h0b5b093_0 - pip: - astunparse==1.6.3 - azure-core==1.10.0 - azure-storage-blob==12.7.0 - databricks-cli==0.14.1 - diskcache==5.1.0 - docker==4.4.1 - gorilla==0.3.0 - horovod==0.20.3 - joblibspark==0.3.0 - keras-preprocessing==1.1.2 - koalas==1.5.0 - mleap==0.16.1 - mlflow==1.13.1 - msrest==0.6.19 - opt-einsum==3.3.0 - petastorm==0.9.7"
    },
    {
        "id": 306,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "- mlflow==1.13.1 - msrest==0.6.19 - opt-einsum==3.3.0 - petastorm==0.9.7 - pyarrow==1.0.1 - pyyaml==5.4 - querystring-parser==1.2.4 - seaborn==0.10.0 - spark-tensorflow-distributor==0.1.0 - tensorboard==2.3.0 - tensorboard-plugin-wit==1.8.0 - tensorflow==2.3.1 - tensorflow-estimator==2.3.0 - termcolor==1.1.0 - xgboost==1.3.1 prefix: /databricks/conda/envs/databricks-ml-gpu"
    },
    {
        "id": 307,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.6ml.html",
        "content": "Spark packages containing Python modules  \nSpark Package  \nPython Module  \nVersion  \ngraphframes  \ngraphframes  \n0.8.1-db1-spark3.0  \nR libraries  \nThe R libraries are identical to the R Libraries in Databricks Runtime 7.6.  \nJava and Scala libraries (Scala 2.12 cluster)  \nIn addition to Java and Scala libraries in Databricks Runtime 7.6, Databricks Runtime 7.6 ML contains the following JARs:  \nCPU clusters  \nGroup ID  \nArtifact ID  \nVersion  \ncom.typesafe.akka  \nakka-actor_2.12  \n2.5.23  \nml.combust.mleap  \nmleap-databricks-runtime_2.12  \n0.17.3-4882dc3  \nml.dmlc  \nxgboost4j-spark_2.12  \n1.2.0  \nml.dmlc  \nxgboost4j_2.12  \n1.2.0  \norg.mlflow  \nmlflow-client  \n1.13.1  \norg.scala-lang.modules  \nscala-java8-compat_2.12  \n0.8.0  \norg.tensorflow  \nspark-tensorflow-connector_2.12  \n1.15.0  \nGPU clusters  \nGroup ID  \nArtifact ID  \nVersion  \ncom.typesafe.akka  \nakka-actor_2.12  \n2.5.23  \nml.combust.mleap  \nmleap-databricks-runtime_2.12  \n0.17.3-4882dc3  \nml.dmlc  \nxgboost4j-spark-gpu_2.12  \n1.2.0  \nml.dmlc  \nxgboost4j-gpu_2.12  \n1.2.0  \norg.mlflow  \nmlflow-client  \n1.13.1  \norg.scala-lang.modules  \nscala-java8-compat_2.12  \n0.8.0  \norg.tensorflow  \nspark-tensorflow-connector_2.12  \n1.15.0"
    },
    {
        "id": 308,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-connect/python/jupyter-notebook.html",
        "content": "Use classic Jupyter Notebook with Databricks Connect for Python  \nNote  \nThis article covers Databricks Connect for Databricks Runtime 13.3 LTS and above.  \nThis article covers how to use Databricks Connect for Python with classic Jupyter Notebook. Databricks Connect enables you to connect popular notebook servers, IDEs, and other custom applications to Databricks clusters. See What is Databricks Connect?.  \nNote  \nBefore you begin to use Databricks Connect, you must set up the Databricks Connect client.  \nTo use Databricks Connect with classic Jupyter Notebook and Python, follow these instructions.  \nTo install classic Jupyter Notebook, with your Python virtual environment activated, run the following command from your terminal or Command Prompt:  \npip3 install notebook  \nTo start classic Jupyter Notebook in your web browser, run the following command from your activated Python virtual environment:  \njupyter notebook  \nIf classic Jupyter Notebook does not appear in your web browser, copy the URL that starts with localhost or 127.0.0.1 from your virtual environment, and enter it in your web browser\u2019s address bar.  \nCreate a new notebook: in classic Jupyter Notebook, on the Files tab, click New > Python 3 (ipykernel).  \nIn the notebook\u2019s first cell, enter either the example code or your own code. If you use your own code, at minimum you must initialize DatabricksSession as shown in the example code.  \nTo run the notebook, click Cell > Run All. All Python code runs locally, while all PySpark code involving DataFrame operations runs on the cluster in the remote Databricks workspace and run responses are sent back to the local caller.  \nTo debug the notebook, add the following line of code at the beginning of your notebook:  \nfrom IPython.core.debugger import set_trace  \nAnd then call set_trace() to enter debug statements at that point of notebook execution. All Python code is debugged locally, while all PySpark code continues to run on the cluster in the remote Databricks workspace. The core Spark engine code cannot be debugged directly from the client."
    },
    {
        "id": 309,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-connect/python/jupyter-notebook.html",
        "content": "To shut down classic Jupyter Notebook, click File > Close and Halt. If the classic Jupyter Notebook process is still running in your terminal or Command Prompt, stop this process by pressing Ctrl + c and then entering y to confirm."
    },
    {
        "id": 310,
        "url": "https://docs.databricks.com/en/dev-tools/cli/sync-commands.html",
        "content": "sync command group  \nNote  \nThis information applies to Databricks CLI versions 0.205 and above, which are in Public Preview. To find your version of the Databricks CLI, run databricks -v.  \nAlso, note that the sync command group can synchronize file changes from a local development machine only to workspace user (/Users) files in your Databricks workspace. It cannot synchronize to DBFS (dbfs:/) files. To synchronize file changes from a local development machine to DBFS (dbfs:/) in your Databricks workspace, use the dbx sync utility.  \nThe sync command group within the Databricks CLI enables one-way synchronization of file changes within a local filesystem directory, to a directory within a remote Databricks workspace.  \nNote  \nsync commands cannot synchronize file changes from a directory within a remote Databricks workspace, back to a directory within a local filesystem.  \nYou run sync commands by appending them to databricks sync. To display help for the sync command, run databricks sync -h.  \nImportant  \nTo install the Databricks CLI, see Install or update the Databricks CLI. To configure authentication for the Databricks CLI, see Authentication for the Databricks CLI.  \nIncrementally sync local file changes to a remote directory"
    },
    {
        "id": 311,
        "url": "https://docs.databricks.com/en/dev-tools/cli/sync-commands.html",
        "content": "Incrementally sync local file changes to a remote directory\nTo perform a single, incremental, one-way synchronization of file changes within a local filesystem directory, to a directory within a remote Databricks workspace, run the sync command, as follows:  \ndatabricks sync <local-directory-path> <remote-directory-path>  \nFor example, to do a one-time, one-way, incremental synchronization of all file changes in the folder named my-folder within the local current working directory, to a specific path within the remote workspace, run the following command:  \ndatabricks sync ./my-folder/ /Users/someone@example.com/  \nIn this example, only file changes since the last run of the sync command are synchronized to /Users/someone@example.com/. By default, the workspace URL within the caller\u2019s DEFAULT profile is used to determine the remote workspace to sync to.\n\nFully sync local file changes to a remote directory"
    },
    {
        "id": 312,
        "url": "https://docs.databricks.com/en/dev-tools/cli/sync-commands.html",
        "content": "Fully sync local file changes to a remote directory\nTo perform a single, full, one-way synchronization of file changes within a local filesystem directory to a directory within a remote Databricks workspace, regardless of when the last sync command was run, use the --full option, for example:  \ndatabricks sync ./my-folder/ /Users/someone@example.com/ --full\n\nContinuously sync local file changes to a remote directory"
    },
    {
        "id": 313,
        "url": "https://docs.databricks.com/en/dev-tools/cli/sync-commands.html",
        "content": "Continuously sync local file changes to a remote directory\nTo turn on continuous, one-way synchronization of file changes within a local filesystem directory, to a directory within a remote Databricks workspace, use the --watch option, for example:  \ndatabricks sync ./my-folder/ /Users/someone@example.com/ --watch  \nOne-way synchronization continues until the command is stopped from the terminal, typically by pressing Ctrl + c or Ctrl + z.  \nPolling for possible synchronization events happens once per second by default. To change this interval, use the --interval option along with the number of seconds to poll followed by the character s, for example for five seconds:  \ndatabricks sync ./my-folder/ /Users/someone@example.com/ --watch --interval 5s\n\nChange the sync progress output format"
    },
    {
        "id": 314,
        "url": "https://docs.databricks.com/en/dev-tools/cli/sync-commands.html",
        "content": "Change the sync progress output format\nSync progress information is output to the terminal in text format by default. To specify the sync progress output format, use the --output option, specifying either text (the default, if --output is not otherwise specified) or json, for example:  \ndatabricks sync ./my-folder/ /Users/someone@example.com/ --output json"
    },
    {
        "id": 315,
        "url": "https://docs.databricks.com/en/delta-live-tables/dlt-tutorials.html",
        "content": "Tutorials: Implement ETL workflows with Delta Live Tables  \nDelta Live Tables provides a simple declarative approach to build ETL and machine learning pipelines on batch or streaming data, while automating operational complexities such as infrastructure management, task orchestration, error handling and recovery, and performance optimization. You can use the following tutorials to get started with Delta Live Tables, perform common data transformation tasks, and implement more advanced data processing workflows.  \nCreate your first pipeline with Delta Live Tables\nCreate your first pipeline with Delta Live Tables\nTo help you learn about the features of the Delta Live Tables framework and how to implement pipelines, this tutorial walks you through creating and running your first pipeline. The tutorial includes an end-to-end example of a pipeline that ingests data, cleans and prepares the data, and performs transformations on the prepared data. See Tutorial: Run your first Delta Live Tables pipeline.\n\nProgrammatically create multiple tables with Python"
    },
    {
        "id": 316,
        "url": "https://docs.databricks.com/en/delta-live-tables/dlt-tutorials.html",
        "content": "Programmatically create multiple tables with Python\nNote  \nPatterns shown in this article cannot be easily completed with only SQL. Because Python datasets can be defined against any query that returns a DataFrame, you can use spark.sql() as necessary to use SQL syntax within Python functions.  \nYou can use Python user-defined functions (UDFs) in your SQL queries, but you must define these UDFs in Python files in the same pipeline before calling them in SQL source files. See User-defined scalar functions - Python.  \nMany workflows require the implementation of multiple data processing flows or dataset definitions that are identical or differ by only a few parameters. This redundancy can result in pipelines that are error-prone and difficult to maintain. To address this redundancy, you can use a metaprogramming pattern with Python. For an example demonstrating how to use this pattern to call a function invoked multiple times to create different tables, see Programmatically create multiple tables.\n\nInclude a Delta Live Tables pipeline in a Databricks workflow"
    },
    {
        "id": 317,
        "url": "https://docs.databricks.com/en/delta-live-tables/dlt-tutorials.html",
        "content": "Include a Delta Live Tables pipeline in a Databricks workflow\nIn addition to creating end-to-end data processing workflows with Delta Live Tables, you can also use a Delta Live Tables pipeline as a task in a workflow that implements complex data processing and analysis tasks. The tutorial in Use Databricks SQL in a Databricks job walks through creating an end-to-end Databricks workflow that includes a Delta Live Tables pipeline to prepare data for analysis and visualization with Databricks SQL."
    },
    {
        "id": 318,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/library-dependencies.html",
        "content": "Databricks Asset Bundles library dependencies  \nThis article describes the syntax for declaring Databricks Asset Bundles library dependencies. Bundles enable programmatic management of Databricks workflows. See What are Databricks Asset Bundles?.  \nIn addition to notebooks, your Databricks jobs will likely depend on libraries in order to work as expected. Databricks Asset Bundles dependencies for local development are specified in the requirements*.txt file at the root of the bundle project, but job task library dependencies are declared in your bundle configuration files and are often necessary as part of the job task type specification.  \nBundles provide support for the following library dependencies for Databricks jobs:  \nPython wheel file  \nJAR file (Java or Scala)  \nPyPI, Maven, or CRAN packages  \nNote  \nWhether or not a library is supported depends on the cluster configuration for the job and the library source. For complete library support information, see Libraries.  \nPython wheel file"
    },
    {
        "id": 319,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/library-dependencies.html",
        "content": "Python wheel file\nTo add a Python wheel file to a job task, in libraries specify a whl mapping for each library to be installed. You can install a wheel file from workspace files, Unity Catalog volumes, cloud object storage, or a local file path.  \nImportant  \nLibraries can be installed from DBFS when using Databricks Runtime 14.3 LTS and below. However, any workspace user can modify library files stored in DBFS. To improve the security of libraries in a Databricks workspace, storing library files in the DBFS root is deprecated and disabled by default in Databricks Runtime 15.1 and above. See Storing libraries in DBFS root is deprecated and disabled by default.  \nInstead, Databricks recommends uploading all libraries, including Python libraries, JAR files, and Spark connectors, to workspace files or Unity Catalog volumes, or using library package repositories. If your workload does not support these patterns, you can also use libraries stored in cloud object storage.  \nThe following example shows how to install three Python wheel files for a job task.  \nThe first Python wheel file was either previously uploaded to the Databricks workspace or added as an include item in the sync mapping, and is in the same local folder as the bundle configuration file.  \nThe second Python wheel file is in the specified workspace files location in the Databricks workspace.  \nThe third Python wheel file was previously uploaded to the volume named my-volume in the Databricks workspace.  \nresources: jobs: my_job: # ... tasks: - task_key: my_task # ... libraries: - whl: ./my-wheel-0.1.0.whl - whl: /Workspace/Shared/Libraries/my-wheel-0.0.1-py3-none-any.whl - whl: /Volumes/main/default/my-volume/my-wheel-0.1.0.whl"
    },
    {
        "id": 320,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/library-dependencies.html",
        "content": "JAR file\nTo add a JAR file to a job task, in libraries specify a jar mapping for each library to be installed. You can install a JAR from workspace files, Unity Catalog volumes, cloud object storage, or a local file path.  \nImportant  \nLibraries can be installed from DBFS when using Databricks Runtime 14.3 LTS and below. However, any workspace user can modify library files stored in DBFS. To improve the security of libraries in a Databricks workspace, storing library files in the DBFS root is deprecated and disabled by default in Databricks Runtime 15.1 and above. See Storing libraries in DBFS root is deprecated and disabled by default.  \nInstead, Databricks recommends uploading all libraries, including Python libraries, JAR files, and Spark connectors, to workspace files or Unity Catalog volumes, or using library package repositories. If your workload does not support these patterns, you can also use libraries stored in cloud object storage.  \nThe following example shows how to install a JAR file that was previously uploaded to the volume named my-volume in the Databricks workspace.  \nresources: jobs: my_job: # ... tasks: - task_key: my_task # ... libraries: - jar: /Volumes/main/default/my-volume/my-java-library-1.0.jar"
    },
    {
        "id": 321,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/library-dependencies.html",
        "content": "PyPI package\nPyPI package\nTo add a PyPI package to a job task definition, in libraries, specify a pypi mapping for each PyPI package to be installed. For each mapping, specify the following:  \nFor package, specify the name of the PyPI package to install. An optional exact version specification is also supported.  \nOptionally, for repo, specify the repository where the PyPI package can be found. If not specified, the default pip index is used (https://pypi.org/simple/).  \nThe following example shows how to install two PyPI packages.  \nThe first PyPI package uses the specified package version and the default pip index.  \nThe second PyPI package uses the specified package version and the explicitly specified pip index.  \nresources: jobs: my_job: # ... tasks: - task_key: my_task # ... libraries: - pypi: package: wheel==0.41.2 - pypi: package: numpy==1.25.2 repo: https://pypi.org/simple/\n\nMaven package"
    },
    {
        "id": 322,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/library-dependencies.html",
        "content": "Maven package\nTo add a Maven package to a job task definition, in libraries, specify a maven mapping for each Maven package to be installed. For each mapping, specify the following:  \nFor coordinates, specify the Gradle-style Maven coordinates for the package.  \nOptionally, for repo, specify the Maven repo to install the Maven package from. If omitted, both the Maven Central Repository and the Spark Packages Repository are searched.  \nOptionally, for exclusions, specify any dependencies to explicitly exclude. See Maven dependency exclusions.  \nThe following example shows how to install two Maven packages.  \nThe first Maven package uses the specified package coordinates and searches for this package in both the Maven Central Repository and the Spark Packages Repository.  \nThe second Maven package uses the specified package coordinates, searches for this package only in the Maven Central Repository, and does not include any of this package\u2019s dependencies that match the specified pattern.  \nresources: jobs: my_job: # ... tasks: - task_key: my_task # ... libraries: - maven: coordinates: com.databricks:databricks-sdk-java:0.8.1 - maven: coordinates: com.databricks:databricks-dbutils-scala_2.13:0.1.4 repo: https://mvnrepository.com/ exclusions: - org.scala-lang:scala-library:2.13.0-RC*"
    },
    {
        "id": 323,
        "url": "https://docs.databricks.com/en/connect/storage/tutorial-s3-instance-profile.html",
        "content": "Tutorial: Configure S3 access with an instance profile  \nNote  \nThis article describes legacy patterns for configuring access to Amazon S3. Databricks recommends using Unity Catalog external locations to manage access to data stored in cloud object storage. See Connect to cloud object storage using Unity Catalog.  \nThis tutorial walks you through how to create an instance profile with read, write, update, and delete permissions on a single S3 bucket. You can grant privileges for multiple buckets using a single IAM role and instance profile. It is also possible to use instance profiles to grant only read and list permissions on S3.  \nAdministrators configure IAM roles in AWS, link them to a Databricks workspace, and grant access to privileged users to associate instance profiles with compute. All users that have access to compute resources with an instance profile attached to it gain the privileges granted by the instance profile.  \nBefore you begin"
    },
    {
        "id": 324,
        "url": "https://docs.databricks.com/en/connect/storage/tutorial-s3-instance-profile.html",
        "content": "Before you begin\nThis tutorial is designed for workspace administrators. You must have sufficient privileges in the AWS account containing your Databricks workspace, and be a Databricks workspace administrator.  \nThis tutorial assumes the following existing permissions and assets:  \nPrivileges to edit the IAM role used to deploy the Databricks workspace.  \nPrivileges to create new IAM roles in AWS.  \nPrivileges to edit permissions on an S3 bucket.\n\nStep 1: Create an instance profile using the AWS console"
    },
    {
        "id": 325,
        "url": "https://docs.databricks.com/en/connect/storage/tutorial-s3-instance-profile.html",
        "content": "Step 1: Create an instance profile using the AWS console\nIn the AWS console, go to the IAM service.  \nClick the Roles tab in the sidebar.  \nClick Create role.  \nUnder Trusted entity type, select AWS service.  \nUnder Use case, select EC2.  \nClick Next.  \nAt the bottom of the page, click Next.  \nIn the Role name field, type a role name.  \nClick Create role.  \nIn the role list, click the role.  \nAdd an inline policy to the role. This policy grants access to the S3 bucket.  \nIn the Permissions tab, click Add permissions > Create inline policy.  \nClick the JSON tab.  \nCopy this policy and set <s3-bucket-name> to the name of your bucket.  \n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\" ], \"Resource\": [ \"arn:aws:s3:::<s3-bucket-name>\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", \"s3:GetObject\", \"s3:DeleteObject\", \"s3:PutObjectAcl\" ], \"Resource\": [ \"arn:aws:s3:::<s3-bucket-name>/*\" ] } ] }  \nClick Review policy.  \nIn the Name field, type a policy name.  \nClick Create policy.  \nIn the role summary, copy the Role ARN.  \nNote  \nIf you intend to enable encryption for the S3 bucket, you must add the IAM role as a Key User for the KMS key provided in the configuration. See Configure encryption for S3 with KMS."
    },
    {
        "id": 326,
        "url": "https://docs.databricks.com/en/connect/storage/tutorial-s3-instance-profile.html",
        "content": "Step 2: Enable the policy to work with serverless resources\nThis step ensures that your instance profile also works for configuring SQL warehouses with instance profiles. See Enable data access configuration.  \nIn the role list, click your instance profile.  \nSelect the Trust Relationships tab.  \nClick Edit Trust Policy.  \nWithin the existing Statement array, append the following JSON block to the end of the existing trust policy. Ensure that you don\u2019t overwrite the existing policy.  \n{ \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::790110701330:role/serverless-customer-resource-role\" ] }, \"Action\": \"sts:AssumeRole\", \"Condition\": { \"StringEquals\": { \"sts:ExternalId\": [ \"databricks-serverless-<YOUR-WORKSPACE-ID1>\", \"databricks-serverless-<YOUR-WORKSPACE-ID2>\" ] } } }  \nThe only thing you need to change in the statement is the workspace IDs. Replace the YOUR_WORKSPACE-IDs with one or more Databricks workspace IDs for the workspaces that will use this role.  \nNote  \nTo get your workspace ID, check the URL when you\u2019re using your workspace. For example, in https://<databricks-instance>/?o=6280049833385130, the number after o= is the workspace ID.  \nDo not edit the principal of the policy. The Principal.AWS field must continue to have the value arn:aws:iam::790110701330:role/serverless-customer-resource-role. This references a serverless compute role managed by Databricks.  \nClick Review policy.  \nClick Save changes."
    },
    {
        "id": 327,
        "url": "https://docs.databricks.com/en/connect/storage/tutorial-s3-instance-profile.html",
        "content": "Step 3: Create the bucket policy\nAt a minimum, the S3 policy must include the ListBucket and GetObject actions, which provide read-only access to a bucket. Delta Lake uses DeleteObject and PutObject permissions during regular operations. The permissions in the example policy below are the recommended defaults for clusters that read and write data.  \nNote  \nS3 buckets have universally unique names and do not require an account ID for universal identification. If you choose to link an S3 bucket to an IAM role and Databricks workspace in a different AWS account, you must specify the account ID when configuring your S3 bucket policy.  \nGo to your S3 console. From the Buckets list, select the bucket for which you want to create a policy.  \nClick Permissions.  \nUnder Bucket policy, click Edit.  \nPaste in a policy. A sample cross-account bucket IAM policy could be the following, replacing <aws-account-id-databricks> with the AWS account ID where the Databricks environment is deployed, <iam-role-for-s3-access> with the instance profile role, and <s3-bucket-name> with the bucket name."
    },
    {
        "id": 328,
        "url": "https://docs.databricks.com/en/connect/storage/tutorial-s3-instance-profile.html",
        "content": "{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Example permissions\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::<aws-account-id-databricks>:role/<iam-role-for-s3-access>\" }, \"Action\": [ \"s3:GetBucketLocation\", \"s3:ListBucket\" ], \"Resource\": \"arn:aws:s3:::<s3-bucket-name>\" }, { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::<aws-account-id-databricks>:role/<iam-role-for-s3-access>\" }, \"Action\": [ \"s3:PutObject\", \"s3:GetObject\", \"s3:DeleteObject\", \"s3:PutObjectAcl\" ], \"Resource\": \"arn:aws:s3:::<s3-bucket-name>/*\" } ] }  \nClick Save."
    },
    {
        "id": 329,
        "url": "https://docs.databricks.com/en/connect/storage/tutorial-s3-instance-profile.html",
        "content": "Step 4: Locate the IAM role that created the Databricks deployment\nStep 4: Locate the IAM role that created the Databricks deployment\nIf you don\u2019t know which IAM role created the Databricks deployment, do the following:  \nAs an account admin, log in to the account console.  \nGo to Workspaces and click your workspace name.  \nIn the Credentials box, note the role name at the end of the Role ARN.  \nFor example, in the Role ARN arn:aws:iam::123456789123:role/finance-prod, the role name is finance-prod.\n\nStep 5: Add the S3 IAM role to the EC2 policy"
    },
    {
        "id": 330,
        "url": "https://docs.databricks.com/en/connect/storage/tutorial-s3-instance-profile.html",
        "content": "Step 5: Add the S3 IAM role to the EC2 policy\nIn the AWS console, go to the IAM service.  \nClick the Roles tab in the sidebar.  \nClick the role that created the Databricks deployment.  \nOn the Permissions tab, click the policy.  \nClick Edit Policy.  \nAppend the following block to the end of the Statement array. Ensure that you don\u2019t overwrite any of the existing policy. Replace <iam-role-for-s3-access> with the role you created in Tutorial: Configure S3 access with an instance profile:  \n{ \"Effect\": \"Allow\", \"Action\": \"iam:PassRole\", \"Resource\": \"arn:aws:iam::<aws-account-id-databricks>:role/<iam-role-for-s3-access>\" }  \nClick Review policy.  \nClick Save changes.\n\nStep 6: Add the instance profile to Databricks"
    },
    {
        "id": 331,
        "url": "https://docs.databricks.com/en/connect/storage/tutorial-s3-instance-profile.html",
        "content": "Step 6: Add the instance profile to Databricks\nAs a workspace admin, go to the settings page.  \nClick the Security tab.  \nClick Manage next to Instance profiles.  \nClick Add Instance Profile.  \nPaste your instance profile ARN into the Instance profile ARN field. If you don\u2019t have the ARN, see Tutorial: Configure S3 access with an instance profile.  \nFor serverless SQL to work with your instance profile, you might need to explicitly specify the role ARN associated with your instance profile in the IAM role ARN field.  \nThis is only a required step if your instance profile\u2019s associated role name (the text after the last slash in the role ARN) and the instance profile name (the text after the last slash in the instance profile ARN) do not match. To confirm whether this applies to you:  \nIn the AWS console, go to the IAM service\u2019s Roles tab. It lists the IAM roles in your account.  \nClick the role with the name that matches the instance profile name in the Databricks SQL admin settings in the Data Security section for the Instance Profile field that you found earlier in this section.  \nIn the summary area, find the Role ARN and Instance Profile ARNs fields and see if they match.  \nIf they do not match, paste the role ARN into the IAM role ARN field. If the names match, you do not need to set the IAM role ARN field.  \nOnly if you are setting up IAM credential passthrough, select the Meta Instance Profile property.  \nDatabricks validates that the instance profile ARN is both syntactically and semantically correct. To validate semantic correctness, Databricks does a dry run by launching a cluster with this instance profile. Any failure in this dry run produces a validation error in the UI. Validation of the instance profile can fail if the instance profile contains the tag-enforcement policy, preventing you from adding a legitimate instance profile. If the validation fails and you still want to add the instance profile, select the Skip Validation checkbox.  \nClick Add."
    },
    {
        "id": 332,
        "url": "https://docs.databricks.com/en/connect/storage/tutorial-s3-instance-profile.html",
        "content": "Manage instance profiles\nManage instance profiles\nWorkspace admins can manage manage access to instance profiles and update them. See Manage instance profiles in Databricks.\n\nNext steps\nNext steps\nEnable data access configuration"
    },
    {
        "id": 333,
        "url": "https://docs.databricks.com/en/delta-live-tables/index.html",
        "content": "What is Delta Live Tables?  \nDelta Live Tables is a declarative framework for building reliable, maintainable, and testable data processing pipelines. You define the transformations to perform on your data and Delta Live Tables manages task orchestration, cluster management, monitoring, data quality, and error handling.  \nInstead of defining your data pipelines using a series of separate Apache Spark tasks, you define streaming tables and materialized views that the system should create and keep up to date. Delta Live Tables manages how your data is transformed based on queries you define for each processing step. You can also enforce data quality with Delta Live Tables expectations, which allow you to define expected data quality and specify how to handle records that fail those expectations.  \nTo learn more about the benefits of building and running your ETL pipelines with Delta Live Tables, see the Delta Live Tables product page.  \nWhat are Delta Live Tables datasets?"
    },
    {
        "id": 334,
        "url": "https://docs.databricks.com/en/delta-live-tables/index.html",
        "content": "What are Delta Live Tables datasets?\nDelta Live Tables datasets are the streaming tables, materialized views, and views maintained as the results of declarative queries. The following table describes how each dataset is processed:  \nDataset type  \nHow are records processed through defined queries?  \nStreaming table  \nEach record is processed exactly once. This assumes an append-only source.  \nMaterialized views  \nRecords are processed as required to return accurate results for the current data state. Materialized views should be used for data processing tasks such as transformations, aggregations, or pre-computing slow queries and frequently used computations.  \nViews  \nRecords are processed each time the view is queried. Use views for intermediate transformations and data quality checks that should not be published to public datasets.  \nThe following sections provide more detailed descriptions of each dataset type. To learn more about selecting dataset types to implement your data processing requirements, see When to use views, materialized views, and streaming tables.  \nStreaming table  \nA streaming table is a Delta table with extra support for streaming or incremental data processing. Streaming tables allow you to process a growing dataset, handling each row only once. Because most datasets grow continuously over time, streaming tables are good for most ingestion workloads. Streaming tables are optimal for pipelines that require data freshness and low latency. Streaming tables can also be useful for massive scale transformations, as results can be incrementally calculated as new data arrives, keeping results up to date without needing to fully recompute all source data with each update. Streaming tables are designed for data sources that are append-only.  \nNote  \nAlthough, by default, streaming tables require append-only data sources, when a streaming source is another streaming table that requires updates or deletes, you can override this behavior with the skipChangeCommits flag.  \nMaterialized view"
    },
    {
        "id": 335,
        "url": "https://docs.databricks.com/en/delta-live-tables/index.html",
        "content": "Note  \nAlthough, by default, streaming tables require append-only data sources, when a streaming source is another streaming table that requires updates or deletes, you can override this behavior with the skipChangeCommits flag.  \nMaterialized view  \nA materialized view is a view where the results have been precomputed. Materialized views are refreshed according to the update schedule of the pipeline in which they\u2019re contained. Materialized views are powerful because they can handle any changes in the input. Each time the pipeline updates, query results are recalculated to reflect changes in upstream datasets that might have occurred because of compliance, corrections, aggregations, or general CDC. Delta Live Tables implements materialized views as Delta tables, but abstracts away complexities associated with efficient application of updates, allowing users to focus on writing queries.  \nViews  \nAll views in Databricks compute results from source datasets as they are queried, leveraging caching optimizations when available. Delta Live Tables does not publish views to the catalog, so views can be referenced only within the pipeline in which they are defined. Views are useful as intermediate queries that should not be exposed to end users or systems. Databricks recommends using views to enforce data quality constraints or transform and enrich datasets that drive multiple downstream queries."
    },
    {
        "id": 336,
        "url": "https://docs.databricks.com/en/delta-live-tables/index.html",
        "content": "Declare your first datasets in Delta Live Tables\nDeclare your first datasets in Delta Live Tables\nDelta Live Tables introduces new syntax for Python and SQL. To get started with Delta Live Tables syntax, see the Python and SQL examples in Tutorial: Run your first Delta Live Tables pipeline.  \nNote  \nDelta Live Tables separates dataset definitions from update processing, and Delta Live Tables notebooks are not intended for interactive execution. See What is a Delta Live Tables pipeline?.\n\nWhat is a Delta Live Tables pipeline?"
    },
    {
        "id": 337,
        "url": "https://docs.databricks.com/en/delta-live-tables/index.html",
        "content": "What is a Delta Live Tables pipeline?\nA pipeline is the main unit used to configure and run data processing workflows with Delta Live Tables.  \nA pipeline contains materialized views and streaming tables declared in Python or SQL source files. Delta Live Tables infers the dependencies between these tables, ensuring updates occur in the correct order. For each dataset, Delta Live Tables compares the current state with the desired state and proceeds to create or update datasets using efficient processing methods.  \nThe settings of Delta Live Tables pipelines fall into two broad categories:  \nConfigurations that define a collection of notebooks or files (known as source code or libraries) that use Delta Live Tables syntax to declare datasets.  \nConfigurations that control pipeline infrastructure, dependency management, how updates are processed, and how tables are saved in the workspace.  \nMost configurations are optional, but some require careful attention, especially when configuring production pipelines. These include the following:  \nTo make data available outside the pipeline, you must declare a target schema to publish to the Hive metastore or a target catalog and target schema to publish to Unity Catalog.  \nData access permissions are configured through the cluster used for execution. Make sure your cluster has appropriate permissions configured for data sources and the target storage location, if specified.  \nFor details on using Python and SQL to write source code for pipelines, see Delta Live Tables SQL language reference and Delta Live Tables Python language reference.  \nFor more on pipeline settings and configurations, see Manage configuration of Delta Live Tables pipelines."
    },
    {
        "id": 338,
        "url": "https://docs.databricks.com/en/delta-live-tables/index.html",
        "content": "Deploy your first pipeline and trigger updates\nDeploy your first pipeline and trigger updates\nBefore processing data with Delta Live Tables, you must configure a pipeline. Once a pipeline is configured, you can trigger an update to calculate results for each dataset in your pipeline. To get started using Delta Live Tables pipelines, see Tutorial: Run your first Delta Live Tables pipeline.\n\nWhat is a pipeline update?\nWhat is a pipeline update?\nPipelines deploy infrastructure and recompute data state when you start an update. An update does the following:  \nStarts a cluster with the correct configuration.  \nDiscovers all the tables and views defined, and checks for any analysis errors such as invalid column names, missing dependencies, and syntax errors.  \nCreates or updates tables and views with the most recent data available.  \nPipelines can be run continuously or on a schedule depending on your use case\u2019s cost and latency requirements. See Run an update on a Delta Live Tables pipeline.\n\nIngest data with Delta Live Tables"
    },
    {
        "id": 339,
        "url": "https://docs.databricks.com/en/delta-live-tables/index.html",
        "content": "Ingest data with Delta Live Tables\nDelta Live Tables supports all data sources available in Databricks.  \nDatabricks recommends using streaming tables for most ingestion use cases. For files arriving in cloud object storage, Databricks recommends Auto Loader. You can directly ingest data with Delta Live Tables from most message buses.  \nFor more information about configuring access to cloud storage, see Cloud storage configuration.  \nFor formats not supported by Auto Loader, you can use Python or SQL to query any format supported by Apache Spark. See Load data with Delta Live Tables.\n\nMonitor and enforce data quality\nMonitor and enforce data quality\nYou can use expectations to specify data quality controls on the contents of a dataset. Unlike a CHECK constraint in a traditional database which prevents adding any records that fail the constraint, expectations provide flexibility when processing data that fails data quality requirements. This flexibility allows you to process and store data that you expect to be messy and data that must meet strict quality requirements. See Manage data quality with Delta Live Tables.\n\nHow are Delta Live Tables and Delta Lake related?"
    },
    {
        "id": 340,
        "url": "https://docs.databricks.com/en/delta-live-tables/index.html",
        "content": "How are Delta Live Tables and Delta Lake related?\nDelta Live Tables extends the functionality of Delta Lake. Because tables created and managed by Delta Live Tables are Delta tables, they have the same guarantees and features provided by Delta Lake. See What is Delta Lake?.  \nDelta Live Tables adds several table properties in addition to the many table properties that can be set in Delta Lake. See Delta Live Tables properties reference and Delta table properties reference.\n\nHow tables are created and managed by Delta Live Tables\nHow tables are created and managed by Delta Live Tables\nDatabricks automatically manages tables created with Delta Live Tables, determining how updates need to be processed to correctly compute the current state of a table and performing a number of maintenance and optimization tasks.  \nFor most operations, you should allow Delta Live Tables to process all updates, inserts, and deletes to a target table. For details and limitations, see Retain manual deletes or updates.\n\nMaintenance tasks performed by Delta Live Tables"
    },
    {
        "id": 341,
        "url": "https://docs.databricks.com/en/delta-live-tables/index.html",
        "content": "Maintenance tasks performed by Delta Live Tables\nDelta Live Tables performs maintenance tasks within 24 hours of a table being updated. Maintenance can improve query performance and reduce cost by removing old versions of tables. By default, the system performs a full OPTIMIZE operation followed by VACUUM. You can disable OPTIMIZE for a table by setting pipelines.autoOptimize.managed = false in the table properties for the table. Maintenance tasks are performed only if a pipeline update has run in the 24 hours before the maintenance tasks are scheduled.\n\nLimitations\nLimitations\nThe following limitations apply:  \nAll tables created and updated by Delta Live Tables are Delta tables.  \nDelta Live Tables tables can only be defined once, meaning they can only be the target of a single operation in all Delta Live Tables pipelines.  \nIdentity columns are not supported with tables that are the target of APPLY CHANGES INTO and might be recomputed during updates for materialized views. For this reason, Databricks recommends only using identity columns with streaming tables in Delta Live Tables. See Use identity columns in Delta Lake.  \nA Databricks workspace is limited to 100 concurrent pipeline updates.\n\nAdditional resources"
    },
    {
        "id": 342,
        "url": "https://docs.databricks.com/en/delta-live-tables/index.html",
        "content": "Additional resources\nDelta Live Tables has full support in the Databricks REST API. See Delta Live Tables API guide.  \nFor pipeline and table settings, see Delta Live Tables properties reference.  \nDelta Live Tables SQL language reference.  \nDelta Live Tables Python language reference."
    },
    {
        "id": 343,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-connect/python/jupyterlab.html",
        "content": "Use JupyterLab with Databricks Connect for Python  \nNote  \nThis article covers Databricks Connect for Databricks Runtime 13.3 LTS and above.  \nThis article covers how to use Databricks Connect for Python with JupyterLab. Databricks Connect enables you to connect popular notebook servers, IDEs, and other custom applications to Databricks clusters. See What is Databricks Connect?.  \nNote  \nBefore you begin to use Databricks Connect, you must set up the Databricks Connect client.  \nTo use Databricks Connect with JupyterLab and Python, follow these instructions.  \nTo install JupyterLab, with your Python virtual environment activated, run the following command from your terminal or Command Prompt:  \npip3 install jupyterlab  \nTo start JupyterLab in your web browser, run the following command from your activated Python virtual environment:  \njupyter lab  \nIf JupyterLab does not appear in your web browser, copy the URL that starts with localhost or 127.0.0.1 from your virtual environment, and enter it in your web browser\u2019s address bar.  \nCreate a new notebook: in JupyterLab, click File > New > Notebook on the main menu, select Python 3 (ipykernel) and click Select.  \nIn the notebook\u2019s first cell, enter either the example code or your own code. If you use your own code, at minimum you must initialize DatabricksSession as shown in the example code.  \nTo run the notebook, click Run > Run All Cells. All code runs locally, while all code involving DataFrame operations runs on the cluster in the remote Databricks workspace and run responses are sent back to the local caller.  \nTo debug the notebook, click the bug (Enable Debugger) icon next to Python 3 (ipykernel) in the notebook\u2019s toolbar. Set one or more breakpoints, and then click Run > Run All Cells. All code is debugged locally, while all Spark code continues to run on the cluster in the remote Databricks workspace. The core Spark engine code cannot be debugged directly from the client.  \nTo shut down JupyterLab, click File > Shut Down. If the JupyterLab process is still running in your terminal or Command Prompt, stop this process by pressing Ctrl + c and then entering y to confirm."
    },
    {
        "id": 344,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-connect/python/jupyterlab.html",
        "content": "To shut down JupyterLab, click File > Shut Down. If the JupyterLab process is still running in your terminal or Command Prompt, stop this process by pressing Ctrl + c and then entering y to confirm.  \nFor more specific debug instructions, see Debugger."
    },
    {
        "id": 345,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "Databricks Utilities (dbutils) reference  \nThis article is a reference for Databricks Utilities (dbutils). dbutils utilities are available in Python, R, and Scala notebooks. You can use the utilities to:  \nWork with files and object storage efficiently.  \nWork with secrets.  \nHow to: List utilities, list commands, display command help  \nUtilities: credentials, data, fs, jobs, library, notebook, secrets, widgets, Utilities API library  \nList available utilities"
    },
    {
        "id": 346,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "List available utilities\nTo list available utilities along with a short description for each utility, run dbutils.help() for Python or Scala.  \nThis example lists available commands for the Databricks Utilities.  \ndbutils.help()  \ndbutils.help()  \nThis module provides various utilities for users to interact with the rest of Databricks. credentials: DatabricksCredentialUtils\u00a0-> Utilities for interacting with credentials within notebooks data: DataUtils\u00a0-> Utilities for understanding and interacting with datasets (EXPERIMENTAL) fs: DbfsUtils\u00a0-> Manipulates the Databricks filesystem (DBFS) from the console jobs: JobsUtils\u00a0-> Utilities for leveraging jobs features library: LibraryUtils\u00a0-> Utilities for session isolated libraries meta: MetaUtils\u00a0-> Methods to hook into the compiler (EXPERIMENTAL) notebook: NotebookUtils\u00a0-> Utilities for the control flow of a notebook (EXPERIMENTAL) preview: Preview\u00a0-> Utilities under preview category secrets: SecretUtils\u00a0-> Provides utilities for leveraging secrets within notebooks widgets: WidgetsUtils\u00a0-> Methods to create and get bound value of input widgets inside notebooks"
    },
    {
        "id": 347,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "List available commands for a utility\nTo list available commands for a utility along with a short description of each command, run .help() after the programmatic name for the utility.  \nThis example lists available commands for the Databricks File System (DBFS) utility.  \ndbutils.fs.help()  \ndbutils.fs.help()  \ndbutils.fs.help()"
    },
    {
        "id": 348,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "dbutils.fs provides utilities for working with FileSystems. Most methods in this package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or another FileSystem URI. For more info about a method, use dbutils.fs.help(\"methodName\"). In notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps straightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\" translates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\". fsutils cp(from: String, to: String, recurse: boolean = false): boolean -> Copies a file or directory, possibly across FileSystems head(file: String, maxBytes: int = 65536): String -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8 ls(dir: String): Seq -> Lists the contents of a directory mkdirs(dir: String): boolean -> Creates the given directory if it does not exist, also creating any necessary parent directories mv(from: String, to: String, recurse: boolean = false): boolean -> Moves a file or directory, possibly across FileSystems put(file: String, contents: String, overwrite: boolean = false): boolean -> Writes the given String out to a file, encoded in UTF-8 rm(dir: String, recurse: boolean = false): boolean -> Removes a file or directory mount mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean -> Mounts the given source directory into DBFS at the given mount point mounts: Seq -> Displays information about what is mounted within DBFS refreshMounts: boolean -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information"
    },
    {
        "id": 349,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "DBFS at the given mount point mounts: Seq -> Displays information about what is mounted within DBFS refreshMounts: boolean -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information unmount(mountPoint: String): boolean -> Deletes a DBFS mount point updateMount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean -> Similar to mount(), but updates an existing mount point instead of creating a new one"
    },
    {
        "id": 350,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "Display help for a command\nDisplay help for a command\nTo display help for a command, run .help(\"<command-name>\") after the command name.  \nThis example displays help for the DBFS copy command.  \ndbutils.fs.help(\"cp\")  \ndbutils.fs.help(\"cp\")  \ndbutils.fs.help(\"cp\")  \n/** * Copies a file or directory, possibly across FileSystems. * * Example: cp(\"/mnt/my-folder/a\", \"dbfs:/a/b\") * * @param from FileSystem URI of the source file or directory * @param to FileSystem URI of the destination file or directory * @param recurse if true, all files and directories will be recursively copied * @return true if all files were successfully copied */ cp(from: java.lang.String, to: java.lang.String, recurse: boolean = false): boolean\n\nCredentials utility (dbutils.credentials)"
    },
    {
        "id": 351,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "Credentials utility (dbutils.credentials)\nCommands: assumeRole, showCurrentRole, showRoles  \nThe credentials utility allows you to interact with credentials within notebooks. This utility is usable only on clusters with credential passthrough enabled. To list the available commands, run dbutils.credentials.help().  \nassumeRole(role: String): boolean -> Sets the role ARN to assume when looking for credentials to authenticate with S3 showCurrentRole: List -> Shows the currently set role showRoles: List -> Shows the set of possible assumed roles  \nassumeRole command (dbutils.credentials.assumeRole)  \nSets the Amazon Resource Name (ARN) for the AWS Identity and Access Management (IAM) role to assume when looking for credentials to authenticate with Amazon S3. After you run this command, you can run S3 access commands, such as sc.textFile(\"s3a://my-bucket/my-file.csv\") to access an object.  \nTo display help for this command, run dbutils.credentials.help(\"assumeRole\").  \ndbutils.credentials.assumeRole(\"arn:aws:iam::123456789012:roles/my-role\") # Out[1]: True  \ndbutils.credentials.assumeRole(\"arn:aws:iam::123456789012:roles/my-role\") # TRUE  \ndbutils.credentials.assumeRole(\"arn:aws:iam::123456789012:roles/my-role\") // res0: Boolean = true  \nshowCurrentRole command (dbutils.credentials.showCurrentRole)  \nLists the currently set AWS Identity and Access Management (IAM) role.  \nTo display help for this command, run dbutils.credentials.help(\"showCurrentRole\").  \ndbutils.credentials.showCurrentRole() # Out[1]: ['arn:aws:iam::123456789012:role/my-role-a']"
    },
    {
        "id": 352,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "dbutils.credentials.showCurrentRole() # Out[1]: ['arn:aws:iam::123456789012:role/my-role-a']  \ndbutils.credentials.showCurrentRole() # [[1]] # [1] \"arn:aws:iam::123456789012:role/my-role-a\"  \ndbutils.credentials.showCurrentRole() // res0: java.util.List[String] = [arn:aws:iam::123456789012:role/my-role-a]  \nshowRoles command (dbutils.credentials.showRoles)  \nLists the set of possible assumed AWS Identity and Access Management (IAM) roles.  \nTo display help for this command, run dbutils.credentials.help(\"showRoles\").  \ndbutils.credentials.showRoles() # Out[1]: ['arn:aws:iam::123456789012:role/my-role-a', 'arn:aws:iam::123456789012:role/my-role-b']  \ndbutils.credentials.showRoles() # [[1]] # [1] \"arn:aws:iam::123456789012:role/my-role-a\" # # [[2]] # [1] \"arn:aws:iam::123456789012:role/my-role-b\"  \ndbutils.credentials.showRoles() // res0: java.util.List[String] = [arn:aws:iam::123456789012:role/my-role-a, arn:aws:iam::123456789012:role/my-role-b]"
    },
    {
        "id": 353,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "Data utility (dbutils.data)\nPreview  \nThis feature is in Public Preview.  \nNote  \nAvailable in Databricks Runtime 9.0 and above.  \nCommands: summarize  \nThe data utility allows you to understand and interpret datasets. To list the available commands, run dbutils.data.help().  \ndbutils.data provides utilities for understanding and interpreting datasets. This module is currently in preview and may be unstable. For more info about a method, use dbutils.data.help(\"methodName\"). summarize(df: Object, precise: boolean): void -> Summarize a Spark DataFrame and visualize the statistics to get quick insights  \nsummarize command (dbutils.data.summarize)  \nCalculates and displays summary statistics of an Apache Spark DataFrame or pandas DataFrame. This command is available for Python, Scala and R.  \nCaution  \nThis command analyzes the complete contents of the DataFrame. Running this command for very large DataFrames can be very expensive.  \nTo display help for this command, run dbutils.data.help(\"summarize\").  \nIn Databricks Runtime 10.4 LTS and above, you can use the additional precise parameter to adjust the precision of the computed statistics.  \nNote  \nThis feature is in Public Preview.  \nWhen precise is set to false (the default), some returned statistics include approximations to reduce run time.  \nThe number of distinct values for categorical columns may have ~5% relative error for high-cardinality columns.  \nThe frequent value counts may have an error of up to 0.01% when the number of distinct values is greater than 10000.  \nThe histograms and percentile estimates may have an error of up to 0.01% relative to the total number of rows.  \nWhen precise is set to true, the statistics are computed with higher precision. All statistics except for the histograms and percentiles for numeric columns are now exact.  \nThe histograms and percentile estimates may have an error of up to 0.0001% relative to the total number of rows.  \nThe tooltip at the top of the data summary output indicates the mode of current run."
    },
    {
        "id": 354,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "The histograms and percentile estimates may have an error of up to 0.0001% relative to the total number of rows.  \nThe tooltip at the top of the data summary output indicates the mode of current run.  \nThis example displays summary statistics for an Apache Spark DataFrame with approximations enabled by default. To see the results, run this command in a notebook. This example is based on Sample datasets.  \ndf = spark.read.format('csv').load( '/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv', header=True, inferSchema=True ) dbutils.data.summarize(df)  \ndf <- read.df(\"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\", source = \"csv\", header=\"true\", inferSchema = \"true\") dbutils.data.summarize(df)  \nval df = spark.read.format(\"csv\") .option(\"inferSchema\", \"true\") .option(\"header\", \"true\") .load(\"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\") dbutils.data.summarize(df)  \nNote that the visualization uses SI notation to concisely render numerical values smaller than 0.01 or larger than 10000. As an example, the numerical value 1.25e-15 will be rendered as 1.25f. One exception: the visualization uses \u201cB\u201d for 1.0e9 (giga) instead of \u201cG\u201d."
    },
    {
        "id": 355,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "File system utility (dbutils.fs)\nWarning  \nThe Python implementation of all dbutils.fs methods uses snake_case rather than camelCase for keyword formatting.  \nFor example: while dbutils.fs.help() displays the option extraConfigs for dbutils.fs.mount(), in Python you would use the keyword extra_configs.  \nCommands: cp, head, ls, mkdirs, mount, mounts, mv, put, refreshMounts, rm, unmount, updateMount  \nThe file system utility allows you to access What is DBFS?, making it easier to use Databricks as a file system.  \nIn notebooks, you can also use the %fs magic command to access DBFS. For example %fs ls /Volumes/main/default/my-volume/ is the same as dbutils.fs.ls(\"/Volumes/main/default/my-volume/\"). See magic commands.  \nTo list the available commands, run dbutils.fs.help()."
    },
    {
        "id": 356,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "dbutils.fs provides utilities for working with FileSystems. Most methods in this package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or another FileSystem URI. For more info about a method, use dbutils.fs.help(\"methodName\"). In notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps straightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\" translates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\". fsutils cp(from: String, to: String, recurse: boolean = false): boolean -> Copies a file or directory, possibly across FileSystems head(file: String, maxBytes: int = 65536): String -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8 ls(dir: String): Seq -> Lists the contents of a directory mkdirs(dir: String): boolean -> Creates the given directory if it does not exist, also creating any necessary parent directories mv(from: String, to: String, recurse: boolean = false): boolean -> Moves a file or directory, possibly across FileSystems put(file: String, contents: String, overwrite: boolean = false): boolean -> Writes the given String out to a file, encoded in UTF-8 rm(dir: String, recurse: boolean = false): boolean -> Removes a file or directory mount mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean -> Mounts the given source directory into DBFS at the given mount point mounts: Seq -> Displays information about what is mounted within DBFS refreshMounts: boolean -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information"
    },
    {
        "id": 357,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "DBFS at the given mount point mounts: Seq -> Displays information about what is mounted within DBFS refreshMounts: boolean -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information unmount(mountPoint: String): boolean -> Deletes a DBFS mount point updateMount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean -> Similar to mount(), but updates an existing mount point instead of creating a new one"
    },
    {
        "id": 358,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "cp command (dbutils.fs.cp)  \nCopies a file or directory, possibly across filesystems.  \nTo display help for this command, run dbutils.fs.help(\"cp\").  \nThis example copies the file named data.csv from /Volumes/main/default/my-volume/ to new-data.csv in the same volume.  \ndbutils.fs.cp(\"/Volumes/main/default/my-volume/data.csv\", \"/Volumes/main/default/my-volume/new-data.csv\") # Out[4]: True  \ndbutils.fs.cp(\"/Volumes/main/default/my-volume/data.csv\", \"/Volumes/main/default/my-volume/new-data.csv\") # [1] TRUE  \ndbutils.fs.cp(\"/Volumes/main/default/my-volume/data.csv\", \"/Volumes/main/default/my-volume/new-data.csv\") // res3: Boolean = true  \nhead command (dbutils.fs.head)  \nReturns up to the specified maximum number bytes of the given file. The bytes are returned as a UTF-8 encoded string.  \nTo display help for this command, run dbutils.fs.help(\"head\").  \nThis example displays the first 25 bytes of the file data.csv located in /Volumes/main/default/my-volume/.  \ndbutils.fs.head(\"/Volumes/main/default/my-volume/data.csv\", 25) # [Truncated to first 25 bytes] # Out[12]: 'Year,First Name,County,Se'  \ndbutils.fs.head(\"/Volumes/main/default/my-volume/data.csv\", 25) # [1] \"Year,First Name,County,Se\""
    },
    {
        "id": 359,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "dbutils.fs.head(\"/Volumes/main/default/my-volume/data.csv\", 25) # [1] \"Year,First Name,County,Se\"  \ndbutils.fs.head(\"/Volumes/main/default/my-volume/data.csv\", 25) // [Truncated to first 25 bytes] // res4: String = // \"Year,First Name,County,Se\"  \nls command (dbutils.fs.ls)  \nLists the contents of a directory.  \nTo display help for this command, run dbutils.fs.help(\"ls\").  \nThis example displays information about the contents of /Volumes/main/default/my-volume/. The modificationTime field is available in Databricks Runtime 10.4 LTS and above. In R, modificationTime is returned as a string.  \ndbutils.fs.ls(\"/Volumes/main/default/my-volume/\") # Out[13]: [FileInfo(path='dbfs:/Volumes/main/default/my-volume/data.csv', name='data.csv', size=2258987, modificationTime=1711357839000)]  \ndbutils.fs.ls(\"/Volumes/main/default/my-volume/\") # For prettier results from dbutils.fs.ls(<dir>), please use `%fs ls <dir>` # [[1]] # [[1]]$path # [1] \"/Volumes/main/default/my-volume/data.csv\" # [[1]]$name # [1] \"data.csv\" # [[1]]$size # [1] 2258987 # [[1]]$isDir # [1] FALSE # [[1]]$isFile # [1] TRUE # [[1]]$modificationTime # [1] \"1711357839000\""
    },
    {
        "id": 360,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "dbutils.fs.ls(\"/tmp\") // res6: Seq[com.databricks.backend.daemon.dbutils.FileInfo] = WrappedArray(FileInfo(/Volumes/main/default/my-volume/data.csv, 2258987, 1711357839000))  \nmkdirs command (dbutils.fs.mkdirs)  \nCreates the given directory if it does not exist. Also creates any necessary parent directories.  \nTo display help for this command, run dbutils.fs.help(\"mkdirs\").  \nThis example creates the directory my-data within /Volumes/main/default/my-volume/.  \ndbutils.fs.mkdirs(\"/Volumes/main/default/my-volume/my-data\") # Out[15]: True  \ndbutils.fs.mkdirs(\"/Volumes/main/default/my-volume/my-data\") # [1] TRUE  \ndbutils.fs.mkdirs(\"/Volumes/main/default/my-volume/my-data\") // res7: Boolean = true  \nmount command (dbutils.fs.mount)  \nMounts the specified source directory into DBFS at the specified mount point.  \nTo display help for this command, run dbutils.fs.help(\"mount\").  \naws_bucket_name = \"my-bucket\" mount_name = \"s3-my-bucket\" dbutils.fs.mount(\"s3a://%s\" % aws_bucket_name, \"/mnt/%s\" % mount_name)  \nval AwsBucketName = \"my-bucket\" val MountName = \"s3-my-bucket\" dbutils.fs.mount(s\"s3a://$AwsBucketName\", s\"/mnt/$MountName\")  \nFor additional code examples, see Connect to Amazon S3.  \nmounts command (dbutils.fs.mounts)  \nDisplays information about what is currently mounted within DBFS."
    },
    {
        "id": 361,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "For additional code examples, see Connect to Amazon S3.  \nmounts command (dbutils.fs.mounts)  \nDisplays information about what is currently mounted within DBFS.  \nTo display help for this command, run dbutils.fs.help(\"mounts\").  \nWarning  \nCall dbutils.fs.refreshMounts() on all other running clusters to propagate the new mount. See refreshMounts command (dbutils.fs.refreshMounts).  \ndbutils.fs.mounts() # Out[11]: [MountInfo(mountPoint='/mnt/databricks-results', source='databricks-results', encryptionType='sse-s3')]  \ndbutils.fs.mounts()  \nFor additional code examples, see Connect to Amazon S3.  \nmv command (dbutils.fs.mv)  \nMoves a file or directory, possibly across filesystems. A move is a copy followed by a delete, even for moves within filesystems.  \nTo display help for this command, run dbutils.fs.help(\"mv\").  \nThis example moves the file rows.csv from /Volumes/main/default/my-volume/ to /Volumes/main/default/my-volume/my-data/.  \ndbutils.fs.mv(\"/Volumes/main/default/my-volume/rows.csv\", \"/Volumes/main/default/my-volume/my-data/\") # Out[2]: True  \ndbutils.fs.mv(\"/Volumes/main/default/my-volume/rows.csv\", \"/Volumes/main/default/my-volume/my-data/\") # [1] TRUE  \ndbutils.fs.mv(\"/Volumes/main/default/my-volume/rows.csv\", \"/Volumes/main/default/my-volume/my-data/\") // res1: Boolean = true  \nput command (dbutils.fs.put)  \nWrites the specified string to a file. The string is UTF-8 encoded."
    },
    {
        "id": 362,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "put command (dbutils.fs.put)  \nWrites the specified string to a file. The string is UTF-8 encoded.  \nTo display help for this command, run dbutils.fs.help(\"put\").  \nThis example writes the string Hello, Databricks! to a file named hello.txt in /Volumes/main/default/my-volume/. If the file exists, it will be overwritten.  \ndbutils.fs.put(\"/Volumes/main/default/my-volume/hello.txt\", \"Hello, Databricks!\", True) # Wrote 2258987 bytes. # Out[6]: True  \ndbutils.fs.put(\"/Volumes/main/default/my-volume/hello.txt\", \"Hello, Databricks!\", TRUE) # [1] TRUE  \ndbutils.fs.put(\"/Volumes/main/default/my-volume/hello.txt\", \"Hello, Databricks!\", true) // Wrote 2258987 bytes. // res2: Boolean = true  \nrefreshMounts command (dbutils.fs.refreshMounts)  \nForces all machines in the cluster to refresh their mount cache, ensuring they receive the most recent information.  \nTo display help for this command, run dbutils.fs.help(\"refreshMounts\").  \ndbutils.fs.refreshMounts()  \ndbutils.fs.refreshMounts()  \nFor additional code examples, see Connect to Amazon S3.  \nrm command (dbutils.fs.rm)  \nRemoves a file or directory and optionally all of its contents. If a file is specified, the recurse parameter is ignored. If a directory is specified, an error occurs if recurse is disabled and the directory is not empty.  \nTo display help for this command, run dbutils.fs.help(\"rm\").  \nThis example removes the directory /Volumes/main/default/my-volume/my-data/ including the contents of the directory.  \ndbutils.fs.rm(\"/Volumes/main/default/my-volume/my-data/\", True) # Out[8]: True"
    },
    {
        "id": 363,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "dbutils.fs.rm(\"/Volumes/main/default/my-volume/my-data/\", True) # Out[8]: True  \ndbutils.fs.rm(\"/Volumes/main/default/my-volume/my-data/\", TRUE) # [1] TRUE  \ndbutils.fs.rm(\"/Volumes/main/default/my-volume/my-data/\", true) // res6: Boolean = true  \nunmount command (dbutils.fs.unmount)  \nDeletes a DBFS mount point.  \nWarning  \nTo avoid errors, never modify a mount point while other jobs are reading or writing to it. After modifying a mount, always run dbutils.fs.refreshMounts() on all other running clusters to propagate any mount updates. See refreshMounts command (dbutils.fs.refreshMounts).  \nTo display help for this command, run dbutils.fs.help(\"unmount\").  \ndbutils.fs.unmount(\"/mnt/<mount-name>\")  \nFor additional code examples, see Connect to Amazon S3.  \nupdateMount command (dbutils.fs.updateMount)  \nSimilar to the dbutils.fs.mount command, but updates an existing mount point instead of creating a new one. Returns an error if the mount point is not present.  \nTo display help for this command, run dbutils.fs.help(\"updateMount\").  \nWarning  \nTo avoid errors, never modify a mount point while other jobs are reading or writing to it. After modifying a mount, always run dbutils.fs.refreshMounts() on all other running clusters to propagate any mount updates. See refreshMounts command (dbutils.fs.refreshMounts).  \nThis command is available in Databricks Runtime 10.4 LTS and above.  \naws_bucket_name = \"my-bucket\" mount_name = \"s3-my-bucket\" dbutils.fs.updateMount(\"s3a://%s\" % aws_bucket_name, \"/mnt/%s\" % mount_name)"
    },
    {
        "id": 364,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "val AwsBucketName = \"my-bucket\" val MountName = \"s3-my-bucket\" dbutils.fs.updateMount(s\"s3a://$AwsBucketName\", s\"/mnt/$MountName\")"
    },
    {
        "id": 365,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "Jobs utility (dbutils.jobs)\nSubutilities: taskValues  \nNote  \nThis utility is available only for Python.  \nThe jobs utility allows you to leverage jobs features. To display help for this utility, run dbutils.jobs.help().  \nProvides utilities for leveraging jobs features. taskValues: TaskValuesUtils -> Provides utilities for leveraging job task values  \ntaskValues subutility (dbutils.jobs.taskValues)  \nCommands: get, set  \nNote  \nThis subutility is available only for Python.  \nProvides commands for leveraging job task values.  \nUse this sub utility to set and get arbitrary values during a job run. These values are called task values. You can access task values in downstream tasks in the same job run. For example, you can communicate identifiers or metrics, such as information about the evaluation of a machine learning model, between different tasks within a job run. Each task can set multiple task values, get them, or both. Each task value has a unique key within the same task. This unique key is known as the task value\u2019s key. A task value is accessed with the task name and the task value\u2019s key.  \nTo display help for this subutility, run dbutils.jobs.taskValues.help().  \nget command (dbutils.jobs.taskValues.get)  \nNote  \nThis command is available only for Python.  \nOn Databricks Runtime 10.4 and earlier, if get cannot find the task, a Py4JJavaError is raised instead of a ValueError.  \nGets the contents of the specified task value for the specified task in the current job run.  \nTo display help for this command, run dbutils.jobs.taskValues.help(\"get\").  \nFor example:  \ndbutils.jobs.taskValues.get(taskKey = \"my-task\", \\ key = \"my-key\", \\ default = 7, \\ debugValue = 42)  \nIn the preceding example:  \ntaskKey is the name of the task that set the task value. If the command cannot find this task, a ValueError is raised."
    },
    {
        "id": 366,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "In the preceding example:  \ntaskKey is the name of the task that set the task value. If the command cannot find this task, a ValueError is raised.  \nkey is the name of the task value\u2019s key that you set with the set command (dbutils.jobs.taskValues.set). If the command cannot find this task value\u2019s key, a ValueError is raised (unless default is specified).  \ndefault is an optional value that is returned if key cannot be found. default cannot be None.  \ndebugValue is an optional value that is returned if you try to get the task value from within a notebook that is running outside of a job. This can be useful during debugging when you want to run your notebook manually and return some value instead of raising a TypeError by default. debugValue cannot be None.  \nIf you try to get a task value from within a notebook that is running outside of a job, this command raises a TypeError by default. However, if the debugValue argument is specified in the command, the value of debugValue is returned instead of raising a TypeError.  \nset command (dbutils.jobs.taskValues.set)  \nNote  \nThis command is available only for Python.  \nSets or updates a task value. You can set up to 250 task values for a job run.  \nTo display help for this command, run dbutils.jobs.taskValues.help(\"set\").  \nSome examples include:  \ndbutils.jobs.taskValues.set(key = \"my-key\", \\ value = 5) dbutils.jobs.taskValues.set(key = \"my-other-key\", \\ value = \"my other value\")  \nIn the preceding examples:  \nkey is the task value\u2019s key. This key must be unique to the task. That is, if two different tasks each set a task value with key K, these are two different task values that have the same key K.  \nvalue is the value for this task value\u2019s key. This command must be able to represent the value internally in JSON format. The size of the JSON representation of the value cannot exceed 48 KiB.  \nIf you try to set a task value from within a notebook that is running outside of a job, this command does nothing."
    },
    {
        "id": 367,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "Library utility (dbutils.library)\nLibrary utility (dbutils.library)\nMost methods in the dbutils.library submodule are deprecated. See Library utility (dbutils.library) (legacy).  \nYou might need to programmatically restart the Python process on Databricks to ensure that locally installed or upgraded libraries function correctly in the Python kernel for your current SparkSession. To do this, run the dbutils.library.restartPython command. See Restart the Python process on Databricks.\n\nNotebook utility (dbutils.notebook)"
    },
    {
        "id": 368,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "Notebook utility (dbutils.notebook)\nCommands: exit, run  \nThe notebook utility allows you to chain together notebooks and act on their results. See Run a Databricks notebook from another notebook.  \nTo list the available commands, run dbutils.notebook.help().  \nexit(value: String): void -> This method lets you exit a notebook with a value run(path: String, timeoutSeconds: int, arguments: Map): String -> This method runs a notebook and returns its exit value.  \nexit command (dbutils.notebook.exit)  \nExits a notebook with a value.  \nTo display help for this command, run dbutils.notebook.help(\"exit\").  \nThis example exits the notebook with the value Exiting from My Other Notebook.  \ndbutils.notebook.exit(\"Exiting from My Other Notebook\") # Notebook exited: Exiting from My Other Notebook  \ndbutils.notebook.exit(\"Exiting from My Other Notebook\") # Notebook exited: Exiting from My Other Notebook  \ndbutils.notebook.exit(\"Exiting from My Other Notebook\") // Notebook exited: Exiting from My Other Notebook  \nNote  \nIf the run has a query with structured streaming running in the background, calling dbutils.notebook.exit() does not terminate the run. The run will continue to execute for as long as query is executing in the background. You can stop the query running in the background by clicking Cancel in the cell of the query or by running query.stop(). When the query stops, you can terminate the run with dbutils.notebook.exit().  \nrun command (dbutils.notebook.run)  \nRuns a notebook and returns its exit value. The notebook will run in the current cluster by default.  \nNote  \nThe maximum length of the string value returned from the run command is 5 MB. See Get the output for a single run (GET /jobs/runs/get-output).  \nTo display help for this command, run dbutils.notebook.help(\"run\").  \nThis example runs a notebook named My Other Notebook in the same location as the calling notebook. The called notebook ends with the line of code dbutils.notebook.exit(\"Exiting from My Other Notebook\"). If the called notebook does not finish running within 60 seconds, an exception is thrown."
    },
    {
        "id": 369,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "dbutils.notebook.run(\"My Other Notebook\", 60) # Out[14]: 'Exiting from My Other Notebook'  \ndbutils.notebook.run(\"My Other Notebook\", 60) // res2: String = Exiting from My Other Notebook"
    },
    {
        "id": 370,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "Secrets utility (dbutils.secrets)\nCommands: get, getBytes, list, listScopes  \nThe secrets utility allows you to store and access sensitive credential information without making them visible in notebooks. See Secret management and Use the secrets in a notebook. To list the available commands, run dbutils.secrets.help().  \nget(scope: String, key: String): String -> Gets the string representation of a secret value with scope and key getBytes(scope: String, key: String): byte[] -> Gets the bytes representation of a secret value with scope and key list(scope: String): Seq -> Lists secret metadata for secrets within a scope listScopes: Seq -> Lists secret scopes  \nget command (dbutils.secrets.get)  \nGets the string representation of a secret value for the specified secrets scope and key.  \nWarning  \nAdministrators, secret creators, and users granted permission can read Databricks secrets. While Databricks makes an effort to redact secret values that might be displayed in notebooks, it is not possible to prevent such users from reading secrets. For more information, see Secret redaction.  \nTo display help for this command, run dbutils.secrets.help(\"get\").  \nThis example gets the string representation of the secret value for the scope named my-scope and the key named my-key.  \ndbutils.secrets.get(scope=\"my-scope\", key=\"my-key\") # Out[14]: '[REDACTED]'  \ndbutils.secrets.get(scope=\"my-scope\", key=\"my-key\") # [1] \"[REDACTED]\"  \ndbutils.secrets.get(scope=\"my-scope\", key=\"my-key\") // res0: String = [REDACTED]  \ngetBytes command (dbutils.secrets.getBytes)  \nGets the bytes representation of a secret value for the specified scope and key.  \nTo display help for this command, run dbutils.secrets.help(\"getBytes\").  \nThis example gets the byte representation of the secret value (in this example, a1!b2@c3#) for the scope named my-scope and the key named my-key."
    },
    {
        "id": 371,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "This example gets the byte representation of the secret value (in this example, a1!b2@c3#) for the scope named my-scope and the key named my-key.  \ndbutils.secrets.getBytes(scope=\"my-scope\", key=\"my-key\") # Out[1]: b'a1!b2@c3#'  \ndbutils.secrets.getBytes(scope=\"my-scope\", key=\"my-key\") # [1] 61 31 21 62 32 40 63 33 23  \ndbutils.secrets.getBytes(scope=\"my-scope\", key=\"my-key\") // res1: Array[Byte] = Array(97, 49, 33, 98, 50, 64, 99, 51, 35)  \nlist command (dbutils.secrets.list)  \nLists the metadata for secrets within the specified scope.  \nTo display help for this command, run dbutils.secrets.help(\"list\").  \nThis example lists the metadata for secrets within the scope named my-scope.  \ndbutils.secrets.list(\"my-scope\") # Out[10]: [SecretMetadata(key='my-key')]  \ndbutils.secrets.list(\"my-scope\") # [[1]] # [[1]]$key # [1] \"my-key\"  \ndbutils.secrets.list(\"my-scope\") // res2: Seq[com.databricks.dbutils_v1.SecretMetadata] = ArrayBuffer(SecretMetadata(my-key))  \nlistScopes command (dbutils.secrets.listScopes)  \nLists the available scopes.  \nTo display help for this command, run dbutils.secrets.help(\"listScopes\").  \nThis example lists the available scopes.  \ndbutils.secrets.listScopes() # Out[14]: [SecretScope(name='my-scope')]  \ndbutils.secrets.listScopes() # [[1]] # [[1]]$name # [1] \"my-scope\""
    },
    {
        "id": 372,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "dbutils.secrets.listScopes() # [[1]] # [[1]]$name # [1] \"my-scope\"  \ndbutils.secrets.listScopes() // res3: Seq[com.databricks.dbutils_v1.SecretScope] = ArrayBuffer(SecretScope(my-scope))"
    },
    {
        "id": 373,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "Widgets utility (dbutils.widgets)\nCommands: combobox, dropdown, get, getArgument, multiselect, remove, removeAll, text  \nThe widgets utility allows you to parameterize notebooks. See Databricks widgets.  \nTo list the available commands, run dbutils.widgets.help().  \ncombobox(name: String, defaultValue: String, choices: Seq, label: String): void -> Creates a combobox input widget with a given name, default value and choices dropdown(name: String, defaultValue: String, choices: Seq, label: String): void -> Creates a dropdown input widget a with given name, default value and choices get(name: String): String -> Retrieves current value of an input widget getAll: map -> Retrieves a map of all widget names and their values getArgument(name: String, optional: String): String -> (DEPRECATED) Equivalent to get multiselect(name: String, defaultValue: String, choices: Seq, label: String): void -> Creates a multiselect input widget with a given name, default value and choices remove(name: String): void -> Removes an input widget from the notebook removeAll: void -> Removes all widgets in the notebook text(name: String, defaultValue: String, label: String): void -> Creates a text input widget with a given name and default value  \ncombobox command (dbutils.widgets.combobox)  \nCreates and displays a combobox widget with the specified programmatic name, default value, choices, and optional label.  \nTo display help for this command, run dbutils.widgets.help(\"combobox\").  \nThis example creates and displays a combobox widget with the programmatic name fruits_combobox. It offers the choices apple, banana, coconut, and dragon fruit and is set to the initial value of banana. This combobox widget has an accompanying label Fruits. This example ends by printing the initial value of the combobox widget, banana."
    },
    {
        "id": 374,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "dbutils.widgets.combobox( name='fruits_combobox', defaultValue='banana', choices=['apple', 'banana', 'coconut', 'dragon fruit'], label='Fruits' ) print(dbutils.widgets.get(\"fruits_combobox\")) # banana  \ndbutils.widgets.combobox( name='fruits_combobox', defaultValue='banana', choices=list('apple', 'banana', 'coconut', 'dragon fruit'), label='Fruits' ) print(dbutils.widgets.get(\"fruits_combobox\")) # [1] \"banana\"  \ndbutils.widgets.combobox( \"fruits_combobox\", \"banana\", Array(\"apple\", \"banana\", \"coconut\", \"dragon fruit\"), \"Fruits\" ) print(dbutils.widgets.get(\"fruits_combobox\")) // banana  \nCREATE WIDGET COMBOBOX fruits_combobox DEFAULT \"banana\" CHOICES SELECT * FROM (VALUES (\"apple\"), (\"banana\"), (\"coconut\"), (\"dragon fruit\")) SELECT :fruits_combobox -- banana  \ndropdown command (dbutils.widgets.dropdown)  \nCreates and displays a dropdown widget with the specified programmatic name, default value, choices, and optional label.  \nTo display help for this command, run dbutils.widgets.help(\"dropdown\").  \nThis example creates and displays a dropdown widget with the programmatic name toys_dropdown. It offers the choices alphabet blocks, basketball, cape, and doll and is set to the initial value of basketball. This dropdown widget has an accompanying label Toys. This example ends by printing the initial value of the dropdown widget, basketball."
    },
    {
        "id": 375,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "dbutils.widgets.dropdown( name='toys_dropdown', defaultValue='basketball', choices=['alphabet blocks', 'basketball', 'cape', 'doll'], label='Toys' ) print(dbutils.widgets.get(\"toys_dropdown\")) # basketball  \ndbutils.widgets.dropdown( name='toys_dropdown', defaultValue='basketball', choices=list('alphabet blocks', 'basketball', 'cape', 'doll'), label='Toys' ) print(dbutils.widgets.get(\"toys_dropdown\")) # [1] \"basketball\"  \ndbutils.widgets.dropdown( \"toys_dropdown\", \"basketball\", Array(\"alphabet blocks\", \"basketball\", \"cape\", \"doll\"), \"Toys\" ) print(dbutils.widgets.get(\"toys_dropdown\")) // basketball  \nCREATE WIDGET DROPDOWN toys_dropdown DEFAULT \"basketball\" CHOICES SELECT * FROM (VALUES (\"alphabet blocks\"), (\"basketball\"), (\"cape\"), (\"doll\")) SELECT :toys_dropdown -- basketball  \nget command (dbutils.widgets.get)  \nGets the current value of the widget with the specified programmatic name. This programmatic name can be either:  \nThe name of a custom widget in the notebook, for example fruits_combobox or toys_dropdown.  \nThe name of a custom parameter passed to the notebook as part of a notebook task, for example name or age. For more information, see the coverage of parameters for notebook tasks in the Create a job UI or the notebook_params field in the Trigger a new job run (POST /jobs/run-now) operation in the Jobs API.  \nTo display help for this command, run dbutils.widgets.help(\"get\").  \nThis example gets the value of the widget that has the programmatic name fruits_combobox.  \ndbutils.widgets.get('fruits_combobox') # banana"
    },
    {
        "id": 376,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "This example gets the value of the widget that has the programmatic name fruits_combobox.  \ndbutils.widgets.get('fruits_combobox') # banana  \ndbutils.widgets.get('fruits_combobox') # [1] \"banana\"  \ndbutils.widgets.get(\"fruits_combobox\") // res6: String = banana  \nSELECT :fruits_combobox -- banana  \nThis example gets the value of the notebook task parameter that has the programmatic name age. This parameter was set to 35 when the related notebook task was run.  \ndbutils.widgets.get('age') # 35  \ndbutils.widgets.get('age') # [1] \"35\"  \ndbutils.widgets.get(\"age\") // res6: String = 35  \nSELECT :age -- 35  \ngetAll command (dbutils.widgets.getAll)  \nGets a mapping of all current widget names and values. This can be especially useful to quickly pass widget values to a spark.sql() query.  \nThis command is available in Databricks Runtime 13.3 LTS and above. It is only available for Python and Scala.  \nTo display help for this command, run dbutils.widgets.help(\"getAll\").  \nThis example gets the map of widget values and passes it as parameter arguments in a Spark SQL query.  \ndf = spark.sql(\"SELECT * FROM table where col1 = :param\", dbutils.widgets.getAll()) df.show() # Query output  \nval df = spark.sql(\"SELECT * FROM table where col1 = :param\", dbutils.widgets.getAll()) df.show() // res6: Query output  \ngetArgument command (dbutils.widgets.getArgument)  \nGets the current value of the widget with the specified programmatic name. If the widget does not exist, an optional message can be returned.  \nNote  \nThis command is deprecated. Use dbutils.widgets.get instead."
    },
    {
        "id": 377,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "Gets the current value of the widget with the specified programmatic name. If the widget does not exist, an optional message can be returned.  \nNote  \nThis command is deprecated. Use dbutils.widgets.get instead.  \nTo display help for this command, run dbutils.widgets.help(\"getArgument\").  \nThis example gets the value of the widget that has the programmatic name fruits_combobox. If this widget does not exist, the message Error: Cannot find fruits combobox is returned.  \ndbutils.widgets.getArgument('fruits_combobox', 'Error: Cannot find fruits combobox') # Deprecation warning: Use dbutils.widgets.text() or dbutils.widgets.dropdown() to create a widget and dbutils.widgets.get() to get its bound value. # Out[3]: 'banana'  \ndbutils.widgets.getArgument('fruits_combobox', 'Error: Cannot find fruits combobox') # Deprecation warning: Use dbutils.widgets.text() or dbutils.widgets.dropdown() to create a widget and dbutils.widgets.get() to get its bound value. # [1] \"banana\"  \ndbutils.widgets.getArgument(\"fruits_combobox\", \"Error: Cannot find fruits combobox\") // command-1234567890123456:1: warning: method getArgument in trait WidgetsUtils is deprecated: Use dbutils.widgets.text() or dbutils.widgets.dropdown() to create a widget and dbutils.widgets.get() to get its bound value. // dbutils.widgets.getArgument(\"fruits_combobox\", \"Error: Cannot find fruits combobox\") // ^ // res7: String = banana  \nmultiselect command (dbutils.widgets.multiselect)"
    },
    {
        "id": 378,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "multiselect command (dbutils.widgets.multiselect)  \nCreates and displays a multiselect widget with the specified programmatic name, default value, choices, and optional label.  \nTo display help for this command, run dbutils.widgets.help(\"multiselect\").  \nThis example creates and displays a multiselect widget with the programmatic name days_multiselect. It offers the choices Monday through Sunday and is set to the initial value of Tuesday. This multiselect widget has an accompanying label Days of the Week. This example ends by printing the initial value of the multiselect widget, Tuesday.  \ndbutils.widgets.multiselect( name='days_multiselect', defaultValue='Tuesday', choices=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], label='Days of the Week' ) print(dbutils.widgets.get(\"days_multiselect\")) # Tuesday  \ndbutils.widgets.multiselect( name='days_multiselect', defaultValue='Tuesday', choices=list('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'), label='Days of the Week' ) print(dbutils.widgets.get(\"days_multiselect\")) # [1] \"Tuesday\"  \ndbutils.widgets.multiselect( \"days_multiselect\", \"Tuesday\", Array(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"), \"Days of the Week\" ) print(dbutils.widgets.get(\"days_multiselect\")) // Tuesday  \nCREATE WIDGET MULTISELECT days_multiselect DEFAULT \"Tuesday\" CHOICES SELECT * FROM (VALUES (\"Monday\"), (\"Tuesday\"), (\"Wednesday\"), (\"Thursday\"), (\"Friday\"), (\"Saturday\"), (\"Sunday\")) SELECT :days_multiselect -- Tuesday  \nremove command (dbutils.widgets.remove)"
    },
    {
        "id": 379,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "remove command (dbutils.widgets.remove)  \nRemoves the widget with the specified programmatic name.  \nTo display help for this command, run dbutils.widgets.help(\"remove\").  \nImportant  \nIf you add a command to remove a widget, you cannot add a subsequent command to create a widget in the same cell. You must create the widget in another cell.  \nThis example removes the widget with the programmatic name fruits_combobox.  \ndbutils.widgets.remove('fruits_combobox')  \ndbutils.widgets.remove('fruits_combobox')  \ndbutils.widgets.remove(\"fruits_combobox\")  \nREMOVE WIDGET fruits_combobox  \nremoveAll command (dbutils.widgets.removeAll)  \nRemoves all widgets from the notebook.  \nTo display help for this command, run dbutils.widgets.help(\"removeAll\").  \nImportant  \nIf you add a command to remove all widgets, you cannot add a subsequent command to create any widgets in the same cell. You must create the widgets in another cell.  \nThis example removes all widgets from the notebook.  \ndbutils.widgets.removeAll()  \ndbutils.widgets.removeAll()  \ndbutils.widgets.removeAll()  \ntext command (dbutils.widgets.text)  \nCreates and displays a text widget with the specified programmatic name, default value, and optional label.  \nTo display help for this command, run dbutils.widgets.help(\"text\").  \nThis example creates and displays a text widget with the programmatic name your_name_text. It is set to the initial value of Enter your name. This text widget has an accompanying label Your name. This example ends by printing the initial value of the text widget, Enter your name.  \ndbutils.widgets.text( name='your_name_text', defaultValue='Enter your name', label='Your name' ) print(dbutils.widgets.get(\"your_name_text\")) # Enter your name"
    },
    {
        "id": 380,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "dbutils.widgets.text( name='your_name_text', defaultValue='Enter your name', label='Your name' ) print(dbutils.widgets.get(\"your_name_text\")) # [1] \"Enter your name\"  \ndbutils.widgets.text( \"your_name_text\", \"Enter your name\", \"Your name\" ) print(dbutils.widgets.get(\"your_name_text\")) // Enter your name  \nCREATE WIDGET TEXT your_name_text DEFAULT \"Enter your name\" SELECT :your_name_text -- Enter your name"
    },
    {
        "id": 381,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "Databricks Utilities API library\nImportant  \nThe Databricks Utilities API (dbutils-api) library is deprecated. Although this library is still available, Databricks plans no new feature work for the dbutils-api library.  \nDatabricks recommends that you use one of the following libraries instead:  \nDatabricks Utilities for Scala, with Java  \nDatabricks Utilities for Scala, with Scala  \nTo accelerate application development, it can be helpful to compile, build, and test applications before you deploy them as production jobs. To enable you to compile against Databricks Utilities, Databricks provides the dbutils-api library. You can download the dbutils-api library from the DBUtils API webpage on the Maven Repository website or include the library by adding a dependency to your build file:  \nSBT  \nlibraryDependencies += \"com.databricks\" % \"dbutils-api_TARGET\" % \"VERSION\"  \nMaven  \n<dependency> <groupId>com.databricks</groupId> <artifactId>dbutils-api_TARGET</artifactId> <version>VERSION</version> </dependency>  \nGradle  \ncompile 'com.databricks:dbutils-api_TARGET:VERSION'  \nReplace TARGET with the desired target (for example 2.12) and VERSION with the desired version (for example 0.0.5). For a list of available targets and versions, see the DBUtils API webpage on the Maven Repository website.  \nOnce you build your application against this library, you can deploy the application.  \nImportant  \nThe dbutils-api library allows you to locally compile an application that uses dbutils, but not to run it. To run the application, you must deploy it in Databricks."
    },
    {
        "id": 382,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-utils.html",
        "content": "Limitations\nLimitations\nCalling dbutils inside of executors can produce unexpected results or potentially result in errors.  \nIf you need to run file system operations on executors using dbutils, there are several faster and more scalable alternatives available:  \nFor file copy or move operations, you can check a faster option of running filesystem operations described in Parallelize filesystem operations.  \nFor file system list and delete operations, you can refer to parallel listing and delete methods utilizing Spark in How to list and delete files faster in Databricks.  \nFor information about executors, see Cluster Mode Overview on the Apache Spark website."
    },
    {
        "id": 383,
        "url": "https://docs.databricks.com/en/delta-sharing/access-list.html",
        "content": "Restrict Delta Sharing recipient access using IP access lists (open sharing)  \nThis article describes how data providers can assign IP access lists to control recipient access to shared data.  \nIf you, as a data provider, are using the open Delta Sharing protocol, you can limit a recipient to a restricted set of IP addresses when they access data that you share. This list is independent of Workspace IP access lists. Only allow lists are supported.  \nThe IP access list affects the following:  \nDelta Sharing OSS Protocol REST API access  \nDelta Sharing activation URL access  \nDelta Sharing credential file download  \nEach recipient supports a maximum of 100 IP/CIDR values, where one CIDR counts as a single value. Only IPv4 addresses are supported.  \nAssign an IP access list to a recipient"
    },
    {
        "id": 384,
        "url": "https://docs.databricks.com/en/delta-sharing/access-list.html",
        "content": "Assign an IP access list to a recipient\nYou can assign an IP access list to a recipient using Catalog Explorer or the Databricks Unity Catalog CLI.  \nPermissions required: If you are assigning an IP access list when you create a recipient, you must be a metastore admin or user with the CREATE_RECIPIENT privilege. If you are assigning an IP access list to an existing recipient, you must be the recipient object owner.  \nIn your Databricks workspace, click Catalog.  \nAt the top of the Catalog pane, click the gear icon and select Delta Sharing.  \nAlternatively, from the Quick access page, click the Delta Sharing > button.  \nOn the Shared by me tab, click Recipients and select the recipient.  \nOn the IP access list tab, click Add IP address/CIDRs for each IP address (in single IP address format, like 8.8.8.8) or range of IP addresses (in CIDR format, like 8.8.8.4/10).  \nTo add an IP access list when you create a new recipient, run the following command using the Databricks CLI, replacing <recipient-name> and the IP address values.  \ndatabricks recipients create \\ --json=-'{ \"name\": \"<recipient-name>\", \"authentication_type\": \"<authentication-type>\", \"ip_access_list\": { \"allowed_ip_addresses\": [ \"8.8.8.8\", \"8.8.8.4/10\" ] } }'  \nTo add an IP access list to an existing recipient, run the following command, replacing <recipient-name> and the IP address values.  \ndatabricks recipients update \\ --json='{ \"name\": \"<recipient-name>\", \"ip_access_list\": { \"allowed_ip_addresses\": [ \"8.8.8.8\", \"8.8.8.4/10\" ] } }'"
    },
    {
        "id": 385,
        "url": "https://docs.databricks.com/en/delta-sharing/access-list.html",
        "content": "Remove an IP access list\nRemove an IP access list\nYou can remove a recipient\u2019s IP access list using Catalog Explorer or the Databricks Unity Catalog CLI. If you remove all IP addresses from the list, the recipient can access the shared data from anywhere.  \nPermissions required: Recipient object owner.  \nIn your Databricks workspace, click Catalog.  \nAt the top of the Catalog pane, click the gear icon and select Delta Sharing.  \nAlternatively, from the Quick access page, click the Delta Sharing > button.  \nOn the Shared by me tab, click Recipients and select the recipient.  \nOn the IP access list tab, click the trash can icon next to the IP address you want to delete.  \nUse the Databricks CLI to pass in an empty IP access list:  \ndatabricks recipients update \\ --json='{ \"name\": \"<recipient-name>\", \"ip_access_list\": {} }'\n\nView a recipient\u2019s IP access list"
    },
    {
        "id": 386,
        "url": "https://docs.databricks.com/en/delta-sharing/access-list.html",
        "content": "View a recipient\u2019s IP access list\nYou can view a recipient\u2019s IP access list using Catalog Explorer, the Databricks Unity Catalog CLI, or the DESCRIBE RECIPIENT SQL command in a notebook or Databricks SQL query.  \nPermissions required: Metastore admin, user with the USE RECIPIENT privilege, or the recipient object owner.  \nIn your Databricks workspace, click Catalog.  \nAt the top of the Catalog pane, click the gear icon and select Delta Sharing.  \nAlternatively, from the Quick access page, click the Delta Sharing > button.  \nOn the Shared by me tab, click Recipients and select the recipient.  \nView allowed IP addresses on the IP access list tab.  \nRun the following command using the Databricks CLI.  \ndatabricks recipients get <recipient-name>  \nRun the following command in a notebook or the Databricks SQL query editor.  \nDESCRIBE RECIPIENT <recipient-name>;\n\nAudit logging for Delta Sharing IP access lists"
    },
    {
        "id": 387,
        "url": "https://docs.databricks.com/en/delta-sharing/access-list.html",
        "content": "Audit logging for Delta Sharing IP access lists\nThe following operations trigger audit logs related to IP access lists:  \nRecipient management operations: create, update  \nDenial of access to any of the Delta Sharing OSS Protocol REST API calls  \nDenial of access to Delta Sharing activation URL (open sharing only)  \nDenial of access to Delta Sharing credential file download (open sharing only)  \nTo learn more about how to enable and read audit logs for Delta Sharing, see Audit and monitor data sharing."
    },
    {
        "id": 388,
        "url": "https://docs.databricks.com/en/clean-rooms/manage-clean-room.html",
        "content": "Manage clean rooms  \nPreview  \nThis feature is in Public Preview. To request access, reach out to your Databricks representative.  \nThis article describes how to manage clean rooms, including how to:  \nUpdate a clean room owner and comment  \nAdd, remove, and edit data assets and notebooks  \nGrant access to a clean room  \nMonitor clean room activity  \nDelete a clean room  \nThese tasks can be performed by all collaborators in a clean room.  \nBefore you begin"
    },
    {
        "id": 389,
        "url": "https://docs.databricks.com/en/clean-rooms/manage-clean-room.html",
        "content": "Before you begin\nManaging clean rooms requires the following permissions, depending on the task:  \nTo view a clean room in the clean room list or to view clean room details, you must be the owner of the clean room, a metastore admin, or have one of the following privileges on the clean room: MODIFY CLEAN ROOM, EXECUTE CLEAN ROOM TASK, or BROWSE.  \nTo update the owner of a clean room, you must be the owner of the clean room or a metastore admin.  \nTo add and remove data assets and notebooks in a clean room and to update a comment, you must be the owner of the clean room or have the MODIFY CLEAN ROOM privilege on the clean room.  \nTo grant access to a clean room, you must be the owner or a metastore admin.  \nTo delete a clean room, you must be the owner.  \nNote  \nThe creator is automatically assigned as the owner of the clean room in their Databricks account. The collaborator organization\u2019s metastore admin is automatically assigned ownership of the clean room in their Databricks account. You can transfer ownership. See Manage Unity Catalog object ownership.\n\nUpdate a clean room"
    },
    {
        "id": 390,
        "url": "https://docs.databricks.com/en/clean-rooms/manage-clean-room.html",
        "content": "Update a clean room\nIn your Databricks workspace, click Catalog.  \nOn the Quick access page, click the Clean Rooms > button.  \nAlternatively, click the gear icon at the top of the Catalog pane and select Clean Rooms.  \nSelect the clean room from the list.  \nMake any of the following updates:  \nEdit comment: Click the edit icon next to the comment. Comment changes apply only to the clean room securable in your Unity Catalog metastore. It does not get propogated to other collaborators.  \nTransfer owner: Click the edit icon next to the Owner name.  \nAdd tables and volumes: See Step 3. Add data assets and notebooks to the clean room.  \nRemove tables and volumes: Click the kebab menu (also known as the three-dot menu) on the asset row and select Remove data asset.  \nAdd notebooks: See Step 3. Add data assets and notebooks to the clean room.  \nUpdate notebooks: Click the kebab menu on the notebook row and select Update notebook. In the dialog, browse for and select the updated notebook.  \nMake any updates directly in the notebook before you select it in the Clean Rooms interface.  \nRemove notebooks: Click the kebab menu on the notebook row and select Delete notebook."
    },
    {
        "id": 391,
        "url": "https://docs.databricks.com/en/clean-rooms/manage-clean-room.html",
        "content": "Grant access to a clean room\nThe clean room owner has all privileges on the clean room. The clean room owner and the metastore owner are the only roles that can grant other principals access to the clean room. The creator is automatically assigned as the owner of the clean room in their Databricks account. The collaborator organization\u2019s metastore admin is automatically assigned ownership of the clean room in their Databricks account. You can transfer ownership. See Manage Unity Catalog object ownership.  \nThe owner can grant the following privileges on a clean room:  \nBROWSE, which lets you list the clean room and view details like comment, owner, assets, and run history.  \nMODIFY CLEAN ROOM, which lets you do everything that BROWSE grants, plus add and remove data assets, add and remove notebooks, and update comments.  \nEXECUTE CLEAN ROOM TASK, which lets you do everything that BROWSE grants, plus run notebooks in a clean room.  \nTo grant a principal privileges on a clean room:  \nIn your Databricks workspace, click Catalog.  \nAt the top of the Catalog pane, click the gear icon and select Clean Rooms.  \nAlternatively, from the Quick access page, click the Clean Rooms > button.  \nSelect the clean room from the list.  \nGo to the Permissions tab.  \nSelect the user, group, or service principal, click Grant, and select the privileges you want to grant.  \nFor more information about granting privileges, see Manage privileges in Unity Catalog."
    },
    {
        "id": 392,
        "url": "https://docs.databricks.com/en/clean-rooms/manage-clean-room.html",
        "content": "Monitor clean room tasks\nMonitor clean room tasks\nAll notebook runs from all collaborators are logged on the Runs tab in the Clean Rooms UI. You can filter runs by status and collaborator who ran the task.  \nIn your Databricks workspace, click Catalog.  \nOn the Quick access page, click the Clean Rooms > button.  \nAlternatively, click the gear icon at the top of the Catalog pane and select Clean Rooms.  \nSelect the clean room from the list.  \nGo to the Runs tab.  \nYou can also view runs for a specific notebook by selecting the notebook in the clean room UI and going to the Runs tab on the notebook details page.\n\nMonitor clean room logs\nMonitor clean room logs\nThe clean_room_events system table logs all clean-room related events in your Databricks metastore and enables you to view all actions that you and other collaborators took on your clean rooms.  \nTo learn how to use this system table to gain insight into clean room activities, see Clean room events system table reference.\n\nDelete a clean room"
    },
    {
        "id": 393,
        "url": "https://docs.databricks.com/en/clean-rooms/manage-clean-room.html",
        "content": "Delete a clean room\nWhen you delete a clean room, collaborators are unable to use it, but it appears on their list of clean rooms until they delete the clean room object on their side. When you delete a clean room, running tasks are canceled immediately.  \nIn your Databricks workspace, click Catalog.  \nOn the Quick access page, click the Clean Rooms > button.  \nAlternatively, click the gear icon at the top of the Catalog pane and select Clean Rooms.  \nSelect the clean room from the list.  \nClick the kebab menu on the upper right corner of the page.  \nSelect Delete.  \nConfirm that you want to delete the clean room, and click Delete.\n\nTroubleshoot clean room issues"
    },
    {
        "id": 394,
        "url": "https://docs.databricks.com/en/clean-rooms/manage-clean-room.html",
        "content": "Troubleshoot clean room issues\nYou may encounter the following error messages or issues when you work with clean rooms.  \nOther collaborators have left the clean room, so the clean room is no longer usable. Please delete this clean room securable.  \nIn a no-trust clean room, any collaborator can delete the central clean room. This error message means that the other collaborator deleted the central clean room, and your reference to the central clean room is no longer valid. You can only delete the clean room object in your Databricks workspace. You cannot use it.  \nCollaborator X already has a clean room named Y  \nNo collaborator can change the clean room name. This ensures that all collaborators can reference the clean room name without ambiguity. This error means that the other collaborator already has a clean room with the same name as the one you chose. Choose another clean room name.  \nPlease accept the Serverless terms of service before using Clean Rooms  \nServerless compute is required for central clean rooms. You do not need to enable serverless compute in your own workspace to use clean rooms. But you do need to accept the serverless compute terms of service. See Enable serverless compute. If you need assistance, contact your Databricks representative.  \nInvited collaborator cannot see the clean room  \nOnly a metastore admin can view a clean room when it is initially created in an invited collaborator\u2019s metastore. Some workspaces that are enabled for Unity Catalog don\u2019t have a metastore admin assigned. You must assign the metastore admin role in order to start working with the clean room. See Assign a metastore admin."
    },
    {
        "id": 395,
        "url": "https://docs.databricks.com/en/delta-live-tables/language-references.html",
        "content": "Delta Live Tables language references  \nThis article has information on the programming interfaces available to implement Delta Live Tables pipelines and has links to documentation with detailed specifications and examples for each interface.  \nData loading and transformations are implemented in a Delta Live Tables pipeline by queries that define streaming tables and materialized views. To implement these queries, Delta Live Tables supports SQL and Python interfaces. Because these interfaces provide equivalent functionality for most data processing use cases, pipeline developers can choose the interface that they are most comfortable with. The articles in this section are detailed references for the SQL and Python interfaces and should be used by developers as they implement pipelines in their interface of choice.  \nDelta Live Tables SQL language reference\nDelta Live Tables SQL language reference\nFor pipeline developers familiar with writing queries in SQL, Delta Live Tables has a simple but powerful SQL interface designed to support the loading and transformation of data. To learn about the details of the SQL interface, including how to define streaming tables for tasks such as loading data and materialized views for transforming data, see Delta Live Tables SQL language reference.\n\nDelta Live Tables Python language reference"
    },
    {
        "id": 396,
        "url": "https://docs.databricks.com/en/delta-live-tables/language-references.html",
        "content": "Delta Live Tables Python language reference\nFor Python developers, Delta Live Tables has a Python interface designed to support the loading and transformation of data. For tasks that require processing not supported by SQL, developers can use Python to write pipeline source code that combines Delta Live Tables queries with Python functions that implement the processing not supported by the Delta Live Tables interfaces. To learn about the Delta Live Tables Python interface, including detailed specifications for the Python functions included in the interface, see Delta Live Tables Python language reference."
    },
    {
        "id": 397,
        "url": "https://docs.databricks.com/en/dev-tools/databricks-connect/python/udf.html",
        "content": "User-defined functions in Databricks Connect for Python  \nNote  \nThis article covers Databricks Connect for Databricks Runtime 13.1 and above.  \nThis article describes how to execute UDFs with Databricks Connect for Python. Databricks Connect enables you to connect popular IDEs, notebook servers, and custom applications to Databricks clusters. For the Scala version of this article, see User-defined functions in Databricks Connect for Scala.  \nNote  \nBefore you begin to use Databricks Connect, you must set up the Databricks Connect client.  \nDatabricks Connect for Python supports user-defined functions (UDF). When a Dataframe operation that include UDFs is executed, the UDFs involved are serialized by Databricks Connect and sent over to the server as part of the request.  \nNote  \nSince the user-defined function is serialized and deserialized, the Python version used by the client must match the Python version on the Databricks cluster. To check the cluster\u2019s Python version, see the \u201cSystem Environment\u201d section for the cluster\u2019s Databricks Runtime version in Databricks Runtime release notes versions and compatibility.  \nThe following Python program sets up a simple UDF that squares values in a column.  \nfrom pyspark.sql.functions import col, udf from pyspark.sql.types import IntegerType from databricks.connect import DatabricksSession @udf(returnType=IntegerType()) def double(x): return x * x spark = DatabricksSession.builder.getOrCreate() df = spark.range(1, 2) df = df.withColumn(\"doubled\", double(col(\"id\"))) df.show()"
    },
    {
        "id": 398,
        "url": "https://docs.databricks.com/en/dashboards/tutorials/manage-permissions.html",
        "content": "Manage dashboard permissions using the Workspace API  \nThis tutorial demonstrates how to manage dashboard permissions using the Workspace API. Each step includes a sample request and response and explanations about how to use the API tools and properties together.  \nPrerequisites\nPrerequisites\nYou need a personal access token to connect with your workspace. See Databricks personal access token authentication.  \nYou need the workspace ID of the workspace you want to access. See Workspace instance names, URLs, and IDs  \nFamiliarity with the Databricks REST API reference.\n\nPath parameters"
    },
    {
        "id": 399,
        "url": "https://docs.databricks.com/en/dashboards/tutorials/manage-permissions.html",
        "content": "Path parameters\nEach endpoint request in this article requires two path parameters, workspace_object_type and workspace_object_id.  \nworkspace_object_type: For AI/BI dashboards, the object type is dashboards.  \nworkspace_object_id: This corresponds to the resource_id associated with the dashboard. You can use the GET /api/2.0/workspace/list or GET /api/2.0/workspace/get-status to retrieve that value. It is a 32-character string similar to 01eec14769f616949d7a44244a53ed10.  \nSee Step 1: Explore a workspace directory for an example of listing workspace objects. See GET /api/2.0/workspace/list for details about the the Workspace List API.\n\nGet workspace object permission levels"
    },
    {
        "id": 400,
        "url": "https://docs.databricks.com/en/dashboards/tutorials/manage-permissions.html",
        "content": "Get workspace object permission levels\nThis section uses the Get workspace object permission levels endpoint to get the permission levels that a user can have on a dashboard. See GET /api/workspace/workspace/getpermissionlevels.  \nIn the following example, the request includes sample path parameters described above. The response includes the permissions that can be applied to the dashboard indicated in the request.  \nGET /api/2.0/permissions/dashboards/01eec14769f616949d7a44244a53ed10/permissionLevels Response: { \"permission_levels\": [ { \"permission_level\": \"CAN_READ\", \"description\": \"Can view the Lakeview dashboard\" }, { \"permission_level\": \"CAN_RUN\", \"description\": \"Can view, attach/detach, and run the Lakeview dashboard\" }, { \"permission_level\": \"CAN_EDIT\", \"description\": \"Can view, attach/detach, run, and edit the Lakeview dashboard\" }, { \"permission_level\": \"CAN_MANAGE\", \"description\": \"Can view, attach/detach, run, edit, and change permissions of the Lakeview dashboard\" } ] }"
    },
    {
        "id": 401,
        "url": "https://docs.databricks.com/en/dashboards/tutorials/manage-permissions.html",
        "content": "Get workspace object permission details\nThe Get workspace object permissions endpoint gets the assigned permissions on a specific workspace object. See GET /api/workspace/workspace/getpermissions.  \nThe following example shows a request and response for the dashboard in the previous example. The response includes details about the dashboard and users and groups with permissions on the dashboard. Permissions on this object have been inherited for both items in the access_control_list portion of the response. In the first entry, permissions are inherited from a folder in the workspace. The second entry shows permissions inherited by membership in the group, admins.  \nGET /api/2.0/permissions/dashboards/01eec14769f616949d7a44244a53ed10 Response: { \"object_id\": \"/dashboards/490384175243923\", \"object_type\": \"dashboard\", \"access_control_list\": [ { \"user_name\": \"first.last@example.com\", \"display_name\": \"First Last\", \"all_permissions\": [ { \"permission_level\": \"CAN_MANAGE\", \"inherited\": true, \"inherited_from_object\": [ \"/directories/2951435987702195\" ] } ] }, { \"group_name\": \"admins\", \"all_permissions\": [ { \"permission_level\": \"CAN_MANAGE\", \"inherited\": true, \"inherited_from_object\": [ \"/directories/\" ] } ] } ] }"
    },
    {
        "id": 402,
        "url": "https://docs.databricks.com/en/dashboards/tutorials/manage-permissions.html",
        "content": "Set workspace object permissions\nSet workspace object permissions\nYou can set permissions on dashboards using the Set workspace object permissions endpoint. See PUT /api/workspace/workspace/setpermissions.  \nThe following example gives CAN EDIT permission to all workspace users for the workspace_object_id in the PUT request.  \nPUT /api/2.0/permissions/dashboards/01eec14769f616949d7a44244a53ed10 Request body: { \"access_control_list\": [ { \"group_name\": \"users\", \"permission_level\": \"CAN_EDIT\" } ] }  \nFor AI/BI dashboards, you can use the group All account users to assign view permission to all users registered to the Databricks account. See Share a published dashboard.\n\nUpdate workspace object permissions"
    },
    {
        "id": 403,
        "url": "https://docs.databricks.com/en/dashboards/tutorials/manage-permissions.html",
        "content": "Update workspace object permissions\nThe Update workspace object permissions endpoint performs functions similarly to the Set workspace object permissions endpoint. It assigns permissions using a PATCH request instead of a PUT request.  \nSee PATCH /api/workspace/workspace/updatepermissions.  \nPATCH /api/2.0/permissions/dashboards/01eec14769f616949d7a44244a53ed10 Request body: { \"access_control_list\": [ { \"group_name\": \"account userS\", \"permission_level\": \"CAN_VIEW\" } ] }"
    },
    {
        "id": 404,
        "url": "https://docs.databricks.com/en/data-governance/table-acls/table-acl.html",
        "content": "Enable Hive metastore table access control on a cluster (legacy)  \nThis article describes how to enable table access control for the built-in Hive metastore on a cluster.  \nFor information about how to set privileges on Hive metastore securable objects once table access control has been enabled on a cluster, see Hive metastore privileges and securable objects (legacy).  \nNote  \nHive metastore table access control is a legacy data governance model. Databricks recommends that you use Unity Catalog instead for its simplicity and account-centered governance model. You can upgrade the tables managed by the Hive metastore to the Unity Catalog metastore.  \nEnable table access control for a cluster"
    },
    {
        "id": 405,
        "url": "https://docs.databricks.com/en/data-governance/table-acls/table-acl.html",
        "content": "Enable table access control for a cluster\nTable access control is available in two versions:  \nSQL-only table access control, which restricts users to SQL commands.  \nPython and SQL table access control, which allows users to run SQL, Python, and PySpark commands.  \nTable access control is not supported with Machine Learning Runtime.  \nImportant  \nEven if table access control is enabled for a cluster, Databricks workspace administrators have access to file-level data.  \nSQL-only table access control  \nThis version of table access control restricts users to SQL commands only.  \nTo enable SQL-only table access control on a cluster and restrict that cluster to use only SQL commands, set the following flag in the cluster\u2019s Spark conf:  \nspark.databricks.acl.sqlOnly true  \nNote  \nAccess to SQL-only table access control is not affected by the Enable Table Access Control setting in the admin settings page. That setting controls only the workspace-wide enablement of Python and SQL table access control.  \nPython and SQL table access control  \nThis version of table access control lets users run Python commands that use the DataFrame API as well as SQL. When it is enabled on a cluster, users on that cluster:  \nCan access Spark only using the Spark SQL API or DataFrame API. In both cases, access to tables and views is restricted by administrators according to the Databricks Privileges you can grant on Hive metastore objects.  \nMust run their commands on cluster nodes as a low-privilege user forbidden from accessing sensitive parts of the filesystem or creating network connections to ports other than 80 and 443.  \nOnly built-in Spark functions can create network connections on ports other than 80 and 443.  \nOnly workspace admin users or users with ANY FILE privilege can read data from external databases through the PySpark JDBC connector."
    },
    {
        "id": 406,
        "url": "https://docs.databricks.com/en/data-governance/table-acls/table-acl.html",
        "content": "Only built-in Spark functions can create network connections on ports other than 80 and 443.  \nOnly workspace admin users or users with ANY FILE privilege can read data from external databases through the PySpark JDBC connector.  \nIf you want Python processes to be able to access additional outbound ports, you can set the Spark config spark.databricks.pyspark.iptable.outbound.whitelisted.ports to the ports you want to allow access. The supported format of the configuration value is [port[:port][,port[:port]]...], for example: 21,22,9000:9999. The port must be within the valid range, that is, 0-65535.  \nAttempts to get around these restrictions will fail with an exception. These restrictions are in place so that users can never access unprivileged data through the cluster."
    },
    {
        "id": 407,
        "url": "https://docs.databricks.com/en/data-governance/table-acls/table-acl.html",
        "content": "Enable table access control for your workspace\nEnable table access control for your workspace\nBefore users can configure Python and SQL table access control, a Databricks workspace must enable table access control for the Databricks workspace and deny users access to clusters that are not enabled for table access control.  \nGo to the settings page.  \nClick the Security tab.  \nTurn on the Table Access Control option.  \nEnforce table access control  \nTo ensure that your users access only the data that you want them to, you must restrict your users to clusters with table access control enabled. In particular, you should ensure that:  \nUsers do not have permission to create clusters. If they create a cluster without table access control, they can access any data from that cluster.  \nUsers do not have CAN ATTACH TO permission for any cluster that is not enabled for table access control.  \nSee Compute permissions for more information.\n\nCreate a cluster enabled for table access control\nCreate a cluster enabled for table access control\nTable access control is enabled by default in clusters with Shared access mode.  \nTo create the cluster using the REST API, see Create new cluster.\n\nSet privileges on a data object\nSet privileges on a data object\nSee Hive metastore privileges and securable objects (legacy)."
    },
    {
        "id": 408,
        "url": "https://docs.databricks.com/en/delta/data-skipping.html",
        "content": "Data skipping for Delta Lake  \nNote  \nIn Databricks Runtime 13.3 and above, Databricks recommends using liquid clustering for Delta table layout. Clustering is not compatible with Z-ordering. See Use liquid clustering for Delta tables.  \nData skipping information is collected automatically when you write data into a Delta table. Delta Lake on Databricks takes advantage of this information (minimum and maximum values, null counts, and total records per file) at query time to provide faster queries.  \nYou must have statistics collected for columns that are used in ZORDER statements. See What is Z-ordering?.  \nSpecify Delta statistics columns"
    },
    {
        "id": 409,
        "url": "https://docs.databricks.com/en/delta/data-skipping.html",
        "content": "Specify Delta statistics columns\nBy default, Delta Lake collects statistics on the first 32 columns defined in your table schema. For this collection, each field in a nested column is considered an individual column. You can modify this behavior by setting one of the following table properties:  \nTable property  \nDatabricks Runtime supported  \nDescription  \ndelta.dataSkippingNumIndexedCols  \nAll supported Databricks Runtime versions  \nIncrease or decrease the number of columns on which Delta collects statistics. Depends on column order.  \ndelta.dataSkippingStatsColumns  \nDatabricks Runtime 13.3 LTS and above  \nSpecify a list of column names for which Delta Lake collects statistics. Supersedes dataSkippingNumIndexedCols.  \nTable properties can be set at table creation or with ALTER TABLE statements. See Delta table properties reference.  \nUpdating this property does not automatically recompute statistics for existing data. Rather, it impacts the behavior of future statistics collection when adding or updating data in the table. Delta Lake does not leverage statistics for columns not included in the current list of statistics columns.  \nIn Databricks Runtime 14.3 LTS and above, you can manually trigger the recomputation of statistics for a Delta table using the following command:  \nANALYZE TABLE table_name COMPUTE DELTA STATISTICS  \nNote  \nLong strings are truncated during statistics collection. You might choose to exclude long string columns from statistics collection, especially if the columns aren\u2019t used frequently for filtering queries."
    },
    {
        "id": 410,
        "url": "https://docs.databricks.com/en/delta/data-skipping.html",
        "content": "What is Z-ordering?\nNote  \nDatabricks recommends using liquid clustering for all new Delta tables. You cannot use ZORDER in combination with liquid clustering.  \nZ-ordering is a technique to colocate related information in the same set of files. This co-locality is automatically used by Delta Lake on Databricks data-skipping algorithms. This behavior dramatically reduces the amount of data that Delta Lake on Databricks needs to read. To Z-order data, you specify the columns to order on in the ZORDER BY clause:  \nOPTIMIZE events WHERE date >= current_timestamp() - INTERVAL 1 day ZORDER BY (eventType)  \nIf you expect a column to be commonly used in query predicates and if that column has high cardinality (that is, a large number of distinct values), then use ZORDER BY.  \nYou can specify multiple columns for ZORDER BY as a comma-separated list. However, the effectiveness of the locality drops with each extra column. Z-ordering on columns that do not have statistics collected on them would be ineffective and a waste of resources. This is because data skipping requires column-local stats such as min, max, and count. You can configure statistics collection on certain columns by reordering columns in the schema, or you can increase the number of columns to collect statistics on.  \nNote  \nZ-ordering is not idempotent but aims to be an incremental operation. The time it takes for Z-ordering is not guaranteed to reduce over multiple runs. However, if no new data was added to a partition that was just Z-ordered, another Z-ordering of that partition will not have any effect.  \nZ-ordering aims to produce evenly-balanced data files with respect to the number of tuples, but not necessarily data size on disk. The two measures are most often correlated, but there can be situations when that is not the case, leading to skew in optimize task times.  \nFor example, if you ZORDER BY date and your most recent records are all much wider (for example longer arrays or string values) than the ones in the past, it is expected that the OPTIMIZE job\u2019s task durations will be skewed, as well as the resulting file sizes. This is, however, only a problem for the OPTIMIZE command itself; it should not have any negative impact on subsequent queries."
    },
    {
        "id": 411,
        "url": "https://docs.databricks.com/en/delta-live-tables/manage-pipeline-configurations.html",
        "content": "Manage configuration of Delta Live Tables pipelines  \nBecause Delta Live Tables automates operational complexities such as infrastructure management, task orchestration, error recovery, and performance optimization, many of your pipelines can run with minimal manual configuration. However, Delta Live Tables also allows you to manage configuration for pipelines requiring non-default configurations or to optimize performance and resource usage. These articles provide details on managing configurations for your Delta Live Tables pipelines, including settings that determine how pipelines are run, options for the compute that runs a pipeline, and management of external dependencies such as Python libraries.  \nUse serverless compute to run fully managed pipelines"
    },
    {
        "id": 412,
        "url": "https://docs.databricks.com/en/delta-live-tables/manage-pipeline-configurations.html",
        "content": "Use serverless compute to run fully managed pipelines\nUse serverless DLT pipelines to run pipelines with reliable and fully managed compute resources. With serverless compute, the compute that runs your pipeline is automatically optimized and scaled up and down based on the resources required to run a pipeline. Serverless DLT pipelines supports additional features to improve performance, such as incremental refresh for materialized views, faster startup time for compute resources, and improved processing of streaming workloads. See Create fully managed pipelines using Delta Live Tables with serverless compute.\n\nManage pipeline settings"
    },
    {
        "id": 413,
        "url": "https://docs.databricks.com/en/delta-live-tables/manage-pipeline-configurations.html",
        "content": "Manage pipeline settings\nThe configuration for a Delta Live Tables pipeline includes settings that define the source code implementing the pipeline. It also includes settings that control pipeline infrastructure, dependency management, how updates are processed, and how tables are saved in the workspace. Most configurations are optional, but some require careful attention.  \nTo learn about the configuration options for pipelines and how to use them, see Configure pipeline settings for Delta Live Tables.  \nFor detailed specifications of Delta Live Tables settings, properties that control how tables are managed, and non-settable compute options, see Delta Live Tables properties reference.\n\nManage external dependencies for pipelines that use Python\nManage external dependencies for pipelines that use Python\nDelta Live Tables supports using external dependencies in your pipelines such as Python packages and libraries. To learn about options and recommendations for using dependencies, see Manage Python dependencies for Delta Live Tables pipelines.\n\nUse Python modules stored in your Databricks workspace"
    },
    {
        "id": 414,
        "url": "https://docs.databricks.com/en/delta-live-tables/manage-pipeline-configurations.html",
        "content": "Use Python modules stored in your Databricks workspace\nIn addition to implementing your Python code in Databricks notebooks, you can use Databricks Git Folders or workspace files to store your code as Python modules. Storing your code as Python modules is especially useful when you have common functionality you want to use in multiple pipelines or notebooks in the same pipeline. To learn how to use Python modules with your pipelines, see Import Python modules from Git folders or workspace files.\n\nOptimize pipeline compute utilization\nOptimize pipeline compute utilization\nUse Enhanced Autoscaling to optimize the cluster utilization of your pipelines. Enhanced Autoscaling adds resources only if the system determines those resources will increase pipeline processing speed. Resources are freed when no longer needed, and clusters are shut down as soon as all pipeline updates are complete.  \nTo learn more about Enhanced Autoscaling, including configuration details, see Optimize the cluster utilization of Delta Live Tables pipelines with Enhanced Autoscaling."
    },
    {
        "id": 415,
        "url": "https://docs.databricks.com/en/dev-tools/cli/labs-commands.html",
        "content": "labs command group  \nNote  \nThis information applies to Databricks CLI 0.210.0 and above, which are in Public Preview. To find your version of the Databricks CLI, run databricks -v.  \nThe labs command group within the Databricks CLI enables you to work with available Databricks Labs applications.  \nImportant  \nTo install the Databricks CLI, see Install or update the Databricks CLI. To configure authentication for the Databricks CLI, see Authentication for the Databricks CLI.  \nYou run labs commands by appending them to databricks labs. To display help for the labs command group, run databricks labs -h.  \nList all available Databricks Labs applications\nList all available Databricks Labs applications\nTo show a list of all Databricks Labs applications that are available for installation on your local development machine, run the labs list command, as follows:  \ndatabricks labs list\n\nList all installed Databricks Labs applications\nList all installed Databricks Labs applications\nTo show a list of all Databricks Labs applications that were installed on your local development machine by running the Databricks CLI, run the labs installed command, as follows:  \ndatabricks labs installed"
    },
    {
        "id": 416,
        "url": "https://docs.databricks.com/en/dev-tools/cli/labs-commands.html",
        "content": "Install a Databricks Labs application\nInstall a Databricks Labs application\nTo install an available Databricks Labs application, run the labs install command, followed by the application\u2019s programmatic name, as follows:  \ndatabricks labs install <application-name>\n\nShow information about an installed Databricks Labs application\nShow information about an installed Databricks Labs application\nTo show information about a Databricks Labs application that you installed on your local development machine by running the Databricks CLI, run the labs show command, followed by the application\u2019s programmatic name, as follows:  \ndatabricks labs show <application-name>\n\nUpgrade an installed Databricks Labs application\nUpgrade an installed Databricks Labs application\nTo upgrade a Databricks Labs application that you installed by running the Databricks CLI, run the labs upgrade command, followed by the application\u2019s programmatic name, as follows:  \ndatabricks labs upgrade <application-name>\n\nUninstall a Databricks Labs project"
    },
    {
        "id": 417,
        "url": "https://docs.databricks.com/en/dev-tools/cli/labs-commands.html",
        "content": "Uninstall a Databricks Labs project\nTo uninstall a Databricks Labs application that you installed by running the Databricks CLI, run the labs uninstall command, followed by the application\u2019s programmatic name, as follows:  \ndatabricks labs uninstall <application-name>\n\nClear Databricks Labs cache entries\nClear Databricks Labs cache entries\nTo clear cache entries for Databricks Labs from all relevant locations, run the labs clear-cache command, as follows:  \ndatabricks labs clear-cache\n\nAdditional resources\nAdditional resources\nFor more information about available Databricks Labs applications, see the following:  \nDatabricks Labs on databricks.com  \nDatabricks Labs on GitHub"
    },
    {
        "id": 418,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "Stream processing with Apache Kafka and Databricks  \nThis article describes how you can use Apache Kafka as either a source or a sink when running Structured Streaming workloads on Databricks.  \nFor more Kafka, see the Kafka documentation.  \nRead data from Kafka"
    },
    {
        "id": 419,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "Read data from Kafka\nThe following is an example for a streaming read from Kafka:  \ndf = (spark.readStream .format(\"kafka\") .option(\"kafka.bootstrap.servers\", \"<server:ip>\") .option(\"subscribe\", \"<topic>\") .option(\"startingOffsets\", \"latest\") .load() )  \nDatabricks also supports batch read semantics for Kafka data sources, as shown in the following example:  \ndf = (spark .read .format(\"kafka\") .option(\"kafka.bootstrap.servers\", \"<server:ip>\") .option(\"subscribe\", \"<topic>\") .option(\"startingOffsets\", \"earliest\") .option(\"endingOffsets\", \"latest\") .load() )  \nFor incremental batch loading, Databricks recommends using Kafka with Trigger.AvailableNow. See Configuring incremental batch processing.  \nIn Databricks Runtime 13.3 LTS and above, Databricks provides a SQL function for reading Kafka data. Streaming with SQL is supported only in Delta Live Tables or with streaming tables in Databricks SQL. See read_kafka table-valued function."
    },
    {
        "id": 420,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "Configure Kafka Structured Streaming reader\nDatabricks provides the kafka keyword as a data format to configure connections to Kafka 0.10+.  \nThe following are the most common configurations for Kafka:  \nThere are multiple ways of specifying which topics to subscribe to. You should provide only one of these parameters:  \nOption  \nValue  \nDescription  \nsubscribe  \nA comma-separated list of topics.  \nThe topic list to subscribe to.  \nsubscribePattern  \nJava regex string.  \nThe pattern used to subscribe to topic(s).  \nassign  \nJSON string {\"topicA\":[0,1],\"topic\":[2,4]}.  \nSpecific topicPartitions to consume.  \nOther notable configurations:  \nOption  \nValue  \nDefault Value  \nDescription  \nkafka.bootstrap.servers  \nComma-separated list of host:port.  \nempty  \n[Required] The Kafka bootstrap.servers configuration. If you find there is no data from Kafka, check the broker address list first. If the broker address list is incorrect, there might not be any errors. This is because Kafka client assumes the brokers will become available eventually and in the event of network errors retry forever.  \nfailOnDataLoss  \ntrue or false.  \ntrue  \n[Optional] Whether to fail the query when it\u2019s possible that data was lost. Queries can permanently fail to read data from Kafka due to many scenarios such as deleted topics, topic truncation before processing, and so on. We try to estimate conservatively whether data was possibly lost or not. Sometimes this can cause false alarms. Set this option to false if it does not work as expected, or you want the query to continue processing despite data loss.  \nminPartitions  \nInteger >= 0, 0 = disabled.  \n0 (disabled)"
    },
    {
        "id": 421,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "minPartitions  \nInteger >= 0, 0 = disabled.  \n0 (disabled)  \n[Optional] Minimum number of partitions to read from Kafka. You can configure Spark to use an arbitrary minimum of partitions to read from Kafka using the minPartitions option. Normally Spark has a 1-1 mapping of Kafka topicPartitions to Spark partitions consuming from Kafka. If you set the minPartitions option to a value greater than your Kafka topicPartitions, Spark will divvy up large Kafka partitions to smaller pieces. This option can be set at times of peak loads, data skew, and as your stream is falling behind to increase processing rate. It comes at a cost of initializing Kafka consumers at each trigger, which may impact performance if you use SSL when connecting to Kafka.  \nkafka.group.id  \nA Kafka consumer group ID.  \nnot set  \n[Optional] Group ID to use while reading from Kafka. Use this with caution. By default, each query generates a unique group ID for reading data. This ensures that each query has its own consumer group that does not face interference from any other consumer, and therefore can read all of the partitions of its subscribed topics. In some scenarios (for example, Kafka group-based authorization), you may want to use specific authorized group IDs to read data. You can optionally set the group ID. However, do this with extreme caution as it can cause unexpected behavior.  \nConcurrently running queries (both, batch and streaming) with the same group ID are likely interfere with each other causing each query to read only part of the data.  \nThis may also occur when queries are started/restarted in quick succession. To minimize such issues, set the Kafka consumer configuration session.timeout.ms to be very small.  \nstartingOffsets  \nearliest , latest  \nlatest"
    },
    {
        "id": 422,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "This may also occur when queries are started/restarted in quick succession. To minimize such issues, set the Kafka consumer configuration session.timeout.ms to be very small.  \nstartingOffsets  \nearliest , latest  \nlatest  \n[Optional] The start point when a query is started, either \u201cearliest\u201d which is from the earliest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest. Note: For batch queries, latest (either implicitly or by using -1 in json) is not allowed. For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.  \nSee Structured Streaming Kafka Integration Guide for other optional configurations."
    },
    {
        "id": 423,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "Schema for Kafka records\nSchema for Kafka records\nThe schema of Kafka records is:  \nColumn  \nType  \nkey  \nbinary  \nvalue  \nbinary  \ntopic  \nstring  \npartition  \nint  \noffset  \nlong  \ntimestamp  \nlong  \ntimestampType  \nint  \nThe key and the value are always deserialized as byte arrays with the ByteArrayDeserializer. Use DataFrame operations (such as cast(\"string\")) to explicitly deserialize the keys and values.\n\nWrite data to Kafka"
    },
    {
        "id": 424,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "Write data to Kafka\nThe following is an example for a streaming write to Kafka:  \n(df .writeStream .format(\"kafka\") .option(\"kafka.bootstrap.servers\", \"<server:ip>\") .option(\"topic\", \"<topic>\") .start() )  \nDatabricks also supports batch write semantics to Kafka data sinks, as shown in the following example:  \n(df .write .format(\"kafka\") .option(\"kafka.bootstrap.servers\", \"<server:ip>\") .option(\"topic\", \"<topic>\") .save() )\n\nConfigure Kafka Structured Streaming writer"
    },
    {
        "id": 425,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "Configure Kafka Structured Streaming writer\nImportant  \nDatabricks Runtime 13.3 LTS and above includes a newer version of the kafka-clients library that enables idempotent writes by default. If a Kafka sink uses version 2.8.0 or below with ACLs configured, but without IDEMPOTENT_WRITE enabled, the write fails with the error message org.apache.kafka.common.KafkaException: Cannot execute transactional method because we are in an error state.  \nResolve this error by upgrading to Kafka version 2.8.0 or above, or by setting .option(\u201ckafka.enable.idempotence\u201d, \u201cfalse\u201d) while configuring your Structured Streaming writer.  \nThe schema provided to the DataStreamWriter interacts with the Kafka sink. You can use the following fields:  \nColumn name  \nRequired or optional  \nType  \nkey  \noptional  \nSTRING or BINARY  \nvalue  \nrequired  \nSTRING or BINARY  \nheaders  \noptional  \nARRAY  \ntopic  \noptional (ignored if topic is set as writer option)  \nSTRING  \npartition  \noptional  \nINT  \nThe following are common options set while writing to Kafka:  \nOption  \nValue  \nDefault value  \nDescription  \nkafka.boostrap.servers  \nA comma-separated list of <host:port>  \nnone  \n[Required] The Kafka bootstrap.servers configuration.  \ntopic  \nSTRING  \nnot set  \n[Optional] Sets the topic for all rows to be written. This option overrides any topic column that exists in the data.  \nincludeHeaders  \nBOOLEAN  \nfalse  \n[Optional] Whether to include the Kafka headers in the row.  \nSee Structured Streaming Kafka Integration Guide for other optional configurations."
    },
    {
        "id": 426,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "Retrieve Kafka metrics\nYou can get the average, min, and max of the number of offsets that the streaming query is behind the latest available offset among all the subscribed topics with the avgOffsetsBehindLatest, maxOffsetsBehindLatest, and minOffsetsBehindLatest metrics. See Reading Metrics Interactively.  \nNote  \nAvailable in Databricks Runtime 9.1 and above.  \nGet the estimated total number of bytes that the query process has not consumed from the subscribed topics by examining the value of estimatedTotalBytesBehindLatest. This estimate is based on the batches that were processed in the last 300 seconds. The timeframe that the estimate is based on can be changed by setting the option bytesEstimateWindowLength to a different value. For example, to set it to 10 minutes:  \ndf = (spark.readStream .format(\"kafka\") .option(\"bytesEstimateWindowLength\", \"10m\") # m for minutes, you can also use \"600s\" for 600 seconds )  \nIf you are running the stream in a notebook, you can see these metrics under the Raw Data tab in the streaming query progress dashboard:  \n{ \"sources\" : [ { \"description\" : \"KafkaV2[Subscribe[topic]]\", \"metrics\" : { \"avgOffsetsBehindLatest\" : \"4.0\", \"maxOffsetsBehindLatest\" : \"4\", \"minOffsetsBehindLatest\" : \"4\", \"estimatedTotalBytesBehindLatest\" : \"80.0\" }, } ] }"
    },
    {
        "id": 427,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "Use SSL to connect Databricks to Kafka\nTo enable SSL connections to Kafka, follow the instructions in the Confluent documentation Encryption and Authentication with SSL. You can provide the configurations described there, prefixed with kafka., as options. For example, you specify the trust store location in the property kafka.ssl.truststore.location.  \nDatabricks recommends that you:  \nStore your certificates in cloud object storage. You can restrict access to the certificates only to clusters that can access Kafka. See Data governance with Unity Catalog.  \nStore your certificate passwords as secrets in a secret scope.  \nThe following example uses object storage locations and Databricks secrets to enable an SSL connection:  \ndf = (spark.readStream .format(\"kafka\") .option(\"kafka.bootstrap.servers\", ...) .option(\"kafka.security.protocol\", \"SASL_SSL\") .option(\"kafka.ssl.truststore.location\", <truststore-location>) .option(\"kafka.ssl.keystore.location\", <keystore-location>) .option(\"kafka.ssl.keystore.password\", dbutils.secrets.get(scope=<certificate-scope-name>,key=<keystore-password-key-name>)) .option(\"kafka.ssl.truststore.password\", dbutils.secrets.get(scope=<certificate-scope-name>,key=<truststore-password-key-name>)) )"
    },
    {
        "id": 428,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "Use Amazon Managed Streaming for Kafka with IAM\nPreview  \nThis feature is in Public Preview in Databricks Runtime 13.3 LTS and above.  \nYou can use Databricks to connect to Amazon Managed Streaming for Kafka (MSK) using IAM. For configuration instructions for MSK, see Amazon MSK configuration.  \nNote  \nThe following configurations are only required if you are using IAM to connect to MSK. You can also configure connections to MSK using options provided by the Apache Spark Kafka connector.  \nDatabricks recommends managing your connection to MSK using an instance profile. See Instance profiles.  \nYou must configure the following options to connect to MSK with an instance profile:  \n\"kafka.sasl.mechanism\" -> \"AWS_MSK_IAM\", \"kafka.sasl.jaas.config\" -> \"shadedmskiam.software.amazon.msk.auth.iam.IAMLoginModule required;\", \"kafka.security.protocol\" -> \"SASL_SSL\", \"kafka.sasl.client.callback.handler.class\" -> \"shadedmskiam.software.amazon.msk.auth.iam.IAMClientCallbackHandler\"  \n\"kafka.sasl.mechanism\": \"AWS_MSK_IAM\", \"kafka.sasl.jaas.config\": \"shadedmskiam.software.amazon.msk.auth.iam.IAMLoginModule required;\", \"kafka.security.protocol\": \"SASL_SSL\", \"kafka.sasl.client.callback.handler.class\": \"shadedmskiam.software.amazon.msk.auth.iam.IAMClientCallbackHandler\"  \nYou can optionally configure your connection to MSK with an IAM user or IAM role instead of an instance profile. You must provide values for your AWS access key and secret key using the environmental variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. See Reference a secret in an environment variable."
    },
    {
        "id": 429,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "In addition, if you choose to configure your connection using an IAM role, you must modify the value provided to kafka.sasl.jaas.config to include the role ARN, as in the following example: shadedmskiam.software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn=\"arn:aws:iam::123456789012:role/msk_client_role\"."
    },
    {
        "id": 430,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "Service Principal authentication with Microsoft Entra ID and Azure Event Hubs\nDatabricks supports the authentication of Spark jobs with Event Hubs services. This authentication is done via OAuth with Microsoft Entra ID.  \nDatabricks supports Microsoft Entra ID authentication with a client ID and secret in the following compute environments:  \nDatabricks Runtime 12.2 LTS and above on compute configured with single user access mode.  \nDatabricks Runtime 14.3 LTS and above on compute configured with shared access mode.  \nDelta Live Tables pipelines configured without Unity Catalog.  \nDatabricks does not support Microsoft Entra ID authentication with a certificate in any compute environment, or in Delta Live Tables pipelines configured with Unity Catalog.  \nThis authentication does not work on shared clusters or on Unity Catalog Delta Live Tables.  \nConfiguring the Structured Streaming Kafka Connector  \nTo perform authentication with Microsoft Entra ID, you\u2019ll need the following values:  \nA tenant ID. You can find this in the Microsoft Entra ID services tab.  \nA clientID (also known as Application ID).  \nA client secret. Once you have this, you should add it as a secret to your Databricks Workspace. To add this secret, see Secret management.  \nAn EventHubs topic. You can find a list of topics in the Event Hubs section under the Entities section on a specific Event Hubs Namespace page. To work with multiple topics, you can set the IAM role at the Event Hubs level.  \nAn EventHubs server. You can find this on the overview page of your specific Event Hubs namespace:  \nAdditionally, to use Entra ID, we need to tell Kafka to use the OAuth SASL mechanism (SASL is a generic protocol, and OAuth is a type of SASL \u201cmechanism\u201d):  \nkafka.security.protocol should be SASL_SSL  \nkafka.sasl.mechanism should be OAUTHBEARER  \nkafka.sasl.login.callback.handler.class should be a fully qualified name of the Java class with a value of kafkashaded to the login callback handler of our shaded Kafka class. See the following example for the exact class.  \nExample  \nNext, let\u2019s look at a running example:"
    },
    {
        "id": 431,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "# This is the only section you need to modify for auth purposes! # ------------------------------ tenant_id = \"...\" client_id = \"...\" client_secret = dbutils.secrets.get(\"your-scope\", \"your-secret-name\") event_hubs_server = \"...\" event_hubs_topic = \"...\" # ------------------------------- sasl_config = f'kafkashaded.org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required clientId=\"{client_id}\" clientSecret=\"{client_secret}\" scope=\"https://{event_hubs_server}/.default\" ssl.protocol=\"SSL\";' kafka_options = { # Port 9093 is the EventHubs Kafka port \"kafka.bootstrap.servers\": f\"{event_hubs_server}:9093\", \"kafka.sasl.jaas.config\": sasl_config, \"kafka.sasl.oauthbearer.token.endpoint.url\": f\"https://login.microsoft.com/{tenant_id}/oauth2/v2.0/token\", \"subscribe\": event_hubs_topic, # You should not need to modify these \"kafka.security.protocol\": \"SASL_SSL\", \"kafka.sasl.mechanism\": \"OAUTHBEARER\", \"kafka.sasl.login.callback.handler.class\": \"kafkashaded.org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler\" } df ="
    },
    {
        "id": 432,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "\"kafkashaded.org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler\" } df = spark.readStream.format(\"kafka\").options(**kafka_options) display(df)"
    },
    {
        "id": 433,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "// This is the only section you need to modify for auth purposes! // ------------------------------- val tenantId = \"...\" val clientId = \"...\" val clientSecret = dbutils.secrets.get(\"your-scope\", \"your-secret-name\") val eventHubsServer = \"...\" val eventHubsTopic = \"...\" // ------------------------------- val saslConfig = s\"\"\"kafkashaded.org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required clientId=\"$clientId\" clientSecret=\"$clientSecret\" scope=\"https://$eventHubsServer/.default\" ssl.protocol=\"SSL\";\"\"\" val kafkaOptions = Map( // Port 9093 is the EventHubs Kafka port \"kafka.bootstrap.servers\" -> s\"$eventHubsServer:9093\", \"kafka.sasl.jaas.config\" -> saslConfig, \"kafka.sasl.oauthbearer.token.endpoint.url\" -> s\"https://login.microsoft.com/$tenantId/oauth2/v2.0/token\", \"subscribe\" -> eventHubsTopic, // You should not need to modify these \"kafka.security.protocol\" -> \"SASL_SSL\", \"kafka.sasl.mechanism\" -> \"OAUTHBEARER\", \"kafka.sasl.login.callback.handler.class\" -> \"kafkashaded.org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler\" ) val scalaDF = spark.readStream"
    },
    {
        "id": 434,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "-> \"kafkashaded.org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler\" ) val scalaDF = spark.readStream .format(\"kafka\") .options(kafkaOptions) .load() display(scalaDF)"
    },
    {
        "id": 435,
        "url": "https://docs.databricks.com/en/connect/streaming/kafka.html",
        "content": "Handling potential errors  \nStreaming options are not supported.  \nIf you try to use this authentication mechanism in a Delta Live Tables pipeline configured with Unity Catalog you might receive the following error:  \nTo resolve this error, use a supported compute configuration. See Service Principal authentication with Microsoft Entra ID and Azure Event Hubs.  \nFailed to create a new KafkaAdminClient.  \nThis is an internal error that Kafka throws if any of the following authentication options are incorrect:  \nClient ID (also known as Application ID)  \nTenant ID  \nEventHubs server  \nTo resolve the error, verify that the values are correct for these options.  \nAdditionally, you might see this error if you modify the configuration options provided by default in the example (that you were asked not to modify), such as kafka.security.protocol.  \nThere are no records being returned  \nIf you are trying to display or process your DataFrame but aren\u2019t getting results, you will see the following in the UI.  \nThis message means that authentication was successful, but EventHubs didn\u2019t return any data. Some possible (though by no means exhaustive) reasons are:  \nYou specified the wrong EventHubs topic.  \nThe default Kafka configuration option for startingOffsets is latest, and you\u2019re not currently receiving any data through the topic yet. You can set startingOffsetstoearliest to start reading data starting from Kafka\u2019s earliest offsets."
    },
    {
        "id": 436,
        "url": "https://docs.databricks.com/en/catalogs/index.html",
        "content": "What are catalogs in Databricks?  \nA catalog is the primary unit of data organization in the Databricks Unity Catalog data governance model. This article gives an overview of catalogs in Unity Catalog and how best to use them.  \nCatalogs are the first layer in Unity Catalog\u2019s three-level namespace (catalog.schema.table-etc). They contain schemas, which in turn can contain tables, views, volumes, models, and functions. Catalogs are registered in a Unity Catalog metastore in your Databricks account.  \nHow should I organize my data into catalogs?"
    },
    {
        "id": 437,
        "url": "https://docs.databricks.com/en/catalogs/index.html",
        "content": "How should I organize my data into catalogs?\nWhen you design your data governance model, you should give careful thought to the catalogs that you create. As the highest level in your organization\u2019s data governance model, each catalog should represent a logical unit of data isolation and a logical category of data access, allowing an efficient hierarchy of grants to flow down to schemas and the data objects that they contain. Catalogs therefore often mirror organizational units or software development lifecycle scopes. You might choose, for example, to have a catalog for production data and a catalog for development data, or a catalog for non-customer data and one for sensitive customer data.\n\nData isolation using catalogs"
    },
    {
        "id": 438,
        "url": "https://docs.databricks.com/en/catalogs/index.html",
        "content": "Data isolation using catalogs\nEach catalog typically has its own managed storage location to store managed tables and volumes, providing physical data isolation at the catalog level. You can also choose to store data at the metastore level, providing a default storage location for catalogs that don\u2019t have a managed storage location of their own. You can add storage at the schema level for more granular data isolation.  \nBecause your Databricks account has one metastore per region, catalogs are inherently isolated by region.  \nFor more information, see What are database objects in Databricks? and Data is physically separated in storage.\n\nCatalog-level privileges"
    },
    {
        "id": 439,
        "url": "https://docs.databricks.com/en/catalogs/index.html",
        "content": "Catalog-level privileges\nBecause grants on any Unity Catalog object are inherited by children of that object, owning a catalog or having broad privileges on a catalog is very powerful. For example, catalog owners have all privileges on the catalog and the objects in the catalog, and they can grant access to any object in the catalog. Users with SELECT on a catalog can read any table in the catalog. Users with CREATE TABLE on a catalog can create a table in any schema in the catalog.  \nTo enforce the principle of least privilege, where users have the minimum access they need to perform their required tasks, typically you grant access only to the specific objects or level in the hierarchy that the user requires. But catalog-level privileges let the catalog owner manage what lower-level object owners can grant. Even if a user is granted access to a low-level data object like a table, for example, that user cannot access that table unless they also have the USE CATALOG privilege on the catalog that contains the table.  \nFor more information, see Manage Unity Catalog object ownership, General Unity Catalog privilege types, and Data governance and data isolation building blocks.\n\nCatalog types"
    },
    {
        "id": 440,
        "url": "https://docs.databricks.com/en/catalogs/index.html",
        "content": "Catalog types\nWhen you create a catalog, you\u2019re given two options:  \nStandard catalog: the typical catalog, used as the primary unit to organize your data objects in Unity Catalog. This is the catalog type that is discussed in this article.  \nForeign catalog: a Unity Catalog object that is used only in Lakehouse Federation scenarios. A foreign catalog mirrors a database in an external data system, enabling you to perform read-only queries on that data system in your Databricks workspace. See What is Lakehouse Federation?.  \nIn addition to those two catalog types, Databricks provisions the following catalogs automatically when you create a new workspace:  \nhive_metastore catalog: This is the repository of all data managed by the legacy Hive metastore in Databricks workspaces. When an existing non-Unity Catalog workspace is converted to Unity Catalog, all objects that are registered in the legacy Hive metastore are surfaced in Unity Catalog in the hive_metastore catalog. For information about working with the Hive metastore alongside Unity Catalog, see Work with Unity Catalog and the legacy Hive metastore. The Hive metastore is deprecated, and all Databricks workspaces should migrate to Unity Catalog.  \nWorkspace catalog: In all new workspaces, this catalog is created for you by default. Typically, it shares its name with your workspace name. If this catalog exists, all users in your workspace (and only your workspace) have access to it by default, which makes it a convenient place for users to try out the process of creating and accessing data objects in Unity Catalog. See Step 1: Confirm that your workspace is enabled for Unity Catalog."
    },
    {
        "id": 441,
        "url": "https://docs.databricks.com/en/catalogs/index.html",
        "content": "Default catalog\nDefault catalog\nA default catalog is configured for each workspace that is enabled for Unity Catalog. The default catalog lets you perform data operations without specifying a catalog. If you omit the top-level catalog name when you perform data operations, the default catalog is assumed.  \nIf your workspace was enabled for Unity Catalog automatically, the pre-provisioned workspace catalog is specified as the default catalog. A workspace admin can change the default catalog as needed.  \nFor details, see Manage the default catalog.\n\nWorkspace-catalog binding\nWorkspace-catalog binding\nIf you use workspaces to isolate user data access, you might want to use workspace-catalog bindings. Workspace-catalog bindings enable you to limit catalog access by workspace boundaries. For example, you can ensure that workspace admins and users can only access production data in prod_catalog from a production workspace environment, prod_workspace. Catalogs are shared with all workspaces attached to the current metastore unless you specify a binding. See Organize your data and Limit catalog access to specific workspaces.  \nIf your workspace was enabled for Unity Catalog automatically, the pre-provisioned workspace catalog is bound to your workspace by default."
    },
    {
        "id": 442,
        "url": "https://docs.databricks.com/en/catalogs/index.html",
        "content": "More information\nMore information\nCreate catalogs Manage catalogs Manage the default catalog Limit catalog access to specific workspaces"
    },
    {
        "id": 443,
        "url": "https://docs.databricks.com/en/connect/external-systems/mysql.html",
        "content": "Query MySQL with Databricks  \nThis example queries MySQL using its JDBC driver. For more details on reading, writing, configuring parallelism, and query pushdown, see Query databases using JDBC.  \nExperimental  \nThe configurations described in this article are Experimental. Experimental features are provided as-is and are not supported by Databricks through customer technical support. To get full query federation support, you should instead use Lakehouse Federation, which enables your Databricks users to take advantage of Unity Catalog syntax and data governance tools.  \nUsing JDBC"
    },
    {
        "id": 444,
        "url": "https://docs.databricks.com/en/connect/external-systems/mysql.html",
        "content": "Using JDBC\ndriver = \"com.mysql.cj.jdbc.Driver\" database_host = \"<database-host-url>\" database_port = \"3306\" # update if you use a non-default port database_name = \"<database-name>\" table = \"<table-name>\" user = \"<username>\" password = \"<password>\" url = f\"jdbc:mysql://{database_host}:{database_port}/{database_name}\" remote_table = (spark.read .format(\"jdbc\") .option(\"driver\", driver) .option(\"url\", url) .option(\"dbtable\", table) .option(\"user\", user) .option(\"password\", password) .load() )  \nval driver = \"com.mysql.cj.jdbc.Driver\" val database_host = \"<database-host-url>\" val database_port = \"3306\" # update if you use a non-default port val database_name = \"<database-name>\" val table = \"<table-name>\" val user = \"<username>\" val password = \"<password>\" val url = s\"jdbc:mysql://${database_host}:${database_port}/${database_name}\" val remote_table = spark.read .format(\"jdbc\") .option(\"driver\", driver) .option(\"url\", url) .option(\"dbtable\", table) .option(\"user\", user) .option(\"password\", password) .load()"
    },
    {
        "id": 445,
        "url": "https://docs.databricks.com/en/connect/external-systems/mysql.html",
        "content": "Using the MySQL connector in Databricks Runtime\nUsing Databricks Runtime 11.3 LTS and above, you can use the named connector to query MySQL. See the following examples:  \nremote_table = (spark.read .format(\"mysql\") .option(\"dbtable\", \"table_name\") .option(\"host\", \"database_hostname\") .option(\"port\", \"3306\") # Optional - will use default port 3306 if not specified. .option(\"database\", \"database_name\") .option(\"user\", \"username\") .option(\"password\", \"password\") .load() )  \nDROP TABLE IF EXISTS mysql_table; CREATE TABLE mysql_table USING mysql OPTIONS ( dbtable '<table-name>', host '<database-host-url>', port '3306', /* Optional - will use default port 3306 if not specified. */ database '<database-name>', user '<username>', password '<password>' ); SELECT * from mysql_table;  \nval remote_table = spark.read .format(\"mysql\") .option(\"dbtable\", \"table_name\") .option(\"host\", \"database_hostname\") .option(\"port\", \"3306\") # Optional - will use default port 3306 if not specified. .option(\"database\", \"database_name\") .option(\"user\", \"username\") .option(\"password\", \"password\") .load()"
    },
    {
        "id": 446,
        "url": "https://docs.databricks.com/en/dev-tools/datagrip.html",
        "content": "DataGrip integration with Databricks  \nNote  \nThis article covers DataGrip, which is neither provided nor supported by Databricks. To contact the provider, see DataGrip Support.  \nDataGrip is an integrated development environment (IDE) for database developers that provides a query console, schema navigation, explain plans, smart code completion, real-time analysis and quick fixes, refactorings, version control integration, and other features.  \nThis article describes how to use your local development machine to install, configure, and use DataGrip to work with databases in Databricks.  \nNote  \nThis article was tested with macOS, Databricks JDBC Driver version 2.6.36, and DataGrip version 2023.3.1.  \nRequirements"
    },
    {
        "id": 447,
        "url": "https://docs.databricks.com/en/dev-tools/datagrip.html",
        "content": "Requirements\nBefore you install DataGrip, your local development machine must meet the following requirements:  \nA Linux, macOS, or Windows operating system.  \nDownload the Databricks JDBC Driver onto your local development machine, extracting the DatabricksJDBC42.jar file from the downloaded DatabricksJDBC42-<version>.zip file.  \nA Databricks cluster or SQL warehouse to connect with DataGrip.\n\nStep 1: Install DataGrip\nStep 1: Install DataGrip\nDownload and install DataGrip.  \nLinux: Download the .zip file, extract its contents, and then follow the instructions in the Install-Linux-tar.txt file.  \nmacOS: Download and run the .dmg file.  \nWindows: Download and run the .exe file.  \nFor more information, see Install DataGrip on the DataGrip website.\n\nStep 2: Configure the Databricks JDBC Driver for DataGrip"
    },
    {
        "id": 448,
        "url": "https://docs.databricks.com/en/dev-tools/datagrip.html",
        "content": "Step 2: Configure the Databricks JDBC Driver for DataGrip\nSet up DataGrip with information about the Databricks JDBC Driver that you downloaded earlier.  \nStart DataGrip.  \nClick File > Data Sources.  \nIn the Data Sources and Drivers dialog box, click the Drivers tab.  \nClick the + (Driver) button to add a driver.  \nFor Name, enter Databricks.  \nOn the General tab, in the Driver Files list, click the + (Add) button.  \nClick Custom JARs.  \nBrowse to and select the DatabricksJDBC42.jar file that you extracted earlier, and then click Open.  \nFor Class, select com.databricks.client.jdbc.Driver.  \nClick OK.\n\nStep 3: Connect DataGrip to your Databricks databases"
    },
    {
        "id": 449,
        "url": "https://docs.databricks.com/en/dev-tools/datagrip.html",
        "content": "Step 3: Connect DataGrip to your Databricks databases\nIn DataGrip, click File > Data Sources.  \nOn the Data Sources tab, click the + (Add) button.  \nSelect the Databricks driver that you added in the preceding step.  \nOn the General tab, for URL, enter the value of the JDBC URL field for your Databricks resource. For the JDBC URL field syntax, see Authentication settings for the Databricks JDBC Driver.  \nClick Test Connection.  \nTip  \nYou should start your resource before testing your connection. Otherwise the test might take several minutes to complete while the resource starts.  \nIf the connection succeeds, on the Schemas tab, check the boxes for the schemas that you want to be able to access, for example All schemas.  \nClick OK.  \nRepeat the instructions in this step for each resource that you want DataGrip to access.\n\nStep 4: Use DataGrip to browse tables"
    },
    {
        "id": 450,
        "url": "https://docs.databricks.com/en/dev-tools/datagrip.html",
        "content": "Step 4: Use DataGrip to browse tables\nUse DataGrip to access tables in your Databricks workspace.  \nIn DataGrip, in the Database window, expand your resource node, expand the schema you want to browse, and then expand tables.  \nDouble-click a table. The first set of rows from the table are displayed.  \nRepeat the instructions in this step to access additional tables.  \nTo access tables in other schemas, in the Database window\u2019s toolbar, click the Data Source Properties icon. In the Data Sources and Drivers dialog box, on the Schemas tab, check the box for each additional schema you want to access, and then click OK.\n\nStep 5: Use DataGrip to run SQL statements"
    },
    {
        "id": 451,
        "url": "https://docs.databricks.com/en/dev-tools/datagrip.html",
        "content": "Step 5: Use DataGrip to run SQL statements\nUse DataGrip to load the sample trips table from the samples catalog\u2019s nyctaxi schema.  \nIn DataGrip, click File > New > SQL File.  \nEnter a name for the file, for example select_trips.  \nIn the select_trips.sql file tab, enter the following SQL statement:  \nSELECT * FROM samples.nyctaxi.trips;  \nSelect the SELECT statement.  \nOn the file tab\u2019s toolbar, click the Execute icon.\n\nNext steps\nNext steps\nLearn more about the Query console in DataGrip.  \nLearn about the Data editor in DataGrip.  \nLearn more about the various tool windows in DataGrip.  \nLearn how to search in DataGrip.  \nLearn how to export data in DataGrip.  \nLearn how to find and replace text using regular expressions in DataGrip.\n\nAdditional resources\nAdditional resources\nDataGrip documentation  \nDataGrip Support"
    },
    {
        "id": 452,
        "url": "https://docs.databricks.com/en/data-governance/unity-catalog/manage-privileges/allowlist.html",
        "content": "Allowlist libraries and init scripts on shared compute  \nIn Databricks Runtime 13.3 LTS and above, you can add libraries and init scripts to the allowlist in Unity Catalog. This allows users to leverage these artifacts on compute configured with shared access mode.  \nYou can allowlist a directory or filepath before that directory or file exists. See Upload files to a Unity Catalog volume.  \nNote  \nYou must be a metastore admin or have the MANAGE ALLOWLIST privilege to modify the allowlist. See MANAGE ALLOWLIST.  \nImportant  \nLibraries used as JDBC drivers or custom Spark data sources on Unity Catalog-enabled shared compute require ANY FILE permissions.  \nSome installed libraries store data of all users in one common temp directory. These libraries might compromise user isolation.  \nHow to add items to the allowlist"
    },
    {
        "id": 453,
        "url": "https://docs.databricks.com/en/data-governance/unity-catalog/manage-privileges/allowlist.html",
        "content": "How to add items to the allowlist\nYou can add items to the allowlist with Catalog Explorer or the REST API.  \nTo open the dialog for adding items to the allowlist in Catalog Explorer, do the following:  \nIn your Databricks workspace, click Catalog.  \nClick to open the metastore details and permissions UI.  \nSelect Allowed JARs/Init Scripts.  \nClick Add.  \nImportant  \nThis option only displays for sufficiently privileged users. If you cannot access the allowlist UI, contact your metastore admin for assistance in allowlisting libraries and init scripts.\n\nAdd an init script to the allowlist\nAdd an init script to the allowlist\nComplete the following steps in the allowlist dialog to add an init script to the allowlist:  \nFor Type, select Init Script.  \nFor Source Type, select Volume or the object storage protocol.  \nSpecify the source path to add to the allowlist. See How are permissions on paths enforced in the allowlist?.\n\nAdd a JAR to the allowlist"
    },
    {
        "id": 454,
        "url": "https://docs.databricks.com/en/data-governance/unity-catalog/manage-privileges/allowlist.html",
        "content": "Add a JAR to the allowlist\nComplete the following steps in the allowlist dialog to add a JAR to the allowlist:  \nFor Type, select JAR.  \nFor Source Type, select Volume or the object storage protocol.  \nSpecify the source path to add to the allowlist. See How are permissions on paths enforced in the allowlist?.\n\nAdd Maven coordinates to the allowlist\nAdd Maven coordinates to the allowlist\nComplete the following steps in the allowlist dialog to add Maven coordinates to the allowlist:  \nFor Type, select Maven.  \nFor Source Type, select Coordinates.  \nEnter coordinates in the following format: groudId:artifactId:version.  \nYou can include all versions of a library by allowlisting the following format: groudId:artifactId.  \nYou can include all artifacts in a group by allowlisting the following format: groupId.\n\nHow are permissions on paths enforced in the allowlist?"
    },
    {
        "id": 455,
        "url": "https://docs.databricks.com/en/data-governance/unity-catalog/manage-privileges/allowlist.html",
        "content": "How are permissions on paths enforced in the allowlist?\nYou can use the allowlist to grant access to JARs or init scripts stored in Unity Catalog volumes and object storage. If you add a path for a directory rather than a file, allowlist permissions propagate to contained files and directories.  \nPrefix matching is used for all artifacts stored in Unity Catalog volumes or object storage. To prevent prefix matching at a given directory level, include a trailing slash (/). For example, /Volumes/prod-libraries/ will not perform prefix matching for files prefixed with prod-libraries. Instead, all files and directories within /Volumes/prod-libraries/ are added to the allowlist.  \nYou can define permissions at the following levels:  \nThe base path for the volume or storage container.  \nA directory nested at any depth from the base path.  \nA single file.  \nAdding a path to the allowlist only means that the path can be used for either init scripts or JAR installation. Databricks still checks for permissions to access data in the specified location.  \nThe principal used must have READ VOLUME permissions on the specified volume. See SELECT.  \nIn single user access mode, the identity of the assigned principal (a user or service principal) is used.  \nIn shared access mode:  \nLibraries use the identity of the library installer.  \nInit scripts use the identity of the cluster owner.  \nNote  \nNo-isolation shared access mode does not support volumes, but uses the same identity assignment as shared access mode.  \nDatabricks recommends configuring all object storage privileges related to init scripts and libraries with read-only permissions. Users with write permissions on these locations can potentially modify code in library files or init scripts.  \nDatabricks recommends using instance profiles to manage access to JARs or init scripts stored in S3. Use the following documentation in the cross-reference link to complete this setup:  \nCreate a IAM role with read and list permissions on your desired buckets. See Tutorial: Configure S3 access with an instance profile.  \nLaunch a cluster with the instance profile. See Instance profiles.  \nNote  \nAllowlist permissions for JARs and init scripts are managed separately. If you use the same location to store both types of objects, you must add the location to the allowlist for each."
    },
    {
        "id": 456,
        "url": "https://docs.databricks.com/en/delta-live-tables/transform.html",
        "content": "Transform data with Delta Live Tables  \nThis article describes how you can use Delta Live Tables to declare transformations on datasets and specify how records are processed through query logic. It also contains some examples of common transformation patterns that can be useful when building out Delta Live Tables pipelines.  \nYou can define a dataset against any query that returns a DataFrame. You can use Apache Spark built-in operations, UDFs, custom logic, and MLflow models as transformations in your Delta Live Tables pipeline. Once data has been ingested into your Delta Live Tables pipeline, you can define new datasets against upstream sources to create new streaming tables, materialized views, and views.  \nTo learn how to effectively perform stateful processing with Delta Live Tables, see Optimize stateful processing in Delta Live Tables with watermarks.  \nWhen to use views, materialized views, and streaming tables"
    },
    {
        "id": 457,
        "url": "https://docs.databricks.com/en/delta-live-tables/transform.html",
        "content": "When to use views, materialized views, and streaming tables\nTo ensure your pipelines are efficient and maintainable, choose the best dataset type when you implement your pipeline queries.  \nConsider using a view when:  \nYou have a large or complex query that you want to break into easier-to-manage queries.  \nYou want to validate intermediate results using expectations.  \nYou want to reduce storage and compute costs and do not require the materialization of query results. Because tables are materialized, they require additional computation and storage resources.  \nConsider using a materialized view when:  \nMultiple downstream queries consume the table. Because views are computed on demand, the view is re-computed every time the view is queried.  \nOther pipelines, jobs, or queries consume the table. Because views are not materialized, you can only use them in the same pipeline.  \nYou want to view the results of a query during development. Because tables are materialized and can be viewed and queried outside of the pipeline, using tables during development can help validate the correctness of computations. After validating, convert queries that do not require materialization into views.  \nConsider using a streaming table when:  \nA query is defined against a data source that is continuously or incrementally growing.  \nQuery results should be computed incrementally.  \nHigh throughput and low latency is desired for the pipeline.  \nNote  \nstreaming tables are always defined against streaming sources. You can also use streaming sources with APPLY CHANGES INTO to apply updates from CDC feeds. See The APPLY CHANGES APIs: Simplify change data capture with Delta Live Tables."
    },
    {
        "id": 458,
        "url": "https://docs.databricks.com/en/delta-live-tables/transform.html",
        "content": "Combine streaming tables and materialized views in a single pipeline\nstreaming tables inherit the processing guarantees of Apache Spark Structured Streaming and are configured to process queries from append-only data sources, where new rows are always inserted into the source table rather than modified.  \nNote  \nAlthough, by default, streaming tables require append-only data sources, when a streaming source is another streaming table that requires updates or deletes, you can override this behavior with the skipChangeCommits flag.  \nA common streaming pattern includes ingesting source data to create the initial datasets in a pipeline. These initial datasets are commonly called bronze tables and often perform simple transformations.  \nBy contrast, the final tables in a pipeline, commonly referred to as gold tables, often require complicated aggregations or reading from sources that are the targets of an APPLY CHANGES INTO operation. Because these operations inherently create updates rather than appends, they are not supported as inputs to streaming tables. These transformations are better suited for materialized views.  \nBy mixing streaming tables and materialized views into a single pipeline, you can simplify your pipeline, avoid costly re-ingestion or re-processing of raw data, and have the full power of SQL to compute complex aggregations over an efficiently encoded and filtered dataset. The following example illustrates this type of mixed processing:  \nNote  \nThese examples use Auto Loader to load files from cloud storage. To load files with Auto Loader in a Unity Catalog enabled pipeline, you must use external locations. To learn more about using Unity Catalog with Delta Live Tables, see Use Unity Catalog with your Delta Live Tables pipelines."
    },
    {
        "id": 459,
        "url": "https://docs.databricks.com/en/delta-live-tables/transform.html",
        "content": "@dlt.table def streaming_bronze(): return ( # Since this is a streaming source, this table is incremental. spark.readStream.format(\"cloudFiles\") .option(\"cloudFiles.format\", \"json\") .load(\"s3://path/to/raw/data\") ) @dlt.table def streaming_silver(): # Since we read the bronze table as a stream, this silver table is also # updated incrementally. return dlt.read_stream(\"streaming_bronze\").where(...) @dlt.table def live_gold(): # This table will be recomputed completely by reading the whole silver table # when it is updated. return dlt.read(\"streaming_silver\").groupBy(\"user_id\").count()  \nCREATE OR REFRESH STREAMING TABLE streaming_bronze AS SELECT * FROM cloud_files( \"s3://path/to/raw/data\", \"json\" ) CREATE OR REFRESH STREAMING TABLE streaming_silver AS SELECT * FROM STREAM(LIVE.streaming_bronze) WHERE... CREATE OR REFRESH MATERIALIZED VIEW mv_gold AS SELECT count(*) FROM LIVE.streaming_silver GROUP BY user_id  \nLearn more about using Auto Loader to efficiently read JSON files from S3 for incremental processing."
    },
    {
        "id": 460,
        "url": "https://docs.databricks.com/en/delta-live-tables/transform.html",
        "content": "Stream-static joins\nStream-static joins\nStream-static joins are a good choice when denormalizing a continuous stream of append-only data with a primarily static dimension table.  \nWith each pipeline update, new records from the stream are joined with the most current snapshot of the static table. If records are added or updated in the static table after corresponding data from the streaming table has been processed, the resultant records are not recalculated unless a full refresh is performed.  \nIn pipelines configured for triggered execution, the static table returns results as of the time the update started. In pipelines configured for continuous execution, each time the table processes an update, the most recent version of the static table is queried.  \nThe following is an example of a stream-static join:  \n@dlt.table def customer_sales(): return dlt.read_stream(\"sales\").join(dlt.read(\"customers\"), [\"customer_id\"], \"left\")  \nCREATE OR REFRESH STREAMING TABLE customer_sales AS SELECT * FROM STREAM(LIVE.sales) INNER JOIN LEFT LIVE.customers USING (customer_id)\n\nCalculate aggregates efficiently"
    },
    {
        "id": 461,
        "url": "https://docs.databricks.com/en/delta-live-tables/transform.html",
        "content": "Calculate aggregates efficiently\nYou can use streaming tables to incrementally calculate simple distributive aggregates like count, min, max, or sum, and algebraic aggregates like average or standard deviation. Databricks recommends incremental aggregation for queries with a limited number of groups, for example, a query with a GROUP BY country clause. Only new input data is read with each update.  \nTo learn more about writing Delta Live Tables queries that perform incremental aggregations, see Perform windowed aggregations with watermarks.\n\nUse MLflow models in a Delta Live Tables pipeline"
    },
    {
        "id": 462,
        "url": "https://docs.databricks.com/en/delta-live-tables/transform.html",
        "content": "Use MLflow models in a Delta Live Tables pipeline\nNote  \nTo use MLflow models in a Unity Catalog-enabled pipeline, your pipeline must be configured to use the preview channel. To use the current channel, you must configure your pipeline to publish to the Hive metastore.  \nYou can use MLflow-trained models in Delta Live Tables pipelines. MLflow models are treated as transformations in Databricks, meaning they act upon a Spark DataFrame input and return results as a Spark DataFrame. Because Delta Live Tables defines datasets against DataFrames, you can convert Apache Spark workloads that leverage MLflow to Delta Live Tables with just a few lines of code. For more on MLflow, see ML lifecycle management using MLflow.  \nIf you already have a Python notebook calling an MLflow model, you can adapt this code to Delta Live Tables by using the @dlt.table decorator and ensuring functions are defined to return transformation results. Delta Live Tables does not install MLflow by default, so make sure you %pip install mlflow and import mlflow and dlt at the top of your notebook. For an introduction to Delta Live Tables syntax, see Implement a Delta Live Tables pipeline with Python.  \nTo use MLflow models in Delta Live Tables, complete the following steps:  \nObtain the run ID and model name of the MLflow model. The run ID and model name are used to construct the URI of the MLflow model.  \nUse the URI to define a Spark UDF to load the MLflow model.  \nCall the UDF in your table definitions to use the MLflow model.  \nThe following example shows the basic syntax for this pattern:"
    },
    {
        "id": 463,
        "url": "https://docs.databricks.com/en/delta-live-tables/transform.html",
        "content": "Use the URI to define a Spark UDF to load the MLflow model.  \nCall the UDF in your table definitions to use the MLflow model.  \nThe following example shows the basic syntax for this pattern:  \n%pip install mlflow import dlt import mlflow run_id= \"<mlflow-run-id>\" model_name = \"<the-model-name-in-run>\" model_uri = f\"runs:/{run_id}/{model_name}\" loaded_model_udf = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri) @dlt.table def model_predictions(): return dlt.read(<input-data>) .withColumn(\"prediction\", loaded_model_udf(<model-features>))  \nAs a complete example, the following code defines a Spark UDF named loaded_model_udf that loads an MLflow model trained on loan risk data. The data columns used to make the prediction are passed as an argument to the UDF. The table loan_risk_predictions calculates predictions for each row in loan_risk_input_data."
    },
    {
        "id": 464,
        "url": "https://docs.databricks.com/en/delta-live-tables/transform.html",
        "content": "%pip install mlflow import dlt import mlflow from pyspark.sql.functions import struct run_id = \"mlflow_run_id\" model_name = \"the_model_name_in_run\" model_uri = f\"runs:/{run_id}/{model_name}\" loaded_model_udf = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri) categoricals = [\"term\", \"home_ownership\", \"purpose\", \"addr_state\",\"verification_status\",\"application_type\"] numerics = [\"loan_amnt\", \"emp_length\", \"annual_inc\", \"dti\", \"delinq_2yrs\", \"revol_util\", \"total_acc\", \"credit_length_in_years\"] features = categoricals + numerics @dlt.table( comment=\"GBT ML predictions of loan risk\", table_properties={ \"quality\": \"gold\" } ) def loan_risk_predictions(): return dlt.read(\"loan_risk_input_data\") .withColumn('predictions', loaded_model_udf(struct(features)))"
    },
    {
        "id": 465,
        "url": "https://docs.databricks.com/en/delta-live-tables/transform.html",
        "content": "Retain manual deletes or updates\nDelta Live Tables allows you to manually delete or update records from a table and do a refresh operation to recompute downstream tables.  \nBy default, Delta Live Tables recomputes table results based on input data each time a pipeline is updated, so you must ensure the deleted record isn\u2019t reloaded from the source data. Setting the pipelines.reset.allowed table property to false prevents refreshes to a table but does not prevent incremental writes to the tables or prevent new data from flowing into the table.  \nThe following diagram illustrates an example using two streaming tables:  \nraw_user_table ingests raw user data from a source.  \nbmi_table incrementally computes BMI scores using weight and height from raw_user_table.  \nYou want to manually delete or update user records from the raw_user_table and recompute the bmi_table.  \nThe following code demonstrates setting the pipelines.reset.allowed table property to false to disable full refresh for raw_user_table so that intended changes are retained over time, but downstream tables are recomputed when a pipeline update is run:  \nCREATE OR REFRESH STREAMING TABLE raw_user_table TBLPROPERTIES(pipelines.reset.allowed = false) AS SELECT * FROM cloud_files(\"/databricks-datasets/iot-stream/data-user\", \"csv\"); CREATE OR REFRESH STREAMING TABLE bmi_table AS SELECT userid, (weight/2.2) / pow(height*0.0254,2) AS bmi FROM STREAM(LIVE.raw_user_table);"
    },
    {
        "id": 466,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/job-task-types.html",
        "content": "Add tasks to jobs in Databricks Asset Bundles  \nThis article provides examples of various types of tasks that you can add to Databricks jobs in Databricks Asset Bundles. See What are Databricks Asset Bundles?.  \nMost job task types have task-specific parameters among their supported settings, but you can also define job parameters that get passed to tasks. Dynamic value references are supported for job parameters, which enable passing values specific to the job run between tasks. See Pass context about job runs into job tasks.  \nNote  \nYou can override job task settings. See Override job tasks settings in Databricks Asset Bundles.  \nTip  \nTo quickly generate resource configuration for an existing job using the Databricks CLI, you can use the bundle generate job command. See bundle commands.  \nNotebook task"
    },
    {
        "id": 467,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/job-task-types.html",
        "content": "Notebook task\nYou use this task to run a notebook.  \nThe following example adds a notebook task to a job and sets a job parameter named my_job_run_id. The path for the notebook to deploy is relative to the configuration file in which this task is declared. The task gets the notebook from its deployed location in the Databricks workspace. (Ellipses indicate omitted content, for brevity.)  \n# ... resources: jobs: my-notebook-job: name: my-notebook-job # ... tasks: - task_key: my-notebook-task notebook_task: notebook_path: ./my-notebook.ipynb parameters: - name: my_job_run_id default: \"{{job.run_id}}\" # ... # ...  \nFor additional mappings that you can set for this task, see tasks > notebook_task in the create job operation\u2019s request payload as defined in POST /api/2.1/jobs/create in the REST API reference, expressed in YAML format. See also \u201cNotebook\u201d in Task type options and Pass parameters to a Databricks job task."
    },
    {
        "id": 468,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/job-task-types.html",
        "content": "Python script task\nPython script task\nYou use this task to run a Python file.  \nThe following example adds a Python script task to a job. The path for the Python file to deploy is relative to the configuration file in which this task is declared. The task gets the Python file from its deployed location in the Databricks workspace. (Ellipses indicate omitted content, for brevity.)  \n# ... resources: jobs: my-python-script-job: name: my-python-script-job # ... tasks: - task_key: my-python-script-task spark_python_task: python_file: ./my-script.py # ... # ...  \nFor additional mappings that you can set for this task, see tasks > spark_python_task in the create job operation\u2019s request payload as defined in POST /api/2.1/jobs/create in the REST API reference, expressed in YAML format. See also \u201cPython script\u201d in Task type options and Pass parameters to a Databricks job task.\n\nPython wheel task"
    },
    {
        "id": 469,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/job-task-types.html",
        "content": "Python wheel task\nYou use this task to run a Python wheel file.  \nThe following example adds a Python wheel task to a job. The path for the Python wheel file to deploy is relative to the configuration file in which this task is declared. See Databricks Asset Bundles library dependencies. (Ellipses indicate omitted content, for brevity.)  \n# ... resources: jobs: my-python-wheel-job: name: my-python-wheel-job # ... tasks: - task_key: my-python-wheel-task python_wheel_task: entry_point: run package_name: my_package libraries: - whl: ./my_package/dist/my_package-*.whl # ... # ...  \nFor additional mappings that you can set for this task, see tasks > python_wheel_task in the create job operation\u2019s request payload as defined in POST /api/2.1/jobs/create in the REST API reference, expressed in YAML format. See also Develop a Python wheel file using Databricks Asset Bundles, and \u201cPython Wheel\u201d in Task type options and Pass parameters to a Databricks job task."
    },
    {
        "id": 470,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/job-task-types.html",
        "content": "JAR task\nYou use this task to run a JAR. You can reference local JAR libraries or those in a workspace, a Unity Catalog volume, or an external cloud storage location. See Databricks Asset Bundles library dependencies.  \nThe following example adds a JAR task to a job. The path for the JAR is to the specified volume location. (Ellipses indicate omitted content, for brevity.)  \n# ... resources: jobs: my-jar-job: name: my-jar-job # ... tasks: - task_key: my-jar-task spark_jar_task: main_class_name: org.example.com.Main libraries: - jar: /Volumes/main/default/my-volume/my-project-0.1.0-SNAPSHOT.jar # ... # ...  \nFor additional mappings that you can set for this task, see tasks > spark_jar_task in the create job operation\u2019s request payload as defined in POST /api/2.1/jobs/create in the REST API reference, expressed in YAML format. See also \u201cJAR\u201d in Task type options and Pass parameters to a Databricks job task."
    },
    {
        "id": 471,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/job-task-types.html",
        "content": "SQL file task\nYou use this task to run a SQL file located in a workspace or a remote Git repository.  \nThe following example adds a SQL file task to a job. This SQL file task uses the specified SQL warehouse to run the specified SQL file. (Ellipses indicate omitted content, for brevity.)  \n# ... resources: jobs: my-sql-file-job: name: my-sql-file-job # ... tasks: - task_key: my-sql-file-task sql_task: file: path: /Users/someone@example.com/hello-world.sql source: WORKSPACE warehouse_id: 1a111111a1111aa1 # ... # ...  \nTo get a SQL warehouse\u2019s ID, open the SQL warehouse\u2019s settings page, then copy the ID found in parentheses after the name of the warehouse in the Name field on the Overview tab.  \nFor additional mappings that you can set for this task, see tasks > sql_task > file in the create job operation\u2019s request payload as defined in POST /api/2.1/jobs/create in the REST API reference, expressed in YAML format. See also \u201cSQL: File\u201d in Task type options."
    },
    {
        "id": 472,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/job-task-types.html",
        "content": "Delta Live Tables pipeline task\nYou use this task to run a Delta Live Tables pipeline. See What is Delta Live Tables?.  \nThe following example adds a Delta Live Tables pipeline task to a job. This Delta Live Tables pipeline task runs the specified pipeline. (Ellipses indicate omitted content, for brevity.)  \n# ... resources: jobs: my-pipeline-job: name: my-pipeline-job # ... tasks: - task_key: my-pipeline-task pipeline_task: pipeline_id: 11111111-1111-1111-1111-111111111111 # ... # ...  \nYou can get a pipelines\u2019s ID by opening the pipeline in the workspace and copying the Pipeline ID value on the Pipeline details tab of the pipeline\u2019s settings page.  \nFor additional mappings that you can set for this task, see tasks > pipeline_task in the create job operation\u2019s request payload as defined in POST /api/2.1/jobs/create in the REST API reference, expressed in YAML format. See also \u201cDelta Live Tables Pipeline\u201d in Task type options."
    },
    {
        "id": 473,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/job-task-types.html",
        "content": "dbt task\nYou use this task to run one or more dbt commands. See Connect to dbt Cloud.  \nThe following example adds a dbt task to a job. This dbt task uses the specified SQL warehouse to run the specified dbt commands.  \n# ... resources: jobs: my-dbt-job: name: my-dbt-job # ... tasks: - task_key: my-dbt-task dbt_task: commands: - \"dbt deps\" - \"dbt seed\" - \"dbt run\" project_directory: /Users/someone@example.com/Testing warehouse_id: 1a111111a1111aa1 libraries: - pypi: package: \"dbt-databricks>=1.0.0,<2.0.0\" # ... # ...  \nTo get a SQL warehouse\u2019s ID, open the SQL warehouse\u2019s settings page, then copy the ID found in parentheses after the name of the warehouse in the Name field on the Overview tab.  \nFor additional mappings that you can set for this task, see tasks > dbt_task in the create job operation\u2019s request payload as defined in POST /api/2.1/jobs/create in the REST API reference, expressed in YAML format. See also \u201cdbt\u201d in Task type options.  \nDatabricks Asset Bundles also includes a dbt-sql project template that defines a job with a dbt task, as well as dbt profiles for deployed dbt jobs. For information about Databricks Asset Bundles templates, see Use a default bundle template."
    },
    {
        "id": 474,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/job-task-types.html",
        "content": "Run job task\nYou use this task to run another job.  \nThe following example contains a run job task in the second job that runs the first job.  \n# ... resources: jobs: my-first-job: name: my-first-job tasks: - task_key: my-first-job-task new_cluster: spark_version: \"13.3.x-scala2.12\" node_type_id: \"i3.xlarge\" num_workers: 2 notebook_task: notebook_path: ./src/test.py my_second_job: name: my-second-job tasks: - task_key: my-second-job-task run_job_task: job_id: ${resources.jobs.my-first-job.id} # ...  \nThis example uses a substitution to retrieve the ID of the job to run. To get a job\u2019s ID from the UI, open the job in the workspace and copy the ID from the Job ID value in the Job details tab of the jobs\u2019s settings page.  \nFor additional mappings that you can set for this task, see tasks > run_job_task in the create job operation\u2019s request payload as defined in POST /api/2.1/jobs/create in the REST API reference, expressed in YAML format."
    },
    {
        "id": 475,
        "url": "https://docs.databricks.com/en/dev-tools/visual-studio-code.html",
        "content": "How can I use Visual Studio Code with Databricks?  \nVisual Studio Code by Microsoft is a lightweight but powerful source code editor which runs on your desktop and is available for Windows, macOS, and Linux. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for other languages and runtimes (such as C++, C#, Java, Python, PHP, Go, and .NET). Visual Studio Code combines the simplicity of a source code editor with powerful developer tooling, like IntelliSense code completion and debugging. You can use Visual Studio Code on your local development machine to write, run, and debug code in Databricks, interact with Databricks SQL warehouses in remote Databricks workspaces, and more, as follows:  \nName  \nUse this when you want to\u2026  \nDatabricks extension for Visual Studio Code  \nUse Visual Studio Code to write and run local Python, R, Scala, and SQL code on a remote Databricks workspace.  \nDatabricks Connect in Visual Studio Code with Python  \nUse Visual Studio Code to write, run, and debug local Python code on a remote Databricks workspace.  \nDatabricks Connect in Visual Studio Code with Scala  \nUse Visual Studio Code to write, run, and debug local Scala code on a remote Databricks workspace.  \nDatabricks Asset Bundles  \nUse Visual Studio Code to make authoring, deploying, and running bundles easier. Databricks Asset Bundles (or bundles for short) enable you to programmatically define, deploy, and run Databricks jobs, Delta Live Tables pipelines, and MLOps Stacks by using CI/CD best practices and workflows.  \nDatabricks CLI  \nUse the built-in Terminal in Visual Studio Code to work with Databricks from the command line.  \nDatabricks SDKs  \nUse the built-in programming language support in Visual Studio Code to write, run, and debug Python, Java, and Go code that works with Databricks.  \nDatabricks Driver for SQLTools  \nUse a graphical user interface in Visual Studio Code to query Databricks SQL warehouses in remote Databricks workspaces.  \nDatabricks SQL connectors, drivers, and APIs"
    },
    {
        "id": 476,
        "url": "https://docs.databricks.com/en/dev-tools/visual-studio-code.html",
        "content": "Databricks Driver for SQLTools  \nUse a graphical user interface in Visual Studio Code to query Databricks SQL warehouses in remote Databricks workspaces.  \nDatabricks SQL connectors, drivers, and APIs  \nUse the built-in programming language support in Visual Studio Code to write, run, and debug Python, Go, JavaScript, TypeScript, and Node.js code that works with Databricks SQL warehouses in remote Databricks workspaces.  \nProvision infrastructure  \nUse third-party plugins such as the Hashicorp Terraform Extension for Visual Studio Code to make it easier to provision Databricks infrastructure with Terraform and follow infrastructure-as-code (IaC) best practices. Use the built-in programming language support in Visual Studio Code to write and deploy Python, TypeScript, Java, C#, and Go definitions of Databricks infrastructure through third-party offerings such as the Cloud Development Kit for Terraform (CDKTF) and Pulumi."
    },
    {
        "id": 477,
        "url": "https://docs.databricks.com/en/dbfs/index.html",
        "content": "What is DBFS?  \nThe term DBFS is used to describe two parts of the platform:  \nDBFS root  \nDBFS mounts  \nStoring and accessing data using DBFS root or DBFS mounts is a deprecated pattern and not recommended by Databricks. For recommendations for working with files, see Work with files on Databricks.  \nWhat is the Databricks File System?\nWhat is the Databricks File System?\nThe term DBFS comes from Databricks File System, which describes the distributed file system used by Databricks to interact with cloud-based storage.  \nThe underlying technology associated with DBFS is still part of the Databricks platform. For example, dbfs:/ is an optional scheme when interacting with Unity Catalog volumes.  \nPast and current warnings and caveats about DBFS only apply to the DBFS root or DBFS mounts.\n\nHow does DBFS work with Unity Catalog?"
    },
    {
        "id": 478,
        "url": "https://docs.databricks.com/en/dbfs/index.html",
        "content": "How does DBFS work with Unity Catalog?\nDatabricks recommends using Unity Catalog to manage access to all data.  \nUnity Catalog adds the concepts of external locations, storage credentials, and volumes to help organizations provide the least privileged access to data in cloud object storage.  \nSome security configurations provide direct access to Unity Catalog-managed resources and DBFS, primarily for organizations that have completed migrations or partially migrated to Unity Catalog. See Best practices for DBFS and Unity Catalog.\n\nWhat is the DBFS root?\nWhat is the DBFS root?\nThe DBFS root is a storage location provisioned during workspace creation in the cloud account containing the Databricks workspace. For details on Databricks Filesystem root configuration and deployment, see Create an S3 bucket for workspace deployment.  \nDatabricks does not recommend storing production data, libraries, or scripts in DBFS root. See Recommendations for working with DBFS root.\n\nMount object storage"
    },
    {
        "id": 479,
        "url": "https://docs.databricks.com/en/dbfs/index.html",
        "content": "Mount object storage\nNote  \nDBFS mounts are deprecated. Databricks recommends using Unity Catalog volumes. See What are Unity Catalog volumes?.  \nMounting object storage to DBFS allows you to access objects in object storage as if they were on the local file system. Mounts store Hadoop configurations necessary for accessing storage. For more information, see Mounting cloud object storage on Databricks."
    },
    {
        "id": 480,
        "url": "https://docs.databricks.com/en/delta-sharing/audit-logs.html",
        "content": "Audit and monitor data sharing  \nThis article describes how data providers and recipients can use audit logs to monitor Delta Sharing events. Provider audit logs record actions taken by the provider and actions taken by recipients on the provider\u2019s shared data. Recipient audit logs record events related to the accessing of shares and the management of provider objects.  \nTo view the list Delta Sharing audit log events, see Delta Sharing events.  \nRequirements\nRequirements\nTo access audit logs, an account admin must enable the audit log system table for your Databricks account. See Enable system tables. For information on the audit log system table, see Audit log system table reference.  \nIf you are not an account admin or metastore admin, you must be given access to system.access.audit to read audit logs.\n\nView Delta Sharing events in the audit log\nView Delta Sharing events in the audit log\nIf your account has system tables enabled, audit logs are stored in system.access.audit. If, alternatively, your account has an audit log delivery setup, you need to know the bucket and path where the logs are delivered.\n\nLogged events\nLogged events\nTo view the list of Delta Sharing audit log events, see Delta Sharing events.\n\nView details of a recipient\u2019s query result"
    },
    {
        "id": 481,
        "url": "https://docs.databricks.com/en/delta-sharing/audit-logs.html",
        "content": "View details of a recipient\u2019s query result\nIn the provider logs, the events returned as deltaSharingQueriedTableChanges and deltaSharingQueriedTable are logged after a data recipient\u2019s query gets a response. Providers can view the response.result field of these logs to see more details about what was shared with the recipient. The field can include the following values. This list is not exhaustive."
    },
    {
        "id": 482,
        "url": "https://docs.databricks.com/en/delta-sharing/audit-logs.html",
        "content": "\"checkpointBytes\": \"0\", \"earlyTermination\": \"false\", \"maxRemoveFiles\": \"0\", \"path\": \"file: example/s3/path/golden/snapshot-data0/_delta_log\", \"deltaSharingPartitionFilteringAccessed\": \"false\", \"deltaSharingRecipientId\": \"<redacted>\", \"deltaSharingRecipientIdHash\": \"<recipient-hash-id>\", \"jsonLogFileNum\": \"1\", \"scannedJsonLogActionNum\": \"5\", \"numRecords\": \"3\", \"deltaSharingRecipientMetastoreId\": \"<redacted>\", \"userAgent\": \"Delta-Sharing-Unity-Catalog-Databricks-Auth/1.0 Linux/4.15.0-2068-azure-fips OpenJDK_64-Bit_Server_VM/11.0.7+10-jvmci-20.1-b02 java/11.0.7 scala/2.12.15 java_vendor/GraalVM_Community\", \"jsonLogFileBytes\": \"2846\", \"checkpointFileNum\": \"0\", \"metastoreId\": \"<redacted>\", \"limitHint\": \"Some(1)\", \"tableName\": \"cookie_ingredients\", \"tableId\": \"1234567c-6d8b-45fd-9565-32e9fc23f8f3\", \"activeAddFiles\": \"2\", // number of AddFiles returned in the query \"numAddFiles\": \"2\", // number of AddFiles returned in the query \"numAddCDCFiles\": \"2\", // number of AddFiles returned in the CDF query \"numRemoveFiles\": \"2\", // number of RemoveFiles returned in the query \"numSeenAddFiles\": \"3\","
    },
    {
        "id": 483,
        "url": "https://docs.databricks.com/en/delta-sharing/audit-logs.html",
        "content": "number of AddFiles returned in the CDF query \"numRemoveFiles\": \"2\", // number of RemoveFiles returned in the query \"numSeenAddFiles\": \"3\", \"scannedAddFileSize\": \"1300\", // file size in bytes for the AddFile returned in the query \"scannedAddCDCFileSize\": \"1300\", // file size in bytes for the AddCDCFile returned in the CDF query \"scannedRemoveFileSize\": \"1300\", // file size in bytes for the RemoveFile returned in the query \"scannedCheckpointActionNum\": \"0\", \"tableVersion\": \"0\""
    },
    {
        "id": 484,
        "url": "https://docs.databricks.com/en/delta-sharing/audit-logs.html",
        "content": "Logged errors\nIf an attempted Delta Sharing action fails, the action is logged with the error message in the response.error_message field of the log. Items between < and > characters represent placeholder text.  \nError messages in provider logs  \nDelta Sharing logs the following errors for data providers:  \nDelta Sharing is not enabled on the selected metastore.  \nDatabricksServiceException: FEATURE_DISABLED: Delta Sharing is not enabled  \nAn operation was attempted on a catalog that does not exist.  \nDatabricksServiceException: CATALOG_DOES_NOT_EXIST: Catalog \u2018<catalog>\u2019 does not exist.  \nA user who is not an account admin or metastore admin attempted to perform a privileged operation.  \nDatabricksServiceException: PERMISSION_DENIED: Only administrators can <operation-name> <operation-target>  \nAn operation was attempted on a metastore from a workspace to which the metastore is not assigned.  \nDatabricksServiceException: INVALID_STATE: Workspace <workspace-name> is no longer assigned to this metastore  \nA request was missing the recipient name or share name.  \nDatabricksServiceException: INVALID_PARAMETER_VALUE: CreateRecipient/CreateShare Missing required field: <recipient-name>/<share-name>  \nA request included an invalid recipient name or share name.  \nDatabricksServiceException: INVALID_PARAMETER_VALUE: CreateRecipient/CreateShare <recipient-name>/<share-name> is not a valid name  \nA user attempted to share a table that is not in a Unity Catalog metastore.  \nDatabricksServiceException: INVALID_PARAMETER_VALUE: Only managed or external table on Unity Catalog can be added to a share  \nA user attempted to rotate a recipient that was already in a rotated state and whose previous token had not yet expired.  \nDatabricksServiceException: INVALID_PARAMETER_VALUE: There are already two active tokens for recipient <recipient-name>  \nA user attempted to create a new recipient or share with the same name as an existing one.  \nDatabricksServiceException: RECIPIENT_ALREADY_EXISTS/SHARE_ALREADY_EXISTS: Recipient/Share <name> already exists`  \nA user attempted to perform an operation on a recipient or share that does not exist."
    },
    {
        "id": 485,
        "url": "https://docs.databricks.com/en/delta-sharing/audit-logs.html",
        "content": "DatabricksServiceException: RECIPIENT_ALREADY_EXISTS/SHARE_ALREADY_EXISTS: Recipient/Share <name> already exists`  \nA user attempted to perform an operation on a recipient or share that does not exist.  \nDatabricksServiceException: RECIPIENT_DOES_NOT_EXIST/SHARE_DOES_NOT_EXIST: Recipient/Share '<name>' does not exist  \nA user attempted to add a table to a share, but the table had already been added.  \nDatabricksServiceException: RESOURCE_ALREADY_EXISTS: Shared Table '<name>' already exists  \nA user attempted to perform an operation that referenced a table that does not exist.  \nDatabricksServiceException: TABLE_DOES_NOT_EXIST: Table '<name>' does not exist  \nA user attempted to perform an operation that referenced a schema that did not exist.  \nDatabricksServiceException: SCHEMA_DOES_NOT_EXIST: Schema '<name>' does not exist  \nA user attempted to access a share that does not exist.  \nDatabricksServiceException: SHARE_DOES_NOT_EXIST: Share <share-name> does not exist.  \nError messages in recipient logs  \nDelta Sharing logs the following errors for data recipients:  \nThe user attempted to access a share they do not have permission to access.  \nDatabricksServiceException: PERMISSION_DENIED: User does not have SELECT on Share <share-name>  \nThe user attempted to access a share that does not exist.  \nDatabricksServiceException: SHARE_DOES_NOT_EXIST: Share <share-name> does not exist.  \nThe user attempted to access a table that does not exist in the share.  \nDatabricksServiceException: TABLE_DOES_NOT_EXIST: <table-name> does not exist."
    },
    {
        "id": 486,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/mlops-stacks.html",
        "content": "Databricks Asset Bundles for MLOps Stacks  \nYou can use Databricks Asset Bundles, the Databricks CLI, and the Databricks MLOps Stack repository on GitHub to create MLOps Stacks. An MLOps Stack is an MLOps project on Databricks that follows production best practices out of the box. See What are Databricks Asset Bundles?.  \nTo create, deploy, and run an MLOps Stacks project, complete the following steps:  \nRequirements\nRequirements\nMake sure that the target remote workspace has workspace files enabled. See What are workspace files?.  \nOn your development machine, make sure that Databricks CLI version 0.212.2 or above is installed. To check your installed Databricks CLI version, run the command databricks -v. To update your Databricks CLI version, see Install or update the Databricks CLI. (Bundles do not work with Databricks CLI versions 0.18 and below.)\n\nStep 1: Set up authentication"
    },
    {
        "id": 487,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/mlops-stacks.html",
        "content": "Step 1: Set up authentication\nConfigure the Databricks CLI for authentication.  \nThis article assumes that you want to use OAuth user-to-machine (U2M) authentication and a corresponding Databricks configuration profile named DEFAULT for authentication.  \nNote  \nU2M authentication is appropriate for trying out these steps in real time. For fully automated workflows, Databricks recommends that you use OAuth machine-to-machine (M2M) authentication instead. See the M2M authentication setup instructions in Authentication.  \nUse the Databricks CLI to initiate OAuth token management locally by running the following command for each target workspace.  \nIn the following command, replace <workspace-url> with your Databricks workspace instance URL, for example https://dbc-a1b2345c-d6e7.cloud.databricks.com.  \ndatabricks auth login --host <workspace-url>  \nThe Databricks CLI prompts you to save the information that you entered as a Databricks configuration profile. Press Enter to accept the suggested profile name, or enter the name of a new or existing profile. Any existing profile with the same name is overwritten with the information that you entered. You can use profiles to quickly switch your authentication context across multiple workspaces.  \nTo get a list of any existing profiles, in a separate terminal or command prompt, use the Databricks CLI to run the command databricks auth profiles. To view a specific profile\u2019s existing settings, run the command databricks auth env --profile <profile-name>.  \nIn your web browser, complete the on-screen instructions to log in to your Databricks workspace.  \nTo view a profile\u2019s current OAuth token value and the token\u2019s upcoming expiration timestamp, run one of the following commands:  \ndatabricks auth token --host <workspace-url>  \ndatabricks auth token -p <profile-name>  \ndatabricks auth token --host <workspace-url> -p <profile-name>  \nIf you have multiple profiles with the same --host value, you might need to specify the --host and -p options together to help the Databricks CLI find the correct matching OAuth token information."
    },
    {
        "id": 488,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/mlops-stacks.html",
        "content": "Step 2: Create the bundle project\nUse Databricks Asset Bundle templates to create your MLOps Stacks project\u2019s starter files. To do this, begin by running the following command:  \ndatabricks bundle init mlops-stacks  \nAnswer the on-screen prompts. For guidance on answering these prompts, see Start a new project in the Databricks MLOps Stacks repository on GitHub.  \nThe first prompt offers the option of setting up the ML code components, the CI/CD components, or both. This option simplifies the initial setup as you can choose to create only those components that are immediately relevant. (To set up the other components, run the initialization command again.) Select one of the following:  \nCICD_and_Project (default) - Set up both ML code and CI/CD components.  \nProject_Only - Set up ML code components only. This option is for data scientists to get started.  \nCICD_Only - Set up CI/CD components only. This option is for ML engineers to set up infrastructure.  \nAfter you answer all of the on-screen prompts, the template creates your MLOps Stacks project\u2019s starter files and adds them to your current working directory.  \nCustomize your MLOps Stacks project\u2019s starter files as desired. To do this, follow the guidance in the following files within your new project:  \nRole  \nGoal  \nDocs  \nFirst-time users of this repo  \nUnderstand the ML pipeline and code structure in this repo  \nREADME.md  \nData Scientist  \nGet started writing ML code for a brand new project  \n<project-name>/README.md  \nData Scientist  \nUpdate production ML code (for example, model training logic) for an existing project  \ndocs/ml-pull-request.md  \nData Scientist  \nModify production model ML resources (for example, model training or inference jobs)  \n<project-name>/resources/README.md  \nMLOps / DevOps  \nSet up CI/CD for the current ML project  \ndocs/mlops-setup.md  \nFor customizing experiments, the mappings within an experiment declaration correspond to the create experiment operation\u2019s request payload as defined in POST /api/2.0/mlflow/experiments/create in the REST API reference, expressed in YAML format."
    },
    {
        "id": 489,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/mlops-stacks.html",
        "content": "For customizing jobs, the mappings within a job declaration correspond to the create job operation\u2019s request payload as defined in POST /api/2.1/jobs/create in the REST API reference, expressed in YAML format.  \nTip  \nYou can define, combine, and override the settings for new job clusters in bundles by using the techniques described in Override cluster settings in Databricks Asset Bundles.  \nFor customizing models, the mappings within a model declaration correspond to the create model operation\u2019s request payload as defined in POST /api/2.0/mlflow/registered-models/create in the REST API reference, expressed in YAML format.  \nFor customizing pipelines, the mappings within a pipeline declaration correspond to the create pipeline operation\u2019s request payload as defined in POST /api/2.0/pipelines in the REST API reference, expressed in YAML format."
    },
    {
        "id": 490,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/mlops-stacks.html",
        "content": "Step 3: Validate the bundle project\nStep 3: Validate the bundle project\nCheck whether the bundle configuration is valid. To do this, run the Databricks CLI from the project\u2019s root, where the databricks.yml is located, as follows:  \ndatabricks bundle validate  \nIf a summary of the bundle configuration is returned, then the validation succeeded. If any errors are returned, fix the errors, and then repeat this step.\n\nStep 4: Deploy the bundle\nStep 4: Deploy the bundle\nDeploy the project\u2019s resources and artifacts to the desired remote workspace. To do this, run the Databricks CLI from the project\u2019s root, where the databricks.yml is located, as follows:  \ndatabricks bundle deploy -t <target-name>  \nReplace <target-name> with the name of the desired target within the databricks.yml file, for example dev, test, staging, or prod.\n\nStep 5: Run the deployed bundle"
    },
    {
        "id": 491,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/mlops-stacks.html",
        "content": "Step 5: Run the deployed bundle\nThe project\u2019s deployed Databricks jobs automatically run on their predefined schedules. To run a deployed job immediately, run the Databricks CLI from the project\u2019s root, where the databricks.yml is located, as follows:  \ndatabricks bundle run -t <target-name> <job-name>  \nReplace <target-name> with the name of the desired target within the databricks.yml file where the job was deployed, for example dev, test, staging, or prod.  \nReplace <job-name> with the name of the job in one of the .yml files within <project-name>/databricks-resources, for example batch_inference_job, write_feature_table_job, or model_training_job.  \nA link to the Databricks job appears, which you can copy into your web browser to open the job within the Databricks UI.\n\nStep 6: Delete the deployed bundle (optional)"
    },
    {
        "id": 492,
        "url": "https://docs.databricks.com/en/dev-tools/bundles/mlops-stacks.html",
        "content": "Step 6: Delete the deployed bundle (optional)\nTo delete a deployed project\u2019s resources and artifacts if you no longer need them, run the Databricks CLI from the project\u2019s root, where the databricks.yml is located, as follows:  \ndatabricks bundle destroy -t <target-name>  \nReplace <target-name> with the name of the desired target within the databricks.yml file, for example dev, test, staging, or prod.  \nAnswer the on-screen prompts to confirm the deletion of the previously deployed resources and artifacts."
    },
    {
        "id": 493,
        "url": "https://docs.databricks.com/en/dashboards/tutorials/create-dashboard.html",
        "content": "Create a dashboard  \nLearn how to use the AI/BI dashboard UI to create and share insights. For information about dashboard features, see Dashboards.  \nThe steps in this tutorial demonstrate how to build and share the following dashboard:  \nRequirements\nRequirements\nYou are logged into a Databricks workspace.  \nYou have the SQL entitlement in that workspace.  \nYou have at least CAN USE access to one or more SQL warehouses.\n\nStep 1. Create a dashboard\nStep 1. Create a dashboard\nClick New in the sidebar and select Dashboard.  \nBy default, your new dashboard is automatically named with its creation timestamp and stored in your /Workspace/Users/<username> directory.  \nNote  \nYou can also create a new dashboard from the Dashboards listing page or the Add button in the Workspace menu.\n\nStep 2. Define datasets"
    },
    {
        "id": 494,
        "url": "https://docs.databricks.com/en/dashboards/tutorials/create-dashboard.html",
        "content": "Step 2. Define datasets\nThe Canvas tab is for creating and editing widgets like visualizations, text boxes, and filters. The Data tab is for defining the underlying datasets used in your dashboard.  \nNote  \nAll users can write SQL queries to define a dataset. Users in Unity Catalog-enabled workspaces can instead select a Unity Catalog table or view as a dataset.  \nClick the Data tab.  \nClick Create from SQL and paste in the following query. Then click Run to return a collection of records.  \nSELECT T.tpep_pickup_datetime, T.tpep_dropoff_datetime, T.fare_amount, T.pickup_zip, T.dropoff_zip, T.trip_distance, T.weekday, CASE WHEN T.weekday = 1 THEN 'Sunday' WHEN T.weekday = 2 THEN 'Monday' WHEN T.weekday = 3 THEN 'Tuesday' WHEN T.weekday = 4 THEN 'Wednesday' WHEN T.weekday = 5 THEN 'Thursday' WHEN T.weekday = 6 THEN 'Friday' WHEN T.weekday = 7 THEN 'Saturday' ELSE 'N/A' END AS day_of_week FROM ( SELECT dayofweek(tpep_pickup_datetime) as weekday, * FROM `samples`.`nyctaxi`.`trips` WHERE trip_distance > 0 AND trip_distance < 10 AND fare_amount > 0 AND fare_amount < 50 ) T ORDER BY T.weekday  \nInspect your results. The returned records appear under the editor when the query is finished running.  \nChange the name of your query. Your newly defined dataset is autosaved with the name, \u201cUntitled dataset.\u201d Double click on the title to rename it, \u201cTaxicab data\u201d.  \nNote  \nThis query accesses data from the samples catalog on Databricks. The table includes publicly available taxicab data from New York City in 2016. Query results are limited to valid rides that are under 10 miles and cost less than fifty dollars."
    },
    {
        "id": 495,
        "url": "https://docs.databricks.com/en/dashboards/tutorials/create-dashboard.html",
        "content": "Step 3. Create and place a visualization\nStep 3. Create and place a visualization\nTo create your first visualization, complete the following steps:  \nClick the Canvas tab.  \nClick Create a visualization to create a visualization widget and use your mouse to place it in the canvas.\n\nStep 4. Configure your visualization"
    },
    {
        "id": 496,
        "url": "https://docs.databricks.com/en/dashboards/tutorials/create-dashboard.html",
        "content": "Step 4. Configure your visualization\nWhen a visualization widget is selected, you can use the configuration panel on the right side of the screen to display your data. As shown in the following image, only one Dataset has been defined, and it is selected automatically.  \nSetup the X-axis  \nIf necessary, select Bar from the Visualization dropdown menu.  \nClick the to choose the data presented along the X-axis. You can use the search bar to search for a field by name. Select tpep_dropoff_datetime.  \nClick the field name you selected to view additional configuration options.  \nAs the Scale Type, select Temporal.  \nFor the Transform selection, choose HOURLY.  \nSetup the Y-axis  \nClick the next to the Y-axis to select the fare_amount for the data presented along the y-axis.  \nClick the field name you selected to view additional configuration options.  \nAs the Scale Type, select Quantitative.  \nFor the Transform selection, choose AVG.  \nOptional: Create visualizations with Databricks Assistant  \nYou can create visualizations using natural language with the Databricks Assistant.  \nTo generate the same chart as above, choose one of the following options:  \nTo create a new visualization widget:  \nClick Create a visualization. The widget appears with the prompt: Describe a chart\u2026.  \nType \u201cBar chart of average fare amount over hourly dropoff time\u201d  \nTo edit an existing widget:  \nClick the Assistant icon. An input prompt appears. Enter a new prompt for your chart. You can ask for a new chart entirely or ask for modifications. For example, you can type, \u201cSwitch to a line chart\u201d to modify the chart type."
    },
    {
        "id": 497,
        "url": "https://docs.databricks.com/en/dashboards/tutorials/create-dashboard.html",
        "content": "Step 5. Clone and modify a visualization\nStep 5. Clone and modify a visualization\nYou can clone an existing chart to create a new visualization.  \nRight-click on your existing chart and then click Clone.  \nWith your new chart selected, use the configuration panel to change the X-axis field to tpep_pickup_datetime. If necessary, choose HOURLY under the Transform type.  \nUse the Color/Group by selector to choose a new color for your new bar chart.\n\nStep 6. Create a scatterplot\nStep 6. Create a scatterplot\nCreate a new scatterplot with colors differentiated by value. To create a scatterplot, complete the following steps:  \nClick the Create a visualization icon to create a new visualization widget.  \nConfigure your chart by making the following selections:  \nDataset: Taxicab data  \nVisualization: Scatter  \nX axis: trip_distance  \nY axis: fare_amount  \nColor/Group by: day_of_week  \nNote  \nAfter colors have been auto-assigned by category, you can change the color associated with a particular value by clicking on the color in the configuration panel.\n\nStep 7. Create dashboard filters"
    },
    {
        "id": 498,
        "url": "https://docs.databricks.com/en/dashboards/tutorials/create-dashboard.html",
        "content": "Step 7. Create dashboard filters\nYou can use filters to make your dashboards interactive. In this step, you create filters on three fields.  \nCreate a date range filter  \nClick Add a filter (field/parameter) to add a filter widget. Place it on the canvas.  \nFrom the Filter dropdown menu in the configuration panel, select Date range picker.  \nSelect the Title checkbox to create a title field on your filter. Click the placeholder title and type Date range to retitle your filter.  \nFrom the Fields menu, select Taxicab_data.tpep_pickup_datetime.  \nCreate a single-select dropdown filter  \nClick Add a Add a filter (field/parameter) to add a filter widget. Place it on the canvas.  \nFrom the Filter dropdown menu in the configuration panel, select Dropdown (single-select).  \nSelect the Title checkbox to create a title field on your filter. Click on the placeholder title and type Dropoff zip code to retitle your filter.  \nFrom the Fields menu, select Taxicab_data.dropoff_zip.  \nClone a filter  \nRight-click on your Dropoff zip code filter. Then, click Clone.  \nClick the to remove the current field. Then, select Taxicab_data.pickup_zip to filter on that field."
    },
    {
        "id": 499,
        "url": "https://docs.databricks.com/en/dashboards/tutorials/create-dashboard.html",
        "content": "Step 8. Resize and arrange charts and filters\nStep 8. Resize and arrange charts and filters\nUse your mouse to arrange and resize your charts and filters.  \nThe following image shows one possible arrangement for this dashboard.\n\nStep 9. Publish and share\nStep 9. Publish and share\nWhile you develop a dashboard, your progress is saved as a draft. To create a clean copy for easy consumption, publish your dashboard.  \nClick the next to the Share button. Then, click Publish.  \nReview People with access and then click Publish. Individual users and groups with at least CAN VIEW permission can see your published dashboard.  \nFollow the link provided in the Publish notification to view your published dashboard.  \nTo change the list of users or groups you want to share the dashboard with, return to your draft dashboard and click the Share button. Add users or groups who you want to share with. Set permission levels as appropriate. See AI/BI dashboard ACLs to learn more about permissions and rights."
    },
    {
        "id": 500,
        "url": "https://docs.databricks.com/en/compute/access-mode-limitations.html",
        "content": "Compute access mode limitations for Unity Catalog  \nDatabricks recommends using Unity Catalog and shared access mode for most workloads. This article outlines limitations and requirements for each access mode with Unity Catalog. For details on access modes, see Access modes.  \nDatabricks recommends using compute policies to simplify configuration options for most users. See Create and manage compute policies.  \nNote  \nNo-isolation shared is a legacy access mode that does not support Unity Catalog.  \nImportant  \nInit scripts and libraries have different support across access modes and Databricks Runtime versions. See Where can init scripts be installed? and Cluster-scoped libraries.  \nSingle user access mode limitations on Unity Catalog"
    },
    {
        "id": 501,
        "url": "https://docs.databricks.com/en/compute/access-mode-limitations.html",
        "content": "Single user access mode limitations on Unity Catalog\nSingle user access mode on Unity Catalog has the following limitations. These are in addition to the general limitations for all Unity Catalog access mode. See General limitations for Unity Catalog.  \nFine-grained access control limitations for Unity Catalog single user access mode  \nOn Databricks Runtime 15.3 and below, fine-grained access control on single-user compute is not supported. Specifically:  \nYou cannot access a table that has a row filter or column mask.  \nYou cannot access dynamic views.  \nTo read from any view, you must have SELECT on all tables and views that are referenced by the view.  \nTo query dynamic views, views on which you don\u2019t have SELECT on the underlying tables and views, and tables with row filters or column masks, use one of the following:  \nA SQL warehouse.  \nCompute with shared access mode.  \nCompute with single-user access mode on Databricks Runtime 15.4 LTS or above (Public Preview).  \nDatabricks Runtime 15.4 LTS and above support fine-grained access control on single-user compute. To take advantage of the data filtering provided in Databricks Runtime 15.4 LTS and above, you must also verify that your workspace is enabled for serverless compute, because the data filtering functionality that supports fine-grained access controls runs on serverless compute. You might therefore be charged for serverless compute resources when you use single-user compute to run data filtering operations. See Fine-grained access control on single-user compute.  \nStreaming table and materialized view limitations for Unity Catalog single user access mode  \nOn Databricks Runtime 15.3 and below, you cannot use single-user compute to query tables that were created using a Delta Live Tables pipeline, including streaming tables and materialized views, if those tables are owned by other users. The user who creates a table is the owner.  \nTo query tables created by Delta Live Tables and owned by other users, use one of the following:  \nA SQL warehouse.  \nCompute with shared access mode on Databricks Runtime 13.3 LTS or above.  \nCompute with single-user access mode on Databricks Runtime 15.4 LTS or above (Public Preview)."
    },
    {
        "id": 502,
        "url": "https://docs.databricks.com/en/compute/access-mode-limitations.html",
        "content": "Compute with shared access mode on Databricks Runtime 13.3 LTS or above.  \nCompute with single-user access mode on Databricks Runtime 15.4 LTS or above (Public Preview).  \nDatabricks Runtime 15.4 LTS and above support queries on Delta Live Tables-generated tables on single-user compute. To take advantage of the data filtering provided in Databricks Runtime 15.4 LTS and above, you must also verify that your workspace is enabled for serverless compute, because the data filtering functionality that supports Delta Live Tables-generated tables runs on serverless compute. You might therefore be charged for serverless compute resources when you use single-user compute to run data filtering operations. See Fine-grained access control on single-user compute.  \nStreaming limitations for Unity Catalog single user access mode  \nAsynchronous checkpointing is not supported in Databricks Runtime 11.3 LTS and below.  \nStreamingQueryListener requires Databricks Runtime 15.1 or above to use credentials or interact with objects managed by Unity Catalog on single user compute."
    },
    {
        "id": 503,
        "url": "https://docs.databricks.com/en/compute/access-mode-limitations.html",
        "content": "Shared access mode limitations on Unity Catalog\nShared access mode in Unity Catalog has the following limitations. These are in addition to the general limitations for all Unity Catalog access modes. See General limitations for Unity Catalog.  \nDatabricks Runtime ML and Spark Machine Learning Library (MLlib) are not supported.  \nSpark-submit jobs are not supported.  \nIn Databricks Runtime 13.3 and above, individual rows must not exceed 128MB.  \nPySpark UDFs cannot access Git folders, workspace files, or volumes to import modules in Databricks Runtime 14.2 and below.  \nDBFS root and mounts do not support FUSE.  \nWhen you use shared access mode with credential passthrough, Unity Catalog features are disabled.  \nCustom containers are not supported.  \nLanguage support for Unity Catalog shared access mode  \nR is not supported.  \nScala is supported in Databricks Runtime 13.3 and above.  \nIn Databricks Runtime 15.4 LTS and above, all Java or Scala libraries (JAR files) bundled with Databricks Runtime are available on compute in Unity Catalog access modes.  \nFor Databricks Runtime 15.3 or below on compute that uses shared access mode, set the Spark config spark.databricks.scala.kernel.fullClasspath.enabled to true.  \nSpark API limitations and requirements for Unity Catalog shared access mode  \nRDD APIs are not supported.  \nDBUtils and other clients that directly read the data from cloud storage are only supported when you use an external location to access the storage location. See Create an external location to connect cloud storage to Databricks.  \nSpark Context (sc),spark.sparkContext, and sqlContext are not supported for Scala in any Databricks Runtime and are not supported for Python in Databricks Runtime 14.0 and above.  \nDatabricks recommends using the spark variable to interact with the SparkSession instance."
    },
    {
        "id": 504,
        "url": "https://docs.databricks.com/en/compute/access-mode-limitations.html",
        "content": "Databricks recommends using the spark variable to interact with the SparkSession instance.  \nThe following sc functions are also not supported: emptyRDD, range, init_batched_serializer, parallelize, pickleFile, textFile, wholeTextFiles, binaryFiles, binaryRecords, sequenceFile, newAPIHadoopFile, newAPIHadoopRDD, hadoopFile, hadoopRDD, union, runJob, setSystemProperty, uiWebUrl, stop, setJobGroup, setLocalProperty, getConf.  \nThe following Scala Dataset API operations require Databricks Runtime 15.4 LTS or above: map, mapPartitions, foreachPartition, flatMap, reduce and filter.  \nUDF limitations and requirements for Unity Catalog shared access mode  \nUser-defined functions (UDFs) have the following limitations with shared access mode:  \nHive UDFs are not supported.  \napplyInPandas and mapInPandas require Databricks Runtime 14.3 or above.  \nScala scalar UDFs require Databricks Runtime 14.2 or above. Other Scala UDFs and UDAFs are not supported.  \nIn Databricks Runtime 14.2 and below, using a custom version of grpc, pyarrow, or protobuf in a PySpark UDF through notebook-scoped or cluster-scoped libraries is not supported because the installed version is always preferred. To find the version of installed libraries, see the System Environment section of the specific Databricks Runtime version release notes.  \nPython scalar UDFs and Pandas UDFs require Databricks Runtime 13.3 LTS or above. Other Python UDFs, including UDAFs, UDTFs, and Pandas on Spark are not supported.  \nSee User-defined functions (UDFs) in Unity Catalog.  \nStreaming limitations and requirements for Unity Catalog shared access mode  \nNote  \nSome of the listed Kafka options have limited support when used for supported configurations on Databricks. See Stream processing with Apache Kafka and Databricks.  \nFor Scala, foreach, foreachBatch, StreamingListeners, and FlatMapGroupWithState are not supported."
    },
    {
        "id": 505,
        "url": "https://docs.databricks.com/en/compute/access-mode-limitations.html",
        "content": "For Scala, foreach, foreachBatch, StreamingListeners, and FlatMapGroupWithState are not supported.  \nFor Python, foreachBatch has the following behavior changes in Databricks Runtime 14.0 and above:  \nprint() commands write output to the driver logs.  \nYou cannot access the dbutils.widgets submodule inside the function.  \nAny files, modules, or objects referenced in the function must be serializable and available on Spark.  \nFor Scala, from_avro requires Databricks Runtime 14.2 or above.  \napplyInPandasWithState requires Databricks Runtime 14.3 LTS or above.  \nWorking with socket sources is not supported.  \nThe sourceArchiveDir must be in the same external location as the source when you use option(\"cleanSource\", \"archive\") with a data source managed by Unity Catalog.  \nFor Kafka sources and sinks, the following options are not supported:  \nkafka.sasl.client.callback.handler.class  \nkafka.sasl.login.callback.handler.class  \nkafka.sasl.login.class  \nkafka.partition.assignment.strategy  \nThe following Kafka options are not supported in Databricks Runtime 13.3 LTS and above but unsupported in Databricks Runtime 12.2 LTS. You can only specify external locations managed by Unity Catalog for these options:  \nkafka.ssl.truststore.location  \nkafka.ssl.keystore.location  \nStreamingQueryListener requires Databricks Runtime 14.3 LTS or above to use credentials or interact with objects managed by Unity Catalog on shared compute.  \nInstance profiles to configure access to external sources such as Kafka or Kinesis for streaming workloads are not supported.  \nNetwork and file system access limitations and requirements for Unity Catalog shared access mode  \nYou must run commands on compute nodes as a low-privilege user forbidden from accessing sensitive parts of the filesystem.  \nIn Databricks Runtime 11.3 LTS and below, you can only create network connections to ports 80 and 443."
    },
    {
        "id": 506,
        "url": "https://docs.databricks.com/en/compute/access-mode-limitations.html",
        "content": "In Databricks Runtime 11.3 LTS and below, you can only create network connections to ports 80 and 443.  \nYou cannot connect to the instance metadata service (IMDS), other EC2 instances, or any other services running in the Databricks VPC. This prevents access to any service that uses the IMDS, such as boto3 and the AWS CLI."
    },
    {
        "id": 507,
        "url": "https://docs.databricks.com/en/compute/access-mode-limitations.html",
        "content": "General limitations for Unity Catalog\nGeneral limitations for Unity Catalog\nThe following limitations apply to all Unity Catalog-enabled access modes.  \nUDFs  \nGraviton instance support for UDFs on Unity Catalog-enabled clusters is available in Databricks Runtime 15.2 and above. Additional limitations exist for shared access mode. See UDF limitations and requirements for Unity Catalog shared access mode.  \nStreaming limitations for Unity Catalog  \nApache Spark continuous processing mode is not supported. See Continuous Processing in the Spark Structured Streaming Programming Guide.  \nSee also Streaming limitations for Unity Catalog single user access mode and Streaming limitations and requirements for Unity Catalog shared access mode.  \nFor more on streaming with Unity Catalog, see Using Unity Catalog with Structured Streaming."
    },
    {
        "id": 508,
        "url": "https://docs.databricks.com/en/dev-tools/terraform/e2-workspace.html",
        "content": "Create Databricks workspaces using Terraform  \nDatabricks offers guidance about how to create Databricks workspaces with the Databricks Terraform provider along with all required infrastructure on AWS. The guidance applies only to Databricks accounts on the E2 version of the platform. All new Databricks accounts and most existing accounts are now E2.  \nFor details, see Provisioning AWS Databricks E2 in the Databricks Terraform provider documentation."
    },
    {
        "id": 509,
        "url": "https://docs.databricks.com/en/delta/delta-change-data-feed.html",
        "content": "Use Delta Lake change data feed on Databricks  \nChange data feed allows Databricks to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records change events for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated.  \nImportant  \nChange data feed works in tandem with table history to provide change information. Because cloning a Delta table creates a separate history, the change data feed on cloned tables doesn\u2019t match that of the original table.  \nIncrementally process change data"
    },
    {
        "id": 510,
        "url": "https://docs.databricks.com/en/delta/delta-change-data-feed.html",
        "content": "Incrementally process change data\nDatabricks recommends using change data feed in combination with Structured Streaming to incrementally process changes from Delta tables. You must use Structured Streaming for Databricks to automatically track versions for your table\u2019s change data feed.  \nNote  \nDelta Live Tables provides functionality for easy propagation of change data and storing results as SCD (slowly changing dimension) type 1 or type 2 tables. See The APPLY CHANGES APIs: Simplify change data capture with Delta Live Tables.  \nTo read the change data feed from a table, you must enable change data feed on that table. See Enable change data feed.  \nSet the option readChangeFeed to true when configuring a stream against a table to read the change data feed, as shown in the following syntax example:  \n(spark.readStream .option(\"readChangeFeed\", \"true\") .table(\"myDeltaTable\") )  \nspark.readStream .option(\"readChangeFeed\", \"true\") .table(\"myDeltaTable\")  \nBy default, the stream returns the latest snapshot of the table when the stream first starts as an INSERT and future changes as change data.  \nChange data commits as part of the Delta Lake transaction, and becomes available at the same time the new data commits to the table.  \nYou can optionally specify a starting version. See Should I specify a starting version?.  \nChange data feed also supports batch execution, which requires specifying a starting version. See Read changes in batch queries.  \nOptions like rate limits (maxFilesPerTrigger, maxBytesPerTrigger) and excludeRegex are also supported when reading change data.  \nRate limiting can be atomic for versions other than the starting snapshot version. That is, the entire commit version will be rate limited or the entire commit will be returned."
    },
    {
        "id": 511,
        "url": "https://docs.databricks.com/en/delta/delta-change-data-feed.html",
        "content": "Should I specify a starting version?\nYou can optionally specify a starting version if you want to ignore changes that happened before a particular version. You can specify a version using a timestamp or the version ID number recorded in the Delta transaction log.  \nNote  \nA starting version is required for batch reads, and many batch patterns can benefit from setting an optional ending version.  \nWhen you\u2019re configuring Structured Streaming workloads involving change data feed, it\u2019s important to understand how specifying a starting version impacts processing.  \nMany streaming workloads, especially new data processing pipelines, benefit from the default behavior. With the default behavior, the first batch is processed when the stream first records all existing records in the table as INSERT operations in the change data feed.  \nIf your target table already contains all the records with appropriate changes up to a certain point, specify a starting version to avoid processing the source table state as INSERT events.  \nThe following example syntax recovering from a streaming failure in which the checkpoint was corrupted. In this example, assume the following conditions:  \nChange data feed was enabled on the source table at table creation.  \nThe target downstream table has processed all changes up to and including version 75.  \nVersion history for the source table is available for versions 70 and above.  \n(spark.readStream .option(\"readChangeFeed\", \"true\") .option(\"startingVersion\", 76) .table(\"source_table\") )  \nspark.readStream .option(\"readChangeFeed\", \"true\") .option(\"startingVersion\", 76) .table(\"source_table\")  \nIn this example, you must also specify a new checkpoint location.  \nImportant  \nIf you specify a starting version, the stream fails to start from a new checkpoint if the starting version is no longer present in the table history. Delta Lake cleans up historic versions automatically, meaning that all specified starting versions are eventually deleted.  \nSee Can I use change data feed to replay the entire history of a table?."
    },
    {
        "id": 512,
        "url": "https://docs.databricks.com/en/delta/delta-change-data-feed.html",
        "content": "Read changes in batch queries\nYou can use batch query syntax to read all changes starting from a particular version or to read changes within a specified range of versions.  \nYou specify a version as an integer and a timestamps as a string in the format yyyy-MM-dd[ HH:mm:ss[.SSS]].  \nThe start and end versions are inclusive in the queries. To read the changes from a particular start version to the latest version of the table, specify only the starting version.  \nIf you provide a version lower or timestamp older than one that has recorded change events\u2014that is, when the change data feed was enabled\u2014an error is thrown indicating that the change data feed was not enabled.  \nThe following syntax examples demonstrate using starting and ending version options with batch reads:  \n-- version as ints or longs e.g. changes from version 0 to 10 SELECT * FROM table_changes('tableName', 0, 10) -- timestamp as string formatted timestamps SELECT * FROM table_changes('tableName', '2021-04-21 05:45:46', '2021-05-21 12:00:00') -- providing only the startingVersion/timestamp SELECT * FROM table_changes('tableName', 0) -- database/schema names inside the string for table name, with backticks for escaping dots and special characters SELECT * FROM table_changes('dbName.`dotted.tableName`', '2021-04-21 06:45:46' , '2021-05-21 12:00:00')"
    },
    {
        "id": 513,
        "url": "https://docs.databricks.com/en/delta/delta-change-data-feed.html",
        "content": "# version as ints or longs spark.read \\ .option(\"readChangeFeed\", \"true\") \\ .option(\"startingVersion\", 0) \\ .option(\"endingVersion\", 10) \\ .table(\"myDeltaTable\") # timestamps as formatted timestamp spark.read \\ .option(\"readChangeFeed\", \"true\") \\ .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\ .option(\"endingTimestamp\", '2021-05-21 12:00:00') \\ .table(\"myDeltaTable\") # providing only the startingVersion/timestamp spark.read \\ .option(\"readChangeFeed\", \"true\") \\ .option(\"startingVersion\", 0) \\ .table(\"myDeltaTable\")  \n// version as ints or longs spark.read .option(\"readChangeFeed\", \"true\") .option(\"startingVersion\", 0) .option(\"endingVersion\", 10) .table(\"myDeltaTable\") // timestamps as formatted timestamp spark.read .option(\"readChangeFeed\", \"true\") .option(\"startingTimestamp\", \"2021-04-21 05:45:46\") .option(\"endingTimestamp\", \"2021-05-21 12:00:00\") .table(\"myDeltaTable\") // providing only the startingVersion/timestamp spark.read .option(\"readChangeFeed\", \"true\") .option(\"startingVersion\", 0) .table(\"myDeltaTable\")  \nNote  \nBy default, if a user passes in a version or timestamp exceeding the last commit on a table, the error timestampGreaterThanLatestCommit is thrown. In Databricks Runtime 11.3 LTS and above, change data feed can handle the out of range version case if the user sets the following configuration to true:  \nset spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled = true;"
    },
    {
        "id": 514,
        "url": "https://docs.databricks.com/en/delta/delta-change-data-feed.html",
        "content": "set spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled = true;  \nIf you provide a start version greater than the last commit on a table or a start timestamp newer than the last commit on a table, then when the preceding configuration is enabled, an empty read result is returned.  \nIf you provide an end version greater than the last commit on a table or an end timestamp newer than the last commit on a table, then when the preceding configuration is enabled in batch read mode, all changes between the start version and the last commit are be returned."
    },
    {
        "id": 515,
        "url": "https://docs.databricks.com/en/delta/delta-change-data-feed.html",
        "content": "What is the schema for the change data feed?\nWhat is the schema for the change data feed?\nWhen you read from the change data feed for a table, the schema for the latest table version is used.  \nNote  \nMost schema change and evolution operations are fully supported. Table with column mapping enabled do not support all use cases and demonstrate different behavior. See Change data feed limitations for tables with column mapping enabled.  \nIn addition to the data columns from the schema of the Delta table, change data feed contains metadata columns that identify the type of change event:  \nColumn name  \nType  \nValues  \n_change_type  \nString  \ninsert, update_preimage , update_postimage, delete (1)  \n_commit_version  \nLong  \nThe Delta log or table version containing the change.  \n_commit_timestamp  \nTimestamp  \nThe timestamp associated when the commit was created.  \n(1) preimage is the value before the update, postimage is the value after the update.  \nNote  \nYou cannot enable change data feed on a table if the schema contains columns with the same names as these added columns. Rename columns in the table to resolve this conflict before trying to enable change data feed.\n\nEnable change data feed"
    },
    {
        "id": 516,
        "url": "https://docs.databricks.com/en/delta/delta-change-data-feed.html",
        "content": "Enable change data feed\nYou can only read the change data feed for enabled tables. You must explicitly enable the change data feed option using one of the following methods:  \nNew table: Set the table property delta.enableChangeDataFeed = true in the CREATE TABLE command.  \nCREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES (delta.enableChangeDataFeed = true)  \nExisting table: Set the table property delta.enableChangeDataFeed = true in the ALTER TABLE command.  \nALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)  \nAll new tables:  \nset spark.databricks.delta.properties.defaults.enableChangeDataFeed = true;  \nImportant  \nOnly changes made after you enable the change data feed are recorded. Past changes to a table are not captured.\n\nChange data storage"
    },
    {
        "id": 517,
        "url": "https://docs.databricks.com/en/delta/delta-change-data-feed.html",
        "content": "Change data storage\nEnabling change data feed causes a small increase in storage costs for a table. Change data records are generated as the query runs, and are generally much smaller than the total size of rewritten files.  \nDatabricks records change data for UPDATE, DELETE, and MERGE operations in the _change_data folder under the table directory. Some operations, such as insert-only operations and full-partition deletions, do not generate data in the _change_data directory because Databricks can efficiently compute the change data feed directly from the transaction log.  \nAll reads against data files in the _change_data folder should go through supported Delta Lake APIs.  \nThe files in the _change_data folder follow the retention policy of the table. Change data feed data is deleted when the VACUUM command runs.\n\nCan I use change data feed to replay the entire history of a table?"
    },
    {
        "id": 518,
        "url": "https://docs.databricks.com/en/delta/delta-change-data-feed.html",
        "content": "Can I use change data feed to replay the entire history of a table?\nChange data feed is not intended to serve as a permanent record of all changes to a table. Change data feed only records changes that occur after it\u2019s enabled.  \nChange data feed and Delta Lake allow you to always reconstruct a full snapshot of a source table, meaning you can start a new streaming read against a table with change data feed enabled and capture the current version of that table and all changes that occur after.  \nYou must treat records in the change data feed as transient and only accessible for a specified retention window. The Delta transaction log removes table versions and their corresponding change data feed versions at regular intervals. When a version is removed from the transaction log, you can no longer read the change data feed for that version.  \nIf your use case requires maintaining a permanent history of all changes to a table, you should use incremental logic to write records from the change data feed to a new table. The following code example demonstrates using trigger.AvailableNow, which leverages the incremental processing of Structured Streaming but processes available data as a batch workload. You can schedule this workload asynchronously with your main processing pipelines to create a backup of change data feed for auditing purposes or full replayability.  \n(spark.readStream .option(\"readChangeFeed\", \"true\") .table(\"source_table\") .writeStream .option(\"checkpointLocation\", <checkpoint-path>) .trigger(availableNow=True) .toTable(\"target_table\") )  \nspark.readStream .option(\"readChangeFeed\", \"true\") .table(\"source_table\") .writeStream .option(\"checkpointLocation\", <checkpoint-path>) .trigger(Trigger.AvailableNow) .toTable(\"target_table\")"
    },
    {
        "id": 519,
        "url": "https://docs.databricks.com/en/delta/delta-change-data-feed.html",
        "content": "Change data feed limitations for tables with column mapping enabled\nWith column mapping enabled on a Delta table, you can drop or rename columns in the table without rewriting data files for existing data. With column mapping enabled, change data feed has limitations after performing non-additive schema changes such as renaming or dropping a column, changing data type, or nullability changes.  \nImportant  \nYou cannot read change data feed for a transaction or range in which a non-additive schema change occurs using batch semantics.  \nIn Databricks Runtime 12.2 LTS and below, tables with column mapping enabled that have experienced non-additive schema changes do not support streaming reads on change data feed. See Streaming with column mapping and schema changes.  \nIn Databricks Runtime 11.3 LTS and below, you cannot read change data feed for tables with column mapping enabled that have experienced column renaming or dropping.  \nIn Databricks Runtime 12.2 LTS and above, you can perform batch reads on change data feed for tables with column mapping enabled that have experienced non-additive schema changes. Instead of using the schema of the latest version of the table, read operations use the schema of the end version of the table specified in the query. Queries still fail if the version range specified spans a non-additive schema change."
    },
    {
        "id": 520,
        "url": "https://docs.databricks.com/en/generative-ai/retrieval-augmented-generation.html",
        "content": "What is Mosaic AI Agent Framework?  \nPreview  \nThis feature is in Public Preview.  \nAgent Framework comprises a set of tools on Databricks designed to help developers build, deploy, and evaluate production-quality agents like Retrieval Augmented Generation (RAG) applications.  \nThis article covers what Agent Framework is and the benefits of developing RAG applications on Databricks.  \nAgent Framework lets developers iterate quickly on all aspects of RAG development using an end-to-end LLMOps workflow.  \nRequirements\nRequirements\nPartner-Powered AI assistive features must be enabled for your workspace.  \nAll components of an agentic application must be in a single workspace. For example, in the case of a RAG application, the serving model and the vector search instance need to be in the same workspace.\n\nProduction-level RAG development"
    },
    {
        "id": 521,
        "url": "https://docs.databricks.com/en/generative-ai/retrieval-augmented-generation.html",
        "content": "Production-level RAG development\nQuickly iterate on agent development using the following features:  \nCreate and log agents using any library and MLflow. Parameterize your agents to experiment and iterate on agent development quickly.  \nDeploy agents to production with native support for token streaming and request/response logging, plus a built-in review app to get user feedback for your agent.  \nAgent tracing lets you log, analyze, and compare traces across your agent code to debug and understand how your agent responds to requests.\n\nWhat is RAG?"
    },
    {
        "id": 522,
        "url": "https://docs.databricks.com/en/generative-ai/retrieval-augmented-generation.html",
        "content": "What is RAG?\nRAG is a generative AI design technique that enhances large language models (LLM) with external knowledge. This technique improves LLMs in the following ways:  \nProprietary knowledge: RAG can include proprietary information not initially used to train the LLM, such as memos, emails, and documents to answer domain-specific questions.  \nUp-to-date information: A RAG application can supply the LLM with information from updated data sources.  \nCiting sources: RAG enables LLMs to cite specific sources, allowing users to verify the factual accuracy of responses.  \nData security and access control lists (ACL): The retrieval step can be designed to selectively retrieve personal or proprietary information based on user credentials.  \nCompound AI systems  \nA RAG application is an example of a compound AI system: it expands on the language capabilities of the LLM by combining it with other tools and procedures.  \nIn the simplest form, a RAG application does the following:  \nRetrieval: The user\u2019s request is used to query an outside data store, such as a vector store, a text keyword search, or a SQL database. The goal is to get supporting data for the LLM\u2019s response.  \nAugmentation: The retrieved data is combined with the user\u2019s request, often using a template with additional formatting and instructions, to create a prompt.  \nGeneration: The prompt is passed to the LLM, which then generates a response to the query.  \nRAG and gen AI use cases  \nThe following table lists a few RAG use cases.  \nUse case  \nDescription  \nQ&A chatbots  \nUse LLMs with chatbots to derive accurate answers from company documents and knowledge bases. Chatbots can automate customer support and follow up on website leads to quickly answer questions and resolve issues.  \nSearch augmentation  \nUse LLMs with search engines to augment search results with LLM-generated answers, making it easier for users to find the information they need.  \nKnowledge engine  \nUse company data, like HR and compliance documents, as context for LLMs to let employees easily get answers to questions about benefits, policies, security, and compliance."
    },
    {
        "id": 523,
        "url": "https://docs.databricks.com/en/generative-ai/retrieval-augmented-generation.html",
        "content": "Unstructured vs. structured RAG data\nUnstructured vs. structured RAG data\nRAG architecture can work with either unstructured or structured supporting data. The data you use with RAG depends on your use case.  \nUnstructured data: Data without a specific structure or organization. Documents that include text and images or multimedia content such as audio or videos.  \nPDFs  \nGoogle/Office documents  \nWikis  \nImages  \nVideos  \nStructured data: Tabular data arranged in rows and columns with a specific schema, such as tables in a database.  \nCustomer records in a BI or Data Warehouse system  \nTransaction data from a SQL database  \nData from application APIs (e.g., SAP, Salesforce, etc.)  \nThe following sections describe a RAG application for unstructured data.\n\nRAG data pipeline"
    },
    {
        "id": 524,
        "url": "https://docs.databricks.com/en/generative-ai/retrieval-augmented-generation.html",
        "content": "RAG data pipeline\nThe RAG data pipeline pre-processes and indexes documents for fast and accurate retrieval.  \nThe diagram below shows a sample data pipeline for an unstructured dataset using a semantic search algorithm. Databricks Jobs orchestrate each step.  \nData ingestion - Ingest data from your proprietary source. Store this data in a Delta table or Unity Catalog Volume.  \nDocument processing: You can perform these tasks using Databricks Jobs, Databricks Notebooks, and Delta Live Tables.  \nParse raw documents: Transform the raw data into a usable format. For example, extracting the text, tables, and images from a collection of PDFs or using optical character recognition techniques to extract text from images.  \nExtract metadata: Extract document metadata such as document titles, page numbers, and URLs to help the retrieval step query more accurately.  \nChunk documents: Split the data into chunks that fit into the LLM context window. Retrieving these focused chunks, rather than entire documents, gives the LLM more targeted content to generate responses.  \nEmbedding chunks - An embedding model consumes the chunks to create numerical representations of the information called vector embeddings. Vectors represent the semantic meaning of the text, not just surface-level keywords. In this scenario, you compute the embeddings and use Model Serving to serve the embedding model.  \nEmbedding storage - Store the vector embeddings and the chunk\u2019s text in a Delta table synced with Vector Search.  \nVector database - As part of Vector Search, embeddings and metadata are indexed and stored in a vector database for easy querying by the RAG agent. When a user makes a query, their request is embedded into a vector. The database then uses the vector index to find and return the most similar chunks.  \nEach step involves engineering decisions that impact the RAG application\u2019s quality. For example, choosing the right chunk size in step (3) ensures the LLM receives specific yet contextualized information, while selecting an appropriate embedding model in step (4) determines the accuracy of the chunks returned during retrieval.  \nDatabricks Vector Search  \nComputing similarity is often computationally expensive, but vector indexes like Databricks Vector Search optimize this by efficiently organizing embeddings. Vector searches quickly rank the most relevant results without comparing each embedding to the user\u2019s query individually."
    },
    {
        "id": 525,
        "url": "https://docs.databricks.com/en/generative-ai/retrieval-augmented-generation.html",
        "content": "Computing similarity is often computationally expensive, but vector indexes like Databricks Vector Search optimize this by efficiently organizing embeddings. Vector searches quickly rank the most relevant results without comparing each embedding to the user\u2019s query individually.  \nVector Search automatically syncs new embeddings added to your Delta table and updates the Vector Search index."
    },
    {
        "id": 526,
        "url": "https://docs.databricks.com/en/generative-ai/retrieval-augmented-generation.html",
        "content": "What is a RAG agent?\nA Retrieval Augmented Generation (RAG) agent is a key part of a RAG application that enhances the capabilities of large language models (LLMs) by integrating external data retrieval. The RAG agent processes user queries, retrieves relevant data from a vector database, and passes this data to an LLM to generate a response.  \nTools like LangChain or Pyfunc link these steps by connecting their inputs and outputs.  \nThe diagram below shows a RAG agent for a chatbot and the Databricks features used to build each agent.  \nQuery preprocessing - A user submits a query, which is then preprocessed to make it suitable for querying the vector database. This may involve placing the request in a template or extracting keywords.  \nQuery vectorization - Use Model Serving to embed the request using the same embedding model used to embed the chunks in the data pipeline. These embeddings enable comparison of the semantic similarity between the request and the preprocessed chunks.  \nRetrieval phase - The retriever, an application responsible for fetching relevant information, takes the vectorized query and performs a vector similarity search using Vector Search. The most relevant data chunks are ranked and retrieved based on their similarity to the query.  \nPrompt augmentation - The retriever combines the retrieved data chunks with the original query to provide additional context to the LLM. The prompt is carefully structured to ensure that the LLM understands the context of the query. Often, the LLM has a template for formatting the response. This process of adjusting the prompt is known as prompt engineering.  \nLLM Generation phase - The LLM generates a response using the augmented query enriched by the retrieval results. The LLM can be a custom model or a foundation model.  \nPost-processing - The LLM\u2019s response may be processed to apply additional business logic, add citations, or otherwise refine the generated text based on predefined rules or constraints  \nVarious guardrails may be applied throughout this process to ensure compliance with enterprise policies. This might involve filtering for appropriate requests, checking user permissions before accessing data sources, and using content moderation techniques on the generated responses."
    },
    {
        "id": 527,
        "url": "https://docs.databricks.com/en/generative-ai/retrieval-augmented-generation.html",
        "content": "Evaluation & monitoring\nEvaluation & monitoring\nEvaluation and monitoring help determine if your RAG application meets your quality, cost, and latency requirements. Evaluation occurs during development, while monitoring happens once the application is deployed to production.  \nRAG over unstructured data has many components that impact quality. For example, data formatting changes can influence the retrieved chunks and the LLM\u2019s ability to generate relevant responses. So, it\u2019s important to evaluate individual components in addition to the overall application.\n\nFor more information, see Introduction to Mosaic AI Agent Evaluation.  \nRegion availability\nRegion availability\nFor regional availability of Agent Framework, see Features with limited regional availability"
    },
    {
        "id": 528,
        "url": "https://docs.databricks.com/en/error-messages/incomplete-type-definition-error-class.html",
        "content": "INCOMPLETE_TYPE_DEFINITION error class  \nSQLSTATE: 42K01  \nIncomplete complex type:  \nARRAY\nARRAY\nThe definition of \u201cARRAY\u201d type is incomplete. You must provide an element type. For example: \u201cARRAY`<elementType>`\u201d.\n\nMAP\nMAP\nThe definition of \u201cMAP\u201d type is incomplete. You must provide a key type and a value type. For example: \u201cMAP<`TIMESTAMP`, `INT`>\u201d.\n\nSTRUCT\nSTRUCT\nThe definition of \u201cSTRUCT\u201d type is incomplete. You must provide at least one field type. For example: \u201cSTRUCT<name `STRING`, phone `DECIMAL`(10, 0)>\u201d."
    },
    {
        "id": 529,
        "url": "https://docs.databricks.com/en/error-messages/wrong-num-args-error-class.html",
        "content": "WRONG_NUM_ARGS error class  \nSQLSTATE: 42605  \nThe <functionName> requires <expectedNum> parameters but the actual number is <actualNum>.  \nWITHOUT_SUGGESTION\nWITHOUT_SUGGESTION\nPlease, refer to \u2018<docroot>/sql-ref-functions.html\u2019 for a fix.\n\nWITH_SUGGESTION\nWITH_SUGGESTION\nIf you have to call this function with <legacyNum> parameters, set the legacy configuration <legacyConfKey> to <legacyConfValue>."
    },
    {
        "id": 530,
        "url": "https://docs.databricks.com/en/error-messages/missing-aggregation-error-class.html",
        "content": "MISSING_AGGREGATION error class  \nSQLSTATE: 42803  \nThe non-aggregating expression <expression> is based on columns which are not participating in the GROUP BY clause.  \nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use <expressionAnyValue> if you do not care which of the values within a group is returned.  \nParameters\nParameters\nexpression: Non aggregating, non grouping expression in the SELECT list.  \nexpressionAnyValue: expression wrapped in an any_value() aggregate function.\n\nExplanation"
    },
    {
        "id": 531,
        "url": "https://docs.databricks.com/en/error-messages/missing-aggregation-error-class.html",
        "content": "Explanation\nWithin the context of a query with a GROUP BY clause, the local column-references in the SELECT list must be:  \nConsumed as an argument to an aggregate function, or  \nPart of an expression which matches an expression on the GROUP BY clause.  \nA local column reference is a column that has been resolved to a table-reference in the query\u2019s FROM clause.  \nIn other words: Column-references must either be part of the grouping keys, or they must be part of the aggregation.  \nDatabricks matches expressions on best effort: For example it will recognize: SELECT c1 + 5 FROM T GROUP BY 5 + c1 as mathing expressions. But SELECT c1 FROM T GROUP BY c1 + 5 is not a match.\n\nMitigation"
    },
    {
        "id": 532,
        "url": "https://docs.databricks.com/en/error-messages/missing-aggregation-error-class.html",
        "content": "Mitigation\nThe mitigation of the error depends on the cause:  \nDid you miss a grouping column?  \nAdd expression, or the relevant subexpression of expression to the GROUP BY clause.  \nIs the column reference part of a GROUP BY expression which differs from epression?  \nMatch the expression in the SELECT list or simplify the GROUP BY expression.  \nAre you missing the aggregation?  \nWrap the column reference with an aggregate function. If you only want a representative value from the group, you can use any_value(epression).\n\nExamples"
    },
    {
        "id": 533,
        "url": "https://docs.databricks.com/en/error-messages/missing-aggregation-error-class.html",
        "content": "Examples\n-- Sample data > CREATE OR REPLACE TEMPORARY VIEW tasks(name, firstname, task, cost) AS VALUES ('Smith' , 'Sam' , 'UNPIVOT', 10), ('Smith' , 'Sam' , 'LATERAL', 5), ('Shuster', 'Sally' , 'DELETE' , 7), ('Shuster', 'Sally' , 'GRANT' , 8); -- `name` and `firstname` are part of the group by coumns, but incomplete > SELECT name, firstname, sum(cost) FROM tasks GROUP BY firstname || ' ' || name; [MISSING_AGGREGATION] The expression \"name\" is neither present in the group by, nor is it an aggregate function. -- Match the GROUP BY expression > SELECT firstname || ' ' || name, sum(cost) FROM tasks GROUP BY firstname || ' ' || name; Sam Smith 15 Sally Shuster 15 -- Break up the GROUP BY expression > SELECT firstname, name, sum(cost) FROM tasks GROUP BY firstname, name; Sam Smith 15 Sally Shuster 15 -- Missing grouping column > SELECT name, firstname, sum(cost) FROM tasks GROUP BY name; [MISSING_AGGREGATION] The expression \"firstname\" is neither present in the group by, nor is it an aggregate function. -- Add the grouping column > SELECT firstname, name, sum(cost) FROM tasks GROUP BY firstname, name; Sam Smith 15 Sally Shuster 15 -- Missing aggregate > SELECT firstname, name, sum(cost), task FROM tasks GROUP BY firstname, name; [MISSING_AGGREGATION] The expression \"task\" is neither present in the group by, nor is it an aggregate function. -- Add an aggregate > SELECT firstname, name, sum(cost), array_agg(task) FROM tasks GROUP BY firstname, name; Sam Smith 15 [\"UNPIVOT\",\"LATERAL\"] Sally Shuster 15 [\"DELETE\",\"GRANT\"] -- Return any task > SELECT firstname, name, sum(cost), any_value(task) FROM tasks GROUP BY firstname, name; Sam Smith 15 LATERAL Sally Shuster 15 DELETE"
    },
    {
        "id": 534,
        "url": "https://docs.databricks.com/en/error-messages/collection-size-limit-exceeded-error-class.html",
        "content": "COLLECTION_SIZE_LIMIT_EXCEEDED error class  \nSQLSTATE: 54000  \nCan\u2019t create array with <numberOfElements> elements which exceeding the array size limit <maxRoundedArrayLength>,  \nFUNCTION\nFUNCTION\nunsuccessful try to create arrays in the function <functionName>.\n\nINITIALIZE\nINITIALIZE\ncannot initialize an array with specified parameters.\n\nPARAMETER\nPARAMETER\nthe value of parameter(s) <parameter> in the function <functionName> is invalid."
    },
    {
        "id": 535,
        "url": "https://docs.databricks.com/en/files/unzip-files.html",
        "content": "Expand and read Zip compressed files  \nYou can use the unzip Bash command to expand files or directories of files that have been Zip compressed. If you download or encounter a file or directory ending with .zip, expand the data before trying to continue.  \nNote  \nApache Spark provides native codecs for interacting with compressed Parquet files. Most Parquet files written by Databricks end with .snappy.parquet, indicating they use snappy compression.  \nHow to unzip data"
    },
    {
        "id": 536,
        "url": "https://docs.databricks.com/en/files/unzip-files.html",
        "content": "How to unzip data\nThe Databricks %sh magic command enables execution of arbitrary Bash code, including the unzip command.  \nThe following example uses a zipped CSV file downloaded from the internet. See Download data from the internet.  \nNote  \nYou can use the Databricks Utilities to move files to the ephemeral storage attached to the driver before expanding them. You cannot expand zip files while they reside in Unity Catalog volumes. See Databricks Utilities (dbutils) reference.  \nThe following code uses curl to download and then unzip to expand the data:  \n%sh curl https://resources.lendingclub.com/LoanStats3a.csv.zip --output /tmp/LoanStats3a.csv.zip unzip /tmp/LoanStats3a.csv.zip  \nUse dbutils to move the expanded file to a Unity Catalog volume, as follows:  \ndbutils.fs.mv(\"file:/LoanStats3a.csv\", \"/Volumes/my_catalog/my_schema/my_volume/LoanStats3a.csv\")  \nIn this example, the downloaded data has a comment in the first row and a header in the second. Now that the data has been expanded and moved, use standard options for reading CSV files, as in the following example:  \ndf = spark.read.format(\"csv\").option(\"skipRows\", 1).option(\"header\", True).load(\"/Volumes/my_catalog/my_schema/my_volume/LoanStats3a.csv\") display(df)"
    },
    {
        "id": 537,
        "url": "https://docs.databricks.com/en/generative-ai/tutorials/ai-cookbook/quality-overview.html",
        "content": "Improve RAG application quality  \nThis article provides an overview of how you can refine each component to increase the quality of your retrieval augmented generation (RAG) application.  \nThere are myriad \u201cknobs\u201d to tune at every point in both the offline data pipeline, and online RAG chain. While there are countless others, the article focuses on the most important knobs that have the greatest impact on the quality of your RAG application. Databricks recommends starting with these knobs.  \nTwo types of quality considerations"
    },
    {
        "id": 538,
        "url": "https://docs.databricks.com/en/generative-ai/tutorials/ai-cookbook/quality-overview.html",
        "content": "Two types of quality considerations\nFrom a conceptual point of view, it\u2019s helpful to view RAG quality knobs through the lens of the two key types of quality issues:  \nRetrieval quality: Are you retrieving the most relevant information for a given retrieval query?  \nIt\u2019s difficult to generate high-quality RAG output if the context provided to the LLM is missing important information or contains superfluous information.  \nGeneration quality: Given the retrieved information and the original user query, is the LLM generating the most accurate, coherent, and helpful response possible?  \nIssues here can manifest as hallucinations, inconsistent output, or failure to directly address the user query.  \nRAG apps have two components that can be iterated on to address quality challenges: data pipeline and the chain. It\u2019s tempting to assume a clean division between retrieval issues (simply update the data pipeline) and generation issues (update the RAG chain). However, the reality is more nuanced. Retrieval quality can be influenced by both the data pipeline (for example, parsing/chunking strategy, metadata strategy, embedding model) and the RAG chain (for example, user query transformation, number of chunks retrieved, re-ranking). Similarly, generation quality will invariably be impacted by poor retrieval (for example, irrelevant or missing information affecting model output).  \nThis overlap underscores the need for a holistic approach to RAG quality improvement. By understanding which components to change across both the data pipeline and RAG chain, and how these changes affect the overall solution, you can make targeted updates to improve RAG output quality."
    },
    {
        "id": 539,
        "url": "https://docs.databricks.com/en/generative-ai/tutorials/ai-cookbook/quality-overview.html",
        "content": "Data pipeline quality considerations\nData pipeline quality considerations\nKey considerations about the data pipeline:  \nThe composition of the input data corpus.  \nHow raw data is extracted and transformed into a usable format (for example, parsing a PDF document).  \nHow documents are split into smaller chunks and how those chunks are formatted (for example, chunking strategy, and chunk size).  \nThe metadata (like section title or document title) extracted about each document and/or chunk. How this metadata is included (or not included) in each chunk.  \nThe embedding model used to convert text into vector representations for similarity search.\n\nRAG chain\nRAG chain\nThe choice of LLM and its parameters (for example, temperature and max tokens).  \nThe retrieval parameters (for example, the number of chunks or documents retrieved).  \nThe retrieval approach (for example, keyword vs. hybrid vs. semantic search, rewriting the user\u2019s query, transforming a user\u2019s query into filters, or re-ranking).  \nHow to format the prompt with the retrieved context to guide the LLM toward quality output."
    },
    {
        "id": 540,
        "url": "https://docs.databricks.com/en/dev-tools/vscode-ext/dev-tasks/r-scala-sql-notebook-job.html",
        "content": "Run an R, Scala, or SQL notebook as a job by using the Databricks extension for Visual Studio Code  \nThis article describes how to run an R, Scala, or SQL notebook as a Databricks job by using the Databricks extension for Visual Studio Code. See What is the Databricks extension for Visual Studio Code?.  \nTo run a Python notebook as a Databricks job instead, see Run a Python notebook as a job by using the Databricks extension for Visual Studio Code.  \nThis information assumes that you have already installed and set up the Databricks extension for Visual Studio Code. See Install the Databricks extension for Visual Studio Code.  \nWith the extension and your code project opened, do the following:  \nNote  \nThe following procedure uses the Databricks extension for Visual Studio Code, version 1, which is generally available. To complete this procedure for the Databricks extension for Visual Studio Code, version 2, currently in Private Preview, skip ahead to Run an R, Scala, or SQL notebook as a job by using the Databricks extension for Visual Studio Code, version 2.  \nIn your code project, open the R, Scala, or SQL notebook that you want to run as a job.  \nTip  \nTo create an R, Scala, or SQL notebook file in Visual Studio Code, begin by clicking File > New File, select Python File, and save the new file with a .r, .scala, or .sql file extension, respectively.  \nTo turn the .r, .scala, or .sql file into a Databricks notebook, add the special comment Databricks notebook source to the beginning of the file and add the special comment COMMAND ---------- before each cell. Be sure to use the correct comment marker for each language (# for R, // for Scala, and -- for SQL). For more information, see Import a file and convert it to a notebook.  \nThis is similar to the pattern for Python notebooks:  \nIn Run and Debug view (View > Run), select Run on Databricks as Workflow from the drop-down list, and then click the green play arrow (Start Debugging) icon.  \nNote"
    },
    {
        "id": 541,
        "url": "https://docs.databricks.com/en/dev-tools/vscode-ext/dev-tasks/r-scala-sql-notebook-job.html",
        "content": "In Run and Debug view (View > Run), select Run on Databricks as Workflow from the drop-down list, and then click the green play arrow (Start Debugging) icon.  \nNote  \nIf Run on Databricks as Workflow is not available, see Create a custom run configuration for the Databricks extension for Visual Studio Code.  \nA new editor tab appears, titled Databricks Job Run. The notebook runs as a job in the workspace. The notebook and its output are displayed in the new editor tab\u2019s Output area.  \nTo view information about the job run, click the Task run ID link in the Databricks Job Run editor tab. Your workspace opens and the job run\u2019s details are displayed in the workspace.  \nRun an R, Scala, or SQL notebook as a job by using the Databricks extension for Visual Studio Code, version 2"
    },
    {
        "id": 542,
        "url": "https://docs.databricks.com/en/dev-tools/vscode-ext/dev-tasks/r-scala-sql-notebook-job.html",
        "content": "Run an R, Scala, or SQL notebook as a job by using the Databricks extension for Visual Studio Code, version 2\nNote  \nThe Databricks extension for Visual Studio Code, version 2 is in Private Preview.  \nThis procedure assumes that have already installed and set up the Databricks extension for Visual Studio Code, version 2. See Install and open the Databricks extension for Visual Studio Code, version 2.  \nIn your code project, open the R, Scala, or SQL notebook that you want to run as a job.  \nTip  \nTo create an R, Scala, or SQL notebook file in Visual Studio Code, begin by clicking File > New File, select Python File, and save the new file with a .r, .scala, or .sql file extension, respectively.  \nTo turn the .r, .scala, or .sql file into a Databricks notebook, add the special comment Databricks notebook source to the beginning of the file and add the special comment COMMAND ---------- before each cell. Be sure to use the correct comment marker for each language (# for R, // for Scala, and -- for SQL). For more information, see Import a file and convert it to a notebook.  \nThis is similar to the pattern for Python notebooks:  \nIn Run and Debug view (View > Run), select Run on Databricks as Workflow from the drop-down list, and then click the green play arrow (Start Debugging) icon.  \nNote  \nIf Run on Databricks as Workflow is not available, see Create a custom run configuration for the Databricks extension for Visual Studio Code.  \nA new editor tab appears, titled Databricks Job Run. The notebook runs as a job in the workspace. The notebook and its output are displayed in the new editor tab\u2019s Output area.  \nTo view information about the job run, click the Task run ID link in the Databricks Job Run editor tab. Your workspace opens and the job run\u2019s details are displayed in the workspace."
    },
    {
        "id": 543,
        "url": "https://docs.databricks.com/en/error-messages/not-null-constraint-violation-error-class.html",
        "content": "NOT_NULL_CONSTRAINT_VIOLATION error class  \nSQLSTATE: 42000  \nAssigning a NULL is not allowed here.  \nARRAY_ELEMENT\nARRAY_ELEMENT\nThe array <columnPath> is defined to contain only elements that are NOT NULL.\n\nMAP_VALUE\nMAP_VALUE\nThe map <columnPath> is defined to contain only values that are NOT NULL."
    },
    {
        "id": 544,
        "url": "https://docs.databricks.com/en/error-messages/invalid-array-index-in-element-at-error-class.html",
        "content": "INVALID_ARRAY_INDEX_IN_ELEMENT_AT error class  \nSQLSTATE: 22003  \nThe index <indexValue> is out of bounds. The array has <arraySize> elements. Use try_element_at to tolerate accessing element at invalid index and return NULL instead. If necessary set <ansiConfig> to \u201cfalse\u201d to bypass this error.  \nParameters\nParameters\nindexValue: The requested index into the array.  \narraySize: The cardinality of the array.  \nansiConfig: The configuration setting to alter ANSI mode.\n\nExplanation\nExplanation\nindexValue is beyond the boundary of defined array elements for an element_at(arrayExpr, indexValue), or elt(arrayExpr, indexValue) expression.  \nThe value must be between -arraySize and arraySize, excluding 0.\n\nMitigation"
    },
    {
        "id": 545,
        "url": "https://docs.databricks.com/en/error-messages/invalid-array-index-in-element-at-error-class.html",
        "content": "Mitigation\nThe mitigation for this error depends on the cause:  \nIs the cardinality of the array smaller than expected?  \nFix the input array and re-run the query.  \nHas indexValue been computed incorrectly?  \nAdjust indexValue and re-run the query.  \nDo you expect to get a NULL value to be returned for elements outside the cardinality of the index?  \nIf you can change the expression, use try_element_at(arrayExpr, indexValue) to tolerate references out of bound.  \nIf you cannot change the expression, as a last resort, temporarily set the ansiConfig to false to tolerate references out of bound.\n\nExamples"
    },
    {
        "id": 546,
        "url": "https://docs.databricks.com/en/error-messages/invalid-array-index-in-element-at-error-class.html",
        "content": "Examples\n-- An INVALID_ARRAY_INDEX_IN_ELEMENT_AT error because of mismatched indexing > SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(1), (4) AS T(index); [INVALID_ARRAY_INDEX_IN_ELEMENT_AT] The index 4 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to false to bypass this error. -- Increase the aray size to cover the index > SELECT element_at(array('a', 'b', 'c', 'd'), index) FROM VALUES(1), (4) AS T(index); a d -- Adjusting the index to match the array > SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(1), (3) AS T(index); a c -- Tolerating out of bound array index with adjustment to 1-based indexing > SELECT try_element_at(array('a', 'b', 'c'), index) FROM VALUES(1), (4) AS T(index); a NULL -- Tolerating out of bound by setting ansiConfig in Databricks SQL > SET ANSI_MODE = false; > SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(1), (4) AS T(index); a NULL > SET ANSI_MODE = true; -- Tolerating out of bound by setting ansiConfig in Databricks Runtime > SET spark.sql.ansi.enabled = false; > SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(1), (4) AS T(index); a NULL > SET spark.sql.ansi.enabled = true;"
    },
    {
        "id": 547,
        "url": "https://docs.databricks.com/en/error-messages/invalid-array-index-in-element-at-error-class.html",
        "content": "Related\nRelated\n[ ] (bracket sign) operator  \nANSI_MODE  \nelement_at function  \nelt function  \ntry_element_at function"
    },
    {
        "id": 548,
        "url": "https://docs.databricks.com/en/error-messages/materialized-view-operation-not-allowed-error-class.html",
        "content": "MATERIALIZED_VIEW_OPERATION_NOT_ALLOWED error class  \nSQLSTATE: 56038  \nThe materialized view operation <operation> is not allowed:  \nDROP_DELTA_LIVE_TABLE\nDROP_DELTA_LIVE_TABLE\nCannot DROP a Materialized View created from Delta Live Tables, instead remove the Materialized View from the pipeline definition in Delta Live Tables and retry the pipeline again.\n\nMV_NOT_ENABLED\nMV_NOT_ENABLED\nMaterialized view features are not enabled for your workspace. Please reach out to Databricks to enable materialized views for your workspace.\n\nREPLACE_DELTA_LIVE_TABLE\nREPLACE_DELTA_LIVE_TABLE\nCannot REPLACE a Materialized View created from Delta Live Tables, instead update the Materialized View from the pipeline definition in Delta Live Tables and retry the pipeline again.\n\nREQUIRES_DBSQL_PRO_PLUS\nREQUIRES_DBSQL_PRO_PLUS\nCannot <operation> the Materialized View <tableName> from <environment>, please use DBSQL Serverless (recommended) or Pro warehouse.\n\nREQUIRES_SHARED_COMPUTE"
    },
    {
        "id": 549,
        "url": "https://docs.databricks.com/en/error-messages/materialized-view-operation-not-allowed-error-class.html",
        "content": "REQUIRES_SHARED_COMPUTE\nCannot <operation> the Materialized View <tableName> from an Assigned or No Isolation Shared cluster, please use a Shared cluster or a Databricks SQL warehouse instead."
    },
    {
        "id": 550,
        "url": "https://docs.databricks.com/en/integrations/compute-details.html",
        "content": "Get connection details for a Databricks compute resource  \nTo connect a participating app, tool, SDK, or API to a Databricks compute resource such as a Databricks cluster or a Databricks SQL warehouse, you must provide specific information about that cluster or SQL warehouse so that the connection can be made successfully.  \nTo get the connection details for a Databricks cluster:  \nLog in to your Databricks workspace.  \nIn the sidebar, click Compute.  \nIn the list of available clusters, click the target cluster\u2019s name.  \nOn the Configuration tab, expand Advanced options.  \nClick the JDBC/ODBC tab.  \nCopy the connection details that you need, such as Server Hostname, Port, and HTTP Path.  \nTo get the connection details for a Databricks SQL warehouse, do the following:  \nLog in to your Databricks workspace.  \nIn the sidebar, click SQL > SQL Warehouses.  \nIn the list of available warehouses, click the target warehouse\u2019s name.  \nOn the Connection Details tab, copy the connection details that you need, such as Server hostname, Port, and HTTP path."
    },
    {
        "id": 551,
        "url": "https://docs.databricks.com/en/dev-tools/vscode-ext/dev-tasks/env-vars.html",
        "content": "Use environment variables for the Databricks extension for Visual Studio Code  \nThis article describes how to use environment variables for the Databricks extension for Visual Studio Code. See What is the Databricks extension for Visual Studio Code?  \nThis information assumes that you have already installed and set up the Databricks extension for Visual Studio Code. See Install the Databricks extension for Visual Studio Code.  \nVisual Studio Code supports environment variable definitions files for Python projects. This enables you to create a file with the extension .env somewhere on your development machine, and Visual Studio Code will then apply the environment variables within this .env file at run time. For more information, see Environment variable definitions file in the Visual Studio Code documentation.  \nTo have the Databricks extension for Visual Studio Code use your .env file, set databricks.python.envFile within your settings.json file or Extensions > Databricks > Python: Env File within the Settings editor to the absolute path of your .env file.  \nImportant  \nIf you set settings.json, do not set python.envFile to the absolute path of your .env file as described in the Visual Studio Code documentation, as the Databricks extension for Visual Studio Code must override python.envFile for its internal use. Be sure to only set databricks.python.envFile instead."
    },
    {
        "id": 552,
        "url": "https://docs.databricks.com/en/error-messages/invalid-options-error-class.html",
        "content": "INVALID_OPTIONS error class  \nSQLSTATE: 42K06  \nInvalid options:  \nNON_MAP_FUNCTION\nNON_MAP_FUNCTION\nMust use the map() function for options.\n\nNON_STRING_TYPE\nNON_STRING_TYPE\nA type of keys and values in map() must be string, but got <mapType>."
    },
    {
        "id": 553,
        "url": "https://docs.databricks.com/en/generative-ai/tutorials/ai-cookbook/evaluate-rag-overview.html",
        "content": "Overview: Evaluate RAG quality  \nThe old saying \u201cyou can\u2019t manage what you can\u2019t measure\u201d is very relevant for any generative AI application, including RAG applications. For your generative AI application to deliver high-quality and accurate responses, you must be able to define and measure what \u201cquality\u201d means for your use case.  \nThis section deep dives into three critical components of evaluation:  \nDefine \u201cquality\u201d: Evaluation sets  \nAssess performance: Metrics that matter  \nEnable measurement: Supporting infrastructure"
    },
    {
        "id": 554,
        "url": "https://docs.databricks.com/en/error-messages/insufficient-table-property-error-class.html",
        "content": "INSUFFICIENT_TABLE_PROPERTY error class  \nSQLSTATE: none assigned  \nCan\u2019t find table property:  \nMISSING_KEY\nMISSING_KEY\n<key>.\n\nMISSING_KEY_PART\nMISSING_KEY_PART\n<key>, <totalAmountOfParts> parts are expected."
    },
    {
        "id": 555,
        "url": "https://docs.databricks.com/en/error-messages/delta-versions-not-contiguous-error-class.html",
        "content": "DELTA_VERSIONS_NOT_CONTIGUOUS error class  \nSQLSTATE: KD00C  \nVersions (<versionList>) are not contiguous.  \nAWS\nAWS\nThis can happen when files have been manually removed from the Delta log, or due to S3 eventual consistency when a table is deleted and recreated at the same location. Please contact Databricks support to repair the table.\n\nAZURE\nAZURE\nThis can happen when files have been manually removed from the Delta log. Please contact Databricks support to repair the table.\n\nGENERIC\nGENERIC\nThis can happen when files have been manually removed from the Delta log."
    },
    {
        "id": 556,
        "url": "https://docs.databricks.com/en/getting-started/admin-get-started.html",
        "content": "Get started with Databricks administration  \nThis article article provides opinionated guidance for new account and workspace admins looking to take advantage of the administrative and security features available on Databricks. For more in-depth security guidance, see the Security and compliance guide.  \nRequirements\nRequirements\nYou need a Databricks account and workspace. If you haven\u2019t set yours up yet, follow the steps in Get started: Account and workspace setup to get up and running. Once you have a workspace set up, go through the following admin tasks:\n\nStep 1: Build out your team"
    },
    {
        "id": 557,
        "url": "https://docs.databricks.com/en/getting-started/admin-get-started.html",
        "content": "Step 1: Build out your team\nThe best practice for building out your team is to add users and groups to your account by syncing your identity provider (IdP) with Databricks. If you choose to build your team out manually, you can follow the steps in Manage users and Manage groups to add your team through the account console UI.  \nYou should organize your users and service principals into account groups based on permissions and roles. Account groups simplify identity management by making it easier to assign access to workspaces, data, and other securable objects.  \nAfter your team has been added to Databricks, the following tasks are recommended:  \nAssign groups to the workspace  \nAssign the account admin role to a group  \nSet up SSO for your account\n\nStep 2: Configure permissions and access control"
    },
    {
        "id": 558,
        "url": "https://docs.databricks.com/en/getting-started/admin-get-started.html",
        "content": "Step 2: Configure permissions and access control\nWithin a workspace, workspace admins help secure data and control compute usage by giving users access only to the Databricks functionality and data they need.  \nNote  \nAccess control requires the Premium plan or above. If you don\u2019t have it, go to the account console to update your subscription or contact your Databricks account team.  \nThe following articles walk you through enabling and managing key features workspace admins can use to control data access and compute usage:  \nManage data governance and user data access  \nManage access control for clusters, jobs, notebooks, and other workspace objects  \nCreate and manage compute policies\n\nStep 3: Set up account monitoring\nStep 3: Set up account monitoring\nTo control costs and allow your organization to monitor detailed Databricks usage patterns, including audit and billable usage logs, Databricks recommends using system tables (Public Preview). You can also use custom tags to help monitor resources and data objects.  \nMonitor usage with system tables.  \nMonitor usage using tags\n\nStep 4: Implement additional security features"
    },
    {
        "id": 559,
        "url": "https://docs.databricks.com/en/getting-started/admin-get-started.html",
        "content": "Step 4: Implement additional security features\nDatabricks provides a secure networking environment by default, but if your organization has additional needs, you can configure network security features on your Databricks resources. See Customize network security. For an overview of available security features, see Security and compliance guide.\n\nGet Databricks support\nGet Databricks support\nIf you have any questions about setting up Databricks and need live help, please e-mail onboarding-help@databricks.com.  \nIf you have a Databricks support package, you can open and manage support cases with Databricks. See Learn how to use Databricks support.  \nIf your organization does not have a Databricks support subscription, or if you are not an authorized contact for your company\u2019s support subscription, you can get answers to many questions in Databricks Office Hours or from the Databricks Community.\n\nDatabricks Academy\nDatabricks Academy\nDatabricks Academy has a free self-paced learning path for platform administrators. Before you can access the course, you first need to register for Databricks Academy if you haven\u2019t already.  \nYou can also sign up to attend a live platform administration training.\n\nAdditional resources"
    },
    {
        "id": 560,
        "url": "https://docs.databricks.com/en/getting-started/admin-get-started.html",
        "content": "Additional resources\nThe following table includes links for further learning:  \nBecome a Databricks expert  \nRun the Get started quickstarts  \nTake training courses at Databricks Academy  \nReview resources such as e-books, webinars, and more  \nLearn industry best practices and news  \nRead Databricks blogs  \nView past Spark + AI Summit sessions  \nFollow in-depth, proven best practices  \nLearn about CI/CD on Databricks  \nLearn how to keep your data lake GDPR and CCPA compliant using Delta Lake  \nMeet HIPAA requirements  \nManage data retention  \nLearn how to migrate workloads to Databricks  \nLearn cluster configuration best practices on Databricks  \nGet involved  \nParticipate in Data + AI Summit  \nInfluence the product roadmap by adding ideas to the Ideas Portal  \nGet help and support  \nHelp Center  \nCommunity Forums"
    },
    {
        "id": 561,
        "url": "https://docs.databricks.com/en/error-messages/restricted-streaming-option-permission-enforced-error-class.html",
        "content": "RESTRICTED_STREAMING_OPTION_PERMISSION_ENFORCED error class  \nSQLSTATE: 0A000  \nThe option <option> has restricted values on Shared clusters for the <source> source.  \nKAFKA_LOGIN_CALLBACK_BUILT_IN\nKAFKA_LOGIN_CALLBACK_BUILT_IN\nThe Kafka login callback class must be a built-in Kafka class, which is one that starts with \u201ckafkashaded.org.apache.kafka\u201d.\n\nKAFKA_OAUTH_ENDPOINT_HTTP_SCHEME\nKAFKA_OAUTH_ENDPOINT_HTTP_SCHEME\nThe Kafka Oauth bearer token endpoint URL must have an HTTP(S) scheme. To use other schemes, please use an Assigned cluster.\n\nKAFKA_OAUTH_ENDPOINT_URL\nKAFKA_OAUTH_ENDPOINT_URL\nThe Kafka Oauth bearer token endpoint URL must be a valid URL."
    },
    {
        "id": 562,
        "url": "https://docs.databricks.com/en/error-messages/dc-servicenow-api-error-error-class.html",
        "content": "DC_SERVICENOW_API_ERROR error class  \nSQLSTATE: KD000  \nError happened in ServiceNow API calls, errorCode: <errorCode>.  \nFETCH_ALL_TABLES_FAILED\nFETCH_ALL_TABLES_FAILED\nFailed to fetch all tables.\n\nFETCH_TABLE_ROWS_FAILED\nFETCH_TABLE_ROWS_FAILED\nFailed to fetch rows for table.\n\nFETCH_TABLE_SCHEMA_FAILED\nFETCH_TABLE_SCHEMA_FAILED\nFailed to fetch table schema for table <tableName>.\n\nJSON_MISSING_REQUIRED_FIELD\nJSON_MISSING_REQUIRED_FIELD\nJSON response is missing required field <fieldName>.\n\nMALFORMED_OAUTH_TOKEN_ENDPOINT_URL\nMALFORMED_OAUTH_TOKEN_ENDPOINT_URL\nMalformed OAuth token endpoint URL, url: <url>.\n\nMALFORMED_TABLE_FETCH_URL\nMALFORMED_TABLE_FETCH_URL\nMalformed table fetch URL, url: <url>.\n\nTABLE_MODIFIED_DURING_READ\nTABLE_MODIFIED_DURING_READ\nTable <tableName> has been modified during read."
    },
    {
        "id": 563,
        "url": "https://docs.databricks.com/en/error-messages/delta-iceberg-compat-v1-violation-error-class.html",
        "content": "DELTA_ICEBERG_COMPAT_V1_VIOLATION error class  \nSQLSTATE: KD00E  \nThe validation of IcebergCompatV1 has failed.  \nDISABLING_REQUIRED_TABLE_FEATURE\nDISABLING_REQUIRED_TABLE_FEATURE\nIcebergCompatV1 requires feature <feature> to be supported and enabled. You cannot drop it from the table. Instead, please disable IcebergCompatV1 first.\n\nINCOMPATIBLE_TABLE_FEATURE\nINCOMPATIBLE_TABLE_FEATURE\nIcebergCompatV1 is incompatible with feature <feature>.\n\nMISSING_REQUIRED_TABLE_FEATURE\nMISSING_REQUIRED_TABLE_FEATURE\nIcebergCompatV1 requires feature <feature> to be supported and enabled.\n\nREPLACE_TABLE_CHANGE_PARTITION_NAMES\nREPLACE_TABLE_CHANGE_PARTITION_NAMES\nIcebergCompatV1 doesn\u2019t support replacing partitioned tables with a differently-named partition spec, because Iceberg-Spark 1.1.0 doesn\u2019t.  \nPrev Partition Spec: <prevPartitionSpec>  \nNew Partition Spec: <newPartitionSpec>\n\nUNSUPPORTED_DATA_TYPE"
    },
    {
        "id": 564,
        "url": "https://docs.databricks.com/en/error-messages/delta-iceberg-compat-v1-violation-error-class.html",
        "content": "UNSUPPORTED_DATA_TYPE\nIcebergCompatV1 doesn\u2019t support schema with MapType or ArrayType or NullType. Your schema:  \n<schema>\n\nWRONG_REQUIRED_TABLE_PROPERTY\nWRONG_REQUIRED_TABLE_PROPERTY\nIcebergCompatV1 requires table property \u2018<key>\u2019 to be set to \u2018<requiredValue>\u2019. Current value: \u2018<actualValue>\u2019."
    },
    {
        "id": 565,
        "url": "https://docs.databricks.com/en/error-messages/invalid-handle-error-class.html",
        "content": "INVALID_HANDLE error class  \nSQLSTATE: HY000  \nThe handle <handle> is invalid.  \nFORMAT\nFORMAT\nHandle must be an UUID string of the format \u201800112233-4455-6677-8899-aabbccddeeff\u2019\n\nOPERATION_ABANDONED\nOPERATION_ABANDONED\nOperation was considered abandoned because of inactivity and removed.\n\nOPERATION_ALREADY_EXISTS\nOPERATION_ALREADY_EXISTS\nOperation already exists.\n\nOPERATION_NOT_FOUND\nOPERATION_NOT_FOUND\nOperation not found.\n\nSESSION_CHANGED\nSESSION_CHANGED\nThe existing Spark server driver instance has restarted. Please reconnect.\n\nSESSION_CLOSED\nSESSION_CLOSED\nSession was closed.\n\nSESSION_NOT_FOUND\nSESSION_NOT_FOUND\nSession not found."
    },
    {
        "id": 566,
        "url": "https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/faq.html",
        "content": "Auto Loader FAQ  \nCommonly asked questions about Databricks Auto Loader.  \nDoes Auto Loader process the file again when the file gets appended or overwritten?\nDoes Auto Loader process the file again when the file gets appended or overwritten?\nFiles are processed exactly once unless cloudFiles.allowOverwrites is enabled. When a file is appended to or overwritten, Databricks cannot guarantee which version of the file will be processed. You should also use caution when enabling cloudFiles.allowOverwrites in file notification mode, where Auto Loader might identify new files through both file notifications and directory listing. Due to the discrepancy between file notification event time and file modification time, Auto Loader might obtain two different timestamps and therefore ingest the same file twice, even when the file is only written once.  \nIn general, Databricks recommends you use Auto Loader to ingest only immutable files and avoid setting cloudFiles.allowOverwrites. If this does not meet your requirements, contact your Databricks account team.\n\nIf my data files do not arrive continuously, but in regular intervals, for example, once a day, should I still use this source and are there any benefits?"
    },
    {
        "id": 567,
        "url": "https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/faq.html",
        "content": "If my data files do not arrive continuously, but in regular intervals, for example, once a day, should I still use this source and are there any benefits?\nIn this case, you can set up a Trigger.AvailableNow (available in Databricks Runtime 10.4 LTS and above) Structured Streaming job and schedule to run after the anticipated file arrival time. Auto Loader works well with both infrequent or frequent updates. Even if the eventual updates are very large, Auto Loader scales well to the input size. Auto Loader\u2019s efficient file discovery techniques and schema evolution capabilities make Auto Loader the recommended method for incremental data ingestion.\n\nWhat happens if I change the checkpoint location when restarting the stream?\nWhat happens if I change the checkpoint location when restarting the stream?\nA checkpoint location maintains important identifying information of a stream. Changing the checkpoint location effectively means that you have abandoned the previous stream and started a new stream.\n\nDo I need to create event notification services beforehand?"
    },
    {
        "id": 568,
        "url": "https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/faq.html",
        "content": "Do I need to create event notification services beforehand?\nNo. If you choose file notification mode and provide the required permissions, Auto Loader can create file notification services for you. See What is Auto Loader file notification mode?\n\nHow do I clean up the event notification resources created by Auto Loader?\nHow do I clean up the event notification resources created by Auto Loader?\nYou can use the cloud resource manager to list and tear down resources. You can also delete these resources manually using the cloud provider\u2019s UI or APIs.\n\nCan I run multiple streaming queries from different input directories on the same bucket/container?\nCan I run multiple streaming queries from different input directories on the same bucket/container?\nYes, as long as they are not parent-child directories; for example, prod-logs/ and prod-logs/usage/ would not work because /usage is a child directory of /prod-logs.\n\nCan I use this feature when there are existing file notifications on my bucket or container?"
    },
    {
        "id": 569,
        "url": "https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/faq.html",
        "content": "Can I use this feature when there are existing file notifications on my bucket or container?\nYes, as long as your input directory does not conflict with the existing notification prefix (for example, the above parent-child directories).\n\nHow does Auto Loader infer schema?"
    },
    {
        "id": 570,
        "url": "https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/faq.html",
        "content": "How does Auto Loader infer schema?\nWhen the DataFrame is first defined, Auto Loader lists your source directory and chooses the most recent (by file modification time) 50 GB of data or 1000 files, and uses those to infer your data schema.  \nAuto Loader also infers partition columns by examining the source directory structure and looks for file paths that contain the /key=value/ structure. If the source directory has an inconsistent structure, for example:  \nbase/path/partition=1/date=2020-12-31/file1.json // inconsistent because date and partition directories are in different orders base/path/date=2020-12-31/partition=2/file2.json // inconsistent because the date directory is missing base/path/partition=3/file3.json  \nAuto Loader infers the partition columns as empty. Use cloudFiles.partitionColumns to explicitly parse columns from the directory structure.\n\nHow does Auto Loader behave when the source folder is empty?"
    },
    {
        "id": 571,
        "url": "https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/faq.html",
        "content": "How does Auto Loader behave when the source folder is empty?\nIf the source directory is empty, Auto Loader requires you to provide a schema as there is no data to perform inference.\n\nWhen does Autoloader infer schema? Does it evolve automatically after every micro-batch?\nWhen does Autoloader infer schema? Does it evolve automatically after every micro-batch?\nThe schema is inferred when the DataFrame is first defined in your code. During each micro-batch, schema changes are evaluated on the fly; therefore, you don\u2019t need to worry about performance hits. When the stream restarts, it picks up the evolved schema from the schema location and starts executing without any overhead from inference.\n\nWhat\u2019s the performance impact on ingesting the data when using Auto Loader schema inference?"
    },
    {
        "id": 572,
        "url": "https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/faq.html",
        "content": "What\u2019s the performance impact on ingesting the data when using Auto Loader schema inference?\nYou should expect schema inference to take a couple of minutes for very large source directories during initial schema inference. You shouldn\u2019t observe significant performance hits otherwise during stream execution. If you run your code in a Databricks notebook, you can see status updates that specify when Auto Loader will be listing your directory for sampling and inferring your data schema.\n\nDue to a bug, a bad file has changed my schema drastically. What should I do to roll back a schema change?\nDue to a bug, a bad file has changed my schema drastically. What should I do to roll back a schema change?\nContact Databricks support for help."
    },
    {
        "id": 573,
        "url": "https://docs.databricks.com/en/error-messages/invalid-interval-format-error-class.html",
        "content": "INVALID_INTERVAL_FORMAT error class  \nSQLSTATE: 22006  \nError parsing \u2018<input>\u2019 to interval. Please ensure that the value provided is in a valid format for defining an interval. You can reference the documentation for the correct format.  \nARITHMETIC_EXCEPTION\nARITHMETIC_EXCEPTION\nUncaught arithmetic exception while parsing \u2018<input>\u2019.\n\nINPUT_IS_EMPTY\nINPUT_IS_EMPTY\nInterval string cannot be empty.\n\nINPUT_IS_NULL\nINPUT_IS_NULL\nInterval string cannot be null.\n\nINVALID_FRACTION\nINVALID_FRACTION\n<unit> cannot have fractional part.\n\nINVALID_PRECISION\nINVALID_PRECISION\nInterval can only support nanosecond precision, <value> is out of range.\n\nINVALID_PREFIX\nINVALID_PREFIX\nInvalid interval prefix <prefix>.\n\nINVALID_UNIT\nINVALID_UNIT\nInvalid unit <unit>.\n\nINVALID_VALUE\nINVALID_VALUE\nInvalid value <value>.\n\nMISSING_NUMBER\nMISSING_NUMBER\nExpect a number after <word> but hit EOL.\n\nMISSING_UNIT\nMISSING_UNIT\nExpect a unit name after <word> but hit EOL.\n\nUNKNOWN_PARSING_ERROR\nUNKNOWN_PARSING_ERROR\nUnknown error when parsing <word>."
    },
    {
        "id": 574,
        "url": "https://docs.databricks.com/en/files/write-data.html",
        "content": "Where does Databricks write data?  \nThis article details locations Databricks writes data with everyday operations and configurations. Because Databricks has a suite of tools that span many technologies and interact with cloud resources in a shared-responsibility model, the default locations used to store data vary based on the execution environment, configurations, and libraries.  \nThe information in this article is meant to help you understand default paths for various operations and how configurations might alter these defaults. Data stewards and administrators looking for guidance on configuring and controlling access to data should see Data governance with Unity Catalog.  \nTo learn about configuring object storage and other data sources, see Connect to data sources.  \nWhat is object storage?"
    },
    {
        "id": 575,
        "url": "https://docs.databricks.com/en/files/write-data.html",
        "content": "What is object storage?\nIn cloud computing, object storage or blob storage refers to storage containers that maintain data as objects, with each object consisting of data, metadata, and a globally unique resource identifier (URI). Object storage data manipulation operations are often limited to creating, reading, updating, and deleting (CRUD) through a REST API interface. Some object storage offerings include features like versioning and lifecycle management. Object storage has the following benefits:  \nHigh availability, durability, and reliability.  \nLower storage costs compared to most other storage options.  \nInfinitely scalable (limited by the total amount of storage available in a given cloud region).  \nMost cloud-based data lakes are built on top of open source data formats in cloud object storage.\n\nHow does Databricks use object storage?"
    },
    {
        "id": 576,
        "url": "https://docs.databricks.com/en/files/write-data.html",
        "content": "How does Databricks use object storage?\nObject storage is the main form of storage used by Databricks for most operations. You configure access to cloud object storage using Unity Catalog storage credentials and external locations. These locations are then used to store data files backing tables and volumes. See Connect to cloud object storage using Unity Catalog.  \nUnless you specifically configure a table against an external data system, all tables created in Databricks store data in cloud object storage.  \nDelta Lake files stored in cloud object storage provide the data foundation for a Databricks lakehouse.\n\nWhat is block storage?"
    },
    {
        "id": 577,
        "url": "https://docs.databricks.com/en/files/write-data.html",
        "content": "What is block storage?\nIn cloud computing, block storage or disk storage refers to storage volumes that correspond to traditional hard disk drives (HDDs) or solid-state drives (SSDs), also known as \u201chard drives.\u201d When deploying block storage in a cloud computing environment, a logical partition of one or more physical drives is typically deployed. Implementations vary slightly between product offerings and cloud vendors, but the following characteristics are usually found across implementations:  \nAll virtual machines (VMs) require an attached block storage volume.  \nFiles and programs installed to a block storage volume persist as long as the block storage volume persists.  \nBlock storage volumes are often used for temporary data storage.  \nBlock storage volumes attached to VMs are usually deleted alongside VMs.\n\nHow does Databricks use block storage?"
    },
    {
        "id": 578,
        "url": "https://docs.databricks.com/en/files/write-data.html",
        "content": "How does Databricks use block storage?\nWhen you turn on compute resources, Databricks configures and deploys VMs and attaches block storage volumes. This block storage is used to store ephemeral data files for the lifetime of the compute resource. These files include the operating system, installed libraries, and data used by the disk cache. While Apache Spark uses block storage in the background for efficient parallelization and data loading, most code run on Databricks does not directly save or load data to block storage.  \nYou can run arbitrary code, such as Python or Bash commands that use the block storage attached to your driver node. See Work with files in ephemeral storage attached to the driver node.\n\nWhere does Unity Catalog store data files?"
    },
    {
        "id": 579,
        "url": "https://docs.databricks.com/en/files/write-data.html",
        "content": "Where does Unity Catalog store data files?\nUnity Catalog relies on administrators to configure relationships between cloud storage and relational objects. The exact location where data resides depends on how administrators have configured relations.  \nData written or uploaded to objects governed by Unity Catalog is stored in one of the following locations:  \nA managed storage location associated with a metastore, catalog, or schema. Data written or uploaded to managed tables and managed volumes use managed storage. See Specify a managed storage location in Unity Catalog.  \nAn external location configured with storage credentials. Data written or uploaded to external tables and external volumes use external storage. See Connect to cloud object storage using Unity Catalog.\n\nWhere does Databricks SQL store data backing tables?\nWhere does Databricks SQL store data backing tables?\nWhen you run a CREATE TABLE statement with Databricks SQL configured with Unity Catalog, the default behavior is to store data files in a managed storage location configured with Unity Catalog. See Where does Unity Catalog store data files?.  \nThe legacy hive_metastore catalog follows different rules. See Work with Unity Catalog and the legacy Hive metastore.\n\nWhere does Delta Live Tables store data files?"
    },
    {
        "id": 580,
        "url": "https://docs.databricks.com/en/files/write-data.html",
        "content": "Where does Delta Live Tables store data files?\nDatabricks recommends using Unity Catalog when creating DLT pipelines. Data is stored in directories in the managed storage location associated with the target schema.  \nYou can optionally configure DLT pipelines using Hive metastore. When configured with Hive metastore, you can specify a storage location on DBFS or cloud object storage. If you do not specify a location, a location on the DBFS root is assigned to your pipeline.\n\nWhere does Apache Spark write data files?"
    },
    {
        "id": 581,
        "url": "https://docs.databricks.com/en/files/write-data.html",
        "content": "Where does Apache Spark write data files?\nDatabricks recommends using object names with Unity Catalog for reading and writing data. You can also write files to Unity Catalog volumes using the following pattern: /Volumes/<catalog>/<schema>/<volume>/<path>/<file-name>. You must have sufficient privileges to upload, create, update, or insert data to Unity Catalog-governed objects.  \nYou can optionally use universal resource indicators (URIs) to specify paths to data files. URIs vary depending on the cloud provider. You must also have write permissions configured for your current compute resource to write to cloud object storage.  \nDatabricks uses the Databricks Filesystem to map Apache Spark read and write commands back to cloud object storage. Each Databricks workspace has a DBFS root storage location configured in the cloud account allocated for the workspace, which all users can access for reading and writing data. Databricks does not recommend using the DBFS root to store any production data. See What is DBFS? and Recommendations for working with DBFS root.\n\nWhere does pandas write data files on Databricks?"
    },
    {
        "id": 582,
        "url": "https://docs.databricks.com/en/files/write-data.html",
        "content": "Where does pandas write data files on Databricks?\nIn Databricks Runtime 14.0 and above, the default current working directory (CWD) for all local Python read and write operations is the directory containing the notebook. If you provide only a filename when saving a data file, pandas saves that data file as a workspace file parallel to your currently running notebook.  \nNot all Databricks Runtime versions support workspace files, and some Databricks Runtime versions have differing behavior depending on whether you use notebooks or Git folders. See What is the default current working directory?.\n\nWhere should I write temporary files on Databricks?"
    },
    {
        "id": 583,
        "url": "https://docs.databricks.com/en/files/write-data.html",
        "content": "Where should I write temporary files on Databricks?\nIf you must write temporary files you do not want to keep after the cluster is shut down, writing the temporary files to $TEMPDIR yields better performance than writing to the current working directory (CWD) if the CWD is in the workspace filesystem. You can also avoid exceeding branch size limits if the code runs in a Repo. For more information, see File and repo size limits.  \nWrite to /local_disk0 if the amount of data to be written is large and you want the storage to autoscale."
    },
    {
        "id": 584,
        "url": "https://docs.databricks.com/en/ingestion/index.html",
        "content": "Ingest data into a Databricks lakehouse  \nDatabricks offers various ways to ingest data from various sources into a lakehouse backed by Delta Lake. This article lists data sources and provides links to steps for ingesting data from each source type.  \nCloud object storage\nCloud object storage\nTo learn about how to configure incremental ingestion from cloud object storage, see Ingest data from cloud object storage.\n\nLakeFlow Connect\nLakeFlow Connect\nDatabricks LakeFlow Connect offers native connectors for ingestion from enterprise applications and databases. The resulting ingestion pipeline is governed by Unity Catalog and is powered by serverless compute and Delta Live Tables.  \nLakeFlow Connect leverages efficient incremental reads and writes to make data ingestion faster, more scalable, and more cost-efficient, while your data remains fresh for downstream consumption.\n\nStreaming sources\nStreaming sources\nDatabricks can integrate with stream messaging services for near-real time data ingestion into a lakehouse. See Streaming and incremental ingestion.\n\nLocal data files\nLocal data files\nYou can securely upload local data files or download files from a public URL. See Upload files to Databricks."
    },
    {
        "id": 585,
        "url": "https://docs.databricks.com/en/ingestion/index.html",
        "content": "Migrate data to Delta Lake\nMigrate data to Delta Lake\nTo learn how to migrate existing data to Delta Lake, see Migrate data to Delta Lake."
    },
    {
        "id": 586,
        "url": "https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/index.html",
        "content": "What is Auto Loader?  \nAuto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without any additional setup.  \nHow does Auto Loader work?\nHow does Auto Loader work?\nAuto Loader incrementally and efficiently processes new data files as they arrive in cloud storage. It provides a Structured Streaming source called cloudFiles. Given an input directory path on the cloud file storage, the cloudFiles source automatically processes new files as they arrive, with the option of also processing existing files in that directory. Auto Loader has support for both Python and SQL in Delta Live Tables.  \nYou can use Auto Loader to process billions of files to migrate or backfill a table. Auto Loader scales to support near real-time ingestion of millions of files per hour.\n\nSupported Auto Loader sources"
    },
    {
        "id": 587,
        "url": "https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/index.html",
        "content": "Supported Auto Loader sources\nAuto Loader can load data files from the following sources:  \nAmazon S3 (s3://)  \nAzure Data Lake Storage Gen2 (ADLS Gen2, abfss://)  \nGoogle Cloud Storage (GCS, gs://)  \nAzure Blob Storage (wasbs://)  \nNote  \nThe legacy Windows Azure Storage Blob driver (WASB) has been deprecated. ABFS has numerous benefits over WASB. See Azure documentation on ABFS. For documentation for working with the legacy WASB driver, see Connect to Azure Blob Storage with WASB (legacy).  \nADLS Gen1 (adl://)  \nNote  \nAzure has announced the pending retirement of Azure Data Lake Storage Gen1. Databricks recommends migrating all data from Azure Data Lake Storage Gen1 to Azure Data Lake Storage Gen2. If you have not yet migrated, see Accessing Azure Data Lake Storage Gen1 from Databricks.  \nDatabricks File System (DBFS, dbfs:/).  \nAuto Loader can ingest JSON, CSV, XML, PARQUET, AVRO, ORC, TEXT, and BINARYFILE file formats."
    },
    {
        "id": 588,
        "url": "https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/index.html",
        "content": "How does Auto Loader track ingestion progress?\nHow does Auto Loader track ingestion progress?\nAs files are discovered, their metadata is persisted in a scalable key-value store (RocksDB) in the checkpoint location of your Auto Loader pipeline. This key-value store ensures that data is processed exactly once.  \nIn case of failures, Auto Loader can resume from where it left off by information stored in the checkpoint location and continue to provide exactly-once guarantees when writing data into Delta Lake. You don\u2019t need to maintain or manage any state yourself to achieve fault tolerance or exactly-once semantics.\n\nIncremental ingestion using Auto Loader with Delta Live Tables"
    },
    {
        "id": 589,
        "url": "https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/index.html",
        "content": "Incremental ingestion using Auto Loader with Delta Live Tables\nDatabricks recommends Auto Loader in Delta Live Tables for incremental data ingestion. Delta Live Tables extends functionality in Apache Spark Structured Streaming and allows you to write just a few lines of declarative Python or SQL to deploy a production-quality data pipeline with:  \nAutoscaling compute infrastructure for cost savings  \nData quality checks with expectations  \nAutomatic schema evolution handling  \nMonitoring via metrics in the event log  \nYou do not need to provide a schema or checkpoint location because Delta Live Tables automatically manages these settings for your pipelines. See Load data with Delta Live Tables.  \nDatabricks also recommends Auto Loader whenever you use Apache Spark Structured Streaming to ingest data from cloud object storage. APIs are available in Python and Scala.\n\nGet started with Databricks Auto Loader"
    },
    {
        "id": 590,
        "url": "https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/index.html",
        "content": "Get started with Databricks Auto Loader\nSee the following articles to get started configuring incremental data ingestion using Auto Loader with Delta Live Tables:  \nTutorial: Run your first ETL workload on Databricks using sample data (Python, SQL notebook)  \nLoad data from cloud object storage into streaming tables using Auto Loader (Notebook: Python, SQL)  \nLoad data from cloud object storage into streaming tables using Auto Loader (Databricks SQL Editor)\n\nExamples: Common Auto Loader patterns\nExamples: Common Auto Loader patterns\nFor examples of common Auto Loader patterns, see Common data loading patterns.\n\nConfigure Auto Loader options\nConfigure Auto Loader options\nYou can tune Auto Loader based on data volume, variety, and velocity.  \nConfigure schema inference and evolution in Auto Loader  \nConfigure Auto Loader for production workloads  \nFor a full list of Auto Loader options, see:  \nAuto Loader options  \nIf you encounter unexpected performance, see the FAQ.\n\nConfigure Auto Loader file detection modes"
    },
    {
        "id": 591,
        "url": "https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/index.html",
        "content": "Configure Auto Loader file detection modes\nAuto Loader supports two file detection modes. See:  \nWhat is Auto Loader directory listing mode?  \nWhat is Auto Loader file notification mode?\n\nBenefits of Auto Loader over using Structured Streaming directly on files"
    },
    {
        "id": 592,
        "url": "https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/index.html",
        "content": "Benefits of Auto Loader over using Structured Streaming directly on files\nIn Apache Spark, you can read files incrementally using spark.readStream.format(fileFormat).load(directory). Auto Loader provides the following benefits over the file source:  \nScalability: Auto Loader can discover billions of files efficiently. Backfills can be performed asynchronously to avoid wasting any compute resources.  \nPerformance: The cost of discovering files with Auto Loader scales with the number of files that are being ingested instead of the number of directories that the files may land in. See What is Auto Loader directory listing mode?.  \nSchema inference and evolution support: Auto Loader can detect schema drifts, notify you when schema changes happen, and rescue data that would have been otherwise ignored or lost. See How does Auto Loader schema inference work?.  \nCost: Auto Loader uses native cloud APIs to get lists of files that exist in storage. In addition, Auto Loader\u2019s file notification mode can help reduce your cloud costs further by avoiding directory listing altogether. Auto Loader can automatically set up file notification services on storage to make file discovery much cheaper."
    },
    {
        "id": 593,
        "url": "https://docs.databricks.com/en/error-messages/insert-column-arity-mismatch-error-class.html",
        "content": "INSERT_COLUMN_ARITY_MISMATCH error class  \nSQLSTATE: 21S01  \nCannot write to <tableName>, the reason is  \nNOT_ENOUGH_DATA_COLUMNS\nNOT_ENOUGH_DATA_COLUMNS\nnot enough data columns:  \nTable columns: <tableColumns>.  \nData columns: <dataColumns>.\n\nTOO_MANY_DATA_COLUMNS\nTOO_MANY_DATA_COLUMNS\ntoo many data columns:  \nTable columns: <tableColumns>.  \nData columns: <dataColumns>."
    },
    {
        "id": 594,
        "url": "https://docs.databricks.com/en/generative-ai/tutorials/ai-cookbook/fundamentals-governance-llmops.html",
        "content": "RAG application governance and LLMOps  \nThis article is a brief overview of governance and LLMOps for RAG applications.  \nGovernance is required to ensure your enterprise data and AI assets are secured with the proper access controls and logging. For details, see AI Governance.  \nLLMOps, and more broadly MLOps, is the set of processes and automation for managing data, code, and models to improve performance, stability, and long-term efficiency of AI systems. For more information about LLMOps, see the Big Book of MLOps."
    },
    {
        "id": 595,
        "url": "https://docs.databricks.com/en/error-messages/upgrade-not-supported-error-class.html",
        "content": "UPGRADE_NOT_SUPPORTED error class  \nSQLSTATE: 0AKUC  \nTable is not eligible for upgrade from Hive Metastore to Unity Catalog. Reason:  \nBUCKETED_TABLE\nBUCKETED_TABLE\nBucketed table.\n\nDBFS_ROOT_LOCATION\nDBFS_ROOT_LOCATION\nTable located on DBFS root.\n\nHIVE_SERDE\nHIVE_SERDE\nHive SerDe table.\n\nNOT_EXTERNAL\nNOT_EXTERNAL\nNot an external table.\n\nUNSUPPORTED_DBFS_LOC\nUNSUPPORTED_DBFS_LOC\nUnsupported DBFS location.\n\nUNSUPPORTED_FILE_SCHEME\nUNSUPPORTED_FILE_SCHEME\nUnsupported file system scheme <scheme>.\n\nUNSUPPORTED_PROVIDER\nUNSUPPORTED_PROVIDER\nUnsupported provider <provider>.\n\nUNSUPPORTED_TABLE_TYPE\nUNSUPPORTED_TABLE_TYPE\nTable type <tableType> is not supported."
    },
    {
        "id": 596,
        "url": "https://docs.databricks.com/en/dev-tools/vscode-ext/dev-tasks/custom-run-config.html",
        "content": "Create a custom run configuration for the Databricks extension for Visual Studio Code  \nThis article describes how to create a custom run configuration for the Databricks extension for Visual Studio Code. See What is the Databricks extension for Visual Studio Code?.  \nThis information assumes that you have already installed and set up the Databricks extension for Visual Studio Code. See Install the Databricks extension for Visual Studio Code.  \nYou can create custom run configurations in Visual Studio Code to do things such as passing custom arguments to a job or a notebook, or creating different run settings for different files. For example, the following custom run configuration passes the --prod argument to the job:  \n{ \"version\": \"0.2.0\", \"configurations\": [ { \"type\": \"databricks-workflow\", \"request\": \"launch\", \"name\": \"Run on Databricks as Workflow\", \"program\": \"${file}\", \"parameters\": {}, \"args\": [\"--prod\"], \"preLaunchTask\": \"databricks: sync\" } ] }  \nTo create a custom run configuration, click Run > Add Configuration from the main menu in Visual Studio Code. Then select either Databricks for a cluster-based run configuration or Databricks: Workflow for a job-based run configuration.  \nBy using custom run configurations, you can also pass in command-line arguments and run your code just by pressing F5. For more information, see Launch configurations in the Visual Studio Code documentation."
    },
    {
        "id": 597,
        "url": "https://docs.databricks.com/en/ingestion/lakeflow-connect/workday/workday-reports/source-setup.html",
        "content": "Configure Workday reports for ingestion  \nPreview  \nLakeFlow Connect is in gated Public Preview. To participate in the preview, contact your Databricks account team.  \nThis article describes how to configure Workday reports for ingestion.  \nGet the Workday report URL\nGet the Workday report URL\nIn Workday, navigate to the report.  \nIn the top blue bar, click the three dots next to the report name, then click Web Service > View URLs.  \nOptionally add any parameter (for example, prompt) values. Then click OK.  \nNext to the JSON section, click the three dots, then click Copy URL. This gives you the report URL. Ensure that you don\u2019t truncate format=json at the end of the URL.\n\n(Optional) Create an integrated system user (ISU) in Workday\n(Optional) Create an integrated system user (ISU) in Workday\nIn the Workday search bar, type Create user.  \nClick Create Integration System User : Task.  \nEnter a username and password.  \nSelect the Do Not Allow UI Sessions checkbox. This user will only access the RaaS API.\n\n(Optional) Create a security group and add the user to it"
    },
    {
        "id": 598,
        "url": "https://docs.databricks.com/en/ingestion/lakeflow-connect/workday/workday-reports/source-setup.html",
        "content": "(Optional) Create a security group and add the user to it\nIn the Workday search bar, type Create Security Group.  \nClick Create Security Group : Task.  \nSet the type to Integration System Security Group (Unconstrained).  \nEnter a name.  \nAdd the ISU user to the group (Type the prefix, press Enter, and pick the correct user). Then click OK.\n\nAdd domain security policies to the security group"
    },
    {
        "id": 599,
        "url": "https://docs.databricks.com/en/ingestion/lakeflow-connect/workday/workday-reports/source-setup.html",
        "content": "Add domain security policies to the security group\nIn the Workday search bar, type View Security Group.  \nClick View Security Group : Report.  \nIn the View Security Group form, type the prefix of the previously created security group, select it, and then click OK.  \nIn the top blue bar, click the three dots next to the report name, then click Web Service > View URLs.  \nSelect Security Group > Maintain Domain Permission for the security group.  \nUnder Integration Permission, add the required domain security policies for GET access. For each policy, type a differentiating prefix in the search bar, press Enter, and select the appropriate option(s). The exact policies that you need depend on the chosen report. Some useful policies to start with include:  \nWorker Data: Current Staffing Information  \nWorker Data: Public Worker Reports  \nWorker Data: Active and Terminated Workers  \nWorker Data: All Positions  \nWorker Data: Business Title on Worker Profile  \nPerson Data: Work Contact Information  \nWorker Data: Workers  \nWorkday Accounts  \nClick OK.  \nClick Done.  \nIn the Workday search bar, type Activate Pending Security Policy Changes and select it. When the form appears, type a comment and click OK.  \nClick Confirm and click OK."
    },
    {
        "id": 600,
        "url": "https://docs.databricks.com/en/ingestion/lakeflow-connect/workday/workday-reports/source-setup.html",
        "content": "Create an API client, add functional scopes, and generate a refresh token for the ISU user"
    },
    {
        "id": 601,
        "url": "https://docs.databricks.com/en/ingestion/lakeflow-connect/workday/workday-reports/source-setup.html",
        "content": "Create an API client, add functional scopes, and generate a refresh token for the ISU user\nIn the Workday search bar, type Register API Client.  \nClick Register API Client for Integrations : Task.  \nEnter a Client Name.  \nClick Non Expiring Refresh Token.  \nIn the Scope search bar, type System and select it.  \nClick OK.  \nCopy the Client ID and Client Secret, then click Done.  \nIn the View Integration System Security Group page, note the functional areas under Domain Security Policies. Then, add these as Scopes/Functional Areas in the API Client:  \nIn the search bar, type View API Client.  \nChoose your API client from the list.  \nIn the top blue bar, click the three dots, then click API Client and Edit API Clients for Integrations.  \nIn the Scope (Functional Areas) field, search for and add the functional areas that you noted.  \nIn the same menu as before, select Manage Refresh Token for Integrations.  \nIn the form, search for the ISU user and select it.  \nClick OK.  \nClick Generate new token and Confirm Delete.  \nCopy the refresh token for later use in the Delta Live Tables notebook.\n\nNext steps\nNext steps\nIngest Workday reports"
    },
    {
        "id": 602,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Error classes in Databricks  \nApplies to: Databricks SQL Databricks Runtime 12.2 and above  \nError classes are descriptive, human-readable, strings unique to the error condition.  \nYou can use error classes to programmatically handle errors in your application without the need to parse the error message.  \nThis is a list of common, named error conditions returned by Databricks.  \nDatabricks Runtime and Databricks SQL"
    },
    {
        "id": 603,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Databricks Runtime and Databricks SQL\nAGGREGATE_FUNCTION_WITH_NONDETERMINISTIC_EXPRESSION  \nSQLSTATE: 42845  \nNon-deterministic expression <sqlExpr> should not appear in the arguments of an aggregate function.  \nAI_FUNCTION_HTTP_PARSE_CAST_ERROR  \nSQLSTATE: 2203G  \nFailed to parse model output when casting to the specified returnType: \u201c<dataType>\u201d, response JSON was: \u201c<responseString>\u201d. Please update the returnType to match the contents of the type represented by the response JSON and then retry the query again.  \nAI_FUNCTION_HTTP_PARSE_COLUMNS_ERROR  \nSQLSTATE: 2203G  \nThe actual model output has more than one column \u201c<responseString>\u201d. However, the specified return type[\u201c<dataType>\u201d] has only one column. Please update the returnType to contain the same number of columns as the model output and then retry the query again.  \nAI_FUNCTION_HTTP_REQUEST_ERROR  \nSQLSTATE: 08000  \nError occurred while making an HTTP request for function <funcName>: <errorMessage>  \nAI_FUNCTION_INVALID_HTTP_RESPONSE  \nSQLSTATE: 08000  \nInvalid HTTP response for function <funcName>: <errorMessage>  \nAI_FUNCTION_INVALID_MAX_WORDS  \nSQLSTATE: 22032  \nThe maximum number of words must be a non-negative integer, but got <maxWords>.  \nAI_FUNCTION_JSON_PARSE_ERROR  \nSQLSTATE: 22000  \nError occurred while parsing the JSON response for function <funcName>: <errorMessage>  \nAI_FUNCTION_MODEL_SCHEMA_PARSE_ERROR  \nSQLSTATE: 2203G  \nFailed to parse the schema for the serving endpoint \u201c<endpointName>\u201d: <errorMessage>, response JSON was: \u201c<responseJson>\u201d.  \nSet the returnType parameter manually in the AI_QUERY function to override schema resolution.  \nAI_FUNCTION_UNSUPPORTED_ERROR  \nSQLSTATE: 56038  \nThe function <funcName> is not supported in the current environment. It is only available in Databricks SQL Pro and Serverless."
    },
    {
        "id": 604,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "AI_FUNCTION_UNSUPPORTED_ERROR  \nSQLSTATE: 56038  \nThe function <funcName> is not supported in the current environment. It is only available in Databricks SQL Pro and Serverless.  \nAI_FUNCTION_UNSUPPORTED_REQUEST  \nSQLSTATE: 0A000  \nFailed to evaluate the SQL function \u201c<functionName>\u201d because the provided argument of <invalidValue> has \u201c<invalidDataType>\u201d, but only the following types are supported: <supportedDataTypes>. Please update the function call to provide an argument of string type and retry the query again.  \nAI_FUNCTION_UNSUPPORTED_RETURN_TYPE  \nSQLSTATE: 0A000  \nAI function: \u201c<functionName>\u201d does not support the following type as return type: \u201c<typeName>\u201d. Return type must be a valid SQL type understood by Catalyst and supported by AI function. Current supported types includes: <supportedValues>  \nAI_INVALID_ARGUMENT_VALUE_ERROR  \nSQLSTATE: 22032  \nProvided value \u201c<argValue>\u201d is not supported by argument \u201c<argName>\u201d. Supported values are: <supportedValues>  \nAI_QUERY_RETURN_TYPE_COLUMN_TYPE_MISMATCH  \nSQLSTATE: 0A000  \nProvided \u201c<sqlExpr>\u201d is not supported by the argument returnType.  \nAI_SEARCH_EMBEDDING_COLUMN_TYPE_UNSUPPORTED_ERROR  \nSQLSTATE: 0A000  \nVector Search with embedding column type <embeddingColumnType> is not supported.  \nAI_SEARCH_INDEX_TYPE_UNSUPPORTED_ERROR  \nSQLSTATE: 0A000  \nVector Search with index type <indexType> is not supported.  \nAI_SEARCH_UNSUPPORTED_NUM_RESULTS_ERROR  \nSQLSTATE: 0A000  \nVector Search with num_results larger than <maxLimit> is not supported. The limit specified was <requestedLimit>. Pleaase try again with num_results <= `<maxLimit>`  \nALL_PARAMETERS_MUST_BE_NAMED  \nSQLSTATE: 07001  \nUsing name parameterized queries requires all parameters to be named. Parameters missing names: <exprs>.  \nALL_PARTITION_COLUMNS_NOT_ALLOWED  \nSQLSTATE: KD005"
    },
    {
        "id": 605,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 07001  \nUsing name parameterized queries requires all parameters to be named. Parameters missing names: <exprs>.  \nALL_PARTITION_COLUMNS_NOT_ALLOWED  \nSQLSTATE: KD005  \nCannot use all columns for partition columns.  \nALTER_TABLE_COLUMN_DESCRIPTOR_DUPLICATE  \nSQLSTATE: 42710  \nALTER TABLE <type> column <columnName> specifies descriptor \u201c<optionName>\u201d more than once, which is invalid.  \nAMBIGUOUS_ALIAS_IN_NESTED_CTE  \nSQLSTATE: 42KD0  \nName <name> is ambiguous in nested CTE.  \nPlease set <config> to \u201cCORRECTED\u201d so that name defined in inner CTE takes precedence. If set it to \u201cLEGACY\u201d, outer CTE definitions will take precedence.  \nSee https://spark.apache.org/docs/latest/sql-migration-guide.html#query-engine\u2019.  \nAMBIGUOUS_COLUMN_OR_FIELD  \nSQLSTATE: 42702  \nColumn or field <name> is ambiguous and has <n> matches.  \nAMBIGUOUS_COLUMN_REFERENCE  \nSQLSTATE: 42702  \nColumn <name> is ambiguous. It\u2019s because you joined several DataFrame together, and some of these DataFrames are the same.  \nThis column points to one of the DataFrames but Spark is unable to figure out which one.  \nPlease alias the DataFrames with different names via DataFrame.alias before joining them,  \nand specify the column using qualified name, e.g. df.alias(\"a\").join(df.alias(\"b\"), col(\"a.id\") > col(\"b.id\")).  \nAMBIGUOUS_CONSTRAINT  \nSQLSTATE: 42K0C  \nAmbiguous reference to constraint <constraint>.  \nAMBIGUOUS_LATERAL_COLUMN_ALIAS  \nSQLSTATE: 42702  \nLateral column alias <name> is ambiguous and has <n> matches.  \nAMBIGUOUS_REFERENCE  \nSQLSTATE: 42704  \nReference <name> is ambiguous, could be: <referenceNames>.  \nAMBIGUOUS_REFERENCE_TO_FIELDS  \nSQLSTATE: 42000"
    },
    {
        "id": 606,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "AMBIGUOUS_REFERENCE  \nSQLSTATE: 42704  \nReference <name> is ambiguous, could be: <referenceNames>.  \nAMBIGUOUS_REFERENCE_TO_FIELDS  \nSQLSTATE: 42000  \nAmbiguous reference to the field <field>. It appears <count> times in the schema.  \nANALYZE_CONSTRAINTS_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nANALYZE CONSTRAINTS is not supported.  \nANSI_CONFIG_CANNOT_BE_DISABLED  \nSQLSTATE: 56038  \nThe ANSI SQL configuration <config> cannot be disabled in this product.  \nARGUMENT_NOT_CONSTANT  \nSQLSTATE: 42K08  \nThe function <functionName> includes a parameter <parameterName> at position <pos> that requires a constant argument. Please compute the argument <sqlExpr> separately and pass the result as a constant.  \nARITHMETIC_OVERFLOW  \nSQLSTATE: 22003  \n<message>.<alternative> If necessary set <config> to \u201cfalse\u201d to bypass this error.  \nFor more details see ARITHMETIC_OVERFLOW  \nASSIGNMENT_ARITY_MISMATCH  \nSQLSTATE: 42802  \nThe number of columns or variables assigned or aliased: <numTarget> does not match the number of source expressions: <numExpr>.  \nAS_OF_JOIN  \nSQLSTATE: 42604  \nInvalid as-of join.  \nFor more details see AS_OF_JOIN  \nAVRO_DEFAULT_VALUES_UNSUPPORTED  \nSQLSTATE: 0A000  \nThe use of default values is not supported when`rescuedDataColumn` is enabled. You may be able to remove this check by setting spark.databricks.sql.avro.rescuedDataBlockUserDefinedSchemaDefaultValue to false, but the default values will not apply and null values will still be used.  \nAVRO_INCOMPATIBLE_READ_TYPE  \nSQLSTATE: 22KD3  \nCannot convert Avro <avroPath> to SQL <sqlPath> because the original encoded data type is <avroType>, however you\u2019re trying to read the field as <sqlType>, which would lead to an incorrect answer."
    },
    {
        "id": 607,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "To allow reading this field, enable the SQL configuration: \u201cspark.sql.legacy.avro.allowIncompatibleSchema\u201d.  \nAVRO_POSITIONAL_FIELD_MATCHING_UNSUPPORTED  \nSQLSTATE: 0A000  \nThe use of positional field matching is not supported when either rescuedDataColumn or failOnUnknownFields is enabled. Remove these options to proceed.  \nBATCH_METADATA_NOT_FOUND  \nSQLSTATE: 42K03  \nUnable to find batch <batchMetadataFile>.  \nBIGQUERY_OPTIONS_ARE_MUTUALLY_EXCLUSIVE  \nSQLSTATE: 42616  \nBigQuery connection credentials must be specified with either the \u2018GoogleServiceAccountKeyJson\u2019 parameter or all of \u2018projectId\u2019, \u2018OAuthServiceAcctEmail\u2019, \u2018OAuthPvtKey\u2019  \nBINARY_ARITHMETIC_OVERFLOW  \nSQLSTATE: 22003  \n<value1> <symbol> <value2> caused overflow.  \nBUILT_IN_CATALOG  \nSQLSTATE: 42832  \n<operation> doesn\u2019t support built-in catalogs.  \nCALL_ON_STREAMING_DATASET_UNSUPPORTED  \nSQLSTATE: 42KDE  \nThe method <methodName> can not be called on streaming Dataset/DataFrame.  \nCANNOT_ALTER_COLLATION_BUCKET_COLUMN  \nSQLSTATE: 428FR  \nALTER TABLE (ALTER|CHANGE) COLUMN cannot change collation of type/subtypes of bucket columns, but found the bucket column <columnName> in the table <tableName>.  \nCANNOT_ALTER_PARTITION_COLUMN  \nSQLSTATE: 428FR  \nALTER TABLE (ALTER|CHANGE) COLUMN is not supported for partition columns, but found the partition column <columnName> in the table <tableName>.  \nCANNOT_ASSIGN_EVENT_TIME_COLUMN_WITHOUT_WATERMARK  \nSQLSTATE: 42611  \nWatermark needs to be defined to reassign event time column. Failed to find watermark definition in the streaming query.  \nCANNOT_CAST_DATATYPE  \nSQLSTATE: 42846  \nCannot cast <sourceType> to <targetType>.  \nCANNOT_CONVERT_PROTOBUF_FIELD_TYPE_TO_SQL_TYPE"
    },
    {
        "id": 608,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 42846  \nCannot cast <sourceType> to <targetType>.  \nCANNOT_CONVERT_PROTOBUF_FIELD_TYPE_TO_SQL_TYPE  \nSQLSTATE: 42846  \nCannot convert Protobuf <protobufColumn> to SQL <sqlColumn> because schema is incompatible (protobufType = <protobufType>, sqlType = <sqlType>).  \nCANNOT_CONVERT_PROTOBUF_MESSAGE_TYPE_TO_SQL_TYPE  \nSQLSTATE: 42846  \nUnable to convert <protobufType> of Protobuf to SQL type <toType>.  \nCANNOT_CONVERT_SQL_TYPE_TO_PROTOBUF_FIELD_TYPE  \nSQLSTATE: 42846  \nCannot convert SQL <sqlColumn> to Protobuf <protobufColumn> because schema is incompatible (protobufType = <protobufType>, sqlType = <sqlType>).  \nCANNOT_CONVERT_SQL_VALUE_TO_PROTOBUF_ENUM_TYPE  \nSQLSTATE: 42846  \nCannot convert SQL <sqlColumn> to Protobuf <protobufColumn> because <data> is not in defined values for enum: <enumString>.  \nCANNOT_COPY_STATE  \nSQLSTATE: 0AKD0  \nCannot copy catalog state like current database and temporary views from Unity Catalog to a legacy catalog.  \nCANNOT_CREATE_DATA_SOURCE_TABLE  \nSQLSTATE: 42KDE  \nFailed to create data source table <tableName>:  \nFor more details see CANNOT_CREATE_DATA_SOURCE_TABLE  \nCANNOT_DECODE_URL  \nSQLSTATE: 22546  \nThe provided URL cannot be decoded: <url>. Please ensure that the URL is properly formatted and try again.  \nCANNOT_DELETE_SYSTEM_OWNED  \nSQLSTATE: 42832  \nSystem owned <resourceType> cannot be deleted.  \nCANNOT_DROP_AMBIGUOUS_CONSTRAINT  \nSQLSTATE: 42K0C  \nCannot drop the constraint with the name <constraintName> shared by a CHECK constraint"
    },
    {
        "id": 609,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "System owned <resourceType> cannot be deleted.  \nCANNOT_DROP_AMBIGUOUS_CONSTRAINT  \nSQLSTATE: 42K0C  \nCannot drop the constraint with the name <constraintName> shared by a CHECK constraint  \nand a PRIMARY KEY or FOREIGN KEY constraint. You can drop the PRIMARY KEY or  \nFOREIGN KEY constraint by queries:  \nALTER TABLE .. DROP PRIMARY KEY or  \nALTER TABLE .. DROP FOREIGN KEY ..  \nCANNOT_ESTABLISH_CONNECTION  \nSQLSTATE: 08001  \nCannot establish connection to remote <jdbcDialectName> database. Please check connection information and credentials e.g. host, port, user, password and database options. ** If you believe the information is correct, please check your workspace\u2019s network setup and ensure it does not have outbound restrictions to the host. Please also check that the host does not block inbound connections from the network where the workspace\u2019s Spark clusters are deployed. ** Detailed error message: <causeErrorMessage>.  \nCANNOT_ESTABLISH_CONNECTION_SERVERLESS  \nSQLSTATE: 08001  \nCannot establish connection to remote <jdbcDialectName> database. Please check connection information and credentials e.g. host, port, user, password and database options. ** If you believe the information is correct, please allow inbound traffic from the Internet to your host, as you are using Serverless Compute. If your network policies do not allow inbound Internet traffic, please use non Serverless Compute, or you may reach out to your Databricks representative to learn about Serverless Private Networking. ** Detailed error message: <causeErrorMessage>.  \nCANNOT_INVOKE_IN_TRANSFORMATIONS  \nSQLSTATE: 0A000  \nDataset transformations and actions can only be invoked by the driver, not inside of other Dataset transformations; for example, dataset1.map(x => dataset2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the dataset1.map transformation. For more information, see SPARK-28702.  \nCANNOT_LOAD_FUNCTION_CLASS  \nSQLSTATE: 46103"
    },
    {
        "id": 610,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "CANNOT_LOAD_FUNCTION_CLASS  \nSQLSTATE: 46103  \nCannot load class <className> when registering the function <functionName>, please make sure it is on the classpath.  \nCANNOT_LOAD_PROTOBUF_CLASS  \nSQLSTATE: 42K03  \nCould not load Protobuf class with name <protobufClassName>. <explanation>.  \nCANNOT_LOAD_STATE_STORE  \nSQLSTATE: 58030  \nAn error occurred during loading state.  \nFor more details see CANNOT_LOAD_STATE_STORE  \nCANNOT_MERGE_INCOMPATIBLE_DATA_TYPE  \nSQLSTATE: 42825  \nFailed to merge incompatible data types <left> and <right>. Please check the data types of the columns being merged and ensure that they are compatible. If necessary, consider casting the columns to compatible data types before attempting the merge.  \nCANNOT_MERGE_SCHEMAS  \nSQLSTATE: 42KD9  \nFailed merging schemas:  \nInitial schema:  \n<left>  \nSchema that cannot be merged with the initial schema:  \n<right>.  \nCANNOT_MODIFY_CONFIG  \nSQLSTATE: 46110  \nCannot modify the value of the Spark config: <key>.  \nSee also https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements\u2019.  \nCANNOT_PARSE_DECIMAL  \nSQLSTATE: 22018  \nCannot parse decimal. Please ensure that the input is a valid number with optional decimal point or comma separators.  \nCANNOT_PARSE_INTERVAL  \nSQLSTATE: 22006  \nUnable to parse <intervalString>. Please ensure that the value provided is in a valid format for defining an interval. You can reference the documentation for the correct format. If the issue persists, please double check that the input value is not null or empty and try again.  \nCANNOT_PARSE_JSON_FIELD  \nSQLSTATE: 2203G  \nCannot parse the field name <fieldName> and the value <fieldValue> of the JSON token type <jsonType> to target Spark data type <dataType>.  \nCANNOT_PARSE_PROTOBUF_DESCRIPTOR  \nSQLSTATE: 22018"
    },
    {
        "id": 611,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "CANNOT_PARSE_PROTOBUF_DESCRIPTOR  \nSQLSTATE: 22018  \nError parsing descriptor bytes into Protobuf FileDescriptorSet.  \nCANNOT_PARSE_TIMESTAMP  \nSQLSTATE: 22007  \n<message>. If necessary set <ansiConfig> to \u201cfalse\u201d to bypass this error.  \nCANNOT_QUERY_TABLE_DURING_INITIALIZATION  \nSQLSTATE: 55019  \nCannot query MV/ST during initialization.  \nFor more details see CANNOT_QUERY_TABLE_DURING_INITIALIZATION  \nCANNOT_READ_ARCHIVED_FILE  \nSQLSTATE: KD003  \nCannot read file at path <path> because it has been archived. Please adjust your query filters to exclude archived files.  \nCANNOT_READ_FILE  \nSQLSTATE: KD003  \nCannot read <format> file at path: <path>.  \nFor more details see CANNOT_READ_FILE  \nCANNOT_READ_SENSITIVE_KEY_FROM_SECURE_PROVIDER  \nSQLSTATE: 42501  \nCannot read sensitive key \u2018<key>\u2019 from secure provider.  \nCANNOT_RECOGNIZE_HIVE_TYPE  \nSQLSTATE: 429BB  \nCannot recognize hive type string: <fieldType>, column: <fieldName>. The specified data type for the field cannot be recognized by Spark SQL. Please check the data type of the specified field and ensure that it is a valid Spark SQL data type. Refer to the Spark SQL documentation for a list of valid data types and their format. If the data type is correct, please ensure that you are using a supported version of Spark SQL.  \nCANNOT_REFERENCE_UC_IN_HMS  \nSQLSTATE: 0AKD0  \nCannot reference a Unity Catalog <objType> in Hive Metastore objects.  \nCANNOT_RENAME_ACROSS_CATALOG  \nSQLSTATE: 0AKD0  \nRenaming a <type> across catalogs is not allowed.  \nCANNOT_RENAME_ACROSS_SCHEMA  \nSQLSTATE: 0AKD0  \nRenaming a <type> across schemas is not allowed.  \nCANNOT_RESOLVE_DATAFRAME_COLUMN  \nSQLSTATE: 42704"
    },
    {
        "id": 612,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 0AKD0  \nRenaming a <type> across schemas is not allowed.  \nCANNOT_RESOLVE_DATAFRAME_COLUMN  \nSQLSTATE: 42704  \nCannot resolve dataframe column <name>. It\u2019s probably because of illegal references like df1.select(df2.col(\"a\")).  \nCANNOT_RESOLVE_STAR_EXPAND  \nSQLSTATE: 42704  \nCannot resolve <targetString>.* given input columns <columns>. Please check that the specified table or struct exists and is accessible in the input columns.  \nCANNOT_RESTORE_PERMISSIONS_FOR_PATH  \nSQLSTATE: 58030  \nFailed to set permissions on created path <path> back to <permission>.  \nCANNOT_SAVE_VARIANT  \nSQLSTATE: 0A000  \nCannot save variant data type into external storage.  \nCANNOT_SHALLOW_CLONE_ACROSS_UC_AND_HMS  \nSQLSTATE: 0AKD0  \nCannot shallow-clone tables across Unity Catalog and Hive Metastore.  \nCANNOT_SHALLOW_CLONE_NESTED  \nSQLSTATE: 0AKUC  \nCannot shallow-clone a table <table> that is already a shallow clone.  \nCANNOT_SHALLOW_CLONE_NON_UC_MANAGED_TABLE_AS_SOURCE_OR_TARGET  \nSQLSTATE: 0AKUC  \nShallow clone is only supported for the MANAGED table type. The table <table> is not MANAGED table.  \nCANNOT_UPDATE_FIELD  \nSQLSTATE: 0A000  \nCannot update <table> field <fieldName> type:  \nFor more details see CANNOT_UPDATE_FIELD  \nCANNOT_UP_CAST_DATATYPE  \nSQLSTATE: 42846  \nCannot up cast <expression> from <sourceType> to <targetType>.  \n<details>  \nCANNOT_VALIDATE_CONNECTION  \nSQLSTATE: 08000  \nValidation of <jdbcDialectName> connection is not supported. Please contact Databricks support for alternative solutions, or set \u201cspark.databricks.testConnectionBeforeCreation\u201d to \u201cfalse\u201d to skip connection testing before creating a connection object.  \nCANNOT_WRITE_STATE_STORE  \nSQLSTATE: 58030"
    },
    {
        "id": 613,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "CANNOT_WRITE_STATE_STORE  \nSQLSTATE: 58030  \nError writing state store files for provider <providerClass>.  \nFor more details see CANNOT_WRITE_STATE_STORE  \nCAST_INVALID_INPUT  \nSQLSTATE: 22018  \nThe value <expression> of the type <sourceType> cannot be cast to <targetType> because it is malformed. Correct the value as per the syntax, or change its target type. Use try_cast to tolerate malformed input and return NULL instead. If necessary set <ansiConfig> to \u201cfalse\u201d to bypass this error.  \nFor more details see CAST_INVALID_INPUT  \nCAST_OVERFLOW  \nSQLSTATE: 22003  \nThe value <value> of the type <sourceType> cannot be cast to <targetType> due to an overflow. Use try_cast to tolerate overflow and return NULL instead. If necessary set <ansiConfig> to \u201cfalse\u201d to bypass this error.  \nCAST_OVERFLOW_IN_TABLE_INSERT  \nSQLSTATE: 22003  \nFail to assign a value of <sourceType> type to the <targetType> type column or variable <columnName> due to an overflow. Use try_cast on the input value to tolerate overflow and return NULL instead.  \nCATALOG_NOT_FOUND  \nSQLSTATE: 42P08  \nThe catalog <catalogName> not found. Consider to set the SQL config <config> to a catalog plugin.  \nCHECKPOINT_RDD_BLOCK_ID_NOT_FOUND  \nSQLSTATE: 56000  \nCheckpoint block <rddBlockId> not found!  \nEither the executor that originally checkpointed this partition is no longer alive, or the original RDD is unpersisted.  \nIf this problem persists, you may consider using rdd.checkpoint() instead, which is slower than local checkpointing but more fault-tolerant.  \nCLASS_NOT_OVERRIDE_EXPECTED_METHOD  \nSQLSTATE: 38000  \n<className> must override either <method1> or <method2>.  \nCLASS_UNSUPPORTED_BY_MAP_OBJECTS  \nSQLSTATE: 0A000  \nMapObjects does not support the class <cls> as resulting collection.  \nCLEANROOM_COMMANDS_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nClean Room commands are not supported"
    },
    {
        "id": 614,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 0A000  \nMapObjects does not support the class <cls> as resulting collection.  \nCLEANROOM_COMMANDS_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nClean Room commands are not supported  \nCLEANROOM_INVALID_SHARED_DATA_OBJECT_NAME  \nSQLSTATE: 42K05  \nInvalid name to reference a <type> inside a Clean Room. Use a <type>\u2019s name inside the clean room following the format of [catalog].[schema].[<type>].  \nIf you are unsure about what name to use, you can run \u201cSHOW ALL IN CLEANROOM [clean_room]\u201d and use the value in the \u201cname\u201d column.  \nCLOUD_FILE_SOURCE_FILE_NOT_FOUND  \nSQLSTATE: 42K03  \nA file notification was received for file: <filePath> but it does not exist anymore. Please ensure that files are not deleted before they are processed. To continue your stream, you can set the Spark SQL configuration <config> to true.  \nCLOUD_PROVIDER_ERROR  \nSQLSTATE: 58000  \nCloud provider error: <message>  \nCLUSTER_BY_AUTO_FEATURE_NOT_ENABLED  \nSQLSTATE: 0A000  \nPlease contact your Databricks representative to enable the cluster-by-auto feature.  \nCLUSTER_BY_AUTO_REQUIRES_CLUSTERING_FEATURE_ENABLED  \nSQLSTATE: 56038  \nPlease enable clusteringTable.enableClusteringTableFeature to use CLUSTER BY AUTO.  \nCLUSTER_BY_AUTO_REQUIRES_PREDICTIVE_OPTIMIZATION  \nSQLSTATE: 56038  \nCLUSTER BY AUTO requires Predictive Optimization to be enabled.  \nCLUSTER_BY_AUTO_UNSUPPORTED_TABLE_TYPE_ERROR  \nSQLSTATE: 56038  \nCLUSTER BY AUTO is only supported on UC Managed tables.  \nCODEC_NOT_AVAILABLE  \nSQLSTATE: 56038  \nThe codec <codecName> is not available.  \nFor more details see CODEC_NOT_AVAILABLE  \nCODEC_SHORT_NAME_NOT_FOUND  \nSQLSTATE: 42704  \nCannot find a short name for the codec <codecName>.  \nCOLLATION_INVALID_NAME  \nSQLSTATE: 42704"
    },
    {
        "id": 615,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 42704  \nCannot find a short name for the codec <codecName>.  \nCOLLATION_INVALID_NAME  \nSQLSTATE: 42704  \nThe value <collationName> does not represent a correct collation name. Suggested valid collation names: [<proposals>].  \nCOLLATION_INVALID_PROVIDER  \nSQLSTATE: 42704  \nThe value <provider> does not represent a correct collation provider. Supported providers are: [<supportedProviders>].  \nCOLLATION_MISMATCH  \nSQLSTATE: 42P21  \nCould not determine which collation to use for string functions and operators.  \nFor more details see COLLATION_MISMATCH  \nCOLLECTION_SIZE_LIMIT_EXCEEDED  \nSQLSTATE: 54000  \nCan\u2019t create array with <numberOfElements> elements which exceeding the array size limit <maxRoundedArrayLength>,  \nFor more details see COLLECTION_SIZE_LIMIT_EXCEEDED  \nCOLUMN_ALIASES_NOT_ALLOWED  \nSQLSTATE: 42601  \nColumn aliases are not allowed in <op>.  \nCOLUMN_ALREADY_EXISTS  \nSQLSTATE: 42711  \nThe column <columnName> already exists. Choose another name or rename the existing column.  \nCOLUMN_MASKS_CHECK_CONSTRAINT_UNSUPPORTED  \nSQLSTATE: 0A000  \nCreating CHECK constraint on table <tableName> with column mask policies is not supported.  \nCOLUMN_MASKS_DUPLICATE_USING_COLUMN_NAME  \nSQLSTATE: 42734  \nA <statementType> statement attempted to assign a column mask policy to a column which included two or more other referenced columns in the USING COLUMNS list with the same name <columnName>, which is invalid.  \nCOLUMN_MASKS_FEATURE_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nColumn mask policies for <tableName> are not supported:  \nFor more details see COLUMN_MASKS_FEATURE_NOT_SUPPORTED  \nCOLUMN_MASKS_INCOMPATIBLE_SCHEMA_CHANGE  \nSQLSTATE: 0A000  \nUnable to <statementType> <columnName> from table <tableName> because it\u2019s referenced in a column mask policy for column <maskedColumn>. The table owner must remove or alter this policy before proceeding.  \nCOLUMN_MASKS_MERGE_UNSUPPORTED_SOURCE"
    },
    {
        "id": 616,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "COLUMN_MASKS_MERGE_UNSUPPORTED_SOURCE  \nSQLSTATE: 0A000  \nMERGE INTO operations do not support column mask policies in source table <tableName>.  \nCOLUMN_MASKS_MERGE_UNSUPPORTED_TARGET  \nSQLSTATE: 0A000  \nMERGE INTO operations do not support writing into table <tableName> with column mask policies.  \nCOLUMN_MASKS_MULTI_PART_TARGET_COLUMN_NAME  \nSQLSTATE: 42K05  \nThis statement attempted to assign a column mask policy to a column <columnName> with multiple name parts, which is invalid.  \nCOLUMN_MASKS_MULTI_PART_USING_COLUMN_NAME  \nSQLSTATE: 42K05  \nThis statement attempted to assign a column mask policy to a column and the USING COLUMNS list included the name <columnName> with multiple name parts, which is invalid.  \nCOLUMN_MASKS_NOT_ENABLED  \nSQLSTATE: 56038  \nSupport for defining column masks is not enabled  \nCOLUMN_MASKS_REQUIRE_UNITY_CATALOG  \nSQLSTATE: 0A000  \nColumn mask policies are only supported in Unity Catalog.  \nCOLUMN_MASKS_TABLE_CLONE_SOURCE_NOT_SUPPORTED  \nSQLSTATE: 0A000  \n<mode> clone from table <tableName> with column mask policies is not supported.  \nCOLUMN_MASKS_TABLE_CLONE_TARGET_NOT_SUPPORTED  \nSQLSTATE: 0A000  \n<mode> clone to table <tableName> with column mask policies is not supported.  \nCOLUMN_MASKS_UNSUPPORTED_CONSTANT_AS_PARAMETER  \nSQLSTATE: 0AKD1  \nUsing a constant as a parameter in a column mask policy is not supported. Please update your SQL command to remove the constant from the column mask definition and then retry the command again.  \nCOLUMN_MASKS_UNSUPPORTED_PROVIDER  \nSQLSTATE: 0A000  \nFailed to execute <statementType> command because assigning column mask policies is not supported for target data source with table provider: \u201c<provider>\u201d.  \nCOLUMN_MASKS_UNSUPPORTED_SUBQUERY  \nSQLSTATE: 0A000  \nCannot perform <operation> for table <tableName> because it contains one or more column mask policies with subquery expression(s), which are not yet supported. Please contact the owner of the table to update the column mask policies in order to continue."
    },
    {
        "id": 617,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "COLUMN_MASKS_USING_COLUMN_NAME_SAME_AS_TARGET_COLUMN  \nSQLSTATE: 42734  \nThe column <columnName> had the same name as the target column, which is invalid; please remove the column from the USING COLUMNS list and retry the command.  \nCOLUMN_NOT_DEFINED_IN_TABLE  \nSQLSTATE: 42703  \n<colType> column <colName> is not defined in table <tableName>, defined table columns are: <tableCols>.  \nCOLUMN_NOT_FOUND  \nSQLSTATE: 42703  \nThe column <colName> cannot be found. Verify the spelling and correctness of the column name according to the SQL config <caseSensitiveConfig>.  \nCOMMA_PRECEDING_CONSTRAINT_ERROR  \nSQLSTATE: 42601  \nUnexpected \u2018,\u2019 before constraint(s) definition. Ensure that the constraint clause does not start with a comma when columns (and expectations) are not defined.  \nCOMMENT_ON_CONNECTION_NOT_IMPLEMENTED_YET  \nSQLSTATE: 42000  \nThe COMMENT ON CONNECTION command is not implemented yet  \nCOMPARATOR_RETURNS_NULL  \nSQLSTATE: 22004  \nThe comparator has returned a NULL for a comparison between <firstValue> and <secondValue>.  \nIt should return a positive integer for \u201cgreater than\u201d, 0 for \u201cequal\u201d and a negative integer for \u201cless than\u201d.  \nTo revert to deprecated behavior where NULL is treated as 0 (equal), you must set \u201cspark.sql.legacy.allowNullComparisonResultInArraySort\u201d to \u201ctrue\u201d.  \nCOMPLEX_EXPRESSION_UNSUPPORTED_INPUT  \nSQLSTATE: 42K09  \nCannot process input data types for the expression: <expression>.  \nFor more details see COMPLEX_EXPRESSION_UNSUPPORTED_INPUT  \nCONCURRENT_QUERY  \nSQLSTATE: 0A000  \nAnother instance of this query [id: <queryId>] was just started by a concurrent session [existing runId: <existingQueryRunId> new runId: <newQueryRunId>].  \nCONCURRENT_STREAM_LOG_UPDATE  \nSQLSTATE: 40000  \nConcurrent update to the log. Multiple streaming jobs detected for <batchId>."
    },
    {
        "id": 618,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "CONCURRENT_STREAM_LOG_UPDATE  \nSQLSTATE: 40000  \nConcurrent update to the log. Multiple streaming jobs detected for <batchId>.  \nPlease make sure only one streaming job runs on a specific checkpoint location at a time.  \nCONFIG_NOT_AVAILABLE  \nSQLSTATE: 42K0I  \nConfiguration <config> is not available.  \nCONFLICTING_PROVIDER  \nSQLSTATE: 22023  \nThe specified provider <provider> is inconsistent with the existing catalog provider <expectedProvider>. Please use \u2018USING <expectedProvider>\u2019 and retry the command.  \nCONNECT  \nSQLSTATE: 56K00  \nGeneric Spark Connect error.  \nFor more details see CONNECT  \nCONNECTION_ALREADY_EXISTS  \nSQLSTATE: 42000  \nCannot create connection <connectionName> because it already exists.  \nChoose a different name, drop or replace the existing connection, or add the IF NOT EXISTS clause to tolerate pre-existing connections.  \nCONNECTION_NAME_CANNOT_BE_EMPTY  \nSQLSTATE: 42000  \nCannot execute this command because the connection name must be non-empty.  \nCONNECTION_NOT_FOUND  \nSQLSTATE: 42000  \nCannot execute this command because the connection name <connectionName> was not found.  \nCONNECTION_OPTION_NOT_SUPPORTED  \nSQLSTATE: 42000  \nConnections of type \u2018<connectionType>\u2019 do not support the following option(s): <optionsNotSupported>. Supported options: <allowedOptions>.  \nCONNECTION_TYPE_NOT_SUPPORTED  \nSQLSTATE: 42000  \nCannot create connection of type \u2018<connectionType>. Supported connection types: <allowedTypes>.  \nCONSTRAINTS_REQUIRE_UNITY_CATALOG  \nSQLSTATE: 0A000  \nTable constraints are only supported in Unity Catalog.  \nCONVERSION_INVALID_INPUT  \nSQLSTATE: 22018  \nThe value <str> (<fmt>) cannot be converted to <targetType> because it is malformed. Correct the value as per the syntax, or change its format. Use <suggestion> to tolerate malformed input and return NULL instead.  \nCOPY_INTO_COLUMN_ARITY_MISMATCH  \nSQLSTATE: 21S01  \nCannot write to <tableName>, the reason is  \nFor more details see COPY_INTO_COLUMN_ARITY_MISMATCH  \nCOPY_INTO_CREDENTIALS_NOT_ALLOWED_ON"
    },
    {
        "id": 619,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 21S01  \nCannot write to <tableName>, the reason is  \nFor more details see COPY_INTO_COLUMN_ARITY_MISMATCH  \nCOPY_INTO_CREDENTIALS_NOT_ALLOWED_ON  \nSQLSTATE: 0A000  \nInvalid scheme <scheme>. COPY INTO source credentials currently only supports s3/s3n/s3a/wasbs/abfss.  \nCOPY_INTO_CREDENTIALS_REQUIRED  \nSQLSTATE: 42601  \nCOPY INTO source credentials must specify <keyList>.  \nCOPY_INTO_DUPLICATED_FILES_COPY_NOT_ALLOWED  \nSQLSTATE: 25000  \nDuplicated files were committed in a concurrent COPY INTO operation. Please try again later.  \nCOPY_INTO_ENCRYPTION_NOT_ALLOWED_ON  \nSQLSTATE: 0A000  \nInvalid scheme <scheme>. COPY INTO source encryption currently only supports s3/s3n/s3a/abfss.  \nCOPY_INTO_ENCRYPTION_NOT_SUPPORTED_FOR_AZURE  \nSQLSTATE: 0A000  \nCOPY INTO encryption only supports ADLS Gen2, or abfss:// file scheme  \nCOPY_INTO_ENCRYPTION_REQUIRED  \nSQLSTATE: 42601  \nCOPY INTO source encryption must specify \u2018<key>\u2019.  \nCOPY_INTO_ENCRYPTION_REQUIRED_WITH_EXPECTED  \nSQLSTATE: 42601  \nInvalid encryption option <requiredKey>. COPY INTO source encryption must specify \u2018<requiredKey>\u2019 = \u2018<keyValue>\u2019.  \nCOPY_INTO_FEATURE_INCOMPATIBLE_SETTING  \nSQLSTATE: 42613  \nThe COPY INTO feature \u2018<feature>\u2019 is not compatible with \u2018<incompatibleSetting>\u2019.  \nCOPY_INTO_NON_BLIND_APPEND_NOT_ALLOWED  \nSQLSTATE: 25000  \nCOPY INTO other than appending data is not allowed to run concurrently with other transactions. Please try again later.  \nCOPY_INTO_ROCKSDB_MAX_RETRY_EXCEEDED  \nSQLSTATE: 25000  \nCOPY INTO failed to load its state, maximum retries exceeded.  \nCOPY_INTO_SCHEMA_MISMATCH_WITH_TARGET_TABLE  \nSQLSTATE: 42KDG  \nA schema mismatch was detected while copying into the Delta table (Table: <table>)."
    },
    {
        "id": 620,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "COPY_INTO_SCHEMA_MISMATCH_WITH_TARGET_TABLE  \nSQLSTATE: 42KDG  \nA schema mismatch was detected while copying into the Delta table (Table: <table>).  \nThis may indicate an issue with the incoming data, or the Delta table schema can be evolved automatically according to the incoming data by setting:  \nCOPY_OPTIONS (\u2018mergeSchema\u2019 = \u2018true\u2019)  \nSchema difference:  \n<schemaDiff>  \nCOPY_INTO_SOURCE_FILE_FORMAT_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nThe format of the source files must be one of CSV, JSON, AVRO, ORC, PARQUET, TEXT, or BINARYFILE. Using COPY INTO on Delta tables as the source is not supported as duplicate data may be ingested after OPTIMIZE operations. This check can be turned off by running the SQL command set spark.databricks.delta.copyInto.formatCheck.enabled = false.  \nCOPY_INTO_SOURCE_SCHEMA_INFERENCE_FAILED  \nSQLSTATE: 42KD9  \nThe source directory did not contain any parsable files of type <format>. Please check the contents of \u2018<source>\u2019.  \nThe error can be silenced by setting \u2018<config>\u2019 to \u2018false\u2019.  \nCOPY_INTO_STATE_INTERNAL_ERROR  \nSQLSTATE: 55019  \nAn internal error occurred while processing COPY INTO state.  \nFor more details see COPY_INTO_STATE_INTERNAL_ERROR  \nCOPY_INTO_SYNTAX_ERROR  \nSQLSTATE: 42601  \nFailed to parse the COPY INTO command.  \nFor more details see COPY_INTO_SYNTAX_ERROR  \nCOPY_INTO_UNSUPPORTED_FEATURE  \nSQLSTATE: 0A000  \nThe COPY INTO feature \u2018<feature>\u2019 is not supported.  \nCREATE_FOREIGN_SCHEMA_NOT_IMPLEMENTED_YET  \nSQLSTATE: 42000  \nThe CREATE FOREIGN SCHEMA command is not implemented yet  \nCREATE_FOREIGN_TABLE_NOT_IMPLEMENTED_YET  \nSQLSTATE: 42000  \nThe CREATE FOREIGN TABLE command is not implemented yet  \nCREATE_OR_REFRESH_MV_ST_ASYNC  \nSQLSTATE: 0A000"
    },
    {
        "id": 621,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "CREATE_FOREIGN_TABLE_NOT_IMPLEMENTED_YET  \nSQLSTATE: 42000  \nThe CREATE FOREIGN TABLE command is not implemented yet  \nCREATE_OR_REFRESH_MV_ST_ASYNC  \nSQLSTATE: 0A000  \nCannot CREATE OR REFRESH Materialized Views or Streaming Tables with ASYNC specified. Please remove ASYNC from the CREATE OR REFRESH statement or use REFRESH ASYNC to refresh existing Materialized Views or Streaming Tables asynchronously.  \nCREATE_PERMANENT_VIEW_WITHOUT_ALIAS  \nSQLSTATE: 0A000  \nNot allowed to create the permanent view <name> without explicitly assigning an alias for the expression <attr>.  \nCREATE_TABLE_COLUMN_DESCRIPTOR_DUPLICATE  \nSQLSTATE: 42710  \nCREATE TABLE column <columnName> specifies descriptor \u201c<optionName>\u201d more than once, which is invalid.  \nCREATE_VIEW_COLUMN_ARITY_MISMATCH  \nSQLSTATE: 21S01  \nCannot create view <viewName>, the reason is  \nFor more details see CREATE_VIEW_COLUMN_ARITY_MISMATCH  \nCREDENTIAL_MISSING  \nSQLSTATE: 42601  \nPlease provide credentials when creating or updating external locations.  \nCSV_ENFORCE_SCHEMA_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nThe CSV option enforceSchema cannot be set when using rescuedDataColumn or failOnUnknownFields, as columns are read by name rather than ordinal.  \nCYCLIC_FUNCTION_REFERENCE  \nSQLSTATE: 42887  \nCyclic function reference detected: <path>.  \nDATABRICKS_DELTA_NOT_ENABLED  \nSQLSTATE: 56038  \nDatabricks Delta is not enabled in your account.<hints>  \nDATATYPE_MISMATCH  \nSQLSTATE: 42K09  \nCannot resolve <sqlExpr> due to data type mismatch:  \nFor more details see DATATYPE_MISMATCH  \nDATATYPE_MISSING_SIZE  \nSQLSTATE: 42K01  \nDataType <type> requires a length parameter, for example <type>(10). Please specify the length.  \nDATA_LINEAGE_SECURE_VIEW_LEAF_NODE_HAS_NO_RELATION  \nSQLSTATE: 25000  \nWrite Lineage unsuccessful: missing corresponding relation with policies for CLM/RLS."
    },
    {
        "id": 622,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "DATA_LINEAGE_SECURE_VIEW_LEAF_NODE_HAS_NO_RELATION  \nSQLSTATE: 25000  \nWrite Lineage unsuccessful: missing corresponding relation with policies for CLM/RLS.  \nDATA_SOURCE_ALREADY_EXISTS  \nSQLSTATE: 42710  \nData source \u2018<provider>\u2019 already exists. Please choose a different name for the new data source.  \nDATA_SOURCE_NOT_EXIST  \nSQLSTATE: 42704  \nData source \u2018<provider>\u2019 not found. Please make sure the data source is registered.  \nDATA_SOURCE_NOT_FOUND  \nSQLSTATE: 42K02  \nFailed to find the data source: <provider>. Make sure the provider name is correct and the package is properly registered and compatible with your Spark version.  \nDATA_SOURCE_OPTION_CONTAINS_INVALID_CHARACTERS  \nSQLSTATE: 42602  \nOption <option> must not be empty and should not contain invalid characters, query strings, or parameters.  \nDATA_SOURCE_OPTION_IS_REQUIRED  \nSQLSTATE: 42601  \nOption <option> is required.  \nDATA_SOURCE_TABLE_SCHEMA_MISMATCH  \nSQLSTATE: 42K03  \nThe schema of the data source table does not match the expected schema. If you are using the DataFrameReader.schema API or creating a table, avoid specifying the schema.  \nData Source schema: <dsSchema>  \nExpected schema: <expectedSchema>  \nDATA_SOURCE_URL_NOT_ALLOWED  \nSQLSTATE: 42KDB  \nJDBC URL is not allowed in data source options, please specify \u2018host\u2019, \u2018port\u2019, and \u2018database\u2019 options instead.  \nDATETIME_OVERFLOW  \nSQLSTATE: 22008  \nDatetime operation overflow: <operation>.  \nDC_API_QUOTA_EXCEEDED  \nSQLSTATE: KD000  \nYou have exceeded the API quota for the data source <sourceName>.  \nFor more details see DC_API_QUOTA_EXCEEDED  \nDC_CONNECTION_ERROR  \nSQLSTATE: KD000  \nFailed to make a connection to the <sourceName> source. Error code: <errorCode>.  \nFor more details see DC_CONNECTION_ERROR  \nDC_DYNAMICS_API_ERROR  \nSQLSTATE: KD000"
    },
    {
        "id": 623,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Failed to make a connection to the <sourceName> source. Error code: <errorCode>.  \nFor more details see DC_CONNECTION_ERROR  \nDC_DYNAMICS_API_ERROR  \nSQLSTATE: KD000  \nError happened in Dynamics API calls, errorCode: <errorCode>.  \nFor more details see DC_DYNAMICS_API_ERROR  \nDC_NETSUITE_ERROR  \nSQLSTATE: KD000  \nError happened in Netsuite JDBC calls, errorCode: <errorCode>.  \nFor more details see DC_NETSUITE_ERROR  \nDC_SCHEMA_CHANGE_ERROR  \nSQLSTATE: none assigned  \nA schema change has occurred in table <tableName> of the <sourceName> source.  \nFor more details see DC_SCHEMA_CHANGE_ERROR  \nDC_SERVICENOW_API_ERROR  \nSQLSTATE: KD000  \nError happened in ServiceNow API calls, errorCode: <errorCode>.  \nFor more details see DC_SERVICENOW_API_ERROR  \nDC_SFDC_BULK_QUERY_JOB_INCOMPLETE  \nSQLSTATE: KD000  \nIngestion for object <objName> is incomplete because the Salesforce API query job took too long, failed, or was manually cancelled.  \nTo try again, you can either re-run the entire pipeline or refresh this specific destination table. If the error persists, file a ticket. Job ID: <jobId>. Job status: <jobStatus>.  \nDC_SHAREPOINT_API_ERROR  \nSQLSTATE: KD000  \nError happened in Sharepoint API calls, errorCode: <errorCode>.  \nFor more details see DC_SHAREPOINT_API_ERROR  \nDC_SOURCE_API_ERROR  \nSQLSTATE: KD000  \nAn error occurred in the <sourceName> API call. Source API type: <apiType>. Error code: <errorCode>.  \nThis can sometimes happen when you\u2019ve reached a <sourceName> API limit. If you haven\u2019t exceeded your API limit, try re-running the connector. If the issue persists, please file a ticket.  \nDC_UNSUPPORTED_ERROR  \nSQLSTATE: 0A000  \nUnsupported error happened in data source <sourceName>."
    },
    {
        "id": 624,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "DC_UNSUPPORTED_ERROR  \nSQLSTATE: 0A000  \nUnsupported error happened in data source <sourceName>.  \nFor more details see DC_UNSUPPORTED_ERROR  \nDC_WORKDAY_RAAS_API_ERROR  \nSQLSTATE: KD000  \nError happened in Workday RAAS API calls, errorCode: <errorCode>.  \nFor more details see DC_WORKDAY_RAAS_API_ERROR  \nDECIMAL_PRECISION_EXCEEDS_MAX_PRECISION  \nSQLSTATE: 22003  \nDecimal precision <precision> exceeds max precision <maxPrecision>.  \nDEFAULT_DATABASE_NOT_EXISTS  \nSQLSTATE: 42704  \nDefault database <defaultDatabase> does not exist, please create it first or change default database to <defaultDatabase>.  \nDEFAULT_FILE_NOT_FOUND  \nSQLSTATE: 42K03  \nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running \u2018REFRESH TABLE tableName\u2019 command in SQL or by recreating the Dataset/DataFrame involved. If disk cache is stale or the underlying files have been removed, you can invalidate disk cache manually by restarting the cluster.  \nDEFAULT_PLACEMENT_INVALID  \nSQLSTATE: 42608  \nA DEFAULT keyword in a MERGE, INSERT, UPDATE, or SET VARIABLE command could not be directly assigned to a target column because it was part of an expression.  \nFor example: UPDATE SET c1 = DEFAULT is allowed, but UPDATE T SET c1 = DEFAULT + 1 is not allowed.  \nDIFFERENT_DELTA_TABLE_READ_BY_STREAMING_SOURCE  \nSQLSTATE: 55019  \nThe streaming query was reading from an unexpected Delta table (id = \u2018<newTableId>\u2019).  \nIt used to read from another Delta table (id = \u2018<oldTableId>\u2019) according to checkpoint.  \nThis may happen when you changed the code to read from a new table or you deleted and  \nre-created a table. Please revert your change or delete your streaming query checkpoint  \nto restart from scratch.  \nDISTINCT_WINDOW_FUNCTION_UNSUPPORTED  \nSQLSTATE: 0A000  \nDistinct window functions are not supported: <windowExpr>.  \nDIVIDE_BY_ZERO  \nSQLSTATE: 22012"
    },
    {
        "id": 625,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "to restart from scratch.  \nDISTINCT_WINDOW_FUNCTION_UNSUPPORTED  \nSQLSTATE: 0A000  \nDistinct window functions are not supported: <windowExpr>.  \nDIVIDE_BY_ZERO  \nSQLSTATE: 22012  \nDivision by zero. Use try_divide to tolerate divisor being 0 and return NULL instead. If necessary set <config> to \u201cfalse\u201d to bypass this error.  \nFor more details see DIVIDE_BY_ZERO  \nDLT_EXPECTATIONS_NOT_SUPPORTED  \nSQLSTATE: 56038  \nExpectations are only supported within a Delta Live Tables pipeline.  \nDLT_VIEW_CLUSTER_BY_NOT_SUPPORTED  \nSQLSTATE: 56038  \nMATERIALIZED VIEWs with a CLUSTER BY clause are supported only in a Delta Live Tables pipeline.  \nDLT_VIEW_LOCATION_NOT_SUPPORTED  \nSQLSTATE: 56038  \nMATERIALIZED VIEW locations are supported only in a Delta Live Tables pipeline.  \nDLT_VIEW_SCHEMA_WITH_TYPE_NOT_SUPPORTED  \nSQLSTATE: 56038  \nMATERIALIZED VIEW schemas with a specified type are supported only in a Delta Live Tables pipeline.  \nDLT_VIEW_TABLE_CONSTRAINTS_NOT_SUPPORTED  \nSQLSTATE: 56038  \nCONSTRAINT clauses in a view are only supported in a Delta Live Tables pipeline.  \nDUPLICATED_FIELD_NAME_IN_ARROW_STRUCT  \nSQLSTATE: 42713  \nDuplicated field names in Arrow Struct are not allowed, got <fieldNames>.  \nDUPLICATED_MAP_KEY  \nSQLSTATE: 23505  \nDuplicate map key <key> was found, please check the input data.  \nIf you want to remove the duplicated keys, you can set <mapKeyDedupPolicy> to \u201cLAST_WIN\u201d so that the key inserted at last takes precedence.  \nDUPLICATED_METRICS_NAME  \nSQLSTATE: 42710  \nThe metric name is not unique: <metricName>. The same name cannot be used for metrics with different results.  \nHowever multiple instances of metrics with with same result and name are allowed (e.g. self-joins).  \nDUPLICATE_ASSIGNMENTS  \nSQLSTATE: 42701  \nThe columns or variables <nameList> appear more than once as assignment targets.  \nDUPLICATE_CLAUSES  \nSQLSTATE: 42614"
    },
    {
        "id": 626,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "DUPLICATE_ASSIGNMENTS  \nSQLSTATE: 42701  \nThe columns or variables <nameList> appear more than once as assignment targets.  \nDUPLICATE_CLAUSES  \nSQLSTATE: 42614  \nFound duplicate clauses: <clauseName>. Please, remove one of them.  \nDUPLICATE_KEY  \nSQLSTATE: 23505  \nFound duplicate keys <keyColumn>.  \nDUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT  \nSQLSTATE: 4274K  \nCall to routine <functionName> is invalid because it includes multiple argument assignments to the same parameter name <parameterName>.  \nFor more details see DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT  \nDUPLICATE_ROUTINE_PARAMETER_NAMES  \nSQLSTATE: 42734  \nFound duplicate name(s) in the parameter list of the user-defined routine <routineName>: <names>.  \nDUPLICATE_ROUTINE_RETURNS_COLUMNS  \nSQLSTATE: 42711  \nFound duplicate column(s) in the RETURNS clause column list of the user-defined routine <routineName>: <columns>.  \nEMITTING_ROWS_OLDER_THAN_WATERMARK_NOT_ALLOWED  \nSQLSTATE: 42815  \nPrevious node emitted a row with eventTime=<emittedRowEventTime> which is older than current_watermark_value=<currentWatermark>  \nThis can lead to correctness issues in the stateful operators downstream in the execution pipeline.  \nPlease correct the operator logic to emit rows after current global watermark value.  \nEMPTY_JSON_FIELD_VALUE  \nSQLSTATE: 42604  \nFailed to parse an empty string for data type <dataType>.  \nEMPTY_LOCAL_FILE_IN_STAGING_ACCESS_QUERY  \nSQLSTATE: 22023  \nEmpty local file in staging <operation> query  \nENCODER_NOT_FOUND  \nSQLSTATE: 42704  \nNot found an encoder of the type <typeName> to Spark SQL internal representation.  \nConsider to change the input type to one of supported at \u2018<docroot>/sql-ref-datatypes.html\u2019.  \nEND_OFFSET_HAS_GREATER_OFFSET_FOR_TOPIC_PARTITION_THAN_LATEST_WITH_TRIGGER_AVAILABLENOW  \nSQLSTATE: KD000"
    },
    {
        "id": 627,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "END_OFFSET_HAS_GREATER_OFFSET_FOR_TOPIC_PARTITION_THAN_LATEST_WITH_TRIGGER_AVAILABLENOW  \nSQLSTATE: KD000  \nSome of partitions in Kafka topic(s) report available offset which is less than end offset during running query with Trigger.AvailableNow. The error could be transient - restart your query, and report if you still see the same issue.  \nlatest offset: <latestOffset>, end offset: <endOffset>  \nEND_OFFSET_HAS_GREATER_OFFSET_FOR_TOPIC_PARTITION_THAN_PREFETCHED  \nSQLSTATE: KD000  \nFor Kafka data source with Trigger.AvailableNow, end offset should have lower or equal offset per each topic partition than pre-fetched offset. The error could be transient - restart your query, and report if you still see the same issue.  \npre-fetched offset: <prefetchedOffset>, end offset: <endOffset>.  \nERROR_READING_AVRO_UNKNOWN_FINGERPRINT  \nSQLSTATE: KD00B  \nError reading avro data \u2013 encountered an unknown fingerprint: <fingerprint>, not sure what schema to use.  \nThis could happen if you registered additional schemas after starting your spark context.  \nEVENT_LOG_REQUIRES_SHARED_COMPUTE  \nSQLSTATE: 42601  \nCannot query event logs from an Assigned or No Isolation Shared cluster, please use a Shared cluster or a Databricks SQL warehouse instead.  \nEVENT_LOG_UNAVAILABLE  \nSQLSTATE: 55019  \nNo event logs available for <tableOrPipeline>. Please try again later after events are generated  \nEVENT_LOG_UNSUPPORTED_TABLE_TYPE  \nSQLSTATE: 42832  \nThe table type of <tableIdentifier> is <tableType>.  \nQuerying event logs only supports Materialized Views, Streaming Tables, or Delta Live Tables pipelines  \nEVENT_TIME_IS_NOT_ON_TIMESTAMP_TYPE  \nSQLSTATE: 42K09  \nThe event time <eventName> has the invalid type <eventType>, but expected \u201cTIMESTAMP\u201d.  \nEXCEED_LIMIT_LENGTH  \nSQLSTATE: 54006  \nExceeds char/varchar type length limitation: <limit>.  \nEXCEPT_NESTED_COLUMN_INVALID_TYPE  \nSQLSTATE: 428H2"
    },
    {
        "id": 628,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "EXCEED_LIMIT_LENGTH  \nSQLSTATE: 54006  \nExceeds char/varchar type length limitation: <limit>.  \nEXCEPT_NESTED_COLUMN_INVALID_TYPE  \nSQLSTATE: 428H2  \nEXCEPT column <columnName> was resolved and expected to be StructType, but found type <dataType>.  \nEXCEPT_OVERLAPPING_COLUMNS  \nSQLSTATE: 42702  \nColumns in an EXCEPT list must be distinct and non-overlapping, but got (<columns>).  \nEXCEPT_RESOLVED_COLUMNS_WITHOUT_MATCH  \nSQLSTATE: 42703  \nEXCEPT columns [<exceptColumns>] were resolved, but do not match any of the columns [<expandedColumns>] from the star expansion.  \nEXCEPT_UNRESOLVED_COLUMN_IN_STRUCT_EXPANSION  \nSQLSTATE: 42703  \nThe column/field name <objectName> in the EXCEPT clause cannot be resolved. Did you mean one of the following: [<objectList>]?  \nNote: nested columns in the EXCEPT clause may not include qualifiers (table name, parent struct column name, etc.) during a struct expansion; try removing qualifiers if they are used with nested columns.  \nEXECUTOR_BROADCAST_JOIN_OOM  \nSQLSTATE: 53200  \nThere is not enough memory to build the broadcast relation <relationClassName>. Relation Size = <relationSize>. Build-side Shuffle Size = <shuffleSize>. Total memory used by this task = <taskMemoryUsage>. Executor Memory Manager Metrics: onHeapExecutionMemoryUsed = <onHeapExecutionMemoryUsed>, offHeapExecutionMemoryUsed = <offHeapExecutionMemoryUsed>, onHeapStorageMemoryUsed = <onHeapStorageMemoryUsed>, offHeapStorageMemoryUsed = <offHeapStorageMemoryUsed>. [shuffleId: <shuffleId>]  \nEXEC_IMMEDIATE_DUPLICATE_ARGUMENT_ALIASES  \nSQLSTATE: 42701  \nThe USING clause of this EXECUTE IMMEDIATE command contained multiple arguments with same alias (<aliases>), which is invalid; please update the command to specify unique aliases and then try it again."
    },
    {
        "id": 629,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 42701  \nThe USING clause of this EXECUTE IMMEDIATE command contained multiple arguments with same alias (<aliases>), which is invalid; please update the command to specify unique aliases and then try it again.  \nEXPECT_PERMANENT_VIEW_NOT_TEMP  \nSQLSTATE: 42809  \n\u2018<operation>\u2019 expects a permanent view but <viewName> is a temp view.  \nEXPECT_TABLE_NOT_VIEW  \nSQLSTATE: 42809  \n\u2018<operation>\u2019 expects a table but <viewName> is a view.  \nFor more details see EXPECT_TABLE_NOT_VIEW  \nEXPECT_VIEW_NOT_TABLE  \nSQLSTATE: 42809  \nThe table <tableName> does not support <operation>.  \nFor more details see EXPECT_VIEW_NOT_TABLE  \nEXPRESSION_DECODING_FAILED  \nSQLSTATE: 42846  \nFailed to decode a row to a value of the expressions: <expressions>.  \nEXPRESSION_ENCODING_FAILED  \nSQLSTATE: 42846  \nFailed to encode a value of the expressions: <expressions> to a row.  \nEXPRESSION_TYPE_IS_NOT_ORDERABLE  \nSQLSTATE: 42822  \nColumn expression <expr> cannot be sorted because its type <exprType> is not orderable.  \nEXTERNAL_TABLE_INVALID_SCHEME  \nSQLSTATE: 0A000  \nExternal tables don\u2019t support the <scheme> scheme.  \nFABRIC_REFRESH_INVALID_SCOPE  \nSQLSTATE: 0A000  \nError running \u2018REFRESH FOREIGN <scope> <name>\u2019. Cannot refresh a Fabric <scope> directly, please use \u2018REFRESH FOREIGN CATALOG <catalogName>\u2019 to refresh the Fabric Catalog instead.  \nFAILED_EXECUTE_UDF  \nSQLSTATE: 39000  \nUser defined function (<functionName>: (<signature>) => <result>) failed due to: <reason>.  \nFAILED_FUNCTION_CALL  \nSQLSTATE: 38000  \nFailed preparing of the function <funcName> for call. Please, double check function\u2019s arguments.  \nFAILED_JDBC  \nSQLSTATE: HV000  \nFailed JDBC <url> on the operation:  \nFor more details see FAILED_JDBC  \nFAILED_PARSE_STRUCT_TYPE  \nSQLSTATE: 22018  \nFailed parsing struct: <raw>."
    },
    {
        "id": 630,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Failed JDBC <url> on the operation:  \nFor more details see FAILED_JDBC  \nFAILED_PARSE_STRUCT_TYPE  \nSQLSTATE: 22018  \nFailed parsing struct: <raw>.  \nFAILED_READ_FILE  \nSQLSTATE: KD001  \nError while reading file <path>.  \nFor more details see FAILED_READ_FILE  \nFAILED_REGISTER_CLASS_WITH_KRYO  \nSQLSTATE: KD000  \nFailed to register classes with Kryo.  \nFAILED_RENAME_PATH  \nSQLSTATE: 42K04  \nFailed to rename <sourcePath> to <targetPath> as destination already exists.  \nFAILED_RENAME_TEMP_FILE  \nSQLSTATE: 58030  \nFailed to rename temp file <srcPath> to <dstPath> as FileSystem.rename returned false.  \nFAILED_ROW_TO_JSON  \nSQLSTATE: 2203G  \nFailed to convert the row value <value> of the class <class> to the target SQL type <sqlType> in the JSON format.  \nFAILED_TO_PARSE_TOO_COMPLEX  \nSQLSTATE: 54001  \nThe statement, including potential SQL functions and referenced views, was too complex to parse.  \nTo mitigate this error divide the statement into multiple, less complex chunks.  \nFEATURE_NOT_ENABLED  \nSQLSTATE: 56038  \nThe feature <featureName> is not enabled. Consider setting the config <configKey> to <configValue> to enable this capability.  \nFEATURE_NOT_ON_CLASSIC_WAREHOUSE  \nSQLSTATE: 56038  \n<feature> is not supported on Classic SQL warehouses. To use this feature, use a Pro or Serverless SQL warehouse. To learn more about warehouse types, see <docLink>  \nFEATURE_REQUIRES_UC  \nSQLSTATE: 0AKUD  \n<feature> is not supported without Unity Catalog. To use this feature, enable Unity Catalog. To learn more about Unity Catalog, see <docLink>  \nFEATURE_UNAVAILABLE  \nSQLSTATE: 56038  \n<feature> is not supported in your environment. To use this feature, please contact Databricks Support.  \nFIELDS_ALREADY_EXISTS  \nSQLSTATE: 42710"
    },
    {
        "id": 631,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "FEATURE_UNAVAILABLE  \nSQLSTATE: 56038  \n<feature> is not supported in your environment. To use this feature, please contact Databricks Support.  \nFIELDS_ALREADY_EXISTS  \nSQLSTATE: 42710  \nCannot <op> column, because <fieldNames> already exists in <struct>.  \nFIELD_NOT_FOUND  \nSQLSTATE: 42704  \nNo such struct field <fieldName> in <fields>.  \nFILE_IN_STAGING_PATH_ALREADY_EXISTS  \nSQLSTATE: 42K04  \nFile in staging path <path> already exists but OVERWRITE is not set  \nFORBIDDEN_OPERATION  \nSQLSTATE: 42809  \nThe operation <statement> is not allowed on the <objectType>: <objectName>.  \nFOREACH_BATCH_USER_FUNCTION_ERROR  \nSQLSTATE: 39000  \nAn error occurred in the user provided function in foreach batch sink. Reason: <reason>  \nFOREIGN_KEY_MISMATCH  \nSQLSTATE: 42830  \nForeign key parent columns <parentColumns> do not match primary key child columns <childColumns>.  \nFOREIGN_OBJECT_NAME_CANNOT_BE_EMPTY  \nSQLSTATE: 42000  \nCannot execute this command because the foreign <objectType> name must be non-empty.  \nFOUND_MULTIPLE_DATA_SOURCES  \nSQLSTATE: 42710  \nDetected multiple data sources with the name \u2018<provider>\u2019. Please check the data source isn\u2019t simultaneously registered and located in the classpath.  \nFROM_JSON_CONFLICTING_SCHEMA_UPDATES  \nSQLSTATE: 42601  \nfrom_json inference encountered conflicting schema updates at: <location>  \nFROM_JSON_CORRUPT_RECORD_COLUMN_IN_SCHEMA  \nSQLSTATE: 42601  \nfrom_json found columnNameOfCorruptRecord (<columnNameOfCorruptRecord>) present  \nin a JSON object and can no longer proceed. Please configure a different value for  \nthe option \u2018columnNameOfCorruptRecord\u2019.  \nFROM_JSON_CORRUPT_SCHEMA  \nSQLSTATE: 42601  \nfrom_json inference could not read the schema stored at: <location>  \nFROM_JSON_INFERENCE_FAILED  \nSQLSTATE: 42601"
    },
    {
        "id": 632,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 42601  \nfrom_json inference could not read the schema stored at: <location>  \nFROM_JSON_INFERENCE_FAILED  \nSQLSTATE: 42601  \nfrom_json was unable to infer the schema. Please provide one instead.  \nFROM_JSON_INFERENCE_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nfrom_json inference is only supported when defining streaming tables  \nFROM_JSON_INVALID_CONFIGURATION  \nSQLSTATE: 42601  \nfrom_json configuration is invalid:  \nFor more details see FROM_JSON_INVALID_CONFIGURATION  \nFROM_JSON_SCHEMA_EVOLUTION_FAILED  \nSQLSTATE: 22KD3  \nfrom_json could not evolve from <old> to <new>  \nFUNCTION_PARAMETERS_MUST_BE_NAMED  \nSQLSTATE: 07001  \nThe function <function> requires named parameters. Parameters missing names: <exprs>. Please update the function call to add names for all parameters, e.g., <function>(param_name => \u2026).  \nGENERATED_COLUMN_WITH_DEFAULT_VALUE  \nSQLSTATE: 42623  \nA column cannot have both a default value and a generation expression but column <colName> has default value: (<defaultValue>) and generation expression: (<genExpr>).  \nGET_TABLES_BY_TYPE_UNSUPPORTED_BY_HIVE_VERSION  \nSQLSTATE: 56038  \nHive 2.2 and lower versions don\u2019t support getTablesByType. Please use Hive 2.3 or higher version.  \nGET_WARMUP_TRACING_FAILED  \nSQLSTATE: 42601  \nFailed to get warmup tracing. Cause: <cause>.  \nGET_WARMUP_TRACING_FUNCTION_NOT_ALLOWED  \nSQLSTATE: 42601  \nFunction get_warmup_tracing() not allowed.  \nGRAPHITE_SINK_INVALID_PROTOCOL  \nSQLSTATE: KD000  \nInvalid Graphite protocol: <protocol>.  \nGRAPHITE_SINK_PROPERTY_MISSING  \nSQLSTATE: KD000  \nGraphite sink requires \u2018<property>\u2019 property.  \nGROUPING_COLUMN_MISMATCH  \nSQLSTATE: 42803  \nColumn of grouping (<grouping>) can\u2019t be found in grouping columns <groupingColumns>."
    },
    {
        "id": 633,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Graphite sink requires \u2018<property>\u2019 property.  \nGROUPING_COLUMN_MISMATCH  \nSQLSTATE: 42803  \nColumn of grouping (<grouping>) can\u2019t be found in grouping columns <groupingColumns>.  \nGROUPING_ID_COLUMN_MISMATCH  \nSQLSTATE: 42803  \nColumns of grouping_id (<groupingIdColumn>) does not match grouping columns (<groupByColumns>).  \nGROUPING_SIZE_LIMIT_EXCEEDED  \nSQLSTATE: 54000  \nGrouping sets size cannot be greater than <maxSize>.  \nGROUP_BY_AGGREGATE  \nSQLSTATE: 42903  \nAggregate functions are not allowed in GROUP BY, but found <sqlExpr>.  \nFor more details see GROUP_BY_AGGREGATE  \nGROUP_BY_POS_AGGREGATE  \nSQLSTATE: 42903  \nGROUP BY <index> refers to an expression <aggExpr> that contains an aggregate function. Aggregate functions are not allowed in GROUP BY.  \nGROUP_BY_POS_OUT_OF_RANGE  \nSQLSTATE: 42805  \nGROUP BY position <index> is not in select list (valid range is [1, <size>]).  \nGROUP_EXPRESSION_TYPE_IS_NOT_ORDERABLE  \nSQLSTATE: 42822  \nThe expression <sqlExpr> cannot be used as a grouping expression because its data type <dataType> is not an orderable data type.  \nHLL_INVALID_INPUT_SKETCH_BUFFER  \nSQLSTATE: 22546  \nInvalid call to <function>; only valid HLL sketch buffers are supported as inputs (such as those produced by the hll_sketch_agg function).  \nHLL_INVALID_LG_K  \nSQLSTATE: 22546  \nInvalid call to <function>; the lgConfigK value must be between <min> and <max>, inclusive: <value>.  \nHLL_UNION_DIFFERENT_LG_K  \nSQLSTATE: 22000  \nSketches have different lgConfigK values: <left> and <right>. Set the allowDifferentLgConfigK parameter to true to call <function> with different lgConfigK values.  \nIDENTIFIER_TOO_MANY_NAME_PARTS"
    },
    {
        "id": 634,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "IDENTIFIER_TOO_MANY_NAME_PARTS  \nSQLSTATE: 42601  \n<identifier> is not a valid identifier as it has more than 2 name parts.  \nILLEGAL_STATE_STORE_VALUE  \nSQLSTATE: 42601  \nIllegal value provided to the State Store  \nFor more details see ILLEGAL_STATE_STORE_VALUE  \nINCOMPARABLE_PIVOT_COLUMN  \nSQLSTATE: 42818  \nInvalid pivot column <columnName>. Pivot columns must be comparable.  \nINCOMPATIBLE_COLUMN_TYPE  \nSQLSTATE: 42825  \n<operator> can only be performed on tables with compatible column types. The <columnOrdinalNumber> column of the <tableOrdinalNumber> table is <dataType1> type which is not compatible with <dataType2> at the same column of the first table.<hint>.  \nINCOMPATIBLE_DATASOURCE_REGISTER  \nSQLSTATE: 56038  \nDetected an incompatible DataSourceRegister. Please remove the incompatible library from classpath or upgrade it. Error: <message>  \nINCOMPATIBLE_DATA_FOR_TABLE  \nSQLSTATE: KD000  \nCannot write incompatible data for the table <tableName>:  \nFor more details see INCOMPATIBLE_DATA_FOR_TABLE  \nINCOMPATIBLE_JOIN_TYPES  \nSQLSTATE: 42613  \nThe join types <joinType1> and <joinType2> are incompatible.  \nINCOMPATIBLE_VIEW_SCHEMA_CHANGE  \nSQLSTATE: 51024  \nThe SQL query of view <viewName> has an incompatible schema change and column <colName> cannot be resolved. Expected <expectedNum> columns named <colName> but got <actualCols>.  \nPlease try to re-create the view by running: <suggestion>.  \nINCOMPLETE_TYPE_DEFINITION  \nSQLSTATE: 42K01  \nIncomplete complex type:  \nFor more details see INCOMPLETE_TYPE_DEFINITION  \nINCONSISTENT_BEHAVIOR_CROSS_VERSION  \nSQLSTATE: 42K0B  \nYou may get a different result due to the upgrading to  \nFor more details see INCONSISTENT_BEHAVIOR_CROSS_VERSION  \nINCORRECT_NUMBER_OF_ARGUMENTS  \nSQLSTATE: 42605  \n<failure>, <functionName> requires at least <minArgs> arguments and at most <maxArgs> arguments."
    },
    {
        "id": 635,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "INCORRECT_NUMBER_OF_ARGUMENTS  \nSQLSTATE: 42605  \n<failure>, <functionName> requires at least <minArgs> arguments and at most <maxArgs> arguments.  \nINCORRECT_RAMP_UP_RATE  \nSQLSTATE: 22003  \nMax offset with <rowsPerSecond> rowsPerSecond is <maxSeconds>, but \u2018rampUpTimeSeconds\u2019 is <rampUpTimeSeconds>.  \nINDETERMINATE_COLLATION  \nSQLSTATE: 42P22  \nFunction called requires knowledge of the collation it should apply, but indeterminate collation was found. Use COLLATE function to set the collation explicitly.  \nINDEX_ALREADY_EXISTS  \nSQLSTATE: 42710  \nCannot create the index <indexName> on table <tableName> because it already exists.  \nINDEX_NOT_FOUND  \nSQLSTATE: 42704  \nCannot find the index <indexName> on table <tableName>.  \nINFINITE_STREAMING_TRIGGER_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nTrigger type <trigger> is not supported for this cluster type.  \nUse a different trigger type e.g. AvailableNow, Once.  \nINSERT_COLUMN_ARITY_MISMATCH  \nSQLSTATE: 21S01  \nCannot write to <tableName>, the reason is  \nFor more details see INSERT_COLUMN_ARITY_MISMATCH  \nINSERT_PARTITION_COLUMN_ARITY_MISMATCH  \nSQLSTATE: 21S01  \nCannot write to \u2018<tableName>\u2019, <reason>:  \nTable columns: <tableColumns>.  \nPartition columns with static values: <staticPartCols>.  \nData columns: <dataColumns>.  \nINSUFFICIENT_PERMISSIONS  \nSQLSTATE: 42501  \nInsufficient privileges:  \n<report>  \nINSUFFICIENT_PERMISSIONS_EXT_LOC  \nSQLSTATE: 42501  \nUser <user> has insufficient privileges for external location <location>.  \nINSUFFICIENT_PERMISSIONS_NO_OWNER  \nSQLSTATE: 42501  \nThere is no owner for <securableName>. Ask your administrator to set an owner.  \nINSUFFICIENT_PERMISSIONS_OWNERSHIP_SECURABLE  \nSQLSTATE: 42501  \nUser does not own <securableName>.  \nINSUFFICIENT_PERMISSIONS_SECURABLE  \nSQLSTATE: 42501"
    },
    {
        "id": 636,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "INSUFFICIENT_PERMISSIONS_OWNERSHIP_SECURABLE  \nSQLSTATE: 42501  \nUser does not own <securableName>.  \nINSUFFICIENT_PERMISSIONS_SECURABLE  \nSQLSTATE: 42501  \nUser does not have permission <action> on <securableName>.  \nINSUFFICIENT_PERMISSIONS_SECURABLE_PARENT_OWNER  \nSQLSTATE: 42501  \nThe owner of <securableName> is different from the owner of <parentSecurableName>.  \nINSUFFICIENT_PERMISSIONS_STORAGE_CRED  \nSQLSTATE: 42501  \nStorage credential <credentialName> has insufficient privileges.  \nINSUFFICIENT_PERMISSIONS_UNDERLYING_SECURABLES  \nSQLSTATE: 42501  \nUser cannot <action> on <securableName> because of permissions on underlying securables.  \nINSUFFICIENT_PERMISSIONS_UNDERLYING_SECURABLES_VERBOSE  \nSQLSTATE: 42501  \nUser cannot <action> on <securableName> because of permissions on underlying securables:  \n<underlyingReport>  \nINTERVAL_ARITHMETIC_OVERFLOW  \nSQLSTATE: 22015  \n<message>.<alternative>  \nINTERVAL_DIVIDED_BY_ZERO  \nSQLSTATE: 22012  \nDivision by zero. Use try_divide to tolerate divisor being 0 and return NULL instead.  \nINVALID_AGGREGATE_FILTER  \nSQLSTATE: 42903  \nThe FILTER expression <filterExpr> in an aggregate function is invalid.  \nFor more details see INVALID_AGGREGATE_FILTER  \nINVALID_ARRAY_INDEX  \nSQLSTATE: 22003  \nThe index <indexValue> is out of bounds. The array has <arraySize> elements. Use the SQL function get() to tolerate accessing element at invalid index and return NULL instead. If necessary set <ansiConfig> to \u201cfalse\u201d to bypass this error.  \nFor more details see INVALID_ARRAY_INDEX  \nINVALID_ARRAY_INDEX_IN_ELEMENT_AT  \nSQLSTATE: 22003  \nThe index <indexValue> is out of bounds. The array has <arraySize> elements. Use try_element_at to tolerate accessing element at invalid index and return NULL instead. If necessary set <ansiConfig> to \u201cfalse\u201d to bypass this error.  \nFor more details see INVALID_ARRAY_INDEX_IN_ELEMENT_AT"
    },
    {
        "id": 637,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "For more details see INVALID_ARRAY_INDEX_IN_ELEMENT_AT  \nINVALID_BITMAP_POSITION  \nSQLSTATE: 22003  \nThe 0-indexed bitmap position <bitPosition> is out of bounds. The bitmap has <bitmapNumBits> bits (<bitmapNumBytes> bytes).  \nINVALID_BOUNDARY  \nSQLSTATE: 22003  \nThe boundary <boundary> is invalid: <invalidValue>.  \nFor more details see INVALID_BOUNDARY  \nINVALID_BUCKET_COLUMN_DATA_TYPE  \nSQLSTATE: 42601  \nCannot use <type> for bucket column. Collated data types are not supported for bucketing.  \nINVALID_BUCKET_FILE  \nSQLSTATE: 58030  \nInvalid bucket file: <path>.  \nINVALID_BYTE_STRING  \nSQLSTATE: 22P03  \nThe expected format is ByteString, but was <unsupported> (<class>).  \nINVALID_COLUMN_NAME_AS_PATH  \nSQLSTATE: 46121  \nThe datasource <datasource> cannot save the column <columnName> because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.  \nINVALID_COLUMN_OR_FIELD_DATA_TYPE  \nSQLSTATE: 42000  \nColumn or field <name> is of type <type> while it\u2019s required to be <expectedType>.  \nINVALID_CONF_VALUE  \nSQLSTATE: 22022  \nThe value \u2018<confValue>\u2019 in the config \u201c<confName>\u201d is invalid.  \nFor more details see INVALID_CONF_VALUE  \nINVALID_CURRENT_RECIPIENT_USAGE  \nSQLSTATE: 42887  \ncurrent_recipient function can only be used in the CREATE VIEW statement or the ALTER VIEW statement to define a share only view in Unity Catalog.  \nINVALID_CURSOR  \nSQLSTATE: HY109  \nThe cursor is invalid.  \nFor more details see INVALID_CURSOR  \nINVALID_DATETIME_PATTERN  \nSQLSTATE: 22007  \nUnrecognized datetime pattern: <pattern>.  \nFor more details see INVALID_DATETIME_PATTERN  \nINVALID_DEFAULT_VALUE  \nSQLSTATE: 42623  \nFailed to execute <statement> command because the destination column or variable <colName> has a DEFAULT value <defaultValue>,"
    },
    {
        "id": 638,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "INVALID_DEFAULT_VALUE  \nSQLSTATE: 42623  \nFailed to execute <statement> command because the destination column or variable <colName> has a DEFAULT value <defaultValue>,  \nFor more details see INVALID_DEFAULT_VALUE  \nINVALID_DELIMITER_VALUE  \nSQLSTATE: 42602  \nInvalid value for delimiter.  \nFor more details see INVALID_DELIMITER_VALUE  \nINVALID_DEST_CATALOG  \nSQLSTATE: 42809  \nDestination catalog of the SYNC command must be within Unity Catalog. Found <catalog>.  \nINVALID_DRIVER_MEMORY  \nSQLSTATE: F0000  \nSystem memory <systemMemory> must be at least <minSystemMemory>.  \nPlease increase heap size using the \u2013driver-memory option or \u201c<config>\u201d in Spark configuration.  \nINVALID_EMPTY_LOCATION  \nSQLSTATE: 42K05  \nThe location name cannot be empty string, but <location> was given.  \nINVALID_ESC  \nSQLSTATE: 42604  \nFound an invalid escape string: <invalidEscape>. The escape string must contain only one character.  \nINVALID_ESCAPE_CHAR  \nSQLSTATE: 42604  \nEscapeChar should be a string literal of length one, but got <sqlExpr>.  \nINVALID_EXECUTOR_MEMORY  \nSQLSTATE: F0000  \nExecutor memory <executorMemory> must be at least <minSystemMemory>.  \nPlease increase executor memory using the \u2013executor-memory option or \u201c<config>\u201d in Spark configuration.  \nINVALID_EXPRESSION_ENCODER  \nSQLSTATE: 42001  \nFound an invalid expression encoder. Expects an instance of ExpressionEncoder but got <encoderType>. For more information consult \u2018<docroot>/api/java/index.html?org/apache/spark/sql/Encoder.html\u2019.  \nINVALID_EXTRACT_BASE_FIELD_TYPE  \nSQLSTATE: 42000  \nCan\u2019t extract a value from <base>. Need a complex type [STRUCT, ARRAY, MAP] but got <other>.  \nINVALID_EXTRACT_FIELD  \nSQLSTATE: 42601  \nCannot extract <field> from <expr>.  \nINVALID_EXTRACT_FIELD_TYPE  \nSQLSTATE: 42000"
    },
    {
        "id": 639,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "INVALID_EXTRACT_FIELD  \nSQLSTATE: 42601  \nCannot extract <field> from <expr>.  \nINVALID_EXTRACT_FIELD_TYPE  \nSQLSTATE: 42000  \nField name should be a non-null string literal, but it\u2019s <extraction>.  \nINVALID_FIELD_NAME  \nSQLSTATE: 42000  \nField name <fieldName> is invalid: <path> is not a struct.  \nINVALID_FORMAT  \nSQLSTATE: 42601  \nThe format is invalid: <format>.  \nFor more details see INVALID_FORMAT  \nINVALID_FRACTION_OF_SECOND  \nSQLSTATE: 22023  \nThe fraction of sec must be zero. Valid range is [0, 60]. If necessary set <ansiConfig> to \u201cfalse\u201d to bypass this error.  \nINVALID_HANDLE  \nSQLSTATE: HY000  \nThe handle <handle> is invalid.  \nFor more details see INVALID_HANDLE  \nINVALID_IDENTIFIER  \nSQLSTATE: 42602  \nThe unquoted identifier <ident> is invalid and must be back quoted as: <ident>.  \nUnquoted identifiers can only contain ASCII letters (\u2018a\u2019 - \u2018z\u2019, \u2018A\u2019 - \u2018Z\u2019), digits (\u20180\u2019 - \u20189\u2019), and underbar (\u2018_\u2019).  \nUnquoted identifiers must also not start with a digit.  \nDifferent data sources and meta stores may impose additional restrictions on valid identifiers.  \nINVALID_INDEX_OF_ZERO  \nSQLSTATE: 22003  \nThe index 0 is invalid. An index shall be either < 0 or > 0 (the first element has index 1).  \nINVALID_INLINE_TABLE  \nSQLSTATE: 42000  \nInvalid inline table.  \nFor more details see INVALID_INLINE_TABLE  \nINVALID_INTERVAL_FORMAT  \nSQLSTATE: 22006  \nError parsing \u2018<input>\u2019 to interval. Please ensure that the value provided is in a valid format for defining an interval. You can reference the documentation for the correct format.  \nFor more details see INVALID_INTERVAL_FORMAT  \nINVALID_INVERSE_DISTRIBUTION_FUNCTION  \nSQLSTATE: 42K0K  \nInvalid inverse distribution function <funcName>.  \nFor more details see INVALID_INVERSE_DISTRIBUTION_FUNCTION  \nINVALID_JSON_DATA_TYPE  \nSQLSTATE: 2203G"
    },
    {
        "id": 640,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 42K0K  \nInvalid inverse distribution function <funcName>.  \nFor more details see INVALID_INVERSE_DISTRIBUTION_FUNCTION  \nINVALID_JSON_DATA_TYPE  \nSQLSTATE: 2203G  \nFailed to convert the JSON string \u2018<invalidType>\u2019 to a data type. Please enter a valid data type.  \nINVALID_JSON_DATA_TYPE_FOR_COLLATIONS  \nSQLSTATE: 2203G  \nCollations can only be applied to string types, but the JSON data type is <jsonType>.  \nINVALID_JSON_ROOT_FIELD  \nSQLSTATE: 22032  \nCannot convert JSON root field to target Spark type.  \nINVALID_JSON_SCHEMA_MAP_TYPE  \nSQLSTATE: 22032  \nInput schema <jsonSchema> can only contain STRING as a key type for a MAP.  \nINVALID_KRYO_SERIALIZER_BUFFER_SIZE  \nSQLSTATE: F0000  \nThe value of the config \u201c<bufferSizeConfKey>\u201d must be less than 2048 MiB, but got <bufferSizeConfValue> MiB.  \nINVALID_LAMBDA_FUNCTION_CALL  \nSQLSTATE: 42K0D  \nInvalid lambda function call.  \nFor more details see INVALID_LAMBDA_FUNCTION_CALL  \nINVALID_LATERAL_JOIN_TYPE  \nSQLSTATE: 42613  \nThe <joinType> JOIN with LATERAL correlation is not allowed because an OUTER subquery cannot correlate to its join partner. Remove the LATERAL correlation or use an INNER JOIN, or LEFT OUTER JOIN instead.  \nINVALID_LIMIT_LIKE_EXPRESSION  \nSQLSTATE: 42K0E  \nThe limit like expression <expr> is invalid.  \nFor more details see INVALID_LIMIT_LIKE_EXPRESSION  \nINVALID_NON_DETERMINISTIC_EXPRESSIONS  \nSQLSTATE: 42K0E  \nThe operator expects a deterministic expression, but the actual expression is <sqlExprs>.  \nINVALID_NUMERIC_LITERAL_RANGE  \nSQLSTATE: 22003  \nNumeric literal <rawStrippedQualifier> is outside the valid range for <typeName> with minimum value of <minValue> and maximum value of <maxValue>. Please adjust the value accordingly.  \nINVALID_OBSERVED_METRICS"
    },
    {
        "id": 641,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "INVALID_OBSERVED_METRICS  \nSQLSTATE: 42K0E  \nInvalid observed metrics.  \nFor more details see INVALID_OBSERVED_METRICS  \nINVALID_OPTIONS  \nSQLSTATE: 42K06  \nInvalid options:  \nFor more details see INVALID_OPTIONS  \nINVALID_PANDAS_UDF_PLACEMENT  \nSQLSTATE: 0A000  \nThe group aggregate pandas UDF <functionList> cannot be invoked together with as other, non-pandas aggregate functions.  \nINVALID_PARAMETER_MARKER_VALUE  \nSQLSTATE: 22023  \nAn invalid parameter mapping was provided:  \nFor more details see INVALID_PARAMETER_MARKER_VALUE  \nINVALID_PARAMETER_VALUE  \nSQLSTATE: 22023  \nThe value of parameter(s) <parameter> in <functionName> is invalid:  \nFor more details see INVALID_PARAMETER_VALUE  \nINVALID_PARTITION_COLUMN_DATA_TYPE  \nSQLSTATE: 0A000  \nCannot use <type> for partition column.  \nINVALID_PARTITION_OPERATION  \nSQLSTATE: 42601  \nThe partition command is invalid.  \nFor more details see INVALID_PARTITION_OPERATION  \nINVALID_PIPELINE_ID  \nSQLSTATE: 42604  \nPipeline id <pipelineId> is not valid.  \nA pipeline id should be a UUID in the format of \u2018xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\u2019  \nINVALID_PRIVILEGE  \nSQLSTATE: 42852  \nPrivilege <privilege> is not valid for <securable>.  \nINVALID_PROPERTY_KEY  \nSQLSTATE: 42602  \n<key> is an invalid property key, please use quotes, e.g. SET <key>=<value>.  \nINVALID_PROPERTY_VALUE  \nSQLSTATE: 42602  \n<value> is an invalid property value, please use quotes, e.g. SET <key>=<value>  \nINVALID_QUERY_MIXED_QUERY_PARAMETERS  \nSQLSTATE: 42613  \nParameterized query must either use positional, or named parameters, but not both.  \nINVALID_S3_COPY_CREDENTIALS  \nSQLSTATE: 42501  \nCOPY INTO credentials must include AWS_ACCESS_KEY, AWS_SECRET_KEY, and AWS_SESSION_TOKEN.  \nINVALID_SAVE_MODE  \nSQLSTATE: 42000"
    },
    {
        "id": 642,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 42501  \nCOPY INTO credentials must include AWS_ACCESS_KEY, AWS_SECRET_KEY, and AWS_SESSION_TOKEN.  \nINVALID_SAVE_MODE  \nSQLSTATE: 42000  \nThe specified save mode <mode> is invalid. Valid save modes include \u201cappend\u201d, \u201coverwrite\u201d, \u201cignore\u201d, \u201cerror\u201d, \u201cerrorifexists\u201d, and \u201cdefault\u201d.  \nINVALID_SCHEMA  \nSQLSTATE: 42K07  \nThe input schema <inputSchema> is not a valid schema string.  \nFor more details see INVALID_SCHEMA  \nINVALID_SCHEMA_OR_RELATION_NAME  \nSQLSTATE: 42602  \n<name> is not a valid name for tables/schemas. Valid names only contain alphabet characters, numbers and _.  \nINVALID_SCHEME  \nSQLSTATE: 0AKUC  \nUnity catalog does not support <name> as the default file scheme.  \nINVALID_SECRET_LOOKUP  \nSQLSTATE: 22531  \nInvalid secret lookup:  \nFor more details see INVALID_SECRET_LOOKUP  \nINVALID_SET_SYNTAX  \nSQLSTATE: 42000  \nExpected format is \u2018SET\u2019, \u2018SET key\u2019, or \u2018SET key=value\u2019. If you want to include special characters in key, or include semicolon in value, please use backquotes, e.g., SET key=value.  \nINVALID_SHARED_ALIAS_NAME  \nSQLSTATE: 42601  \nThe <sharedObjectType> alias name must be of the form \u201cschema.name\u201d.  \nINVALID_SINGLE_VARIANT_COLUMN  \nSQLSTATE: 42613  \nThe singleVariantColumn option cannot be used if there is also a user specified schema.  \nINVALID_SOURCE_CATALOG  \nSQLSTATE: 42809  \nSource catalog must not be within Unity Catalog for the SYNC command. Found <catalog>.  \nINVALID_SQL_ARG  \nSQLSTATE: 42K08  \nThe argument <name> of sql() is invalid. Consider to replace it either by a SQL literal or by collection constructor functions such as map(), array(), struct().  \nINVALID_SQL_SYNTAX  \nSQLSTATE: 42000  \nInvalid SQL syntax:"
    },
    {
        "id": 643,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "INVALID_SQL_SYNTAX  \nSQLSTATE: 42000  \nInvalid SQL syntax:  \nFor more details see INVALID_SQL_SYNTAX  \nINVALID_STAGING_PATH_IN_STAGING_ACCESS_QUERY  \nSQLSTATE: 42604  \nInvalid staging path in staging <operation> query: <path>  \nINVALID_STATEMENT_FOR_EXECUTE_INTO  \nSQLSTATE: 07501  \nThe INTO clause of EXECUTE IMMEDIATE is only valid for queries but the given statement is not a query: <sqlString>.  \nINVALID_STATEMENT_OR_CLAUSE  \nSQLSTATE: 42601  \nThe statement or clause: <operation> is not valid.  \nINVALID_SUBQUERY_EXPRESSION  \nSQLSTATE: 42823  \nInvalid subquery:  \nFor more details see INVALID_SUBQUERY_EXPRESSION  \nINVALID_TEMP_OBJ_REFERENCE  \nSQLSTATE: 42K0F  \nCannot create the persistent object <objName> of the type <obj> because it references to the temporary object <tempObjName> of the type <tempObj>. Please make the temporary object <tempObjName> persistent, or make the persistent object <objName> temporary.  \nINVALID_TIMESTAMP_FORMAT  \nSQLSTATE: 22000  \nThe provided timestamp <timestamp> doesn\u2019t match the expected syntax <format>.  \nINVALID_TIME_TRAVEL_SPEC  \nSQLSTATE: 42K0E  \nCannot specify both version and timestamp when time travelling the table.  \nINVALID_TIME_TRAVEL_TIMESTAMP_EXPR  \nSQLSTATE: 42K0E  \nThe time travel timestamp expression <expr> is invalid.  \nFor more details see INVALID_TIME_TRAVEL_TIMESTAMP_EXPR  \nINVALID_TYPED_LITERAL  \nSQLSTATE: 42604  \nThe value of the typed literal <valueType> is invalid: <value>.  \nINVALID_UDF_IMPLEMENTATION  \nSQLSTATE: 38000  \nFunction <funcName> does not implement a ScalarFunction or AggregateFunction.  \nINVALID_UPGRADE_SYNTAX  \nSQLSTATE: 42809  \n<command> <supportedOrNot> the source table is in Hive Metastore and the destination table is in Unity Catalog.  \nINVALID_URL  \nSQLSTATE: 22P02"
    },
    {
        "id": 644,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 42809  \n<command> <supportedOrNot> the source table is in Hive Metastore and the destination table is in Unity Catalog.  \nINVALID_URL  \nSQLSTATE: 22P02  \nThe url is invalid: <url>. If necessary set <ansiConfig> to \u201cfalse\u201d to bypass this error.  \nINVALID_USAGE_OF_STAR_OR_REGEX  \nSQLSTATE: 42000  \nInvalid usage of <elem> in <prettyName>.  \nINVALID_UTF8_STRING  \nSQLSTATE: 22029  \nInvalid UTF8 byte sequence found in string: <str>.  \nINVALID_UUID  \nSQLSTATE: 42604  \nInput <uuidInput> is not a valid UUID.  \nThe UUID should be in the format of \u2018xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\u2019  \nPlease check the format of the UUID.  \nINVALID_VARIABLE_TYPE_FOR_QUERY_EXECUTE_IMMEDIATE  \nSQLSTATE: 42K09  \nVariable type must be string type but got <varType>.  \nINVALID_VARIANT_CAST  \nSQLSTATE: 22023  \nThe variant value <value> cannot be cast into <dataType>. Please use try_variant_get instead.  \nINVALID_VARIANT_FROM_PARQUET  \nSQLSTATE: 22023  \nInvalid variant.  \nFor more details see INVALID_VARIANT_FROM_PARQUET  \nINVALID_VARIANT_GET_PATH  \nSQLSTATE: 22023  \nThe path <path> is not a valid variant extraction path in <functionName>.  \nA valid path should start with $ and is followed by zero or more segments like [123], .name, ['name'], or [\"name\"].  \nINVALID_WHERE_CONDITION  \nSQLSTATE: 42903  \nThe WHERE condition <condition> contains invalid expressions: <expressionList>.  \nRewrite the query to avoid window functions, aggregate functions, and generator functions in the WHERE clause.  \nINVALID_WINDOW_SPEC_FOR_AGGREGATION_FUNC  \nSQLSTATE: 42601  \nCannot specify ORDER BY or a window frame for <aggFunc>.  \nINVALID_WRITER_COMMIT_MESSAGE"
    },
    {
        "id": 645,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "INVALID_WINDOW_SPEC_FOR_AGGREGATION_FUNC  \nSQLSTATE: 42601  \nCannot specify ORDER BY or a window frame for <aggFunc>.  \nINVALID_WRITER_COMMIT_MESSAGE  \nSQLSTATE: 42KDE  \nThe data source writer has generated an invalid number of commit messages. Expected exactly one writer commit message from each task, but received <detail>.  \nINVALID_WRITE_DISTRIBUTION  \nSQLSTATE: 42000  \nThe requested write distribution is invalid.  \nFor more details see INVALID_WRITE_DISTRIBUTION  \nJOIN_CONDITION_IS_NOT_BOOLEAN_TYPE  \nSQLSTATE: 42K0E  \nThe join condition <joinCondition> has the invalid type <conditionType>, expected \u201cBOOLEAN\u201d.  \nKAFKA_DATA_LOSS  \nSQLSTATE: 22000  \nSome data may have been lost because they are not available in Kafka any more;  \neither the data was aged out by Kafka or the topic may have been deleted before all the data in the  \ntopic was processed.  \nIf you don\u2019t want your streaming query to fail on such cases, set the source option failOnDataLoss to false.  \nReason:  \nFor more details see KAFKA_DATA_LOSS  \nKINESIS_COULD_NOT_READ_SHARD_UNTIL_END_OFFSET  \nSQLSTATE: 22000  \nCould not read until the desired sequence number <endSeqNum> for shard <shardId> in  \nkinesis stream <stream> with consumer mode <consumerMode>. The query will fail due to  \npotential data loss. The last read record was at sequence number <lastSeqNum>.  \nThis can happen if the data with endSeqNum has already been aged out, or the Kinesis stream was  \ndeleted and reconstructed with the same name. The failure behavior can be overridden  \nby setting spark.databricks.kinesis.failOnDataLoss to false in spark configuration.  \nKINESIS_FETCHED_SHARD_LESS_THAN_TRACKED_SHARD  \nSQLSTATE: 42K04  \nThe minimum fetched shardId from Kinesis (<fetchedShardId>)  \nis less than the minimum tracked shardId (<trackedShardId>).  \nThis is unexpected and occurs when a Kinesis stream is deleted and recreated with the same name,"
    },
    {
        "id": 646,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "is less than the minimum tracked shardId (<trackedShardId>).  \nThis is unexpected and occurs when a Kinesis stream is deleted and recreated with the same name,  \nand a streaming query using this Kinesis stream is restarted using an existing checkpoint location.  \nRestart the streaming query with a new checkpoint location, or create a stream with a new name.  \nKINESIS_RECORD_SEQ_NUMBER_ORDER_VIOLATION  \nSQLSTATE: 22000  \nFor shard <shard>, the last record read from Kinesis in previous fetches has sequence number <lastSeqNum>,  \nwhich is greater than the record read in current fetch with sequence number <recordSeqNum>.  \nThis is unexpected and can happen when the start position of retry or next fetch is incorrectly initialized, and may result in duplicate records downstream.  \nKRYO_BUFFER_OVERFLOW  \nSQLSTATE: 54006  \nKryo serialization failed: <exceptionMsg>. To avoid this, increase \u201c<bufferSizeConfKey>\u201d value.  \nLOAD_DATA_PATH_NOT_EXISTS  \nSQLSTATE: 42K03  \nLOAD DATA input path does not exist: <path>.  \nLOCAL_MUST_WITH_SCHEMA_FILE  \nSQLSTATE: 42601  \nLOCAL must be used together with the schema of file, but got: <actualSchema>.  \nLOCATION_ALREADY_EXISTS  \nSQLSTATE: 42710  \nCannot name the managed table as <identifier>, as its associated location <location> already exists. Please pick a different table name, or remove the existing location first.  \nLOST_TOPIC_PARTITIONS_IN_END_OFFSET_WITH_TRIGGER_AVAILABLENOW  \nSQLSTATE: KD000  \nSome of partitions in Kafka topic(s) have been lost during running query with Trigger.AvailableNow. The error could be transient - restart your query, and report if you still see the same issue.  \ntopic-partitions for latest offset: <tpsForLatestOffset>, topic-partitions for end offset: <tpsForEndOffset>  \nMALFORMED_AVRO_MESSAGE  \nSQLSTATE: KD000"
    },
    {
        "id": 647,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "topic-partitions for latest offset: <tpsForLatestOffset>, topic-partitions for end offset: <tpsForEndOffset>  \nMALFORMED_AVRO_MESSAGE  \nSQLSTATE: KD000  \nMalformed Avro messages are detected in message deserialization. Parse Mode: <mode>. To process malformed Avro message as null result, try setting the option \u2018mode\u2019 as \u2018PERMISSIVE\u2019.  \nMALFORMED_CSV_RECORD  \nSQLSTATE: KD000  \nMalformed CSV record: <badRecord>  \nMALFORMED_RECORD_IN_PARSING  \nSQLSTATE: 22023  \nMalformed records are detected in record parsing: <badRecord>.  \nParse Mode: <failFastMode>. To process malformed records as null result, try setting the option \u2018mode\u2019 as \u2018PERMISSIVE\u2019.  \nFor more details see MALFORMED_RECORD_IN_PARSING  \nMALFORMED_VARIANT  \nSQLSTATE: 22023  \nVariant binary is malformed. Please check the data source is valid.  \nMANAGED_TABLE_WITH_CRED  \nSQLSTATE: 42613  \nCreate managed table with storage credential is not supported.  \nMATERIALIZED_VIEW_MESA_REFRESH_WITHOUT_PIPELINE_ID  \nSQLSTATE: 55019  \nCannot <refreshType> the materialized view because it predates having a pipelineId. To enable <refreshType> please drop and recreate the materialized view.  \nMATERIALIZED_VIEW_OPERATION_NOT_ALLOWED  \nSQLSTATE: 56038  \nThe materialized view operation <operation> is not allowed:  \nFor more details see MATERIALIZED_VIEW_OPERATION_NOT_ALLOWED  \nMATERIALIZED_VIEW_OUTPUT_WITHOUT_EXPLICIT_ALIAS  \nSQLSTATE: 0A000  \nOutput expression <expression> in a materialized view must be explicitly aliased.  \nMATERIALIZED_VIEW_OVER_STREAMING_QUERY_INVALID  \nSQLSTATE: 42000  \nMaterialized View <name> could not be created with streaming query. Please use CREATE [OR REFRESH] STREAMING TABLE or remove the STREAM keyword to your FROM clause to turn this relation into a batch query instead.  \nMATERIALIZED_VIEW_UNSUPPORTED_OPERATION  \nSQLSTATE: 0A000  \nOperation <operation> is not supported on Materialized Views for this version.  \nMAX_NUMBER_VARIABLES_IN_SESSION_EXCEEDED  \nSQLSTATE: 54KD1"
    },
    {
        "id": 648,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 0A000  \nOperation <operation> is not supported on Materialized Views for this version.  \nMAX_NUMBER_VARIABLES_IN_SESSION_EXCEEDED  \nSQLSTATE: 54KD1  \nCannot create the new variable <variableName> because the number of variables in the session exceeds the maximum allowed number (<maxNumVariables>).  \nMAX_RECORDS_PER_FETCH_INVALID_FOR_KINESIS_SOURCE  \nSQLSTATE: 22023  \nmaxRecordsPerFetch needs to be a positive integer less than or equal to <kinesisRecordLimit>  \nMERGE_CARDINALITY_VIOLATION  \nSQLSTATE: 23K01  \nThe ON search condition of the MERGE statement matched a single row from the target table with multiple rows of the source table.  \nThis could result in the target row being operated on more than once with an update or delete operation and is not allowed.  \nMETRIC_CONSTRAINT_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nMETRIC CONSTRAINT is not enabled.  \nMETRIC_STORE_INVALID_ARGUMENT_VALUE_ERROR  \nSQLSTATE: 22023  \nProvided value \u201c<argValue>\u201d is not supported by argument \u201c<argName>\u201d for the METRIC_STORE table function.  \nFor more details see METRIC_STORE_INVALID_ARGUMENT_VALUE_ERROR  \nMETRIC_STORE_UNSUPPORTED_ERROR  \nSQLSTATE: 56038  \nMetric Store function <functionName> is currently disabled in this environment.  \nMISMATCHED_TOPIC_PARTITIONS_BETWEEN_END_OFFSET_AND_PREFETCHED  \nSQLSTATE: KD000  \nKafka data source in Trigger.AvailableNow should provide the same topic partitions in pre-fetched offset to end offset for each microbatch. The error could be transient - restart your query, and report if you still see the same issue.  \ntopic-partitions for pre-fetched offset: <tpsForPrefetched>, topic-partitions for end offset: <tpsForEndOffset>.  \nMISSING_AGGREGATION  \nSQLSTATE: 42803  \nThe non-aggregating expression <expression> is based on columns which are not participating in the GROUP BY clause."
    },
    {
        "id": 649,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "MISSING_AGGREGATION  \nSQLSTATE: 42803  \nThe non-aggregating expression <expression> is based on columns which are not participating in the GROUP BY clause.  \nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use <expressionAnyValue> if you do not care which of the values within a group is returned.  \nFor more details see MISSING_AGGREGATION  \nMISSING_CONNECTION_OPTION  \nSQLSTATE: 42000  \nConnections of type \u2018<connectionType>\u2019 must include the following option(s): <requiredOptions>.  \nMISSING_GROUP_BY  \nSQLSTATE: 42803  \nThe query does not include a GROUP BY clause. Add GROUP BY or turn it into the window functions using OVER clauses.  \nMISSING_NAME_FOR_CHECK_CONSTRAINT  \nSQLSTATE: 42000  \nCHECK constraint must have a name.  \nMISSING_PARAMETER_FOR_KAFKA  \nSQLSTATE: 42KDF  \nParameter <parameterName> is required for Kafka, but is not specified in <functionName>.  \nMISSING_PARAMETER_FOR_ROUTINE  \nSQLSTATE: 42KDF  \nParameter <parameterName> is required, but is not specified in <functionName>.  \nMODIFY_BUILTIN_CATALOG  \nSQLSTATE: 42832  \nModifying built-in catalog <catalogName> is not supported.  \nMULTIPLE_LOAD_PATH  \nSQLSTATE: 42000  \nDatabricks Delta does not support multiple input paths in the load() API.  \npaths: <pathList>. To build a single DataFrame by loading  \nmultiple paths from the same Delta table, please load the root path of  \nthe Delta table with the corresponding partition filters. If the multiple paths  \nare from different Delta tables, please use Dataset\u2019s union()/unionByName() APIs  \nto combine the DataFrames generated by separate load() API calls.  \nMULTIPLE_MATCHING_CONSTRAINTS  \nSQLSTATE: 42891  \nFound at least two matching constraints with the given condition.  \nMULTIPLE_TIME_TRAVEL_SPEC  \nSQLSTATE: 42K0E  \nCannot specify time travel in both the time travel clause and options.  \nMULTIPLE_XML_DATA_SOURCE  \nSQLSTATE: 42710"
    },
    {
        "id": 650,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "MULTIPLE_TIME_TRAVEL_SPEC  \nSQLSTATE: 42K0E  \nCannot specify time travel in both the time travel clause and options.  \nMULTIPLE_XML_DATA_SOURCE  \nSQLSTATE: 42710  \nDetected multiple data sources with the name <provider> (<sourceNames>). Please specify the fully qualified class name or remove <externalSource> from the classpath.  \nMULTI_SOURCES_UNSUPPORTED_FOR_EXPRESSION  \nSQLSTATE: 42K0E  \nThe expression <expr> does not support more than one source.  \nMULTI_UDF_INTERFACE_ERROR  \nSQLSTATE: 0A000  \nNot allowed to implement multiple UDF interfaces, UDF class <className>.  \nMUTUALLY_EXCLUSIVE_CLAUSES  \nSQLSTATE: 42613  \nMutually exclusive clauses or options <clauses>. Please remove one of these clauses.  \nMV_ST_ALTER_QUERY_INCORRECT_BACKING_TYPE  \nSQLSTATE: 42601  \nThe input query expects a <expectedType>, but the underlying table is a <givenType>.  \nNAMED_PARAMETERS_NOT_SUPPORTED  \nSQLSTATE: 4274K  \nNamed parameters are not supported for function <functionName>; please retry the query with positional arguments to the function call instead.  \nNAMED_PARAMETERS_NOT_SUPPORTED_FOR_SQL_UDFS  \nSQLSTATE: 0A000  \nCannot call function <functionName> because named argument references for SQL UDF are not supported. In this case, the named argument reference was <argument>.  \nNAMED_PARAMETER_SUPPORT_DISABLED  \nSQLSTATE: 0A000  \nCannot call function <functionName> because named argument references are not enabled here.  \nIn this case, the named argument reference was <argument>.  \nSet \u201cspark.sql.allowNamedFunctionArguments\u201d to \u201ctrue\u201d to turn on feature.  \nNAMESPACE_ALREADY_EXISTS  \nSQLSTATE: 42000  \nCannot create namespace <nameSpaceName> because it already exists.  \nChoose a different name, drop the existing namespace, or add the IF NOT EXISTS clause to tolerate pre-existing namespace.  \nNAMESPACE_NOT_EMPTY  \nSQLSTATE: 42000"
    },
    {
        "id": 651,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Choose a different name, drop the existing namespace, or add the IF NOT EXISTS clause to tolerate pre-existing namespace.  \nNAMESPACE_NOT_EMPTY  \nSQLSTATE: 42000  \nCannot drop a namespace <nameSpaceNameName> because it contains objects.  \nUse DROP NAMESPACE \u2026 CASCADE to drop the namespace and all its objects.  \nNAMESPACE_NOT_FOUND  \nSQLSTATE: 42000  \nThe namespace <nameSpaceName> cannot be found. Verify the spelling and correctness of the namespace.  \nIf you did not qualify the name with, verify the current_schema() output, or qualify the name with the correctly.  \nTo tolerate the error on drop use DROP NAMESPACE IF EXISTS.  \nNATIVE_XML_DATA_SOURCE_NOT_ENABLED  \nSQLSTATE: 56038  \nNative XML Data Source is not enabled in this cluster.  \nNESTED_AGGREGATE_FUNCTION  \nSQLSTATE: 42607  \nIt is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query.  \nNESTED_EXECUTE_IMMEDIATE  \nSQLSTATE: 07501  \nNested EXECUTE IMMEDIATE commands are not allowed. Please ensure that the SQL query provided (<sqlString>) does not contain another EXECUTE IMMEDIATE command.  \nNONEXISTENT_FIELD_NAME_IN_LIST  \nSQLSTATE: HV091  \nField(s) <nonExistFields> do(es) not exist. Available fields: <fieldNames>  \nNON_FOLDABLE_ARGUMENT  \nSQLSTATE: 42K08  \nThe function <funcName> requires the parameter <paramName> to be a foldable expression of the type <paramType>, but the actual argument is a non-foldable.  \nNON_LAST_MATCHED_CLAUSE_OMIT_CONDITION  \nSQLSTATE: 42613  \nWhen there are more than one MATCHED clauses in a MERGE statement, only the last MATCHED clause can omit the condition.  \nNON_LAST_NOT_MATCHED_BY_SOURCE_CLAUSE_OMIT_CONDITION  \nSQLSTATE: 42613  \nWhen there are more than one NOT MATCHED BY SOURCE clauses in a MERGE statement, only the last NOT MATCHED BY SOURCE clause can omit the condition.  \nNON_LAST_NOT_MATCHED_BY_TARGET_CLAUSE_OMIT_CONDITION"
    },
    {
        "id": 652,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "When there are more than one NOT MATCHED BY SOURCE clauses in a MERGE statement, only the last NOT MATCHED BY SOURCE clause can omit the condition.  \nNON_LAST_NOT_MATCHED_BY_TARGET_CLAUSE_OMIT_CONDITION  \nSQLSTATE: 42613  \nWhen there are more than one NOT MATCHED [BY TARGET] clauses in a MERGE statement, only the last NOT MATCHED [BY TARGET] clause can omit the condition.  \nNON_LITERAL_PIVOT_VALUES  \nSQLSTATE: 42K08  \nLiteral expressions required for pivot values, found <expression>.  \nNON_PARTITION_COLUMN  \nSQLSTATE: 42000  \nPARTITION clause cannot contain the non-partition column: <columnName>.  \nNON_TIME_WINDOW_NOT_SUPPORTED_IN_STREAMING  \nSQLSTATE: 42KDE  \nWindow function is not supported in <windowFunc> (as column <columnName>) on streaming DataFrames/Datasets.  \nStructured Streaming only supports time-window aggregation using the WINDOW function. (window specification: <windowSpec>)  \nNOT_ALLOWED_IN_FROM  \nSQLSTATE: 42601  \nNot allowed in the FROM clause:  \nFor more details see NOT_ALLOWED_IN_FROM  \nNOT_A_CONSTANT_STRING  \nSQLSTATE: 42601  \nThe expression <expr> used for the routine or clause <name> must be a constant STRING which is NOT NULL.  \nFor more details see NOT_A_CONSTANT_STRING  \nNOT_A_PARTITIONED_TABLE  \nSQLSTATE: 42809  \nOperation <operation> is not allowed for <tableIdentWithDB> because it is not a partitioned table.  \nNOT_A_SCALAR_FUNCTION  \nSQLSTATE: 42887  \n<functionName> appears as a scalar expression here, but the function was defined as a table function. Please update the query to move the function call into the FROM clause, or redefine <functionName> as a scalar function instead.  \nNOT_A_TABLE_FUNCTION  \nSQLSTATE: 42887  \n<functionName> appears as a table function here, but the function was defined as a scalar function. Please update the query to move the function call outside the FROM clause, or redefine <functionName> as a table function instead.  \nNOT_NULL_ASSERT_VIOLATION"
    },
    {
        "id": 653,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "NOT_NULL_ASSERT_VIOLATION  \nSQLSTATE: 42000  \nNULL value appeared in non-nullable field: <walkedTypePath>If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (such as java.lang.Integer instead of int/scala.Int).  \nNOT_NULL_CONSTRAINT_VIOLATION  \nSQLSTATE: 42000  \nAssigning a NULL is not allowed here.  \nFor more details see NOT_NULL_CONSTRAINT_VIOLATION  \nNOT_SUPPORTED_CHANGE_COLUMN  \nSQLSTATE: 0A000  \nALTER TABLE ALTER/CHANGE COLUMN is not supported for changing <table>\u2019s column <originName> with type <originType> to <newName> with type <newType>.  \nNOT_SUPPORTED_COMMAND_FOR_V2_TABLE  \nSQLSTATE: 0A000  \n<cmd> is not supported for v2 tables.  \nNOT_SUPPORTED_COMMAND_WITHOUT_HIVE_SUPPORT  \nSQLSTATE: 0A000  \n<cmd> is not supported, if you want to enable it, please set \u201cspark.sql.catalogImplementation\u201d to \u201chive\u201d.  \nNOT_SUPPORTED_IN_JDBC_CATALOG  \nSQLSTATE: 0A000  \nNot supported command in JDBC catalog:  \nFor more details see NOT_SUPPORTED_IN_JDBC_CATALOG  \nNOT_SUPPORTED_WITH_DB_SQL  \nSQLSTATE: 0A000  \n<operation> is not supported on a SQL <endpoint>.  \nNOT_SUPPORTED_WITH_SERVERLESS  \nSQLSTATE: 0A000  \n<operation> is not supported on serverless compute.  \nNOT_UNRESOLVED_ENCODER  \nSQLSTATE: 42601  \nUnresolved encoder expected, but <attr> was found.  \nNO_DEFAULT_COLUMN_VALUE_AVAILABLE  \nSQLSTATE: 42608  \nCan\u2019t determine the default value for <colName> since it is not nullable and it has no default value.  \nNO_HANDLER_FOR_UDAF  \nSQLSTATE: 42000  \nNo handler for UDAF \u2018<functionName>\u2019. Use sparkSession.udf.register(\u2026) instead.  \nNO_MERGE_ACTION_SPECIFIED"
    },
    {
        "id": 654,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 42000  \nNo handler for UDAF \u2018<functionName>\u2019. Use sparkSession.udf.register(\u2026) instead.  \nNO_MERGE_ACTION_SPECIFIED  \nSQLSTATE: 42K0E  \ndf.mergeInto needs to be followed by at least one of whenMatched/whenNotMatched/whenNotMatchedBySource.  \nNO_PARENT_EXTERNAL_LOCATION_FOR_PATH  \nSQLSTATE: none assigned  \nNo parent external location was found for path \u2018<path>\u2019. Please create an external location on one of the parent paths and then retry the query or command again.  \nNO_SQL_TYPE_IN_PROTOBUF_SCHEMA  \nSQLSTATE: 42S22  \nCannot find <catalystFieldPath> in Protobuf schema.  \nNO_STORAGE_LOCATION_FOR_TABLE  \nSQLSTATE: none assigned  \nNo storage location was found for table \u2018<tableId>\u2019 when generating table credentials. Please verify the table type and the table location URL and then retry the query or command again.  \nNO_SUCH_CATALOG_EXCEPTION  \nSQLSTATE: none assigned  \nCatalog \u2018<catalog>\u2019 was not found. Please verify the catalog name and then retry the query or command again.  \nNO_SUCH_CLEANROOM_EXCEPTION  \nSQLSTATE: none assigned  \nThe clean room \u2018<cleanroom>\u2019 does not exist. Please verify that the clean room name is spelled correctly and matches the name of a valid existing clean room and then retry the query or command again.  \nNO_SUCH_EXTERNAL_LOCATION_EXCEPTION  \nSQLSTATE: none assigned  \nThe external location \u2018<externalLocation>\u2019 does not exist. Please verify that the external location name is correct and then retry the query or command again.  \nNO_SUCH_METASTORE_EXCEPTION  \nSQLSTATE: none assigned  \nThe metastore was not found. Please ask your account administrator to assign a metastore to the current workspace and then retry the query or command again.  \nNO_SUCH_PROVIDER_EXCEPTION  \nSQLSTATE: none assigned  \nThe share provider \u2018<providerName>\u2019 does not exist. Please verify the share provider name is spelled correctly and matches the name of a valid existing provider name and then retry the query or command again.  \nNO_SUCH_RECIPIENT_EXCEPTION  \nSQLSTATE: none assigned"
    },
    {
        "id": 655,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "NO_SUCH_RECIPIENT_EXCEPTION  \nSQLSTATE: none assigned  \nThe recipient \u2018<recipient>\u2019 does not exist. Please verify that the recipient name is spelled correctly and matches the name of a valid existing recipient and then retry the query or command again.  \nNO_SUCH_SHARE_EXCEPTION  \nSQLSTATE: none assigned  \nThe share \u2018<share>\u2019 does not exist. Please verify that the share name is spelled correctly and matches the name of a valid existing share and then retry the query or command again.  \nNO_SUCH_STORAGE_CREDENTIAL_EXCEPTION  \nSQLSTATE: none assigned  \nThe storage credential \u2018<storageCredential>\u2019 does not exist. Please verify that the storage credential name is spelled correctly and matches the name of a valid existing storage credential and then retry the query or command again.  \nNO_SUCH_USER_EXCEPTION  \nSQLSTATE: none assigned  \nThe user \u2018<userName>\u2019 does not exist. Please verify that the user to whom you grant permission or alter ownership is spelled correctly and matches the name of a valid existing user and then retry the query or command again.  \nNO_UDF_INTERFACE  \nSQLSTATE: 38000  \nUDF class <className> doesn\u2019t implement any UDF interface.  \nNULLABLE_COLUMN_OR_FIELD  \nSQLSTATE: 42000  \nColumn or field <name> is nullable while it\u2019s required to be non-nullable.  \nNULLABLE_ROW_ID_ATTRIBUTES  \nSQLSTATE: 42000  \nRow ID attributes cannot be nullable: <nullableRowIdAttrs>.  \nNULL_MAP_KEY  \nSQLSTATE: 2200E  \nCannot use null as map key.  \nNULL_QUERY_STRING_EXECUTE_IMMEDIATE  \nSQLSTATE: 22004  \nExecute immediate requires a non-null variable as the query string, but the provided variable <varName> is null.  \nNUMERIC_OUT_OF_SUPPORTED_RANGE  \nSQLSTATE: 22003  \nThe value <value> cannot be interpreted as a numeric since it has more than 38 digits.  \nNUMERIC_VALUE_OUT_OF_RANGE  \nSQLSTATE: 22003  \nFor more details see NUMERIC_VALUE_OUT_OF_RANGE  \nNUM_COLUMNS_MISMATCH  \nSQLSTATE: 42826"
    },
    {
        "id": 656,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "NUMERIC_VALUE_OUT_OF_RANGE  \nSQLSTATE: 22003  \nFor more details see NUMERIC_VALUE_OUT_OF_RANGE  \nNUM_COLUMNS_MISMATCH  \nSQLSTATE: 42826  \n<operator> can only be performed on inputs with the same number of columns, but the first input has <firstNumColumns> columns and the <invalidOrdinalNum> input has <invalidNumColumns> columns.  \nNUM_TABLE_VALUE_ALIASES_MISMATCH  \nSQLSTATE: 42826  \nNumber of given aliases does not match number of output columns.  \nFunction name: <funcName>; number of aliases: <aliasesNum>; number of output columns: <outColsNum>.  \nOAUTH_CUSTOM_IDENTITY_CLAIM_NOT_PROVIDED  \nSQLSTATE: 22KD2  \nNo custom identity claim was provided.  \nONLY_SECRET_FUNCTION_SUPPORTED_HERE  \nSQLSTATE: 42K0E  \nCalling function <functionName> is not supported in this <location>; <supportedFunctions> supported here.  \nONLY_SUPPORTED_WITH_UC_SQL_CONNECTOR  \nSQLSTATE: 0A000  \nSQL operation <operation> is only supported on Databricks SQL connectors with Unity Catalog support.  \nOPERATION_CANCELED  \nSQLSTATE: HY008  \nOperation has been canceled.  \nOPERATION_REQUIRES_UNITY_CATALOG  \nSQLSTATE: 0AKUD  \nOperation <operation> requires Unity Catalog enabled.  \nOP_NOT_SUPPORTED_READ_ONLY  \nSQLSTATE: 42KD1  \n<plan> is not supported in read-only session mode.  \nORDER_BY_POS_OUT_OF_RANGE  \nSQLSTATE: 42805  \nORDER BY position <index> is not in select list (valid range is [1, <size>]).  \nPARSE_EMPTY_STATEMENT  \nSQLSTATE: 42617  \nSyntax error, unexpected empty statement.  \nPARSE_SYNTAX_ERROR  \nSQLSTATE: 42601  \nSyntax error at or near <error> <hint>.  \nPARTITIONS_ALREADY_EXIST  \nSQLSTATE: 428FT  \nCannot ADD or RENAME TO partition(s) <partitionList> in table <tableName> because they already exist."
    },
    {
        "id": 657,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "PARTITIONS_ALREADY_EXIST  \nSQLSTATE: 428FT  \nCannot ADD or RENAME TO partition(s) <partitionList> in table <tableName> because they already exist.  \nChoose a different name, drop the existing partition, or add the IF NOT EXISTS clause to tolerate a pre-existing partition.  \nPARTITIONS_NOT_FOUND  \nSQLSTATE: 428FT  \nThe partition(s) <partitionList> cannot be found in table <tableName>.  \nVerify the partition specification and table name.  \nTo tolerate the error on drop use ALTER TABLE \u2026 DROP IF EXISTS PARTITION.  \nPARTITION_LOCATION_ALREADY_EXISTS  \nSQLSTATE: 42K04  \nPartition location <locationPath> already exists in table <tableName>.  \nPARTITION_LOCATION_IS_NOT_UNDER_TABLE_DIRECTORY  \nSQLSTATE: 42KD5  \nFailed to execute the ALTER TABLE SET PARTITION LOCATION statement, because the  \npartition location <location> is not under the table directory <table>.  \nTo fix it, please set the location of partition to a subdirectory of <table>.  \nPARTITION_METADATA  \nSQLSTATE: 0AKUC  \n<action> is not allowed on table <tableName> since storing partition metadata is not supported in Unity Catalog.  \nPATH_ALREADY_EXISTS  \nSQLSTATE: 42K04  \nPath <outputPath> already exists. Set mode as \u201coverwrite\u201d to overwrite the existing path.  \nPATH_NOT_FOUND  \nSQLSTATE: 42K03  \nPath does not exist: <path>.  \nPIVOT_VALUE_DATA_TYPE_MISMATCH  \nSQLSTATE: 42K09  \nInvalid pivot value \u2018<value>\u2019: value data type <valueType> does not match pivot column data type <pivotType>.  \nPROCEDURE_ARGUMENT_NUMBER_MISMATCH  \nSQLSTATE: 42605  \nProcedure <procedureName> expects <expected> arguments, but <actual> were provided.  \nPROCEDURE_CREATION_EMPTY_ROUTINE  \nSQLSTATE: 0A000  \nCREATE PROCEDURE with an empty routine definition is not allowed.  \nPROCEDURE_CREATION_PARAMETER_OUT_INOUT_WITH_DEFAULT  \nSQLSTATE: 42601"
    },
    {
        "id": 658,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "PROCEDURE_CREATION_EMPTY_ROUTINE  \nSQLSTATE: 0A000  \nCREATE PROCEDURE with an empty routine definition is not allowed.  \nPROCEDURE_CREATION_PARAMETER_OUT_INOUT_WITH_DEFAULT  \nSQLSTATE: 42601  \nThe parameter <parameterName> is defined with parameter mode <parameterMode>. OUT and INOUT parameter cannot be omitted when invoking a routine and therefore do not support a DEFAULT expression. To proceed, remove the DEFAULT clause or change the parameter mode to IN.  \nPROCEDURE_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nStored procedure is not supported  \nPROCEDURE_NOT_SUPPORTED_WITH_HMS  \nSQLSTATE: 0A000  \nStored procedure is not supported with Hive Metastore. Please use Unity Catalog instead.  \nPROTOBUF_DEPENDENCY_NOT_FOUND  \nSQLSTATE: 42K0G  \nCould not find dependency: <dependencyName>.  \nPROTOBUF_DESCRIPTOR_FILE_NOT_FOUND  \nSQLSTATE: 42K0G  \nError reading Protobuf descriptor file at path: <filePath>.  \nPROTOBUF_FIELD_MISSING  \nSQLSTATE: 42K0G  \nSearching for <field> in Protobuf schema at <protobufSchema> gave <matchSize> matches. Candidates: <matches>.  \nPROTOBUF_FIELD_MISSING_IN_SQL_SCHEMA  \nSQLSTATE: 42K0G  \nFound <field> in Protobuf schema but there is no match in the SQL schema.  \nPROTOBUF_FIELD_TYPE_MISMATCH  \nSQLSTATE: 42K0G  \nType mismatch encountered for field: <field>.  \nPROTOBUF_JAVA_CLASSES_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nJava classes are not supported for <protobufFunction>. Contact Databricks Support about alternate options.  \nPROTOBUF_MESSAGE_NOT_FOUND  \nSQLSTATE: 42K0G  \nUnable to locate Message <messageName> in Descriptor.  \nPROTOBUF_TYPE_NOT_SUPPORT  \nSQLSTATE: 42K0G  \nProtobuf type not yet supported: <protobufType>.  \nPS_FETCH_RETRY_EXCEPTION  \nSQLSTATE: 22000"
    },
    {
        "id": 659,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "PROTOBUF_TYPE_NOT_SUPPORT  \nSQLSTATE: 42K0G  \nProtobuf type not yet supported: <protobufType>.  \nPS_FETCH_RETRY_EXCEPTION  \nSQLSTATE: 22000  \nTask in pubsub fetch stage cannot be retried. Partition <partitionInfo> in stage <stageInfo>, TID <taskId>.  \nPS_INVALID_EMPTY_OPTION  \nSQLSTATE: 42000  \n<key> cannot be an empty string.  \nPS_INVALID_KEY_TYPE  \nSQLSTATE: 22000  \nInvalid key type for PubSub dedup: <key>.  \nPS_INVALID_OPTION  \nSQLSTATE: 42000  \nThe option <key> is not supported by PubSub. It can only be used in testing.  \nPS_INVALID_OPTION_TYPE  \nSQLSTATE: 42000  \nInvalid type for <key>. Expected type of <key> to be type <type>.  \nPS_INVALID_READ_LIMIT  \nSQLSTATE: 42000  \nInvalid read limit on PubSub stream: <limit>.  \nPS_INVALID_UNSAFE_ROW_CONVERSION_FROM_PROTO  \nSQLSTATE: 22000  \nInvalid UnsafeRow to decode to PubSubMessageMetadata, the desired proto schema is: <protoSchema>. The input UnsafeRow might be corrupted: <unsafeRow>.  \nPS_MISSING_AUTH_INFO  \nSQLSTATE: 42000  \nFailed to find complete PubSub authentication information.  \nPS_MISSING_REQUIRED_OPTION  \nSQLSTATE: 42000  \nCould not find required option: <key>.  \nPS_MOVING_CHECKPOINT_FAILURE  \nSQLSTATE: 22000  \nFail to move raw data checkpoint files from <src> to destination directory: <dest>.  \nPS_MULTIPLE_FAILED_EPOCHS  \nSQLSTATE: 22000  \nPubSub stream cannot be started as there is more than one failed fetch: <failedEpochs>.  \nPS_OPTION_NOT_IN_BOUNDS  \nSQLSTATE: 22000  \n<key> must be within the following bounds (<min>, <max>) exclusive of both bounds.  \nPS_PROVIDE_CREDENTIALS_WITH_OPTION  \nSQLSTATE: 42000"
    },
    {
        "id": 660,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 22000  \n<key> must be within the following bounds (<min>, <max>) exclusive of both bounds.  \nPS_PROVIDE_CREDENTIALS_WITH_OPTION  \nSQLSTATE: 42000  \nShared clusters do not support authentication with instance profiles. Provide credentials to the stream directly using .option().  \nPS_SPARK_SPECULATION_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nPubSub source connector is only available in cluster with spark.speculation disabled.  \nPS_UNABLE_TO_CREATE_SUBSCRIPTION  \nSQLSTATE: 42000  \nAn error occurred while trying to create subscription <subId> on topic <topicId>. Please check that there are sufficient permissions to create a subscription and try again.  \nPS_UNABLE_TO_PARSE_PROTO  \nSQLSTATE: 22000  \nUnable to parse serialized bytes to generate proto.  \nPS_UNSUPPORTED_GET_OFFSET_CALL  \nSQLSTATE: 0A000  \ngetOffset is not supported without supplying a limit.  \nPYTHON_DATA_SOURCE_ERROR  \nSQLSTATE: 38000  \nFailed to <action> Python data source <type>: <msg>  \nPYTHON_STREAMING_DATA_SOURCE_RUNTIME_ERROR  \nSQLSTATE: 38000  \nFailed when Python streaming data source perform <action>: <msg>  \nQUERIED_TABLE_INCOMPATIBLE_WITH_COLUMN_MASK_POLICY  \nSQLSTATE: 428HD  \nUnable to access referenced table because a previously assigned column mask is currently incompatible with the table schema; to continue, please contact the owner of the table to update the policy:  \nFor more details see QUERIED_TABLE_INCOMPATIBLE_WITH_COLUMN_MASK_POLICY  \nQUERIED_TABLE_INCOMPATIBLE_WITH_ROW_LEVEL_SECURITY_POLICY  \nSQLSTATE: 428HD  \nUnable to access referenced table because a previously assigned row level security policy is currently incompatible with the table schema; to continue, please contact the owner of the table to update the policy:  \nFor more details see QUERIED_TABLE_INCOMPATIBLE_WITH_ROW_LEVEL_SECURITY_POLICY  \nREAD_CURRENT_FILE_NOT_FOUND  \nSQLSTATE: 42K03  \n<message>"
    },
    {
        "id": 661,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "For more details see QUERIED_TABLE_INCOMPATIBLE_WITH_ROW_LEVEL_SECURITY_POLICY  \nREAD_CURRENT_FILE_NOT_FOUND  \nSQLSTATE: 42K03  \n<message>  \nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running \u2018REFRESH TABLE tableName\u2019 command in SQL or by recreating the Dataset/DataFrame involved.  \nREAD_FILES_AMBIGUOUS_ROUTINE_PARAMETERS  \nSQLSTATE: 4274K  \nThe invocation of function <functionName> has <parameterName> and <alternativeName> set, which are aliases of each other. Please set only one of them.  \nREAD_TVF_UNEXPECTED_REQUIRED_PARAMETER  \nSQLSTATE: 4274K  \nThe function <functionName> required parameter <parameterName> must be assigned at position <expectedPos> without the name.  \nRECIPIENT_EXPIRATION_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nOnly TIMESTAMP/TIMESTAMP_LTZ/TIMESTAMP_NTZ types are supported for recipient expiration timestamp.  \nRECURSIVE_PROTOBUF_SCHEMA  \nSQLSTATE: 42K0G  \nFound recursive reference in Protobuf schema, which can not be processed by Spark by default: <fieldDescriptor>. try setting the option recursive.fields.max.depth 0 to 10. Going beyond 10 levels of recursion is not allowed.  \nRECURSIVE_VIEW  \nSQLSTATE: 42K0H  \nRecursive view <viewIdent> detected (cycle: <newPath>).  \nREF_DEFAULT_VALUE_IS_NOT_ALLOWED_IN_PARTITION  \nSQLSTATE: 42601  \nReferences to DEFAULT column values are not allowed within the PARTITION clause.  \nRELATION_LARGER_THAN_8G  \nSQLSTATE: 54000  \nCan not build a <relationName> that is larger than 8G.  \nREMOTE_FUNCTION_HTTP_FAILED_ERROR  \nSQLSTATE: 57012  \nThe remote HTTP request failed with code <errorCode>, and error message <errorMessage>  \nREMOTE_FUNCTION_HTTP_RESULT_PARSE_ERROR  \nSQLSTATE: 22032"
    },
    {
        "id": 662,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 57012  \nThe remote HTTP request failed with code <errorCode>, and error message <errorMessage>  \nREMOTE_FUNCTION_HTTP_RESULT_PARSE_ERROR  \nSQLSTATE: 22032  \nFailed to evaluate the <functionName> SQL function due to inability to parse the JSON result from the remote HTTP response; the error message is <errorMessage>. Check API documentation: <docUrl>. Please fix the problem indicated in the error message and retry the query again.  \nREMOTE_FUNCTION_HTTP_RESULT_UNEXPECTED_ERROR  \nSQLSTATE: 57012  \nFailed to evaluate the <functionName> SQL function due to inability to process the unexpected remote HTTP response; the error message is <errorMessage>. Check API documentation: <docUrl>. Please fix the problem indicated in the error message and retry the query again.  \nREMOTE_FUNCTION_HTTP_RETRY_TIMEOUT  \nSQLSTATE: 57012  \nThe remote request failed after retrying <N> times; the last failed HTTP error code was <errorCode> and the message was <errorMessage>  \nREMOTE_FUNCTION_MISSING_REQUIREMENTS_ERROR  \nSQLSTATE: 57012  \nFailed to evaluate the <functionName> SQL function because <errorMessage>. Check requirements in <docUrl>. Please fix the problem indicated in the error message and retry the query again.  \nRENAME_SRC_PATH_NOT_FOUND  \nSQLSTATE: 42K03  \nFailed to rename as <sourcePath> was not found.  \nREPEATED_CLAUSE  \nSQLSTATE: 42614  \nThe <clause> clause may be used at most once per <operation> operation.  \nREQUIRED_PARAMETER_ALREADY_PROVIDED_POSITIONALLY  \nSQLSTATE: 4274K  \nThe function <functionName> required parameter <parameterName> has been assigned at position <positionalIndex> without the name.  \nPlease update the function call to either remove the named argument with <parameterName> for this parameter or remove the positional  \nargument at <positionalIndex> and then try the query again.  \nREQUIRED_PARAMETER_NOT_FOUND  \nSQLSTATE: 4274K"
    },
    {
        "id": 663,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "argument at <positionalIndex> and then try the query again.  \nREQUIRED_PARAMETER_NOT_FOUND  \nSQLSTATE: 4274K  \nCannot invoke function <functionName> because the parameter named <parameterName> is required, but the function call did not supply a value. Please update the function call to supply an argument value (either positionally at index <index> or by name) and retry the query again.  \nREQUIRES_SINGLE_PART_NAMESPACE  \nSQLSTATE: 42K05  \n<sessionCatalog> requires a single-part namespace, but got <namespace>.  \nRESERVED_CDC_COLUMNS_ON_WRITE  \nSQLSTATE: 42939  \nThe write contains reserved columns <columnList> that are used  \ninternally as metadata for Change Data Feed. To write to the table either rename/drop  \nthese columns or disable Change Data Feed on the table by setting  \n<config> to false.  \nRESTRICTED_STREAMING_OPTION_PERMISSION_ENFORCED  \nSQLSTATE: 0A000  \nThe option <option> has restricted values on Shared clusters for the <source> source.  \nFor more details see RESTRICTED_STREAMING_OPTION_PERMISSION_ENFORCED  \nROUTINE_ALREADY_EXISTS  \nSQLSTATE: 42723  \nCannot create the routine <routineName> because it already exists.  \nChoose a different name, drop or replace the existing routine, or add the IF NOT EXISTS clause to tolerate a pre-existing routine.  \nROUTINE_NOT_FOUND  \nSQLSTATE: 42883  \nThe routine <routineName> cannot be found. Verify the spelling and correctness of the schema and catalog.  \nIf you did not qualify the name with a schema and catalog, verify the current_schema() output, or qualify the name with the correct schema and catalog.  \nTo tolerate the error on drop use DROP \u2026 IF EXISTS.  \nROUTINE_PARAMETER_NOT_FOUND  \nSQLSTATE: 42000  \nThe function <functionName> does not support the parameter <parameterName> specified at position <pos>.<suggestion>  \nROUTINE_USES_SYSTEM_RESERVED_CLASS_NAME  \nSQLSTATE: 42939  \nThe function <routineName> cannot be created because the specified classname \u2018<className>\u2019 is reserved for system use. Please rename the class and try again."
    },
    {
        "id": 664,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 42939  \nThe function <routineName> cannot be created because the specified classname \u2018<className>\u2019 is reserved for system use. Please rename the class and try again.  \nROW_LEVEL_SECURITY_CHECK_CONSTRAINT_UNSUPPORTED  \nSQLSTATE: 0A000  \nCreating CHECK constraint on table <tableName> with row level security policies is not supported.  \nROW_LEVEL_SECURITY_DUPLICATE_COLUMN_NAME  \nSQLSTATE: 42734  \nA <statementType> statement attempted to assign a row level security policy to a table, but two or more referenced columns had the same name <columnName>, which is invalid.  \nROW_LEVEL_SECURITY_FEATURE_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nRow level security policies for <tableName> are not supported:  \nFor more details see ROW_LEVEL_SECURITY_FEATURE_NOT_SUPPORTED  \nROW_LEVEL_SECURITY_INCOMPATIBLE_SCHEMA_CHANGE  \nSQLSTATE: 0A000  \nUnable to <statementType> <columnName> from table <tableName> because it\u2019s referenced in a row level security policy. The table owner must remove or alter this policy before proceeding.  \nROW_LEVEL_SECURITY_MERGE_UNSUPPORTED_SOURCE  \nSQLSTATE: 0A000  \nMERGE INTO operations do not support row level security policies in source table <tableName>.  \nROW_LEVEL_SECURITY_MERGE_UNSUPPORTED_TARGET  \nSQLSTATE: 0A000  \nMERGE INTO operations do not support writing into table <tableName> with row level security policies.  \nROW_LEVEL_SECURITY_MULTI_PART_COLUMN_NAME  \nSQLSTATE: 42K05  \nThis statement attempted to assign a row level security policy to a table, but referenced column <columnName> had multiple name parts, which is invalid.  \nROW_LEVEL_SECURITY_REQUIRE_UNITY_CATALOG  \nSQLSTATE: 0A000  \nRow level security policies are only supported in Unity Catalog.  \nROW_LEVEL_SECURITY_TABLE_CLONE_SOURCE_NOT_SUPPORTED  \nSQLSTATE: 0A000  \n<mode> clone from table <tableName> with row level security policy is not supported.  \nROW_LEVEL_SECURITY_TABLE_CLONE_TARGET_NOT_SUPPORTED  \nSQLSTATE: 0A000"
    },
    {
        "id": 665,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "<mode> clone from table <tableName> with row level security policy is not supported.  \nROW_LEVEL_SECURITY_TABLE_CLONE_TARGET_NOT_SUPPORTED  \nSQLSTATE: 0A000  \n<mode> clone to table <tableName> with row level security policy is not supported.  \nROW_LEVEL_SECURITY_UNSUPPORTED_CONSTANT_AS_PARAMETER  \nSQLSTATE: 0AKD1  \nUsing a constant as a parameter in a row level security policy is not supported. Please update your SQL command to remove the constant from the row filter definition and then retry the command again.  \nROW_LEVEL_SECURITY_UNSUPPORTED_PROVIDER  \nSQLSTATE: 0A000  \nFailed to execute <statementType> command because assigning row level security policy is not supported for target data source with table provider: \u201c<provider>\u201d.  \nROW_SUBQUERY_TOO_MANY_ROWS  \nSQLSTATE: 21000  \nMore than one row returned by a subquery used as a row.  \nRULE_ID_NOT_FOUND  \nSQLSTATE: 22023  \nNot found an id for the rule name \u201c<ruleName>\u201d. Please modify RuleIdCollection.scala if you are adding a new rule.  \nSAMPLE_TABLE_PERMISSIONS  \nSQLSTATE: 42832  \nPermissions not supported on sample databases/tables.  \nSCALAR_SUBQUERY_IS_IN_GROUP_BY_OR_AGGREGATE_FUNCTION  \nSQLSTATE: 0A000  \nThe correlated scalar subquery \u2018<sqlExpr>\u2019 is neither present in GROUP BY, nor in an aggregate function.  \nAdd it to GROUP BY using ordinal position or wrap it in first() (or first_value) if you don\u2019t care which value you get.  \nSCALAR_SUBQUERY_TOO_MANY_ROWS  \nSQLSTATE: 21000  \nMore than one row returned by a subquery used as an expression.  \nSCHEMA_ALREADY_EXISTS  \nSQLSTATE: 42P06  \nCannot create schema <schemaName> because it already exists.  \nChoose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema.  \nSCHEMA_NOT_EMPTY  \nSQLSTATE: 2BP01  \nCannot drop a schema <schemaName> because it contains objects."
    },
    {
        "id": 666,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SCHEMA_NOT_EMPTY  \nSQLSTATE: 2BP01  \nCannot drop a schema <schemaName> because it contains objects.  \nUse DROP SCHEMA \u2026 CASCADE to drop the schema and all its objects.  \nSCHEMA_NOT_FOUND  \nSQLSTATE: 42704  \nThe schema <schemaName> cannot be found. Verify the spelling and correctness of the schema and catalog.  \nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.  \nTo tolerate the error on drop use DROP SCHEMA IF EXISTS.  \nSCHEMA_REGISTRY_CONFIGURATION_ERROR  \nSQLSTATE: 42K0G  \nSchema from schema registry could not be initialized. <reason>.  \nSECOND_FUNCTION_ARGUMENT_NOT_INTEGER  \nSQLSTATE: 22023  \nThe second argument of <functionName> function needs to be an integer.  \nSECRET_FUNCTION_INVALID_LOCATION  \nSQLSTATE: 42K0E  \nCannot execute <commandType> command with one or more non-encrypted references to the SECRET function; please encrypt the result of each such function call with AES_ENCRYPT and try the command again  \nSEED_EXPRESSION_IS_UNFOLDABLE  \nSQLSTATE: 42K08  \nThe seed expression <seedExpr> of the expression <exprWithSeed> must be foldable.  \nSERVER_IS_BUSY  \nSQLSTATE: 08KD1  \nThe server is busy and could not handle the request. Please wait a moment and try again.  \nSORT_BY_WITHOUT_BUCKETING  \nSQLSTATE: 42601  \nsortBy must be used together with bucketBy.  \nSPECIFY_BUCKETING_IS_NOT_ALLOWED  \nSQLSTATE: 42601  \nA CREATE TABLE without explicit column list cannot specify bucketing information.  \nPlease use the form with explicit column list and specify bucketing information.  \nAlternatively, allow bucketing information to be inferred by omitting the clause.  \nSPECIFY_CLUSTER_BY_WITH_BUCKETING_IS_NOT_ALLOWED  \nSQLSTATE: 42908  \nCannot specify both CLUSTER BY and CLUSTERED BY INTO BUCKETS.  \nSPECIFY_CLUSTER_BY_WITH_PARTITIONED_BY_IS_NOT_ALLOWED  \nSQLSTATE: 42908"
    },
    {
        "id": 667,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Cannot specify both CLUSTER BY and CLUSTERED BY INTO BUCKETS.  \nSPECIFY_CLUSTER_BY_WITH_PARTITIONED_BY_IS_NOT_ALLOWED  \nSQLSTATE: 42908  \nCannot specify both CLUSTER BY and PARTITIONED BY.  \nSPECIFY_PARTITION_IS_NOT_ALLOWED  \nSQLSTATE: 42601  \nA CREATE TABLE without explicit column list cannot specify PARTITIONED BY.  \nPlease use the form with explicit column list and specify PARTITIONED BY.  \nAlternatively, allow partitioning to be inferred by omitting the PARTITION BY clause.  \nSQL_CONF_NOT_FOUND  \nSQLSTATE: 42K0I  \nThe SQL config <sqlConf> cannot be found. Please verify that the config exists.  \nSTAGING_PATH_CURRENTLY_INACCESSIBLE  \nSQLSTATE: 22000  \nTransient error while accessing target staging path <path>, please try in a few minutes  \nSTAR_GROUP_BY_POS  \nSQLSTATE: 0A000  \nStar (*) is not allowed in a select list when GROUP BY an ordinal position is used.  \nSTATEFUL_PROCESSOR_CANNOT_PERFORM_OPERATION_WITH_INVALID_HANDLE_STATE  \nSQLSTATE: 42802  \nFailed to perform stateful processor operation=<operationType> with invalid handle state=<handleState>.  \nSTATEFUL_PROCESSOR_CANNOT_PERFORM_OPERATION_WITH_INVALID_TIME_MODE  \nSQLSTATE: 42802  \nFailed to perform stateful processor operation=<operationType> with invalid timeMode=<timeMode>  \nSTATEFUL_PROCESSOR_CANNOT_REINITIALIZE_STATE_ON_KEY  \nSQLSTATE: 42710  \nCannot re-initialize state on the same grouping key during initial state handling for stateful processor. Invalid grouping key=<groupingKey>. Please check your initial state, remove duplicate rows and restart query.  \nSTATEFUL_PROCESSOR_INCORRECT_TIME_MODE_TO_ASSIGN_TTL  \nSQLSTATE: 42802  \nCannot use TTL for state=<stateName> in timeMode=<timeMode>, use TimeMode.ProcessingTime() instead.  \nSTATEFUL_PROCESSOR_TTL_DURATION_MUST_BE_POSITIVE  \nSQLSTATE: 42802"
    },
    {
        "id": 668,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "STATEFUL_PROCESSOR_TTL_DURATION_MUST_BE_POSITIVE  \nSQLSTATE: 42802  \nTTL duration must be greater than zero for State store operation=<operationType> on state=<stateName>.  \nSTATE_STORE_CANNOT_CREATE_COLUMN_FAMILY_WITH_RESERVED_CHARS  \nSQLSTATE: 42802  \nFailed to create column family with unsupported starting character and name=<colFamilyName>.  \nSTATE_STORE_CANNOT_USE_COLUMN_FAMILY_WITH_INVALID_NAME  \nSQLSTATE: 42802  \nFailed to perform column family operation=<operationName> with invalid name=<colFamilyName>. Column family name cannot be empty or include leading/trailing spaces or use the reserved keyword=default  \nSTATE_STORE_HANDLE_NOT_INITIALIZED  \nSQLSTATE: 42802  \nThe handle has not been initialized for this StatefulProcessor.  \nPlease only use the StatefulProcessor within the transformWithState operator.  \nSTATE_STORE_INCORRECT_NUM_ORDERING_COLS_FOR_RANGE_SCAN  \nSQLSTATE: 42802  \nIncorrect number of ordering ordinals=<numOrderingCols> for range scan encoder. The number of ordering ordinals cannot be zero or greater than number of schema columns.  \nSTATE_STORE_INCORRECT_NUM_PREFIX_COLS_FOR_PREFIX_SCAN  \nSQLSTATE: 42802  \nIncorrect number of prefix columns=<numPrefixCols> for prefix scan encoder. Prefix columns cannot be zero or greater than or equal to num of schema columns.  \nSTATE_STORE_INVALID_PROVIDER  \nSQLSTATE: 42K06  \nThe given State Store Provider <inputClass> does not extend org.apache.spark.sql.execution.streaming.state.StateStoreProvider.  \nSTATE_STORE_NULL_TYPE_ORDERING_COLS_NOT_SUPPORTED  \nSQLSTATE: 42802  \nNull type ordering column with name=<fieldName> at index=<index> is not supported for range scan encoder.  \nSTATE_STORE_PROVIDER_DOES_NOT_SUPPORT_FINE_GRAINED_STATE_REPLAY  \nSQLSTATE: 42K06"
    },
    {
        "id": 669,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "STATE_STORE_PROVIDER_DOES_NOT_SUPPORT_FINE_GRAINED_STATE_REPLAY  \nSQLSTATE: 42K06  \nThe given State Store Provider <inputClass> does not extend org.apache.spark.sql.execution.streaming.state.SupportsFineGrainedReplay.  \nTherefore, it does not support option snapshotStartBatchId in state data source.  \nSTATE_STORE_UNSUPPORTED_OPERATION_ON_MISSING_COLUMN_FAMILY  \nSQLSTATE: 42802  \nState store operation=<operationType> not supported on missing column family=<colFamilyName>.  \nSTATE_STORE_VARIABLE_SIZE_ORDERING_COLS_NOT_SUPPORTED  \nSQLSTATE: 42802  \nVariable size ordering column with name=<fieldName> at index=<index> is not supported for range scan encoder.  \nSTATIC_PARTITION_COLUMN_IN_INSERT_COLUMN_LIST  \nSQLSTATE: 42713  \nStatic partition column <staticName> is also specified in the column list.  \nSTDS_COMMITTED_BATCH_UNAVAILABLE  \nSQLSTATE: KD006  \nNo committed batch found, checkpoint location: <checkpointLocation>. Ensure that the query has run and committed any microbatch before stopping.  \nSTDS_CONFLICT_OPTIONS  \nSQLSTATE: 42613  \nThe options <options> cannot be specified together. Please specify the one.  \nSTDS_FAILED_TO_READ_STATE_SCHEMA  \nSQLSTATE: 42K03  \nFailed to read the state schema. Either the file does not exist, or the file is corrupted. options: <sourceOptions>.  \nRerun the streaming query to construct the state schema, and report to the corresponding communities or vendors if the error persists.  \nSTDS_INVALID_OPTION_VALUE  \nSQLSTATE: 42616  \nInvalid value for source option \u2018<optionName>\u2019:  \nFor more details see STDS_INVALID_OPTION_VALUE  \nSTDS_NO_PARTITION_DISCOVERED_IN_STATE_STORE  \nSQLSTATE: KD006  \nThe state does not have any partition. Please double check that the query points to the valid state. options: <sourceOptions>  \nSTDS_OFFSET_LOG_UNAVAILABLE  \nSQLSTATE: KD006"
    },
    {
        "id": 670,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "The state does not have any partition. Please double check that the query points to the valid state. options: <sourceOptions>  \nSTDS_OFFSET_LOG_UNAVAILABLE  \nSQLSTATE: KD006  \nThe offset log for <batchId> does not exist, checkpoint location: <checkpointLocation>.  \nPlease specify the batch ID which is available for querying - you can query the available batch IDs via using state metadata data source.  \nSTDS_OFFSET_METADATA_LOG_UNAVAILABLE  \nSQLSTATE: KD006  \nMetadata is not available for offset log for <batchId>, checkpoint location: <checkpointLocation>.  \nThe checkpoint seems to be only run with older Spark version(s). Run the streaming query with the recent Spark version, so that Spark constructs the state metadata.  \nSTDS_REQUIRED_OPTION_UNSPECIFIED  \nSQLSTATE: 42601  \n\u2018<optionName>\u2019 must be specified.  \nSTREAMING_AQE_NOT_SUPPORTED_FOR_STATEFUL_OPERATORS  \nSQLSTATE: 0A000  \nAdaptive Query Execution is not supported for stateful operators in Structured Streaming.  \nSTREAMING_FROM_MATERIALIZED_VIEW  \nSQLSTATE: 0A000  \nCannot stream from Materialized View <viewName>. Streaming from Materialized Views is not supported.  \nSTREAMING_STATEFUL_OPERATOR_NOT_MATCH_IN_STATE_METADATA  \nSQLSTATE: 42K03  \nStreaming stateful operator name does not match with the operator in state metadata. This likely to happen when user adds/removes/changes stateful operator of existing streaming query.  \nStateful operators in the metadata: [<OpsInMetadataSeq>]; Stateful operators in current batch: [<OpsInCurBatchSeq>].  \nSTREAMING_TABLE_NEEDS_REFRESH  \nSQLSTATE: 55019  \nStreaming table <tableName> needs to be refreshed. Please run CREATE OR REFRESH STREAMING TABLE <tableName> AS to update the table.  \nSTREAMING_TABLE_NOT_SUPPORTED  \nSQLSTATE: 56038  \nStreaming Tables can only be created and refreshed in Delta Live Tables and Databricks SQL Warehouses.  \nSTREAMING_TABLE_OPERATION_NOT_ALLOWED  \nSQLSTATE: 42601  \nThe operation <operation> is not allowed:  \nFor more details see STREAMING_TABLE_OPERATION_NOT_ALLOWED"
    },
    {
        "id": 671,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "STREAMING_TABLE_OPERATION_NOT_ALLOWED  \nSQLSTATE: 42601  \nThe operation <operation> is not allowed:  \nFor more details see STREAMING_TABLE_OPERATION_NOT_ALLOWED  \nSTREAMING_TABLE_QUERY_INVALID  \nSQLSTATE: 42000  \nStreaming table <tableName> can only be created from a streaming query. Please add the STREAM keyword to your FROM clause to turn this relation into a streaming query.  \nSTREAM_NOT_FOUND_FOR_KINESIS_SOURCE  \nSQLSTATE: 42K02  \nKinesis stream <streamName> in <region> not found.  \nPlease start a new query pointing to the correct stream name.  \nSTRUCT_ARRAY_LENGTH_MISMATCH  \nSQLSTATE: 2201E  \nInput row doesn\u2019t have expected number of values required by the schema. <expected> fields are required while <actual> values are provided.  \nSUM_OF_LIMIT_AND_OFFSET_EXCEEDS_MAX_INT  \nSQLSTATE: 22003  \nThe sum of the LIMIT clause and the OFFSET clause must not be greater than the maximum 32-bit integer value (2,147,483,647) but found limit = <limit>, offset = <offset>.  \nSYNC_METADATA_DELTA_ONLY  \nSQLSTATE: 0AKDD  \nRepair table sync metadata command is only supported for delta table.  \nSYNC_METADATA_NOT_SUPPORTED  \nSQLSTATE: 0AKUD  \nRepair table sync metadata command is only supported for Unity Catalog tables.  \nSYNC_SRC_TARGET_TBL_NOT_SAME  \nSQLSTATE: 42KD2  \nSource table name <srcTable> must be same as destination table name <destTable>.  \nSYNTAX_DISCONTINUED  \nSQLSTATE: 42601  \nSupport of the clause or keyword: <clause> has been discontinued in this context.  \nFor more details see SYNTAX_DISCONTINUED  \nTABLE_OR_VIEW_ALREADY_EXISTS  \nSQLSTATE: 42P07  \nCannot create table or view <relationName> because it already exists.  \nChoose a different name, drop the existing object, add the IF NOT EXISTS clause to tolerate pre-existing objects, add the OR REPLACE clause to replace the existing materialized view, or add the OR REFRESH clause to refresh the existing streaming table.  \nTABLE_OR_VIEW_NOT_FOUND"
    },
    {
        "id": 672,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "TABLE_OR_VIEW_NOT_FOUND  \nSQLSTATE: 42P01  \nThe table or view <relationName> cannot be found. Verify the spelling and correctness of the schema and catalog.  \nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.  \nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.  \nFor more details see TABLE_OR_VIEW_NOT_FOUND  \nTABLE_VALUED_ARGUMENTS_NOT_YET_IMPLEMENTED_FOR_SQL_FUNCTIONS  \nSQLSTATE: 0A000  \nCannot <action> SQL user-defined function <functionName> with TABLE arguments because this functionality is not yet implemented.  \nTABLE_VALUED_FUNCTION_FAILED_TO_ANALYZE_IN_PYTHON  \nSQLSTATE: 38000  \nFailed to analyze the Python user defined table function: <msg>  \nTABLE_VALUED_FUNCTION_REQUIRED_METADATA_INCOMPATIBLE_WITH_CALL  \nSQLSTATE: 22023  \nFailed to evaluate the table function <functionName> because its table metadata <requestedMetadata>, but the function call <invalidFunctionCallProperty>.  \nTABLE_VALUED_FUNCTION_REQUIRED_METADATA_INVALID  \nSQLSTATE: 22023  \nFailed to evaluate the table function <functionName> because its table metadata was invalid; <reason>.  \nTABLE_VALUED_FUNCTION_TOO_MANY_TABLE_ARGUMENTS  \nSQLSTATE: 54023  \nThere are too many table arguments for table-valued function.  \nIt allows one table argument, but got: <num>.  \nIf you want to allow it, please set \u201cspark.sql.allowMultipleTableArguments.enabled\u201d to \u201ctrue\u201d  \nTABLE_WITH_ID_NOT_FOUND  \nSQLSTATE: 42P01  \nTable with ID <tableId> cannot be found. Verify the correctness of the UUID.  \nTASK_WRITE_FAILED  \nSQLSTATE: 58030  \nTask failed while writing rows to <path>.  \nTEMP_TABLE_OR_VIEW_ALREADY_EXISTS  \nSQLSTATE: 42P07  \nCannot create the temporary view <relationName> because it already exists."
    },
    {
        "id": 673,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Task failed while writing rows to <path>.  \nTEMP_TABLE_OR_VIEW_ALREADY_EXISTS  \nSQLSTATE: 42P07  \nCannot create the temporary view <relationName> because it already exists.  \nChoose a different name, drop or replace the existing view, or add the IF NOT EXISTS clause to tolerate pre-existing views.  \nTEMP_VIEW_NAME_TOO_MANY_NAME_PARTS  \nSQLSTATE: 428EK  \nCREATE TEMPORARY VIEW or the corresponding Dataset APIs only accept single-part view names, but got: <actualName>.  \nUC_BUCKETED_TABLES  \nSQLSTATE: 0AKUC  \nBucketed tables are not supported in Unity Catalog.  \nUC_CATALOG_NAME_NOT_PROVIDED  \nSQLSTATE: 3D000  \nFor Unity Catalog, please specify the catalog name explicitly. E.g. SHOW GRANT your.address@email.com ON CATALOG main.  \nUC_COMMAND_NOT_SUPPORTED  \nSQLSTATE: 0AKUC  \nThe command(s): <commandName> are not supported in Unity Catalog.  \nFor more details see UC_COMMAND_NOT_SUPPORTED  \nUC_COMMAND_NOT_SUPPORTED_IN_SHARED_ACCESS_MODE  \nSQLSTATE: 0AKUC  \nThe command(s): <commandName> are not supported for Unity Catalog clusters in shared access mode. Use single user access mode instead.  \nUC_CREDENTIAL_PURPOSE_NOT_SUPPORTED  \nSQLSTATE: 0AKUC  \nThe specified credential kind is not supported.  \nUC_DATASOURCE_NOT_SUPPORTED  \nSQLSTATE: 0AKUC  \nData source format <dataSourceFormatName> is not supported in Unity Catalog.  \nUC_DATASOURCE_OPTIONS_NOT_SUPPORTED  \nSQLSTATE: 0AKUC  \nData source options are not supported in Unity Catalog.  \nUC_EXTERNAL_VOLUME_MISSING_LOCATION  \nSQLSTATE: 42601  \nLOCATION clause must be present for external volume. Please check the syntax \u2018CREATE EXTERNAL VOLUME \u2026 LOCATION \u2026\u2019 for creating an external volume.  \nUC_FAILED_PROVISIONING_STATE  \nSQLSTATE: 0AKUC"
    },
    {
        "id": 674,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "LOCATION clause must be present for external volume. Please check the syntax \u2018CREATE EXTERNAL VOLUME \u2026 LOCATION \u2026\u2019 for creating an external volume.  \nUC_FAILED_PROVISIONING_STATE  \nSQLSTATE: 0AKUC  \nThe query failed because it attempted to refer to table <tableName> but was unable to do so: <failureReason>. Please update the table <tableName> to ensure it is in an Active provisioning state and then retry the query again.  \nUC_FILE_SCHEME_FOR_TABLE_CREATION_NOT_SUPPORTED  \nSQLSTATE: 0AKUC  \nCreating table in Unity Catalog with file scheme <schemeName> is not supported.  \nInstead, please create a federated data source connection using the CREATE CONNECTION command for the same table provider, then create a catalog based on the connection with a CREATE FOREIGN CATALOG command to reference the tables therein.  \nUC_HIVE_METASTORE_FEDERATION_NOT_ENABLED  \nSQLSTATE: 0A000  \nHive Metastore federation is not enabled on this cluster.  \nAccessing the catalog <catalogName> is not supported on this cluster  \nUC_INVALID_DEPENDENCIES  \nSQLSTATE: 56098  \nDependencies of <viewName> are recorded as <storedDeps> while being parsed as <parsedDeps>. This likely occurred through improper use of a non-SQL API. You can repair dependencies in Databricks Runtime by running ALTER VIEW <viewName> AS <viewText>.  \nUC_INVALID_NAMESPACE  \nSQLSTATE: 0AKUC  \nNested or empty namespaces are not supported in Unity Catalog.  \nUC_INVALID_REFERENCE  \nSQLSTATE: 0AKUC  \nNon-Unity-Catalog object <name> can\u2019t be referenced in Unity Catalog objects.  \nUC_LAKEHOUSE_FEDERATION_WRITES_NOT_ALLOWED  \nSQLSTATE: 56038  \nUnity Catalog Lakehouse Federation write support is not enabled for provider <provider> on this cluster.  \nUC_LOCATION_FOR_MANAGED_VOLUME_NOT_SUPPORTED  \nSQLSTATE: 42601  \nManaged volume does not accept LOCATION clause. Please check the syntax \u2018CREATE VOLUME \u2026\u2019 for creating a managed volume.  \nUC_NOT_ENABLED  \nSQLSTATE: 56038  \nUnity Catalog is not enabled on this cluster."
    },
    {
        "id": 675,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Managed volume does not accept LOCATION clause. Please check the syntax \u2018CREATE VOLUME \u2026\u2019 for creating a managed volume.  \nUC_NOT_ENABLED  \nSQLSTATE: 56038  \nUnity Catalog is not enabled on this cluster.  \nUC_QUERY_FEDERATION_NOT_ENABLED  \nSQLSTATE: 56038  \nUnity Catalog Query Federation is not enabled on this cluster.  \nUC_SERVICE_CREDENTIALS_NOT_ENABLED  \nSQLSTATE: 56038  \nService credentials are not enabled on this cluster.  \nUC_VOLUMES_NOT_ENABLED  \nSQLSTATE: 56038  \nSupport for Unity Catalog Volumes is not enabled on this instance.  \nUC_VOLUMES_SHARING_NOT_ENABLED  \nSQLSTATE: 56038  \nSupport for Volume Sharing is not enabled on this instance.  \nUC_VOLUME_NOT_FOUND  \nSQLSTATE: 42704  \nVolume <name> does not exist. Please use \u2018SHOW VOLUMES\u2019 to list available volumes.  \nUDF_ERROR  \nSQLSTATE: none assigned  \nExecution of function <fn> failed  \nFor more details see UDF_ERROR  \nUDF_LIMITS  \nSQLSTATE: 54KD0  \nOne or more UDF limits were breached.  \nFor more details see UDF_LIMITS  \nUDF_MAX_COUNT_EXCEEDED  \nSQLSTATE: 54KD0  \nExceeded query-wide UDF limit of <maxNumUdfs> UDFs (limited during public preview). Found <numUdfs>. The UDFs were: <udfNames>.  \nUDF_PYSPARK_UNSUPPORTED_TYPE  \nSQLSTATE: 0A000  \nPySpark UDF <udf> (<eval-type>) is not supported on clusters in Shared access mode.  \nUDF_UNSUPPORTED_PARAMETER_DEFAULT_VALUE  \nSQLSTATE: 0A000  \nParameter default value is not supported for user-defined <functionType> function.  \nUDF_USER_CODE_ERROR  \nSQLSTATE: 39000  \nExecution of function <fn> failed.  \nFor more details see UDF_USER_CODE_ERROR  \nUDTF_ALIAS_NUMBER_MISMATCH  \nSQLSTATE: 42802  \nThe number of aliases supplied in the AS clause does not match the number of columns output by the UDTF."
    },
    {
        "id": 676,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "UDTF_ALIAS_NUMBER_MISMATCH  \nSQLSTATE: 42802  \nThe number of aliases supplied in the AS clause does not match the number of columns output by the UDTF.  \nExpected <aliasesSize> aliases, but got <aliasesNames>.  \nPlease ensure that the number of aliases provided matches the number of columns output by the UDTF.  \nUDTF_INVALID_ALIAS_IN_REQUESTED_ORDERING_STRING_FROM_ANALYZE_METHOD  \nSQLSTATE: 42802  \nFailed to evaluate the user-defined table function because its \u2018analyze\u2019 method returned a requested OrderingColumn whose column name expression included an unnecessary alias <aliasName>; please remove this alias and then try the query again.  \nUDTF_INVALID_REQUESTED_SELECTED_EXPRESSION_FROM_ANALYZE_METHOD_REQUIRES_ALIAS  \nSQLSTATE: 42802  \nFailed to evaluate the user-defined table function because its \u2018analyze\u2019 method returned a requested \u2018select\u2019 expression (<expression>) that does not include a corresponding alias; please update the UDTF to specify an alias there and then try the query again.  \nUDTF_PYSPARK_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nPySpark UDTF is not supported on clusters in Shared access mode. To proceed, please create a single-user cluster instead and re-run the notebook.  \nUNABLE_TO_ACQUIRE_MEMORY  \nSQLSTATE: 53200  \nUnable to acquire <requestedBytes> bytes of memory, got <receivedBytes>.  \nUNABLE_TO_CONVERT_TO_PROTOBUF_MESSAGE_TYPE  \nSQLSTATE: 42K0G  \nUnable to convert SQL type <toType> to Protobuf type <protobufType>.  \nUNABLE_TO_FETCH_HIVE_TABLES  \nSQLSTATE: 58030  \nUnable to fetch tables of Hive database: <dbName>. Error Class Name: <className>.  \nUNABLE_TO_INFER_SCHEMA  \nSQLSTATE: 42KD9  \nUnable to infer schema for <format>. It must be specified manually.  \nUNAUTHORIZED_ACCESS  \nSQLSTATE: 42501  \nUnauthorized access:  \n<report>  \nUNBOUND_SQL_PARAMETER"
    },
    {
        "id": 677,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Unable to infer schema for <format>. It must be specified manually.  \nUNAUTHORIZED_ACCESS  \nSQLSTATE: 42501  \nUnauthorized access:  \n<report>  \nUNBOUND_SQL_PARAMETER  \nSQLSTATE: 42P02  \nFound the unbound parameter: <name>. Please, fix args and provide a mapping of the parameter to either a SQL literal or collection constructor functions such as map(), array(), struct().  \nUNCLOSED_BRACKETED_COMMENT  \nSQLSTATE: 42601  \nFound an unclosed bracketed comment. Please, append */ at the end of the comment.  \nUNEXPECTED_INPUT_TYPE  \nSQLSTATE: 42K09  \nParameter <paramIndex> of function <functionName> requires the <requiredType> type, however <inputSql> has the type <inputType>.  \nUNEXPECTED_OPERATOR_IN_STREAMING_VIEW  \nSQLSTATE: 42KDD  \nUnexpected operator <op> in the CREATE VIEW statement as a streaming source.  \nA streaming view query must consist only of SELECT, WHERE, and UNION ALL operations.  \nUNEXPECTED_POSITIONAL_ARGUMENT  \nSQLSTATE: 4274K  \nCannot invoke function <functionName> because it contains positional argument(s) following the named argument assigned to <parameterName>; please rearrange them so the positional arguments come first and then retry the query again.  \nUNEXPECTED_SERIALIZER_FOR_CLASS  \nSQLSTATE: 42846  \nThe class <className> has an unexpected expression serializer. Expects \u201cSTRUCT\u201d or \u201cIF\u201d which returns \u201cSTRUCT\u201d but found <expr>.  \nUNKNOWN_FIELD_EXCEPTION  \nSQLSTATE: KD003  \nEncountered unknown fields during parsing: <unknownFieldBlob>, which can be fixed by an automatic retry: <isRetryable>  \nFor more details see UNKNOWN_FIELD_EXCEPTION  \nUNKNOWN_POSITIONAL_ARGUMENT  \nSQLSTATE: 4274K  \nThe invocation of function <functionName> contains an unknown positional argument <sqlExpr> at position <pos>. This is invalid.  \nUNKNOWN_PROTOBUF_MESSAGE_TYPE  \nSQLSTATE: 42K0G"
    },
    {
        "id": 678,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "UNKNOWN_PROTOBUF_MESSAGE_TYPE  \nSQLSTATE: 42K0G  \nAttempting to treat <descriptorName> as a Message, but it was <containingType>.  \nUNPIVOT_REQUIRES_ATTRIBUTES  \nSQLSTATE: 42K0A  \nUNPIVOT requires all given <given> expressions to be columns when no <empty> expressions are given. These are not columns: [<expressions>].  \nUNPIVOT_REQUIRES_VALUE_COLUMNS  \nSQLSTATE: 42K0A  \nAt least one value column needs to be specified for UNPIVOT, all columns specified as ids.  \nUNPIVOT_VALUE_DATA_TYPE_MISMATCH  \nSQLSTATE: 42K09  \nUnpivot value columns must share a least common type, some types do not: [<types>].  \nUNPIVOT_VALUE_SIZE_MISMATCH  \nSQLSTATE: 428C4  \nAll unpivot value columns must have the same size as there are value column names (<names>).  \nUNRECOGNIZED_PARAMETER_NAME  \nSQLSTATE: 4274K  \nCannot invoke function <functionName> because the function call included a named argument reference for the argument named <argumentName>, but this function does not include any signature containing an argument with this name. Did you mean one of the following? [<proposal>].  \nUNRECOGNIZED_SQL_TYPE  \nSQLSTATE: 42704  \nUnrecognized SQL type - name: <typeName>, id: <jdbcType>.  \nUNRESOLVABLE_TABLE_VALUED_FUNCTION  \nSQLSTATE: 42883  \nCould not resolve <name> to a table-valued function.  \nPlease make sure that <name> is defined as a table-valued function and that all required parameters are provided correctly.  \nIf <name> is not defined, please create the table-valued function before using it.  \nFor more information about defining table-valued functions, please refer to the Apache Spark documentation.  \nUNRESOLVED_ALL_IN_GROUP_BY  \nSQLSTATE: 42803  \nCannot infer grouping columns for GROUP BY ALL based on the select clause. Please explicitly specify the grouping columns.  \nUNRESOLVED_COLUMN  \nSQLSTATE: 42703  \nA column, variable, or function parameter with name <objectName> cannot be resolved.  \nFor more details see UNRESOLVED_COLUMN"
    },
    {
        "id": 679,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "UNRESOLVED_COLUMN  \nSQLSTATE: 42703  \nA column, variable, or function parameter with name <objectName> cannot be resolved.  \nFor more details see UNRESOLVED_COLUMN  \nUNRESOLVED_FIELD  \nSQLSTATE: 42703  \nA field with name <fieldName> cannot be resolved with the struct-type column <columnPath>.  \nFor more details see UNRESOLVED_FIELD  \nUNRESOLVED_MAP_KEY  \nSQLSTATE: 42703  \nCannot resolve column <objectName> as a map key. If the key is a string literal, add the single quotes \u2018\u2019 around it.  \nFor more details see UNRESOLVED_MAP_KEY  \nUNRESOLVED_ROUTINE  \nSQLSTATE: 42883  \nCannot resolve routine <routineName> on search path <searchPath>.  \nFor more details see UNRESOLVED_ROUTINE  \nUNRESOLVED_USING_COLUMN_FOR_JOIN  \nSQLSTATE: 42703  \nUSING column <colName> cannot be resolved on the <side> side of the join. The <side>-side columns: [<suggestion>].  \nUNRESOLVED_VARIABLE  \nSQLSTATE: 42883  \nCannot resolve variable <variableName> on search path <searchPath>.  \nUNSET_NONEXISTENT_PROPERTIES  \nSQLSTATE: 42K0J  \nAttempted to unset non-existent properties [<properties>] in table <table>.  \nUNSUPPORTED_ADD_FILE  \nSQLSTATE: 0A000  \nDon\u2019t support add file.  \nFor more details see UNSUPPORTED_ADD_FILE  \nUNSUPPORTED_ARROWTYPE  \nSQLSTATE: 0A000  \nUnsupported arrow type <typeName>.  \nUNSUPPORTED_BATCH_TABLE_VALUED_FUNCTION  \nSQLSTATE: 42000  \nThe function <funcName> does not support batch queries.  \nUNSUPPORTED_CALL  \nSQLSTATE: 0A000  \nCannot call the method \u201c<methodName>\u201d of the class \u201c<className>\u201d.  \nFor more details see UNSUPPORTED_CALL  \nUNSUPPORTED_CHAR_OR_VARCHAR_AS_STRING  \nSQLSTATE: 0A000  \nThe char/varchar type can\u2019t be used in the table schema."
    },
    {
        "id": 680,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "For more details see UNSUPPORTED_CALL  \nUNSUPPORTED_CHAR_OR_VARCHAR_AS_STRING  \nSQLSTATE: 0A000  \nThe char/varchar type can\u2019t be used in the table schema.  \nIf you want Spark treat them as string type as same as Spark 3.0 and earlier, please set \u201cspark.sql.legacy.charVarcharAsString\u201d to \u201ctrue\u201d.  \nUNSUPPORTED_CLAUSE_FOR_OPERATION  \nSQLSTATE: 0A000  \nThe <clause> is not supported for <operation>.  \nUNSUPPORTED_COLLATION  \nSQLSTATE: 0A000  \nCollation <collationName> is not supported for:  \nFor more details see UNSUPPORTED_COLLATION  \nUNSUPPORTED_COMMON_ANCESTOR_LOC_FOR_FILE_STREAM_SOURCE  \nSQLSTATE: 42616  \nThe common ancestor of source path and sourceArchiveDir should be registered with UC.  \nIf you see this error message, it\u2019s likely that you register the source path and sourceArchiveDir in different external locations.  \nPlease put them into a single external location.  \nUNSUPPORTED_CONSTRAINT_CLAUSES  \nSQLSTATE: 0A000  \nConstraint clauses <clauses> are unsupported.  \nUNSUPPORTED_CONSTRAINT_TYPE  \nSQLSTATE: 42000  \nUnsupported constraint type. Only <supportedConstraintTypes> are supported  \nUNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY  \nSQLSTATE: 0A000  \nUnsupported data source type for direct query on files: <dataSourceType>  \nUNSUPPORTED_DATATYPE  \nSQLSTATE: 0A000  \nUnsupported data type <typeName>.  \nUNSUPPORTED_DATA_SOURCE_SAVE_MODE  \nSQLSTATE: 0A000  \nThe data source \u201c<source>\u201d cannot be written in the <createMode> mode. Please use either the \u201cAppend\u201d or \u201cOverwrite\u201d mode instead.  \nUNSUPPORTED_DATA_TYPE_FOR_DATASOURCE  \nSQLSTATE: 0A000  \nThe <format> datasource doesn\u2019t support the column <columnName> of the type <columnType>.  \nUNSUPPORTED_DATA_TYPE_FOR_ENCODER  \nSQLSTATE: 0A000"
    },
    {
        "id": 681,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "The <format> datasource doesn\u2019t support the column <columnName> of the type <columnType>.  \nUNSUPPORTED_DATA_TYPE_FOR_ENCODER  \nSQLSTATE: 0A000  \nCannot create encoder for <dataType>. Please use a different output data type for your UDF or DataFrame.  \nUNSUPPORTED_DEFAULT_VALUE  \nSQLSTATE: 0A000  \nDEFAULT column values is not supported.  \nFor more details see UNSUPPORTED_DEFAULT_VALUE  \nUNSUPPORTED_DESERIALIZER  \nSQLSTATE: 0A000  \nThe deserializer is not supported:  \nFor more details see UNSUPPORTED_DESERIALIZER  \nUNSUPPORTED_EXPRESSION_GENERATED_COLUMN  \nSQLSTATE: 42621  \nCannot create generated column <fieldName> with generation expression <expressionStr> because <reason>.  \nUNSUPPORTED_EXPR_FOR_OPERATOR  \nSQLSTATE: 42K0E  \nA query operator contains one or more unsupported expressions.  \nConsider to rewrite it to avoid window functions, aggregate functions, and generator functions in the WHERE clause.  \nInvalid expressions: [<invalidExprSqls>]  \nUNSUPPORTED_EXPR_FOR_PARAMETER  \nSQLSTATE: 42K0E  \nA query parameter contains unsupported expression.  \nParameters can either be variables or literals.  \nInvalid expression: [<invalidExprSql>]  \nUNSUPPORTED_EXPR_FOR_WINDOW  \nSQLSTATE: 42P20  \nExpression <sqlExpr> not supported within a window function.  \nUNSUPPORTED_FEATURE  \nSQLSTATE: 0A000  \nThe feature is not supported:  \nFor more details see UNSUPPORTED_FEATURE  \nUNSUPPORTED_FN_TYPE  \nSQLSTATE: 0A000  \nUnsupported user defined function type: <language>  \nUNSUPPORTED_GENERATOR  \nSQLSTATE: 42K0E  \nThe generator is not supported:  \nFor more details see UNSUPPORTED_GENERATOR  \nUNSUPPORTED_GROUPING_EXPRESSION  \nSQLSTATE: 42K0E  \ngrouping()/grouping_id() can only be used with GroupingSets/Cube/Rollup.  \nUNSUPPORTED_INITIAL_POSITION_AND_TRIGGER_PAIR_FOR_KINESIS_SOURCE  \nSQLSTATE: 42616  \n<trigger> with initial position <initialPosition> is not supported with the Kinesis source"
    },
    {
        "id": 682,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "UNSUPPORTED_INITIAL_POSITION_AND_TRIGGER_PAIR_FOR_KINESIS_SOURCE  \nSQLSTATE: 42616  \n<trigger> with initial position <initialPosition> is not supported with the Kinesis source  \nUNSUPPORTED_INSERT  \nSQLSTATE: 42809  \nCan\u2019t insert into the target.  \nFor more details see UNSUPPORTED_INSERT  \nUNSUPPORTED_MANAGED_TABLE_CREATION  \nSQLSTATE: 0AKDD  \nCreating a managed table <tableName> using datasource <dataSource> is not supported. You need to use datasource DELTA or create an external table using CREATE EXTERNAL TABLE <tableName> \u2026 USING <dataSource> \u2026  \nUNSUPPORTED_MERGE_CONDITION  \nSQLSTATE: 42K0E  \nMERGE operation contains unsupported <condName> condition.  \nFor more details see UNSUPPORTED_MERGE_CONDITION  \nUNSUPPORTED_NESTED_ROW_OR_COLUMN_ACCESS_POLICY  \nSQLSTATE: 0A000  \nTable <tableName> has a row level security policy or column mask which indirectly refers to another table with a row level security policy or column mask; this is not supported. Call sequence: <callSequence>  \nUNSUPPORTED_OVERWRITE  \nSQLSTATE: 42902  \nCan\u2019t overwrite the target that is also being read from.  \nFor more details see UNSUPPORTED_OVERWRITE  \nUNSUPPORTED_SAVE_MODE  \nSQLSTATE: 0A000  \nThe save mode <saveMode> is not supported for:  \nFor more details see UNSUPPORTED_SAVE_MODE  \nUNSUPPORTED_STREAMING_OPTIONS_FOR_VIEW  \nSQLSTATE: 0A000  \nUnsupported for streaming a view. Reason:  \nFor more details see UNSUPPORTED_STREAMING_OPTIONS_FOR_VIEW  \nUNSUPPORTED_STREAMING_OPTIONS_PERMISSION_ENFORCED  \nSQLSTATE: 0A000  \nStreaming options <options> are not supported for data source <source> on a shared cluster.  \nUNSUPPORTED_STREAMING_SINK_PERMISSION_ENFORCED  \nSQLSTATE: 0A000  \nData source <sink> is not supported as a streaming sink on a shared cluster.  \nUNSUPPORTED_STREAMING_SOURCE_PERMISSION_ENFORCED  \nSQLSTATE: 0A000  \nData source <source> is not supported as a streaming source on a shared cluster.  \nUNSUPPORTED_STREAMING_TABLE_VALUED_FUNCTION  \nSQLSTATE: 42000"
    },
    {
        "id": 683,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 0A000  \nData source <source> is not supported as a streaming source on a shared cluster.  \nUNSUPPORTED_STREAMING_TABLE_VALUED_FUNCTION  \nSQLSTATE: 42000  \nThe function <funcName> does not support streaming. Please remove the STREAM keyword  \nUNSUPPORTED_STREAM_READ_LIMIT_FOR_KINESIS_SOURCE  \nSQLSTATE: 0A000  \n<streamReadLimit> is not supported with the Kinesis source  \nUNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY  \nSQLSTATE: 0A000  \nUnsupported subquery expression:  \nFor more details see UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY  \nUNSUPPORTED_TIMESERIES_COLUMNS  \nSQLSTATE: 56038  \nCreating primary key with timeseries columns is not supported  \nUNSUPPORTED_TIMESERIES_WITH_MORE_THAN_ONE_COLUMN  \nSQLSTATE: 0A000  \nCreating primary key with more than one timeseries column <colSeq> is not supported  \nUNSUPPORTED_TRIGGER_FOR_KINESIS_SOURCE  \nSQLSTATE: 0A000  \n<trigger> is not supported with the Kinesis source  \nUNSUPPORTED_TYPED_LITERAL  \nSQLSTATE: 0A000  \nLiterals of the type <unsupportedType> are not supported. Supported types are <supportedTypes>.  \nUNTYPED_SCALA_UDF  \nSQLSTATE: 42K0E  \nYou\u2019re using untyped Scala UDF, which does not have the input type information.  \nSpark may blindly pass null to the Scala closure with primitive-type argument, and the closure will see the default value of the Java type for the null argument, e.g. udf((x: Int) => x, IntegerType), the result is 0 for null input. To get rid of this error, you could:  \nuse typed Scala UDF APIs(without return type parameter), e.g. udf((x: Int) => x).  \nuse Java UDF APIs, e.g. udf(new UDF1[String, Integer] { override def call(s: String): Integer = s.length() }, IntegerType), if input types are all non primitive."
    },
    {
        "id": 684,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "set \u201cspark.sql.legacy.allowUntypedScalaUDF\u201d to \u201ctrue\u201d and use this API with caution.  \nUPGRADE_NOT_SUPPORTED  \nSQLSTATE: 0AKUC  \nTable is not eligible for upgrade from Hive Metastore to Unity Catalog. Reason:  \nFor more details see UPGRADE_NOT_SUPPORTED  \nUSER_DEFINED_FUNCTIONS  \nSQLSTATE: 42601  \nUser defined function is invalid:  \nFor more details see USER_DEFINED_FUNCTIONS  \nUSER_RAISED_EXCEPTION  \nSQLSTATE: P0001  \n<errorMessage>  \nUSER_RAISED_EXCEPTION_PARAMETER_MISMATCH  \nSQLSTATE: P0001  \nThe raise_error() function was used to raise error class: <errorClass> which expects parameters: <expectedParms>.  \nThe provided parameters <providedParms> do not match the expected parameters.  \nPlease make sure to provide all expected parameters.  \nUSER_RAISED_EXCEPTION_UNKNOWN_ERROR_CLASS  \nSQLSTATE: P0001  \nThe raise_error() function was used to raise an unknown error class: <errorClass>  \nVARIABLE_ALREADY_EXISTS  \nSQLSTATE: 42723  \nCannot create the variable <variableName> because it already exists.  \nChoose a different name, or drop or replace the existing variable.  \nVARIABLE_NOT_FOUND  \nSQLSTATE: 42883  \nThe variable <variableName> cannot be found. Verify the spelling and correctness of the schema and catalog.  \nIf you did not qualify the name with a schema and catalog, verify the current_schema() output, or qualify the name with the correct schema and catalog.  \nTo tolerate the error on drop use DROP VARIABLE IF EXISTS.  \nVARIANT_CONSTRUCTOR_SIZE_LIMIT  \nSQLSTATE: 22023  \nCannot construct a Variant larger than 16 MiB. The maximum allowed size of a Variant value is 16 MiB.  \nVARIANT_DUPLICATE_KEY  \nSQLSTATE: 22023  \nFailed to build variant because of a duplicate object key <key>.  \nVARIANT_SIZE_LIMIT  \nSQLSTATE: 22023  \nCannot build variant bigger than <sizeLimit> in <functionName>."
    },
    {
        "id": 685,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Failed to build variant because of a duplicate object key <key>.  \nVARIANT_SIZE_LIMIT  \nSQLSTATE: 22023  \nCannot build variant bigger than <sizeLimit> in <functionName>.  \nPlease avoid large input strings to this expression (for example, add function calls(s) to check the expression size and convert it to NULL first if it is too big).  \nVIEW_ALREADY_EXISTS  \nSQLSTATE: 42P07  \nCannot create view <relationName> because it already exists.  \nChoose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects.  \nVIEW_EXCEED_MAX_NESTED_DEPTH  \nSQLSTATE: 54K00  \nThe depth of view <viewName> exceeds the maximum view resolution depth (<maxNestedDepth>).  \nAnalysis is aborted to avoid errors. If you want to work around this, please try to increase the value of \u201cspark.sql.view.maxNestedViewDepth\u201d.  \nVIEW_NOT_FOUND  \nSQLSTATE: 42P01  \nThe view <relationName> cannot be found. Verify the spelling and correctness of the schema and catalog.  \nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.  \nTo tolerate the error on drop use DROP VIEW IF EXISTS.  \nVOLUME_ALREADY_EXISTS  \nSQLSTATE: 42000  \nCannot create volume <relationName> because it already exists.  \nChoose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects.  \nWINDOW_FUNCTION_AND_FRAME_MISMATCH  \nSQLSTATE: 42K0E  \n<funcName> function can only be evaluated in an ordered row-based window frame with a single offset: <windowExpr>.  \nWINDOW_FUNCTION_WITHOUT_OVER_CLAUSE  \nSQLSTATE: 42601  \nWindow function <funcName> requires an OVER clause.  \nWITH_CREDENTIAL  \nSQLSTATE: 42601  \nWITH CREDENTIAL syntax is not supported for <type>.  \nWRITE_STREAM_NOT_ALLOWED  \nSQLSTATE: 42601  \nwriteStream can be called only on streaming Dataset/DataFrame."
    },
    {
        "id": 686,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "WITH CREDENTIAL syntax is not supported for <type>.  \nWRITE_STREAM_NOT_ALLOWED  \nSQLSTATE: 42601  \nwriteStream can be called only on streaming Dataset/DataFrame.  \nWRONG_COLUMN_DEFAULTS_FOR_DELTA_ALTER_TABLE_ADD_COLUMN_NOT_SUPPORTED  \nSQLSTATE: 0AKDC  \nFailed to execute the command because DEFAULT values are not supported when adding new  \ncolumns to previously existing Delta tables; please add the column without a default  \nvalue first, then run a second ALTER TABLE ALTER COLUMN SET DEFAULT command to apply  \nfor future inserted rows instead.  \nWRONG_COLUMN_DEFAULTS_FOR_DELTA_FEATURE_NOT_ENABLED  \nSQLSTATE: 0AKDE  \nFailed to execute <commandType> command because it assigned a column DEFAULT value,  \nbut the corresponding table feature was not enabled. Please retry the command again  \nafter executing ALTER TABLE tableName SET  \nTBLPROPERTIES(\u2018delta.feature.allowColumnDefaults\u2019 = \u2018supported\u2019).  \nWRONG_COMMAND_FOR_OBJECT_TYPE  \nSQLSTATE: 42809  \nThe operation <operation> requires a <requiredType>. But <objectName> is a <foundType>. Use <alternative> instead.  \nWRONG_NUM_ARGS  \nSQLSTATE: 42605  \nThe <functionName> requires <expectedNum> parameters but the actual number is <actualNum>.  \nFor more details see WRONG_NUM_ARGS  \nXML_ROW_TAG_MISSING  \nSQLSTATE: 42KDF  \n<rowTag> option is required for reading files in XML format.  \nXML_UNSUPPORTED_NESTED_TYPES  \nSQLSTATE: 0N000  \nXML doesn\u2019t support <innerDataType> as inner type of <dataType>. Please wrap the <innerDataType> within a StructType field when using it inside <dataType>.  \nXML_WILDCARD_RESCUED_DATA_CONFLICT_ERROR  \nSQLSTATE: 22023  \nRescued data and wildcard column cannot be simultaneously enabled. Remove the wildcardColumnName option.  \nZORDERBY_COLUMN_DOES_NOT_EXIST  \nSQLSTATE: 42703  \nZOrderBy column <columnName> doesn\u2019t exist."
    },
    {
        "id": 687,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Delta Lake\nDELTA_ACTIVE_SPARK_SESSION_NOT_FOUND  \nSQLSTATE: 08003  \nCould not find active SparkSession  \nDELTA_ACTIVE_TRANSACTION_ALREADY_SET  \nSQLSTATE: 0B000  \nCannot set a new txn as active when one is already active  \nDELTA_ADDING_COLUMN_WITH_INTERNAL_NAME_FAILED  \nSQLSTATE: 42000  \nFailed to add column <colName> because the name is reserved.  \nDELTA_ADDING_DELETION_VECTORS_DISALLOWED  \nSQLSTATE: 0A000  \nThe current operation attempted to add a deletion vector to a table that does not permit the creation of new deletion vectors. Please file a bug report.  \nDELTA_ADDING_DELETION_VECTORS_WITH_TIGHT_BOUNDS_DISALLOWED  \nSQLSTATE: 42000  \nAll operations that add deletion vectors should set the tightBounds column in statistics to false. Please file a bug report.  \nDELTA_ADD_COLUMN_AT_INDEX_LESS_THAN_ZERO  \nSQLSTATE: 42KD3  \nIndex <columnIndex> to add column <columnName> is lower than 0  \nDELTA_ADD_COLUMN_PARENT_NOT_STRUCT  \nSQLSTATE: 42KD3  \nCannot add <columnName> because its parent is not a StructType. Found <other>  \nDELTA_ADD_COLUMN_STRUCT_NOT_FOUND  \nSQLSTATE: 42KD3  \nStruct not found at position <position>  \nDELTA_ADD_CONSTRAINTS  \nSQLSTATE: 0A000  \nPlease use ALTER TABLE ADD CONSTRAINT to add CHECK constraints.  \nDELTA_AGGREGATE_IN_GENERATED_COLUMN  \nSQLSTATE: 42621  \nFound <sqlExpr>. A generated column cannot use an aggregate expression  \nDELTA_AGGREGATION_NOT_SUPPORTED  \nSQLSTATE: 42903  \nAggregate functions are not supported in the <operation> <predicate>.  \nDELTA_ALTER_COLLATION_NOT_SUPPORTED_BLOOM_FILTER  \nSQLSTATE: 428FR  \nFailed to change the collation of column <column> because it has a bloom filter index. Please either retain the existing collation or else drop the bloom filter index and then retry the command again to change the collation.  \nDELTA_ALTER_COLLATION_NOT_SUPPORTED_CLUSTER_BY  \nSQLSTATE: 428FR"
    },
    {
        "id": 688,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "DELTA_ALTER_COLLATION_NOT_SUPPORTED_CLUSTER_BY  \nSQLSTATE: 428FR  \nFailed to change the collation of column <column> because it is a clustering column. Please either retain the existing collation or else change the column to a non-clustering column with an ALTER TABLE command and then retry the command again to change the collation.  \nDELTA_ALTER_TABLE_CHANGE_COL_NOT_SUPPORTED  \nSQLSTATE: 42837  \nALTER TABLE CHANGE COLUMN is not supported for changing column <currentType> to <newType>  \nDELTA_ALTER_TABLE_CLUSTER_BY_NOT_ALLOWED  \nSQLSTATE: 42000  \nALTER TABLE CLUSTER BY is supported only for Delta table with Liquid clustering.  \nDELTA_ALTER_TABLE_CLUSTER_BY_ON_PARTITIONED_TABLE_NOT_ALLOWED  \nSQLSTATE: 42000  \nALTER TABLE CLUSTER BY cannot be applied to a partitioned table.  \nDELTA_ALTER_TABLE_RENAME_NOT_ALLOWED  \nSQLSTATE: 42000  \nOperation not allowed: ALTER TABLE RENAME TO is not allowed for managed Delta tables on S3, as eventual consistency on S3 may corrupt the Delta transaction log. If you insist on doing so and are sure that there has never been a Delta table with the new name <newName> before, you can enable this by setting <key> to be true.  \nDELTA_ALTER_TABLE_SET_CLUSTERING_TABLE_FEATURE_NOT_ALLOWED  \nSQLSTATE: 42000  \nCannot enable <tableFeature> table feature using ALTER TABLE SET TBLPROPERTIES. Please use CREATE OR REPLACE TABLE CLUSTER BY to create a Delta table with clustering.  \nDELTA_AMBIGUOUS_DATA_TYPE_CHANGE  \nSQLSTATE: 429BQ  \nCannot change data type of <column> from <from> to <to>. This change contains column removals and additions, therefore they are ambiguous. Please make these changes individually using ALTER TABLE [ADD | DROP | RENAME] COLUMN.  \nDELTA_AMBIGUOUS_PARTITION_COLUMN  \nSQLSTATE: 42702  \nAmbiguous partition column <column> can be <colMatches>.  \nDELTA_AMBIGUOUS_PATHS_IN_CREATE_TABLE  \nSQLSTATE: 42613  \nCREATE TABLE contains two different locations: <identifier> and <location>.  \nYou can remove the LOCATION clause from the CREATE TABLE statement, or set"
    },
    {
        "id": 689,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "DELTA_AMBIGUOUS_PATHS_IN_CREATE_TABLE  \nSQLSTATE: 42613  \nCREATE TABLE contains two different locations: <identifier> and <location>.  \nYou can remove the LOCATION clause from the CREATE TABLE statement, or set  \n<config> to true to skip this check.  \nDELTA_ARCHIVED_FILES_IN_LIMIT  \nSQLSTATE: 42KDC  \nTable <table> does not contain enough records in non-archived files to satisfy specified LIMIT of <limit> records.  \nDELTA_ARCHIVED_FILES_IN_SCAN  \nSQLSTATE: 42KDC  \nFound <numArchivedFiles> potentially archived file(s) in table <table> that need to be scanned as part of this query.  \nArchived files cannot be accessed. The current time until archival is configured as <archivalTime>.  \nPlease adjust your query filters to exclude any archived files.  \nDELTA_BLOCK_COLUMN_MAPPING_AND_CDC_OPERATION  \nSQLSTATE: 42KD4  \nOperation \u201c<opName>\u201d is not allowed when the table has enabled change data feed (CDF) and has undergone schema changes using DROP COLUMN or RENAME COLUMN.  \nDELTA_BLOOM_FILTER_DROP_ON_NON_EXISTING_COLUMNS  \nSQLSTATE: 42703  \nCannot drop bloom filter indices for the following non-existent column(s): <unknownColumns>  \nDELTA_BLOOM_FILTER_OOM_ON_WRITE  \nSQLSTATE: 82100  \nOutOfMemoryError occurred while writing bloom filter indices for the following column(s): <columnsWithBloomFilterIndices>.  \nYou can reduce the memory footprint of bloom filter indices by choosing a smaller value for the \u2018numItems\u2019 option, a larger value for the \u2018fpp\u2019 option, or by indexing fewer columns.  \nDELTA_CANNOT_CHANGE_DATA_TYPE  \nSQLSTATE: 429BQ  \nCannot change data type: <dataType>  \nDELTA_CANNOT_CHANGE_LOCATION  \nSQLSTATE: 42601  \nCannot change the \u2018location\u2019 of the Delta table using SET TBLPROPERTIES. Please use ALTER TABLE SET LOCATION instead.  \nDELTA_CANNOT_CHANGE_PROVIDER  \nSQLSTATE: 42939  \n\u2018provider\u2019 is a reserved table property, and cannot be altered."
    },
    {
        "id": 690,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "DELTA_CANNOT_CHANGE_PROVIDER  \nSQLSTATE: 42939  \n\u2018provider\u2019 is a reserved table property, and cannot be altered.  \nDELTA_CANNOT_CREATE_BLOOM_FILTER_NON_EXISTING_COL  \nSQLSTATE: 42703  \nCannot create bloom filter indices for the following non-existent column(s): <unknownCols>  \nDELTA_CANNOT_CREATE_LOG_PATH  \nSQLSTATE: 42KD5  \nCannot create <path>  \nDELTA_CANNOT_DESCRIBE_VIEW_HISTORY  \nSQLSTATE: 42809  \nCannot describe the history of a view.  \nDELTA_CANNOT_DROP_BLOOM_FILTER_ON_NON_INDEXED_COLUMN  \nSQLSTATE: 42703  \nCannot drop bloom filter index on a non indexed column: <columnName>  \nDELTA_CANNOT_DROP_CHECK_CONSTRAINT_FEATURE  \nSQLSTATE: 0AKDE  \nCannot drop the CHECK constraints table feature.  \nThe following constraints must be dropped first: <constraints>.  \nDELTA_CANNOT_EVALUATE_EXPRESSION  \nSQLSTATE: 0AKDC  \nCannot evaluate expression: <expression>  \nDELTA_CANNOT_FIND_BUCKET_SPEC  \nSQLSTATE: 22000  \nExpecting a bucketing Delta table but cannot find the bucket spec in the table  \nDELTA_CANNOT_GENERATE_CODE_FOR_EXPRESSION  \nSQLSTATE: 0AKDC  \nCannot generate code for expression: <expression>  \nDELTA_CANNOT_MODIFY_APPEND_ONLY  \nSQLSTATE: 42809  \nThis table is configured to only allow appends. If you would like to permit updates or deletes, use \u2018ALTER TABLE <table_name> SET TBLPROPERTIES (<config>=false)\u2019.  \nDELTA_CANNOT_MODIFY_TABLE_PROPERTY  \nSQLSTATE: 42939  \nThe Delta table configuration <prop> cannot be specified by the user  \nDELTA_CANNOT_RECONSTRUCT_PATH_FROM_URI  \nSQLSTATE: 22KD1  \nA uri (<uri>) which can\u2019t be turned into a relative path was found in the transaction log.  \nDELTA_CANNOT_RELATIVIZE_PATH  \nSQLSTATE: 42000  \nA path (<path>) which can\u2019t be relativized with the current input found in the"
    },
    {
        "id": 691,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "DELTA_CANNOT_RELATIVIZE_PATH  \nSQLSTATE: 42000  \nA path (<path>) which can\u2019t be relativized with the current input found in the  \ntransaction log. Please re-run this as:  \n%%scala com.databricks.delta.Delta.fixAbsolutePathsInLog(\u201c<userPath>\u201d, true)  \nand then also run:  \n%%scala com.databricks.delta.Delta.fixAbsolutePathsInLog(\u201c<path>\u201d)  \nDELTA_CANNOT_RENAME_PATH  \nSQLSTATE: 22KD1  \nCannot rename <currentPath> to <newPath>  \nDELTA_CANNOT_REPLACE_MISSING_TABLE  \nSQLSTATE: 42P01  \nTable <tableName> cannot be replaced as it does not exist. Use CREATE OR REPLACE TABLE to create the table.  \nDELTA_CANNOT_RESOLVE_COLUMN  \nSQLSTATE: 42703  \nCan\u2019t resolve column <columnName> in <schema>  \nDELTA_CANNOT_RESTORE_TABLE_VERSION  \nSQLSTATE: 22003  \nCannot restore table to version <version>. Available versions: [<startVersion>, <endVersion>].  \nDELTA_CANNOT_RESTORE_TIMESTAMP_EARLIER  \nSQLSTATE: 22003  \nCannot restore table to timestamp (<requestedTimestamp>) as it is before the earliest version available. Please use a timestamp after (<earliestTimestamp>).  \nDELTA_CANNOT_RESTORE_TIMESTAMP_GREATER  \nSQLSTATE: 22003  \nCannot restore table to timestamp (<requestedTimestamp>) as it is after the latest version available. Please use a timestamp before (<latestTimestamp>)  \nDELTA_CANNOT_SET_LOCATION_ON_PATH_IDENTIFIER  \nSQLSTATE: 42613  \nCannot change the location of a path based table.  \nDELTA_CANNOT_SET_MANAGED_STATS_COLUMNS_PROPERTY  \nSQLSTATE: 42616  \nCannot set delta.managedDataSkippingStatsColumns on non-DLT table  \nDELTA_CANNOT_UPDATE_ARRAY_FIELD  \nSQLSTATE: 429BQ"
    },
    {
        "id": 692,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Cannot set delta.managedDataSkippingStatsColumns on non-DLT table  \nDELTA_CANNOT_UPDATE_ARRAY_FIELD  \nSQLSTATE: 429BQ  \nCannot update %1$s field %2$s type: update the element by updating %2$s.element  \nDELTA_CANNOT_UPDATE_MAP_FIELD  \nSQLSTATE: 429BQ  \nCannot update %1$s field %2$s type: update a map by updating %2$s.key or %2$s.value  \nDELTA_CANNOT_UPDATE_OTHER_FIELD  \nSQLSTATE: 429BQ  \nCannot update <tableName> field of type <typeName>  \nDELTA_CANNOT_UPDATE_STRUCT_FIELD  \nSQLSTATE: 429BQ  \nCannot update <tableName> field <fieldName> type: update struct by adding, deleting, or updating its fields  \nDELTA_CANNOT_USE_ALL_COLUMNS_FOR_PARTITION  \nSQLSTATE: 428FT  \nCannot use all columns for partition columns  \nDELTA_CANNOT_WRITE_INTO_VIEW  \nSQLSTATE: 0A000  \n<table> is a view. Writes to a view are not supported.  \nDELTA_CAST_OVERFLOW_IN_TABLE_WRITE  \nSQLSTATE: 22003  \nFailed to write a value of <sourceType> type into the <targetType> type column <columnName> due to an overflow.  \nUse try_cast on the input value to tolerate overflow and return NULL instead.  \nIf necessary, set <storeAssignmentPolicyFlag> to \u201cLEGACY\u201d to bypass this error or set <updateAndMergeCastingFollowsAnsiEnabledFlag> to true to revert to the old behaviour and follow <ansiEnabledFlag> in UPDATE and MERGE.  \nDELTA_CDC_NOT_ALLOWED_IN_THIS_VERSION  \nSQLSTATE: 0AKDC  \nConfiguration delta.enableChangeDataFeed cannot be set. Change data feed from Delta is not yet available.  \nDELTA_CHANGE_DATA_FEED_INCOMPATIBLE_DATA_SCHEMA  \nSQLSTATE: 0AKDC  \nRetrieving table changes between version <start> and <end> failed because of an incompatible data schema."
    },
    {
        "id": 693,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "DELTA_CHANGE_DATA_FEED_INCOMPATIBLE_DATA_SCHEMA  \nSQLSTATE: 0AKDC  \nRetrieving table changes between version <start> and <end> failed because of an incompatible data schema.  \nYour read schema is <readSchema> at version <readVersion>, but we found an incompatible data schema at version <incompatibleVersion>.  \nIf possible, please retrieve the table changes using the end version\u2019s schema by setting <config> to endVersion, or contact support.  \nDELTA_CHANGE_DATA_FEED_INCOMPATIBLE_SCHEMA_CHANGE  \nSQLSTATE: 0AKDC  \nRetrieving table changes between version <start> and <end> failed because of an incompatible schema change.  \nYour read schema is <readSchema> at version <readVersion>, but we found an incompatible schema change at version <incompatibleVersion>.  \nIf possible, please query table changes separately from version <start> to <incompatibleVersion> - 1, and from version <incompatibleVersion> to <end>.  \nDELTA_CHANGE_DATA_FILE_NOT_FOUND  \nSQLSTATE: 42K03  \nFile <filePath> referenced in the transaction log cannot be found. This can occur when data has been manually deleted from the file system rather than using the table DELETE statement. This request appears to be targeting Change Data Feed, if that is the case, this error can occur when the change data file is out of the retention period and has been deleted by the VACUUM statement. For more information, see <faqPath>  \nDELTA_CHANGE_TABLE_FEED_DISABLED  \nSQLSTATE: 42807  \nCannot write to table with delta.enableChangeDataFeed set. Change data feed from Delta is not available.  \nDELTA_CHECKPOINT_NON_EXIST_TABLE  \nSQLSTATE: 42K03  \nCannot checkpoint a non-existing table <path>. Did you manually delete files in the deltalog directory?  \nDELTA_CLONE_AMBIGUOUS_TARGET  \nSQLSTATE: 42613  \nTwo paths were provided as the CLONE target so it is ambiguous which to use. An external  \nlocation for CLONE was provided at <externalLocation> at the same time as the path  \n<targetIdentifier>.  \nDELTA_CLONE_INCOMPLETE_FILE_COPY  \nSQLSTATE: 42000"
    },
    {
        "id": 694,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "location for CLONE was provided at <externalLocation> at the same time as the path  \n<targetIdentifier>.  \nDELTA_CLONE_INCOMPLETE_FILE_COPY  \nSQLSTATE: 42000  \nFile (<fileName>) not copied completely. Expected file size: <expectedSize>, found: <actualSize>. To continue with the operation by ignoring the file size check set <config> to false.  \nDELTA_CLONE_UNSUPPORTED_SOURCE  \nSQLSTATE: 0AKDC  \nUnsupported <mode> clone source \u2018<name>\u2019, whose format is <format>.  \nThe supported formats are \u2018delta\u2019, \u2018iceberg\u2019 and \u2018parquet\u2019.  \nDELTA_CLUSTERING_CLONE_TABLE_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nCLONE is not supported for Delta table with Liquid clustering for DBR version < 14.0.  \nDELTA_CLUSTERING_COLUMNS_MISMATCH  \nSQLSTATE: 42P10  \nThe provided clustering columns do not match the existing table\u2019s.  \nprovided: <providedClusteringColumns>  \nexisting: <existingClusteringColumns>  \nDELTA_CLUSTERING_COLUMN_MISSING_STATS  \nSQLSTATE: 22000  \nLiquid clustering requires clustering columns to have stats. Couldn\u2019t find clustering column(s) \u2018<columns>\u2019 in stats schema:  \n<schema>  \nDELTA_CLUSTERING_CREATE_EXTERNAL_NON_LIQUID_TABLE_FROM_LIQUID_TABLE  \nSQLSTATE: 22000  \nCreating an external table without liquid clustering from a table directory with liquid clustering is not allowed; path: <path>.  \nDELTA_CLUSTERING_NOT_SUPPORTED  \nSQLSTATE: 42000  \n\u2018<operation>\u2019 does not support clustering.  \nDELTA_CLUSTERING_PHASE_OUT_FAILED  \nSQLSTATE: 0AKDE  \nCannot finish the <phaseOutType> of the table with <tableFeatureToAdd> table feature (reason: <reason>). Please try the OPTIMIZE command again.  \n== Error ==  \n<error>  \nDELTA_CLUSTERING_REPLACE_TABLE_WITH_PARTITIONED_TABLE  \nSQLSTATE: 42000  \nREPLACE a Delta table with Liquid clustering with a partitioned table is not allowed.  \nDELTA_CLUSTERING_SHOW_CREATE_TABLE_WITHOUT_CLUSTERING_COLUMNS  \nSQLSTATE: 0A000"
    },
    {
        "id": 695,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 42000  \nREPLACE a Delta table with Liquid clustering with a partitioned table is not allowed.  \nDELTA_CLUSTERING_SHOW_CREATE_TABLE_WITHOUT_CLUSTERING_COLUMNS  \nSQLSTATE: 0A000  \nSHOW CREATE TABLE is not supported for Delta table with Liquid clustering without any clustering columns.  \nDELTA_CLUSTERING_WITH_DYNAMIC_PARTITION_OVERWRITE  \nSQLSTATE: 42000  \nDynamic partition overwrite mode is not allowed for Delta table with Liquid clustering.  \nDELTA_CLUSTERING_WITH_PARTITION_PREDICATE  \nSQLSTATE: 0A000  \nOPTIMIZE command for Delta table with Liquid clustering doesn\u2019t support partition predicates. Please remove the predicates: <predicates>.  \nDELTA_CLUSTERING_WITH_ZORDER_BY  \nSQLSTATE: 42613  \nOPTIMIZE command for Delta table with Liquid clustering cannot specify ZORDER BY. Please remove ZORDER BY (<zOrderBy>).  \nDELTA_CLUSTER_BY_INVALID_NUM_COLUMNS  \nSQLSTATE: 54000  \nCLUSTER BY for Liquid clustering supports up to <numColumnsLimit> clustering columns, but the table has <actualNumColumns> clustering columns. Please remove the extra clustering columns.  \nDELTA_CLUSTER_BY_SCHEMA_NOT_PROVIDED  \nSQLSTATE: 42908  \nIt is not allowed to specify CLUSTER BY when the schema is not defined. Please define schema for table <tableName>.  \nDELTA_CLUSTER_BY_WITH_BUCKETING  \nSQLSTATE: 42613  \nClustering and bucketing cannot both be specified. Please remove CLUSTERED BY INTO BUCKETS / bucketBy if you want to create a Delta table with clustering.  \nDELTA_CLUSTER_BY_WITH_PARTITIONED_BY  \nSQLSTATE: 42613  \nClustering and partitioning cannot both be specified. Please remove PARTITIONED BY / partitionBy / partitionedBy if you want to create a Delta table with clustering.  \nDELTA_COLLATIONS_NOT_SUPPORTED  \nSQLSTATE: 0AKDC  \nCollations are not supported in Delta Lake.  \nDELTA_COLUMN_DATA_SKIPPING_NOT_SUPPORTED_PARTITIONED_COLUMN  \nSQLSTATE: 0AKDC  \nData skipping is not supported for partition column \u2018<column>\u2019."
    },
    {
        "id": 696,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Collations are not supported in Delta Lake.  \nDELTA_COLUMN_DATA_SKIPPING_NOT_SUPPORTED_PARTITIONED_COLUMN  \nSQLSTATE: 0AKDC  \nData skipping is not supported for partition column \u2018<column>\u2019.  \nDELTA_COLUMN_DATA_SKIPPING_NOT_SUPPORTED_TYPE  \nSQLSTATE: 0AKDC  \nData skipping is not supported for column \u2018<column>\u2019 of type <type>.  \nDELTA_COLUMN_MAPPING_MAX_COLUMN_ID_NOT_SET  \nSQLSTATE: 42703  \nThe max column id property (<prop>) is not set on a column mapping enabled table.  \nDELTA_COLUMN_MAPPING_MAX_COLUMN_ID_NOT_SET_CORRECTLY  \nSQLSTATE: 42703  \nThe max column id property (<prop>) on a column mapping enabled table is <tableMax>, which cannot be smaller than the max column id for all fields (<fieldMax>).  \nDELTA_COLUMN_MISSING_DATA_TYPE  \nSQLSTATE: 42601  \nThe data type of the column <colName> was not provided.  \nDELTA_COLUMN_NOT_FOUND  \nSQLSTATE: 42703  \nUnable to find the column <columnName> given [<columnList>]  \nDELTA_COLUMN_NOT_FOUND_IN_MERGE  \nSQLSTATE: 42703  \nUnable to find the column \u2018<targetCol>\u2019 of the target table from the INSERT columns: <colNames>. INSERT clause must specify value for all the columns of the target table.  \nDELTA_COLUMN_NOT_FOUND_IN_SCHEMA  \nSQLSTATE: 42703  \nCouldn\u2019t find column <columnName> in:  \n<tableSchema>  \nDELTA_COLUMN_PATH_NOT_NESTED  \nSQLSTATE: 42704  \nExpected <columnPath> to be a nested data type, but found <other>. Was looking for the  \nindex of <column> in a nested field.  \nSchema:  \n<schema>  \nDELTA_COLUMN_STRUCT_TYPE_MISMATCH  \nSQLSTATE: 2200G  \nStruct column <source> cannot be inserted into a <targetType> field <targetField> in <targetTable>.  \nDELTA_COMPACTION_VALIDATION_FAILED  \nSQLSTATE: 22000"
    },
    {
        "id": 697,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Struct column <source> cannot be inserted into a <targetType> field <targetField> in <targetTable>.  \nDELTA_COMPACTION_VALIDATION_FAILED  \nSQLSTATE: 22000  \nThe validation of the compaction of path <compactedPath> to <newPath> failed: Please file a bug report.  \nDELTA_COMPLEX_TYPE_COLUMN_CONTAINS_NULL_TYPE  \nSQLSTATE: 22005  \nFound nested NullType in column <columName> which is of <dataType>. Delta doesn\u2019t support writing NullType in complex types.  \nDELTA_CONCURRENT_APPEND  \nSQLSTATE: 2D521  \nConcurrentAppendException: Files were added to <partition> by a concurrent update. <retryMsg> <conflictingCommit>  \nRefer to <docLink> for more details.  \nDELTA_CONCURRENT_DELETE_DELETE  \nSQLSTATE: 2D521  \nConcurrentDeleteDeleteException: This transaction attempted to delete one or more files that were deleted (for example <file>) by a concurrent update. Please try the operation again.<conflictingCommit>  \nRefer to <docLink> for more details.  \nDELTA_CONCURRENT_DELETE_READ  \nSQLSTATE: 2D521  \nConcurrentDeleteReadException: This transaction attempted to read one or more files that were deleted (for example <file>) by a concurrent update. Please try the operation again.<conflictingCommit>  \nRefer to <docLink> for more details.  \nDELTA_CONCURRENT_TRANSACTION  \nSQLSTATE: 2D521  \nConcurrentTransactionException: This error occurs when multiple streaming queries are using the same checkpoint to write into this table. Did you run multiple instances of the same streaming query at the same time?<conflictingCommit>  \nRefer to <docLink> for more details.  \nDELTA_CONCURRENT_WRITE  \nSQLSTATE: 2D521  \nConcurrentWriteException: A concurrent transaction has written new data since the current transaction read the table. Please try the operation again.<conflictingCommit>  \nRefer to <docLink> for more details.  \nDELTA_CONFLICT_SET_COLUMN  \nSQLSTATE: 42701"
    },
    {
        "id": 698,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Refer to <docLink> for more details.  \nDELTA_CONFLICT_SET_COLUMN  \nSQLSTATE: 42701  \nThere is a conflict from these SET columns: <columnList>.  \nDELTA_CONSTRAINT_ALREADY_EXISTS  \nSQLSTATE: 42710  \nConstraint \u2018<constraintName>\u2019 already exists. Please delete the old constraint first.  \nOld constraint:  \n<oldConstraint>  \nDELTA_CONSTRAINT_DATA_TYPE_MISMATCH  \nSQLSTATE: 42K09  \nColumn <columnName> has data type <columnType> and cannot be altered to data type <dataType> because this column is referenced by the following check constraint(s):  \n<constraints>  \nDELTA_CONSTRAINT_DEPENDENT_COLUMN_CHANGE  \nSQLSTATE: 42K09  \nCannot alter column <columnName> because this column is referenced by the following check constraint(s):  \n<constraints>  \nDELTA_CONSTRAINT_DOES_NOT_EXIST  \nSQLSTATE: 42704  \nCannot drop nonexistent constraint <constraintName> from table <tableName>. To avoid throwing an error, provide the parameter IF EXISTS or set the SQL session configuration <config> to <confValue>.  \nDELTA_CONVERSION_NO_PARTITION_FOUND  \nSQLSTATE: 42KD6  \nFound no partition information in the catalog for table <tableName>. Have you run \u201cMSCK REPAIR TABLE\u201d on your table to discover partitions?  \nDELTA_CONVERSION_UNSUPPORTED_COLUMN_MAPPING  \nSQLSTATE: 0AKDC  \nThe configuration \u2018<config>\u2019 cannot be set to <mode> when using CONVERT TO DELTA.  \nDELTA_CONVERT_NON_PARQUET_TABLE  \nSQLSTATE: 0AKDC  \nCONVERT TO DELTA only supports parquet tables, but you are trying to convert a <sourceName> source: <tableId>  \nDELTA_CONVERT_TO_DELTA_ROW_TRACKING_WITHOUT_STATS  \nSQLSTATE: 22000  \nCannot enable row tracking without collecting statistics.  \nIf you want to enable row tracking, do the following:  \nEnable statistics collection by running the command  \nSET <statisticsCollectionPropertyKey> = true  \nRun CONVERT TO DELTA without the NO STATISTICS option.  \nIf you do not want to collect statistics, disable row tracking:"
    },
    {
        "id": 699,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Enable statistics collection by running the command  \nSET <statisticsCollectionPropertyKey> = true  \nRun CONVERT TO DELTA without the NO STATISTICS option.  \nIf you do not want to collect statistics, disable row tracking:  \nDeactivate enabling the table feature by default by running the command:  \nRESET <rowTrackingTableFeatureDefaultKey>  \nDeactivate the table property by default by running:  \nSET <rowTrackingDefaultPropertyKey> = false  \nDELTA_COPY_INTO_TARGET_FORMAT  \nSQLSTATE: 0AKDD  \nCOPY INTO target must be a Delta table.  \nDELTA_CREATE_EXTERNAL_TABLE_WITHOUT_SCHEMA  \nSQLSTATE: 42601  \nYou are trying to create an external table <tableName>  \nfrom <path> using Delta, but the schema is not specified when the  \ninput path is empty.  \nTo learn more about Delta, see <docLink>  \nDELTA_CREATE_EXTERNAL_TABLE_WITHOUT_TXN_LOG  \nSQLSTATE: 42K03  \nYou are trying to create an external table <tableName>  \nfrom %2$s using Delta, but there is no transaction log present at  \n%2$s/_delta_log. Check the upstream job to make sure that it is writing using  \nformat(\u201cdelta\u201d) and that the path is the root of the table.  \nTo learn more about Delta, see <docLink>  \nDELTA_CREATE_TABLE_IDENTIFIER_LOCATION_MISMATCH  \nSQLSTATE: 0AKDC  \nCreating path-based Delta table with a different location isn\u2019t supported. Identifier: <identifier>, Location: <location>  \nDELTA_CREATE_TABLE_MISSING_TABLE_NAME_OR_LOCATION  \nSQLSTATE: 42601  \nTable name or location has to be specified.  \nDELTA_CREATE_TABLE_SCHEME_MISMATCH  \nSQLSTATE: 42KD7  \nThe specified schema does not match the existing schema at <path>.  \n== Specified ==  \n<specifiedSchema>  \n== Existing ==  \n<existingSchema>  \n== Differences ==  \n<schemaDifferences>  \nIf your intention is to keep the existing schema, you can omit the  \nschema from the create table command. Otherwise please ensure that  \nthe schema matches.  \nDELTA_CREATE_TABLE_SET_CLUSTERING_TABLE_FEATURE_NOT_ALLOWED  \nSQLSTATE: 42000"
    },
    {
        "id": 700,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "schema from the create table command. Otherwise please ensure that  \nthe schema matches.  \nDELTA_CREATE_TABLE_SET_CLUSTERING_TABLE_FEATURE_NOT_ALLOWED  \nSQLSTATE: 42000  \nCannot enable <tableFeature> table feature using TBLPROPERTIES. Please use CREATE OR REPLACE TABLE CLUSTER BY to create a Delta table with clustering.  \nDELTA_CREATE_TABLE_WITH_DIFFERENT_CLUSTERING  \nSQLSTATE: 42KD7  \nThe specified clustering columns do not match the existing clustering columns at <path>.  \n== Specified ==  \n<specifiedColumns>  \n== Existing ==  \n<existingColumns>  \nDELTA_CREATE_TABLE_WITH_DIFFERENT_PARTITIONING  \nSQLSTATE: 42KD7  \nThe specified partitioning does not match the existing partitioning at <path>.  \n== Specified ==  \n<specifiedColumns>  \n== Existing ==  \n<existingColumns>  \nDELTA_CREATE_TABLE_WITH_DIFFERENT_PROPERTY  \nSQLSTATE: 42KD7  \nThe specified properties do not match the existing properties at <path>.  \n== Specified ==  \n<specificiedProperties>  \n== Existing ==  \n<existingProperties>  \nDELTA_CREATE_TABLE_WITH_NON_EMPTY_LOCATION  \nSQLSTATE: 42601  \nCannot create table (\u2018<tableId>\u2019). The associated location (\u2018<tableLocation>\u2019) is not empty and also not a Delta table.  \nDELTA_DATA_CHANGE_FALSE  \nSQLSTATE: 0AKDE  \nCannot change table metadata because the \u2018dataChange\u2019 option is set to false. Attempted operation: \u2018<op>\u2019.  \nDELTA_DELETED_PARQUET_FILE_NOT_FOUND  \nSQLSTATE: 42K03  \nFile <filePath> referenced in the transaction log cannot be found. This parquet file may be deleted under Delta\u2019s data retention policy.  \nDefault Delta data retention duration: <logRetentionPeriod>. Modification time of the parquet file: <modificationTime>. Deletion time of the parquet file: <deletionTime>. Deleted on Delta version: <deletionVersion>.  \nDELTA_DELETION_VECTOR_MISSING_NUM_RECORDS  \nSQLSTATE: 2D521"
    },
    {
        "id": 701,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "DELTA_DELETION_VECTOR_MISSING_NUM_RECORDS  \nSQLSTATE: 2D521  \nIt is invalid to commit files with deletion vectors that are missing the numRecords statistic.  \nDELTA_DOMAIN_METADATA_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nDetected DomainMetadata action(s) for domains <domainNames>, but DomainMetadataTableFeature is not enabled.  \nDELTA_DROP_COLUMN_AT_INDEX_LESS_THAN_ZERO  \nSQLSTATE: 42KD8  \nIndex <columnIndex> to drop column is lower than 0  \nDELTA_DROP_COLUMN_ON_SINGLE_FIELD_SCHEMA  \nSQLSTATE: 0AKDC  \nCannot drop column from a schema with a single column. Schema:  \n<schema>  \nDELTA_DUPLICATE_ACTIONS_FOUND  \nSQLSTATE: 2D521  \nFile operation \u2018<actionType>\u2019 for path <path> was specified several times.  \nIt conflicts with <conflictingPath>.  \nIt is not valid for multiple file operations with the same path to exist in a single commit.  \nDELTA_DUPLICATE_COLUMNS_FOUND  \nSQLSTATE: 42711  \nFound duplicate column(s) <coltype>: <duplicateCols>  \nDELTA_DUPLICATE_COLUMNS_ON_INSERT  \nSQLSTATE: 42701  \nDuplicate column names in INSERT clause  \nDELTA_DUPLICATE_COLUMNS_ON_UPDATE_TABLE  \nSQLSTATE: 42701  \n<message>  \nPlease remove duplicate columns before you update your table.  \nDELTA_DUPLICATE_DATA_SKIPPING_COLUMNS  \nSQLSTATE: 42701  \nDuplicated data skipping columns found: <columns>.  \nDELTA_DUPLICATE_DOMAIN_METADATA_INTERNAL_ERROR  \nSQLSTATE: 42601  \nInternal error: two DomainMetadata actions within the same transaction have the same domain <domainName>  \nDELTA_DV_HISTOGRAM_DESERIALIZATON  \nSQLSTATE: 22000  \nCould not deserialize the deleted record counts histogram during table integrity verification.  \nDELTA_DYNAMIC_PARTITION_OVERWRITE_DISABLED  \nSQLSTATE: 0A000  \nDynamic partition overwrite mode is specified by session config or write options, but it is disabled by spark.databricks.delta.dynamicPartitionOverwrite.enabled=false.  \nDELTA_EMPTY_DATA"
    },
    {
        "id": 702,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Dynamic partition overwrite mode is specified by session config or write options, but it is disabled by spark.databricks.delta.dynamicPartitionOverwrite.enabled=false.  \nDELTA_EMPTY_DATA  \nSQLSTATE: 428GU  \nData used in creating the Delta table doesn\u2019t have any columns.  \nDELTA_EMPTY_DIRECTORY  \nSQLSTATE: 42K03  \nNo file found in the directory: <directory>.  \nDELTA_EXCEED_CHAR_VARCHAR_LIMIT  \nSQLSTATE: 22001  \nValue \u201c<value>\u201d exceeds char/varchar type length limitation. Failed check: <expr>.  \nDELTA_FAILED_CAST_PARTITION_VALUE  \nSQLSTATE: 22018  \nFailed to cast partition value <value> to <dataType>  \nDELTA_FAILED_FIND_ATTRIBUTE_IN_OUTPUT_COLUMNS  \nSQLSTATE: 42703  \nCould not find <newAttributeName> among the existing target output <targetOutputColumns>  \nDELTA_FAILED_INFER_SCHEMA  \nSQLSTATE: 42KD9  \nFailed to infer schema from the given list of files.  \nDELTA_FAILED_MERGE_SCHEMA_FILE  \nSQLSTATE: 42KDA  \nFailed to merge schema of file <file>:  \n<schema>  \nDELTA_FAILED_READ_FILE_FOOTER  \nSQLSTATE: KD001  \nCould not read footer for file: <currentFile>  \nDELTA_FAILED_RECOGNIZE_PREDICATE  \nSQLSTATE: 42601  \nCannot recognize the predicate \u2018<predicate>\u2019  \nDELTA_FAILED_SCAN_WITH_HISTORICAL_VERSION  \nSQLSTATE: KD002  \nExpect a full scan of the latest version of the Delta source, but found a historical scan of version <historicalVersion>  \nDELTA_FAILED_TO_MERGE_FIELDS  \nSQLSTATE: 22005  \nFailed to merge fields \u2018<currentField>\u2019 and \u2018<updateField>\u2019  \nDELTA_FEATURES_PROTOCOL_METADATA_MISMATCH  \nSQLSTATE: 0AKDE  \nUnable to operate on this table because the following table features are enabled in metadata but not listed in protocol: <features>.  \nDELTA_FEATURES_REQUIRE_MANUAL_ENABLEMENT  \nSQLSTATE: 0AKDE"
    },
    {
        "id": 703,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Unable to operate on this table because the following table features are enabled in metadata but not listed in protocol: <features>.  \nDELTA_FEATURES_REQUIRE_MANUAL_ENABLEMENT  \nSQLSTATE: 0AKDE  \nYour table schema requires manually enablement of the following table feature(s): <unsupportedFeatures>.  \nTo do this, run the following command for each of features listed above:  \nALTER TABLE table_name SET TBLPROPERTIES ('delta.feature.feature_name' = 'supported')  \nReplace \u201ctable_name\u201d and \u201cfeature_name\u201d with real values.  \nCurrent supported feature(s): <supportedFeatures>.  \nDELTA_FEATURE_DROP_CONFLICT_REVALIDATION_FAIL  \nSQLSTATE: 0AKDE  \nCannot drop feature because a concurrent transaction modified the table.  \nPlease try the operation again.  \n<concurrentCommit>  \nDELTA_FEATURE_DROP_DEPENDENT_FEATURE  \nSQLSTATE: 0AKDE  \nCannot drop table feature <feature> because some other features (<dependentFeatures>) in this table depends on <feature>.  \nConsider dropping them first before dropping this feature.  \nDELTA_FEATURE_DROP_FEATURE_NOT_PRESENT  \nSQLSTATE: 0AKDE  \nCannot drop <feature> from this table because it is not currently present in the table\u2019s protocol.  \nDELTA_FEATURE_DROP_HISTORICAL_VERSIONS_EXIST  \nSQLSTATE: 0AKDE  \nCannot drop <feature> because the Delta log contains historical versions that use the feature.  \nPlease wait until the history retention period (<logRetentionPeriodKey>=<logRetentionPeriod>)  \nhas passed since the feature was last active.  \nAlternatively, please wait for the TRUNCATE HISTORY retention period to expire (<truncateHistoryLogRetentionPeriod>)  \nand then run:  \nALTER TABLE table_name DROP FEATURE feature_name TRUNCATE HISTORY  \nDELTA_FEATURE_DROP_HISTORY_TRUNCATION_NOT_ALLOWED  \nSQLSTATE: 0AKDE  \nThe particular feature does not require history truncation.  \nDELTA_FEATURE_DROP_NONREMOVABLE_FEATURE  \nSQLSTATE: 0AKDE  \nCannot drop <feature> because dropping this feature is not supported.  \nPlease contact Databricks support.  \nDELTA_FEATURE_DROP_UNSUPPORTED_CLIENT_FEATURE  \nSQLSTATE: 0AKDE"
    },
    {
        "id": 704,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 0AKDE  \nCannot drop <feature> because dropping this feature is not supported.  \nPlease contact Databricks support.  \nDELTA_FEATURE_DROP_UNSUPPORTED_CLIENT_FEATURE  \nSQLSTATE: 0AKDE  \nCannot drop <feature> because it is not supported by this Databricks version.  \nConsider using Databricks with a higher version.  \nDELTA_FEATURE_DROP_WAIT_FOR_RETENTION_PERIOD  \nSQLSTATE: 0AKDE  \nDropping <feature> was partially successful.  \nThe feature is now no longer used in the current version of the table. However, the feature  \nis still present in historical versions of the table. The table feature cannot be dropped  \nfrom the table protocol until these historical versions have expired.  \nTo drop the table feature from the protocol, please wait for the historical versions to  \nexpire, and then repeat this command. The retention period for historical versions is  \ncurrently configured as <logRetentionPeriodKey>=<logRetentionPeriod>.  \nAlternatively, please wait for the TRUNCATE HISTORY retention period to expire (<truncateHistoryLogRetentionPeriod>)  \nand then run:  \nALTER TABLE table_name DROP FEATURE feature_name TRUNCATE HISTORY  \nDELTA_FEATURE_REQUIRES_HIGHER_READER_VERSION  \nSQLSTATE: 0AKDE  \nUnable to enable table feature <feature> because it requires a higher reader protocol version (current <current>). Consider upgrading the table\u2019s reader protocol version to <required>, or to a version which supports reader table features. Refer to <docLink> for more information on table protocol versions.  \nDELTA_FEATURE_REQUIRES_HIGHER_WRITER_VERSION  \nSQLSTATE: 0AKDE  \nUnable to enable table feature <feature> because it requires a higher writer protocol version (current <current>). Consider upgrading the table\u2019s writer protocol version to <required>, or to a version which supports writer table features. Refer to <docLink> for more information on table protocol versions.  \nDELTA_FILE_ALREADY_EXISTS  \nSQLSTATE: 42K04  \nExisting file path <path>  \nDELTA_FILE_LIST_AND_PATTERN_STRING_CONFLICT  \nSQLSTATE: 42613  \nCannot specify both file list and pattern string.  \nDELTA_FILE_NOT_FOUND  \nSQLSTATE: 42K03  \nFile path <path>"
    },
    {
        "id": 705,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 42613  \nCannot specify both file list and pattern string.  \nDELTA_FILE_NOT_FOUND  \nSQLSTATE: 42K03  \nFile path <path>  \nDELTA_FILE_NOT_FOUND_DETAILED  \nSQLSTATE: 42K03  \nFile <filePath> referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table DELETE statement. For more information, see <faqPath>  \nDELTA_FILE_OR_DIR_NOT_FOUND  \nSQLSTATE: 42K03  \nNo such file or directory: <path>  \nDELTA_FILE_TO_OVERWRITE_NOT_FOUND  \nSQLSTATE: 42K03  \nFile (<path>) to be rewritten not found among candidate files:  \n<pathList>  \nDELTA_FOUND_MAP_TYPE_COLUMN  \nSQLSTATE: KD003  \nA MapType was found. In order to access the key or value of a MapType, specify one  \nof:  \n<key> or  \n<value>  \nfollowed by the name of the column (only if that column is a struct type).  \ne.g. mymap.key.mykey  \nIf the column is a basic type, mymap.key or mymap.value is sufficient.  \nSchema:  \n<schema>  \nDELTA_GENERATED_COLUMNS_DATA_TYPE_MISMATCH  \nSQLSTATE: 42K09  \nColumn <columnName> has data type <columnType> and cannot be altered to data type <dataType> because this column is referenced by the following generated column(s):  \n<generatedColumns>  \nDELTA_GENERATED_COLUMNS_DEPENDENT_COLUMN_CHANGE  \nSQLSTATE: 42K09  \nCannot alter column <columnName> because this column is referenced by the following generated column(s):  \n<generatedColumns>  \nDELTA_GENERATED_COLUMNS_EXPR_TYPE_MISMATCH  \nSQLSTATE: 42K09  \nThe expression type of the generated column <columnName> is <expressionType>, but the column type is <columnType>  \nDELTA_GENERATED_COLUMN_UPDATE_TYPE_MISMATCH  \nSQLSTATE: 42K09"
    },
    {
        "id": 706,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "The expression type of the generated column <columnName> is <expressionType>, but the column type is <columnType>  \nDELTA_GENERATED_COLUMN_UPDATE_TYPE_MISMATCH  \nSQLSTATE: 42K09  \nColumn <currentName> is a generated column or a column used by a generated column. The data type is <currentDataType> and cannot be converted to data type <updateDataType>  \nDELTA_ICEBERG_COMPAT_VIOLATION  \nSQLSTATE: KD00E  \nThe validation of IcebergCompatV`<version>` has failed.  \nFor more details see DELTA_ICEBERG_COMPAT_VIOLATION  \nDELTA_IDENTITY_COLUMNS_ILLEGAL_STEP  \nSQLSTATE: 42611  \nIDENTITY column step cannot be 0.  \nDELTA_IDENTITY_COLUMNS_UNSUPPORTED_DATA_TYPE  \nSQLSTATE: 428H2  \nDataType <dataType> is not supported for IDENTITY columns.  \nDELTA_IDENTITY_COLUMNS_WITH_GENERATED_EXPRESSION  \nSQLSTATE: 42613  \nIDENTITY column cannot be specified with a generated column expression.  \nDELTA_ILLEGAL_OPTION  \nSQLSTATE: 42616  \nInvalid value \u2018<input>\u2019 for option \u2018<name>\u2019, <explain>  \nDELTA_ILLEGAL_USAGE  \nSQLSTATE: 42601  \nThe usage of <option> is not allowed when <operation> a Delta table.  \nDELTA_INCONSISTENT_BUCKET_SPEC  \nSQLSTATE: 42000  \nBucketSpec on Delta bucketed table does not match BucketSpec from metadata.Expected: <expected>. Actual: <actual>.  \nDELTA_INCONSISTENT_LOGSTORE_CONFS  \nSQLSTATE: F0000  \n(<setKeys>) cannot be set to different values. Please only set one of them, or set them to the same value.  \nDELTA_INCORRECT_ARRAY_ACCESS  \nSQLSTATE: KD003  \nIncorrectly accessing an ArrayType. Use arrayname.element.elementname position to  \nadd to an array.  \nDELTA_INCORRECT_ARRAY_ACCESS_BY_NAME  \nSQLSTATE: KD003  \nAn ArrayType was found. In order to access elements of an ArrayType, specify  \n<rightName> instead of <wrongName>.  \nSchema:  \n<schema>  \nDELTA_INCORRECT_GET_CONF"
    },
    {
        "id": 707,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "An ArrayType was found. In order to access elements of an ArrayType, specify  \n<rightName> instead of <wrongName>.  \nSchema:  \n<schema>  \nDELTA_INCORRECT_GET_CONF  \nSQLSTATE: 42000  \nUse getConf() instead of `conf.getConf()  \nDELTA_INCORRECT_LOG_STORE_IMPLEMENTATION  \nSQLSTATE: 0AKDC  \nThe error typically occurs when the default LogStore implementation, that  \nis, HDFSLogStore, is used to write into a Delta table on a non-HDFS storage system.  \nIn order to get the transactional ACID guarantees on table updates, you have to use the  \ncorrect implementation of LogStore that is appropriate for your storage system.  \nSee <docLink> for details.  \nDELTA_INDEX_LARGER_OR_EQUAL_THAN_STRUCT  \nSQLSTATE: 42KD8  \nIndex <position> to drop column equals to or is larger than struct length: <length>  \nDELTA_INDEX_LARGER_THAN_STRUCT  \nSQLSTATE: 42KD8  \nIndex <index> to add column <columnName> is larger than struct length: <length>  \nDELTA_INSERT_COLUMN_ARITY_MISMATCH  \nSQLSTATE: 42802  \nCannot write to \u2018<tableName>\u2019, <columnName>; target table has <numColumns> column(s) but the inserted data has <insertColumns> column(s)  \nDELTA_INSERT_COLUMN_MISMATCH  \nSQLSTATE: 42802  \nColumn <columnName> is not specified in INSERT  \nDELTA_INVALID_AUTO_COMPACT_TYPE  \nSQLSTATE: 22023  \nInvalid auto-compact type: <value>. Allowed values are: <allowed>.  \nDELTA_INVALID_BUCKET_COUNT  \nSQLSTATE: 22023  \nInvalid bucket count: <invalidBucketCount>. Bucket count should be a positive number that is power of 2 and at least 8. You can use <validBucketCount> instead.  \nDELTA_INVALID_BUCKET_INDEX  \nSQLSTATE: 22023  \nCannot find the bucket column in the partition columns  \nDELTA_INVALID_CALENDAR_INTERVAL_EMPTY  \nSQLSTATE: 2200P  \nInterval cannot be null or blank.  \nDELTA_INVALID_CDC_RANGE  \nSQLSTATE: 22003"
    },
    {
        "id": 708,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Cannot find the bucket column in the partition columns  \nDELTA_INVALID_CALENDAR_INTERVAL_EMPTY  \nSQLSTATE: 2200P  \nInterval cannot be null or blank.  \nDELTA_INVALID_CDC_RANGE  \nSQLSTATE: 22003  \nCDC range from start <start> to end <end> was invalid. End cannot be before start.  \nDELTA_INVALID_CHARACTERS_IN_COLUMN_NAME  \nSQLSTATE: 42K05  \nAttribute name \u201c<columnName>\u201d contains invalid character(s) among \u201d ,;{}()\\n\\t=\u201d. Please use alias to rename it.  \nDELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES  \nSQLSTATE: 42K05  \nFound invalid character(s) among \u2018 ,;{}()nt=\u2019 in the column names of your schema.  \nInvalid column names: <invalidColumnNames>.  \nPlease use other characters and try again.  \nAlternatively, enable Column Mapping to keep using these characters.  \nDELTA_INVALID_CLONE_PATH  \nSQLSTATE: 22KD1  \nThe target location for CLONE needs to be an absolute path or table name. Use an  \nabsolute path instead of <path>.  \nDELTA_INVALID_COLUMN_NAMES_WHEN_REMOVING_COLUMN_MAPPING  \nSQLSTATE: 42K05  \nFound invalid character(s) among \u2018 ,;{}()nt=\u2019 in the column names of your schema.  \nInvalid column names: <invalidColumnNames>.  \nColumn mapping cannot be removed when there are invalid characters in the column names.  \nPlease rename the columns to remove the invalid characters and execute this command again.  \nDELTA_INVALID_FORMAT  \nSQLSTATE: 22000  \nIncompatible format detected.  \nA transaction log for Delta was found at ``<deltaRootPath>/_delta_log,  \nbut you are trying to <operation> <path> using format(\u201c<format>\u201d). You must use  \n\u2018format(\u201cdelta\u201d)\u2019 when reading and writing to a delta table.  \nTo learn more about Delta, see <docLink>  \nDELTA_INVALID_GENERATED_COLUMN_REFERENCES  \nSQLSTATE: 42621  \nA generated column cannot use a non-existent column or another generated column  \nDELTA_INVALID_IDEMPOTENT_WRITES_OPTIONS  \nSQLSTATE: 42616"
    },
    {
        "id": 709,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 42621  \nA generated column cannot use a non-existent column or another generated column  \nDELTA_INVALID_IDEMPOTENT_WRITES_OPTIONS  \nSQLSTATE: 42616  \nInvalid options for idempotent Dataframe writes: <reason>  \nDELTA_INVALID_INTERVAL  \nSQLSTATE: 22006  \n<interval> is not a valid INTERVAL.  \nDELTA_INVALID_INVENTORY_SCHEMA  \nSQLSTATE: 42000  \nThe schema for the specified INVENTORY does not contain all of the required fields. Required fields are: <expectedSchema>  \nDELTA_INVALID_ISOLATION_LEVEL  \nSQLSTATE: 25000  \ninvalid isolation level \u2018<isolationLevel>\u2019  \nDELTA_INVALID_LOGSTORE_CONF  \nSQLSTATE: F0000  \n(<classConfig>) and (<schemeConfig>) cannot be set at the same time. Please set only one group of them.  \nDELTA_INVALID_MANAGED_TABLE_SYNTAX_NO_SCHEMA  \nSQLSTATE: 42000  \nYou are trying to create a managed table <tableName>  \nusing Delta, but the schema is not specified.  \nTo learn more about Delta, see <docLink>  \nDELTA_INVALID_PARTITION_COLUMN  \nSQLSTATE: 42996  \n<columnName> is not a valid partition column in table <tableName>.  \nDELTA_INVALID_PARTITION_COLUMN_NAME  \nSQLSTATE: 42996  \nFound partition columns having invalid character(s) among \u201d ,;{}()nt=\u201d. Please change the name to your partition columns. This check can be turned off by setting spark.conf.set(\u201cspark.databricks.delta.partitionColumnValidity.enabled\u201d, false) however this is not recommended as other features of Delta may not work properly.  \nDELTA_INVALID_PARTITION_COLUMN_TYPE  \nSQLSTATE: 42996  \nUsing column <name> of type <dataType> as a partition column is not supported.  \nDELTA_INVALID_PARTITION_PATH  \nSQLSTATE: 22KD1  \nA partition path fragment should be the form like part1=foo/part2=bar. The partition path: <path>  \nDELTA_INVALID_PROTOCOL_DOWNGRADE  \nSQLSTATE: KD004"
    },
    {
        "id": 710,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "A partition path fragment should be the form like part1=foo/part2=bar. The partition path: <path>  \nDELTA_INVALID_PROTOCOL_DOWNGRADE  \nSQLSTATE: KD004  \nProtocol version cannot be downgraded from <oldProtocol> to <newProtocol>  \nDELTA_INVALID_PROTOCOL_VERSION  \nSQLSTATE: KD004  \nUnsupported Delta protocol version: table \u201c<tableNameOrPath>\u201d requires reader version <readerRequired> and writer version <writerRequired>, but this version of Databricks supports reader versions <supportedReaders> and writer versions <supportedWriters>. Please upgrade to a newer release.  \nDELTA_INVALID_TABLE_VALUE_FUNCTION  \nSQLSTATE: 22000  \nFunction <function> is an unsupported table valued function for CDC reads.  \nDELTA_INVALID_TIMESTAMP_FORMAT  \nSQLSTATE: 22007  \nThe provided timestamp <timestamp> does not match the expected syntax <format>.  \nDELTA_LOG_ALREADY_EXISTS  \nSQLSTATE: 42K04  \nA Delta log already exists at <path>  \nDELTA_LOG_FILE_NOT_FOUND_FOR_STREAMING_SOURCE  \nSQLSTATE: 42K03  \nIf you never deleted it, it\u2019s likely your query is lagging behind. Please delete its checkpoint to restart from scratch. To avoid this happening again, you can update your retention policy of your Delta table  \nDELTA_MATERIALIZED_ROW_TRACKING_COLUMN_NAME_MISSING  \nSQLSTATE: 22000  \nMaterialized <rowTrackingColumn> column name missing for <tableName>.  \nDELTA_MAX_ARRAY_SIZE_EXCEEDED  \nSQLSTATE: 42000  \nPlease use a limit less than Int.MaxValue - 8.  \nDELTA_MAX_COMMIT_RETRIES_EXCEEDED  \nSQLSTATE: 40000  \nThis commit has failed as it has been tried <numAttempts> times but did not succeed.  \nThis can be caused by the Delta table being committed continuously by many concurrent  \ncommits.  \nCommit started at version: <startVersion>  \nCommit failed at version: <failVersion>  \nNumber of actions attempted to commit: <numActions>  \nTotal time spent attempting this commit: <timeSpent> ms  \nDELTA_MAX_LIST_FILE_EXCEEDED  \nSQLSTATE: 42000"
    },
    {
        "id": 711,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Commit failed at version: <failVersion>  \nNumber of actions attempted to commit: <numActions>  \nTotal time spent attempting this commit: <timeSpent> ms  \nDELTA_MAX_LIST_FILE_EXCEEDED  \nSQLSTATE: 42000  \nFile list must have at most <maxFileListSize> entries, had <numFiles>.  \nDELTA_MERGE_ADD_VOID_COLUMN  \nSQLSTATE: 42K09  \nCannot add column <newColumn> with type VOID. Please explicitly specify a non-void type.  \nDELTA_MERGE_INCOMPATIBLE_DATATYPE  \nSQLSTATE: 42K09  \nFailed to merge incompatible data types <currentDataType> and <updateDataType>  \nDELTA_MERGE_INCOMPATIBLE_DECIMAL_TYPE  \nSQLSTATE: 42806  \nFailed to merge decimal types with incompatible <decimalRanges>  \nDELTA_MERGE_MATERIALIZE_SOURCE_FAILED_REPEATEDLY  \nSQLSTATE: 25000  \nKeeping the source of the MERGE statement materialized has failed repeatedly.  \nDELTA_MERGE_MISSING_WHEN  \nSQLSTATE: 42601  \nThere must be at least one WHEN clause in a MERGE statement.  \nDELTA_MERGE_RESOLVED_ATTRIBUTE_MISSING_FROM_INPUT  \nSQLSTATE: 42601  \nResolved attribute(s) <missingAttributes> missing from <input> in operator <merge>  \nDELTA_MERGE_UNEXPECTED_ASSIGNMENT_KEY  \nSQLSTATE: 22005  \nUnexpected assignment key: <unexpectedKeyClass> - <unexpectedKeyObject>  \nDELTA_MERGE_UNRESOLVED_EXPRESSION  \nSQLSTATE: 42601  \nCannot resolve <sqlExpr> in <clause> given <cols>.  \nDELTA_METADATA_CHANGED  \nSQLSTATE: 2D521  \nMetadataChangedException: The metadata of the Delta table has been changed by a concurrent update. Please try the operation again.<conflictingCommit>  \nRefer to <docLink> for more details.  \nDELTA_MISSING_CHANGE_DATA  \nSQLSTATE: KD002  \nError getting change data for range [<startVersion> , <endVersion>] as change data was not  \nrecorded for version [<version>]. If you\u2019ve enabled change data feed on this table,  \nuse DESCRIBE HISTORY to see when it was first enabled."
    },
    {
        "id": 712,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "recorded for version [<version>]. If you\u2019ve enabled change data feed on this table,  \nuse DESCRIBE HISTORY to see when it was first enabled.  \nOtherwise, to start recording change data, use `ALTER TABLE table_name SET TBLPROPERTIES  \n(<key>=true)`.  \nDELTA_MISSING_COLUMN  \nSQLSTATE: 42703  \nCannot find <columnName> in table columns: <columnList>  \nDELTA_MISSING_COMMIT_INFO  \nSQLSTATE: KD004  \nThis table has the feature <featureName> enabled which requires the presence of the CommitInfo action in every commit. However, the CommitInfo action is missing from commit version <version>.  \nDELTA_MISSING_COMMIT_TIMESTAMP  \nSQLSTATE: KD004  \nThis table has the feature <featureName> enabled which requires the presence of commitTimestamp in the CommitInfo action. However, this field has not been set in commit version <version>.  \nDELTA_MISSING_DELTA_TABLE  \nSQLSTATE: 42P01  \n<tableName> is not a Delta table.  \nDELTA_MISSING_DELTA_TABLE_COPY_INTO  \nSQLSTATE: 42P01  \nTable doesn\u2019t exist. Create an empty Delta table first using CREATE TABLE <tableName>.  \nDELTA_MISSING_ICEBERG_CLASS  \nSQLSTATE: 56038  \nIceberg class was not found. Please ensure Delta Iceberg support is installed.  \nPlease refer to <docLink> for more details.  \nDELTA_MISSING_NOT_NULL_COLUMN_VALUE  \nSQLSTATE: 23502  \nColumn <columnName>, which has a NOT NULL constraint, is missing from the data being written into the table.  \nDELTA_MISSING_PARTITION_COLUMN  \nSQLSTATE: 42KD6  \nPartition column <columnName> not found in schema <columnList>  \nDELTA_MISSING_PART_FILES  \nSQLSTATE: 42KD6  \nCouldn\u2019t find all part files of the checkpoint version: <version>  \nDELTA_MISSING_PROVIDER_FOR_CONVERT  \nSQLSTATE: 0AKDC  \nCONVERT TO DELTA only supports parquet tables. Please rewrite your target as parquet.<path> if it\u2019s a parquet directory.  \nDELTA_MISSING_SET_COLUMN  \nSQLSTATE: 42703"
    },
    {
        "id": 713,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "CONVERT TO DELTA only supports parquet tables. Please rewrite your target as parquet.<path> if it\u2019s a parquet directory.  \nDELTA_MISSING_SET_COLUMN  \nSQLSTATE: 42703  \nSET column <columnName> not found given columns: <columnList>.  \nDELTA_MISSING_TRANSACTION_LOG  \nSQLSTATE: 42000  \nIncompatible format detected.  \nYou are trying to <operation> <path> using Delta, but there is no  \ntransaction log present. Check the upstream job to make sure that it is writing  \nusing format(\u201cdelta\u201d) and that you are trying to %1$s the table base path.  \nTo learn more about Delta, see <docLink>  \nDELTA_MODE_NOT_SUPPORTED  \nSQLSTATE: 0AKDC  \nSpecified mode \u2018<mode>\u2019 is not supported. Supported modes are: <supportedModes>  \nDELTA_MULTIPLE_CDC_BOUNDARY  \nSQLSTATE: 42614  \nMultiple <startingOrEnding> arguments provided for CDC read. Please provide one of either <startingOrEnding>Timestamp or <startingOrEnding>Version.  \nDELTA_MULTIPLE_CONF_FOR_SINGLE_COLUMN_IN_BLOOM_FILTER  \nSQLSTATE: 42614  \nMultiple bloom filter index configurations passed to command for column: <columnName>  \nDELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE  \nSQLSTATE: 21506  \nCannot perform Merge as multiple source rows matched and attempted to modify the same  \ntarget row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,  \nwhen multiple source rows match on the same target row, the result may be ambiguous  \nas it is unclear which source row should be used to update or delete the matching  \ntarget row. You can preprocess the source table to eliminate the possibility of  \nmultiple matches. Please refer to  \n<usageReference>  \nDELTA_NAME_CONFLICT_IN_BUCKETED_TABLE  \nSQLSTATE: 42000  \nThe following column name(s) are reserved for Delta bucketed table internal usage only: <names>  \nDELTA_NESTED_FIELDS_NEED_RENAME  \nSQLSTATE: 42K05  \nThe input schema contains nested fields that are capitalized differently than the target table."
    },
    {
        "id": 714,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "DELTA_NESTED_FIELDS_NEED_RENAME  \nSQLSTATE: 42K05  \nThe input schema contains nested fields that are capitalized differently than the target table.  \nThey need to be renamed to avoid the loss of data in these fields while writing to Delta.  \nFields:  \n<fields>.  \nOriginal schema:  \n<schema>  \nDELTA_NESTED_NOT_NULL_CONSTRAINT  \nSQLSTATE: 0AKDC  \nThe <nestType> type of the field <parent> contains a NOT NULL constraint. Delta does not support NOT NULL constraints nested within arrays or maps. To suppress this error and silently ignore the specified constraints, set <configKey> = true.  \nParsed <nestType> type:  \n<nestedPrettyJson>  \nDELTA_NESTED_SUBQUERY_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nNested subquery is not supported in the <operation> condition.  \nDELTA_NEW_CHECK_CONSTRAINT_VIOLATION  \nSQLSTATE: 23512  \n<numRows> rows in <tableName> violate the new CHECK constraint (<checkConstraint>)  \nDELTA_NEW_NOT_NULL_VIOLATION  \nSQLSTATE: 23512  \n<numRows> rows in <tableName> violate the new NOT NULL constraint on <colName>  \nDELTA_NON_BOOLEAN_CHECK_CONSTRAINT  \nSQLSTATE: 42621  \nCHECK constraint \u2018<name>\u2019 (<expr>) should be a boolean expression.  \nDELTA_NON_DETERMINISTIC_EXPRESSION_IN_GENERATED_COLUMN  \nSQLSTATE: 42621  \nFound <expr>. A generated column cannot use a non deterministic expression.  \nDELTA_NON_DETERMINISTIC_FUNCTION_NOT_SUPPORTED  \nSQLSTATE: 0AKDC  \nNon-deterministic functions are not supported in the <operation> <expression>  \nDELTA_NON_LAST_MATCHED_CLAUSE_OMIT_CONDITION  \nSQLSTATE: 42601  \nWhen there are more than one MATCHED clauses in a MERGE statement, only the last MATCHED clause can omit the condition.  \nDELTA_NON_LAST_NOT_MATCHED_BY_SOURCE_CLAUSE_OMIT_CONDITION  \nSQLSTATE: 42601"
    },
    {
        "id": 715,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "DELTA_NON_LAST_NOT_MATCHED_BY_SOURCE_CLAUSE_OMIT_CONDITION  \nSQLSTATE: 42601  \nWhen there are more than one NOT MATCHED BY SOURCE clauses in a MERGE statement, only the last NOT MATCHED BY SOURCE clause can omit the condition.  \nDELTA_NON_LAST_NOT_MATCHED_CLAUSE_OMIT_CONDITION  \nSQLSTATE: 42601  \nWhen there are more than one NOT MATCHED clauses in a MERGE statement, only the last NOT MATCHED clause can omit the condition  \nDELTA_NON_PARSABLE_TAG  \nSQLSTATE: 42601  \nCould not parse tag <tag>.  \nFile tags are: <tagList>  \nDELTA_NON_PARTITION_COLUMN_ABSENT  \nSQLSTATE: KD005  \nData written into Delta needs to contain at least one non-partitioned column.<details>  \nDELTA_NON_PARTITION_COLUMN_REFERENCE  \nSQLSTATE: 42P10  \nPredicate references non-partition column \u2018<columnName>\u2019. Only the partition columns may be referenced: [<columnList>]  \nDELTA_NON_PARTITION_COLUMN_SPECIFIED  \nSQLSTATE: 42P10  \nNon-partitioning column(s) <columnList> are specified where only partitioning columns are expected: <fragment>.  \nDELTA_NON_SINGLE_PART_NAMESPACE_FOR_CATALOG  \nSQLSTATE: 42K05  \nDelta catalog requires a single-part namespace, but <identifier> is multi-part.  \nDELTA_NOT_A_DATABRICKS_DELTA_TABLE  \nSQLSTATE: 42000  \n<table> is not a Delta table. Please drop this table first if you would like to create it with Databricks Delta.  \nDELTA_NOT_A_DELTA_TABLE  \nSQLSTATE: 0AKDD  \n<tableName> is not a Delta table. Please drop this table first if you would like to recreate it with Delta Lake.  \nDELTA_NOT_NULL_COLUMN_NOT_FOUND_IN_STRUCT  \nSQLSTATE: 42K09  \nNot nullable column not found in struct: <struct>  \nDELTA_NOT_NULL_CONSTRAINT_VIOLATED  \nSQLSTATE: 23502  \nNOT NULL constraint violated for column: <columnName>.  \nDELTA_NOT_NULL_NESTED_FIELD  \nSQLSTATE: 0A000"
    },
    {
        "id": 716,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "DELTA_NOT_NULL_CONSTRAINT_VIOLATED  \nSQLSTATE: 23502  \nNOT NULL constraint violated for column: <columnName>.  \nDELTA_NOT_NULL_NESTED_FIELD  \nSQLSTATE: 0A000  \nA non-nullable nested field can\u2019t be added to a nullable parent. Please set the nullability of the parent column accordingly.  \nDELTA_NO_COMMITS_FOUND  \nSQLSTATE: KD006  \nNo commits found at <logPath>  \nDELTA_NO_RECREATABLE_HISTORY_FOUND  \nSQLSTATE: KD006  \nNo recreatable commits found at <logPath>  \nDELTA_NO_RELATION_TABLE  \nSQLSTATE: 42P01  \nTable <tableIdent> not found  \nDELTA_NO_START_FOR_CDC_READ  \nSQLSTATE: 42601  \nNo startingVersion or startingTimestamp provided for CDC read.  \nDELTA_NULL_SCHEMA_IN_STREAMING_WRITE  \nSQLSTATE: 42P18  \nDelta doesn\u2019t accept NullTypes in the schema for streaming writes.  \nDELTA_ONEOF_IN_TIMETRAVEL  \nSQLSTATE: 42601  \nPlease either provide \u2018timestampAsOf\u2019 or \u2018versionAsOf\u2019 for time travel.  \nDELTA_ONLY_OPERATION  \nSQLSTATE: 0AKDD  \n<operation> is only supported for Delta tables.  \nDELTA_OPERATION_MISSING_PATH  \nSQLSTATE: 42601  \nPlease provide the path or table identifier for <operation>.  \nDELTA_OPERATION_NOT_ALLOWED  \nSQLSTATE: 0AKDC  \nOperation not allowed: <operation> is not supported for Delta tables  \nDELTA_OPERATION_NOT_ALLOWED_DETAIL  \nSQLSTATE: 0AKDC  \nOperation not allowed: <operation> is not supported for Delta tables: <tableName>  \nDELTA_OPERATION_NOT_SUPPORTED_FOR_COLUMN_WITH_COLLATION  \nSQLSTATE: 0AKDC  \n<operation> is not supported for column <colName> with non-default collation <collation>.  \nDELTA_OPERATION_NOT_SUPPORTED_FOR_EXPRESSION_WITH_COLLATION  \nSQLSTATE: 0AKDC  \n<operation> is not supported for expression <exprText> because it uses non-default collation.  \nDELTA_OPERATION_ON_TEMP_VIEW_WITH_GENERATED_COLS_NOT_SUPPORTED"
    },
    {
        "id": 717,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 0AKDC  \n<operation> is not supported for expression <exprText> because it uses non-default collation.  \nDELTA_OPERATION_ON_TEMP_VIEW_WITH_GENERATED_COLS_NOT_SUPPORTED  \nSQLSTATE: 0A000  \n<operation> command on a temp view referring to a Delta table that contains generated columns is not supported. Please run the <operation> command on the Delta table directly  \nDELTA_OPERATION_ON_VIEW_NOT_ALLOWED  \nSQLSTATE: 0AKDC  \nOperation not allowed: <operation> cannot be performed on a view.  \nDELTA_OVERWRITE_MUST_BE_TRUE  \nSQLSTATE: 42000  \nCopy option overwriteSchema cannot be specified without setting OVERWRITE = \u2018true\u2019.  \nDELTA_OVERWRITE_SCHEMA_WITH_DYNAMIC_PARTITION_OVERWRITE  \nSQLSTATE: 42613  \n\u2018overwriteSchema\u2019 cannot be used in dynamic partition overwrite mode.  \nDELTA_PARTITION_COLUMN_CAST_FAILED  \nSQLSTATE: 22525  \nFailed to cast value <value> to <dataType> for partition column <columnName>  \nDELTA_PARTITION_COLUMN_NOT_FOUND  \nSQLSTATE: 42703  \nPartition column <columnName> not found in schema [<schemaMap>]  \nDELTA_PARTITION_SCHEMA_IN_ICEBERG_TABLES  \nSQLSTATE: 42613  \nPartition schema cannot be specified when converting Iceberg tables. It is automatically inferred.  \nDELTA_PATH_DOES_NOT_EXIST  \nSQLSTATE: 42K03  \n<path> doesn\u2019t exist, or is not a Delta table.  \nDELTA_PATH_EXISTS  \nSQLSTATE: 42K04  \nCannot write to already existent path <path> without setting OVERWRITE = \u2018true\u2019.  \nDELTA_POST_COMMIT_HOOK_FAILED  \nSQLSTATE: 2DKD0  \nCommitting to the Delta table version <version> succeeded but error while executing post-commit hook <name> <message>  \nDELTA_PROTOCOL_CHANGED  \nSQLSTATE: 2D521  \nProtocolChangedException: The protocol version of the Delta table has been changed by a concurrent update. <additionalInfo> <conflictingCommit>  \nRefer to <docLink> for more details."
    },
    {
        "id": 718,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "ProtocolChangedException: The protocol version of the Delta table has been changed by a concurrent update. <additionalInfo> <conflictingCommit>  \nRefer to <docLink> for more details.  \nDELTA_PROTOCOL_PROPERTY_NOT_INT  \nSQLSTATE: 42K06  \nProtocol property <key> needs to be an integer. Found <value>  \nDELTA_READ_FEATURE_PROTOCOL_REQUIRES_WRITE  \nSQLSTATE: KD004  \nUnable to upgrade only the reader protocol version to use table features. Writer protocol version must be at least <writerVersion> to proceed. Refer to <docLink> for more information on table protocol versions.  \nDELTA_READ_TABLE_WITHOUT_COLUMNS  \nSQLSTATE: 428GU  \nYou are trying to read a Delta table <tableName> that does not have any columns.  \nWrite some new data with the option mergeSchema = true to be able to read the table.  \nDELTA_REGEX_OPT_SYNTAX_ERROR  \nSQLSTATE: 2201B  \nPlease recheck your syntax for \u2018<regExpOption>\u2019  \nDELTA_REPLACE_WHERE_IN_OVERWRITE  \nSQLSTATE: 42613  \nYou can\u2019t use replaceWhere in conjunction with an overwrite by filter  \nDELTA_REPLACE_WHERE_MISMATCH  \nSQLSTATE: 44000  \nWritten data does not conform to partial table overwrite condition or constraint \u2018<replaceWhere>\u2019.  \n<message>  \nDELTA_REPLACE_WHERE_WITH_DYNAMIC_PARTITION_OVERWRITE  \nSQLSTATE: 42613  \nA \u2018replaceWhere\u2019 expression and \u2018partitionOverwriteMode\u2019=\u2019dynamic\u2019 cannot both be set in the DataFrameWriter options.  \nDELTA_REPLACE_WHERE_WITH_FILTER_DATA_CHANGE_UNSET  \nSQLSTATE: 42613  \n\u2018replaceWhere\u2019 cannot be used with data filters when \u2018dataChange\u2019 is set to false. Filters: <dataFilters>  \nDELTA_ROW_ID_ASSIGNMENT_WITHOUT_STATS  \nSQLSTATE: 22000  \nCannot assign row IDs without row count statistics.  \nCollect statistics for the table by running the following code in a Scala notebook and retry:"
    },
    {
        "id": 719,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "DELTA_ROW_ID_ASSIGNMENT_WITHOUT_STATS  \nSQLSTATE: 22000  \nCannot assign row IDs without row count statistics.  \nCollect statistics for the table by running the following code in a Scala notebook and retry:  \nimport com.databricks.sql.transaction.tahoe.DeltaLog import com.databricks.sql.transaction.tahoe.stats.StatisticsCollection import org.apache.spark.sql.catalyst.TableIdentifier val log = DeltaLog.forTable(spark, TableIdentifier(table_name)) StatisticsCollection.recompute(spark, log)  \nDELTA_SCHEMA_CHANGED  \nSQLSTATE: KD007  \nDetected schema change:  \nstreaming source schema: <readSchema>  \ndata file schema: <dataSchema>  \nPlease try restarting the query. If this issue repeats across query restarts without  \nmaking progress, you have made an incompatible schema change and need to start your  \nquery from scratch using a new checkpoint directory.  \nDELTA_SCHEMA_CHANGED_WITH_STARTING_OPTIONS  \nSQLSTATE: KD007  \nDetected schema change in version <version>:  \nstreaming source schema: <readSchema>  \ndata file schema: <dataSchema>  \nPlease try restarting the query. If this issue repeats across query restarts without  \nmaking progress, you have made an incompatible schema change and need to start your  \nquery from scratch using a new checkpoint directory. If the issue persists after  \nchanging to a new checkpoint directory, you may need to change the existing  \n\u2018startingVersion\u2019 or \u2018startingTimestamp\u2019 option to start from a version newer than  \n<version> with a new checkpoint directory.  \nDELTA_SCHEMA_CHANGED_WITH_VERSION  \nSQLSTATE: KD007  \nDetected schema change in version <version>:  \nstreaming source schema: <readSchema>  \ndata file schema: <dataSchema>  \nPlease try restarting the query. If this issue repeats across query restarts without  \nmaking progress, you have made an incompatible schema change and need to start your  \nquery from scratch using a new checkpoint directory.  \nDELTA_SCHEMA_CHANGE_SINCE_ANALYSIS  \nSQLSTATE: KD007  \nThe schema of your Delta table has changed in an incompatible way since your DataFrame"
    },
    {
        "id": 720,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "query from scratch using a new checkpoint directory.  \nDELTA_SCHEMA_CHANGE_SINCE_ANALYSIS  \nSQLSTATE: KD007  \nThe schema of your Delta table has changed in an incompatible way since your DataFrame  \nor DeltaTable object was created. Please redefine your DataFrame or DeltaTable object.  \nChanges:  \n<schemaDiff> <legacyFlagMessage>  \nDELTA_SCHEMA_NOT_PROVIDED  \nSQLSTATE: 42908  \nTable schema is not provided. Please provide the schema (column definition) of the table when using REPLACE table and an AS SELECT query is not provided.  \nDELTA_SCHEMA_NOT_SET  \nSQLSTATE: KD008  \nTable schema is not set. Write data into it or use CREATE TABLE to set the schema.  \nDELTA_SET_LOCATION_SCHEMA_MISMATCH  \nSQLSTATE: 42KD7  \nThe schema of the new Delta location is different than the current table schema.  \noriginal schema:  \n<original>  \ndestination schema:  \n<destination>  \nIf this is an intended change, you may turn this check off by running:  \n%%sql set <config> = true  \nDELTA_SHALLOW_CLONE_FILE_NOT_FOUND  \nSQLSTATE: 42K03  \nFile <filePath> referenced in the transaction log cannot be found. This can occur when data has been manually deleted from the file system rather than using the table DELETE statement. This table appears to be a shallow clone, if that is the case, this error can occur when the original table from which this table was cloned has deleted a file that the clone is still using. If you want any clones to be independent of the original table, use a DEEP clone instead.  \nDELTA_SHARING_CURRENT_RECIPIENT_PROPERTY_UNDEFINED  \nSQLSTATE: 42704  \nThe data is restricted by recipient property <property> that do not apply to the current recipient in the session.  \nFor more details see DELTA_SHARING_CURRENT_RECIPIENT_PROPERTY_UNDEFINED  \nDELTA_SHARING_INVALID_OP_IN_EXTERNAL_SHARED_VIEW  \nSQLSTATE: 42887  \n<operation> cannot be used in Delta Sharing views that are shared cross account.  \nDELTA_SHOW_PARTITION_IN_NON_PARTITIONED_COLUMN  \nSQLSTATE: 42P10"
    },
    {
        "id": 721,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 42887  \n<operation> cannot be used in Delta Sharing views that are shared cross account.  \nDELTA_SHOW_PARTITION_IN_NON_PARTITIONED_COLUMN  \nSQLSTATE: 42P10  \nNon-partitioning column(s) <badCols> are specified for SHOW PARTITIONS  \nDELTA_SHOW_PARTITION_IN_NON_PARTITIONED_TABLE  \nSQLSTATE: 42809  \nSHOW PARTITIONS is not allowed on a table that is not partitioned: <tableName>  \nDELTA_SOURCE_IGNORE_DELETE  \nSQLSTATE: 0A000  \nDetected deleted data (for example <removedFile>) from streaming source at version <version>. This is currently not supported. If you\u2019d like to ignore deletes, set the option \u2018ignoreDeletes\u2019 to \u2018true\u2019. The source table can be found at path <dataPath>.  \nDELTA_SOURCE_TABLE_IGNORE_CHANGES  \nSQLSTATE: 0A000  \nDetected a data update (for example <file>) in the source table at version <version>. This is currently not supported. If this is going to happen regularly and you are okay to skip changes, set the option \u2018skipChangeCommits\u2019 to \u2018true\u2019. If you would like the data update to be reflected, please restart this query with a fresh checkpoint directory or do a full refresh if you are using DLT. If you need to handle these changes, please switch to MVs. The source table can be found at path <dataPath>.  \nDELTA_STARTING_VERSION_AND_TIMESTAMP_BOTH_SET  \nSQLSTATE: 42613  \nPlease either provide \u2018<version>\u2019 or \u2018<timestamp>\u2019  \nDELTA_STATS_COLLECTION_COLUMN_NOT_FOUND  \nSQLSTATE: 42000  \n<statsType> stats not found for column in Parquet metadata: <columnPath>.  \nDELTA_STREAMING_CANNOT_CONTINUE_PROCESSING_POST_SCHEMA_EVOLUTION  \nSQLSTATE: KD002  \nWe\u2019ve detected one or more non-additive schema change(s) (<opType>) between Delta version <previousSchemaChangeVersion> and <currentSchemaChangeVersion> in the Delta streaming source."
    },
    {
        "id": 722,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Please check if you want to manually propagate the schema change(s) to the sink table before we proceed with stream processing using the finalized schema at <currentSchemaChangeVersion>.  \nOnce you have fixed the schema of the sink table or have decided there is no need to fix, you can set (one of) the following SQL configurations to unblock the non-additive schema change(s) and continue stream processing.  \nTo unblock for this particular stream just for this series of schema change(s): set <allowCkptVerKey>` = `<allowCkptVerValue>.  \nTo unblock for this particular stream: set <allowCkptKey>` = `<allowCkptValue>  \nTo unblock for all streams: set <allowAllKey>` = `<allowAllValue>.  \nAlternatively if applicable, you may replace the <allowAllMode> with <opSpecificMode> in the SQL conf to unblock stream for just this schema change type.  \nDELTA_STREAMING_CHECK_COLUMN_MAPPING_NO_SNAPSHOT  \nSQLSTATE: KD002  \nFailed to obtain Delta log snapshot for the start version when checking column mapping schema changes. Please choose a different start version, or force enable streaming read at your own risk by setting \u2018<config>\u2019 to \u2018true\u2019.  \nDELTA_STREAMING_INCOMPATIBLE_SCHEMA_CHANGE  \nSQLSTATE: 42KD4  \nStreaming read is not supported on tables with read-incompatible schema changes (e.g. rename or drop or datatype changes).  \nFor further information and possible next steps to resolve this issue, please review the documentation at <docLink>  \nRead schema: <readSchema>. Incompatible data schema: <incompatibleSchema>.  \nDELTA_STREAMING_INCOMPATIBLE_SCHEMA_CHANGE_USE_SCHEMA_LOG  \nSQLSTATE: 42KD4  \nStreaming read is not supported on tables with read-incompatible schema changes (e.g. rename or drop or datatype changes).  \nPlease provide a \u2018schemaTrackingLocation\u2019 to enable non-additive schema evolution for Delta stream processing.  \nSee <docLink> for more details."
    },
    {
        "id": 723,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Please provide a \u2018schemaTrackingLocation\u2019 to enable non-additive schema evolution for Delta stream processing.  \nSee <docLink> for more details.  \nRead schema: <readSchema>. Incompatible data schema: <incompatibleSchema>.  \nDELTA_STREAMING_METADATA_EVOLUTION  \nSQLSTATE: 22000  \nThe schema, table configuration or protocol of your Delta table has changed during streaming.  \nThe schema or metadata tracking log has been updated.  \nPlease restart the stream to continue processing using the updated metadata.  \nUpdated schema: <schema>.  \nUpdated table configurations: <config>.  \nUpdated table protocol: <protocol>  \nDELTA_STREAMING_SCHEMA_EVOLUTION_UNSUPPORTED_ROW_FILTER_COLUMN_MASKS  \nSQLSTATE: 22000  \nStreaming from source table <tableId> with schema tracking does not support row filters or column masks.  \nPlease drop the row filters or column masks, or disable schema tracking.  \nDELTA_STREAMING_SCHEMA_LOCATION_CONFLICT  \nSQLSTATE: 22000  \nDetected conflicting schema location \u2018<loc>\u2019 while streaming from table or table located at \u2018<table>\u2019.  \nAnother stream may be reusing the same schema location, which is not allowed.  \nPlease provide a new unique schemaTrackingLocation path or streamingSourceTrackingId as a reader option for one of the streams from this table.  \nDELTA_STREAMING_SCHEMA_LOCATION_NOT_UNDER_CHECKPOINT  \nSQLSTATE: 22000  \nSchema location \u2018<schemaTrackingLocation>\u2019 must be placed under checkpoint location \u2018<checkpointLocation>\u2019.  \nDELTA_STREAMING_SCHEMA_LOG_DESERIALIZE_FAILED  \nSQLSTATE: 22000  \nIncomplete log file in the Delta streaming source schema log at \u2018<location>\u2019.  \nThe schema log may have been corrupted. Please pick a new schema location.  \nDELTA_STREAMING_SCHEMA_LOG_INCOMPATIBLE_DELTA_TABLE_ID  \nSQLSTATE: 22000  \nDetected incompatible Delta table id when trying to read Delta stream.  \nPersisted table id: <persistedId>, Table id: <tableId>  \nThe schema log might have been reused. Please pick a new schema location.  \nDELTA_STREAMING_SCHEMA_LOG_INCOMPATIBLE_PARTITION_SCHEMA  \nSQLSTATE: 22000"
    },
    {
        "id": 724,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "The schema log might have been reused. Please pick a new schema location.  \nDELTA_STREAMING_SCHEMA_LOG_INCOMPATIBLE_PARTITION_SCHEMA  \nSQLSTATE: 22000  \nDetected incompatible partition schema when trying to read Delta stream.  \nPersisted schema: <persistedSchema>, Delta partition schema: <partitionSchema>  \nPlease pick a new schema location to reinitialize the schema log if you have manually changed the table\u2019s partition schema recently.  \nDELTA_STREAMING_SCHEMA_LOG_INIT_FAILED_INCOMPATIBLE_METADATA  \nSQLSTATE: 22000  \nWe could not initialize the Delta streaming source schema log because  \nwe detected an incompatible schema or protocol change while serving a streaming batch from table version <a> to <b>.  \nDELTA_STREAMING_SCHEMA_LOG_PARSE_SCHEMA_FAILED  \nSQLSTATE: 22000  \nFailed to parse the schema from the Delta streaming source schema log.  \nThe schema log may have been corrupted. Please pick a new schema location.  \nDELTA_TABLE_ALREADY_CONTAINS_CDC_COLUMNS  \nSQLSTATE: 42711  \nUnable to enable Change Data Capture on the table. The table already contains  \nreserved columns <columnList> that will  \nbe used internally as metadata for the table\u2019s Change Data Feed. To enable  \nChange Data Feed on the table rename/drop these columns.  \nDELTA_TABLE_ALREADY_EXISTS  \nSQLSTATE: 42P07  \nTable <tableName> already exists.  \nDELTA_TABLE_FOR_PATH_UNSUPPORTED_HADOOP_CONF  \nSQLSTATE: 0AKDC  \nCurrently DeltaTable.forPath only supports hadoop configuration keys starting with <allowedPrefixes> but got <unsupportedOptions>  \nDELTA_TABLE_ID_MISMATCH  \nSQLSTATE: KD007  \nThe Delta table at <tableLocation> has been replaced while this command was using the table.  \nTable id was <oldId> but is now <newId>.  \nPlease retry the current command to ensure it reads a consistent view of the table.  \nDELTA_TABLE_LOCATION_MISMATCH  \nSQLSTATE: 42613"
    },
    {
        "id": 725,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Table id was <oldId> but is now <newId>.  \nPlease retry the current command to ensure it reads a consistent view of the table.  \nDELTA_TABLE_LOCATION_MISMATCH  \nSQLSTATE: 42613  \nThe location of the existing table <tableName> is <existingTableLocation>. It doesn\u2019t match the specified location <tableLocation>.  \nDELTA_TABLE_NOT_FOUND  \nSQLSTATE: 42P01  \nDelta table <tableName> doesn\u2019t exist.  \nDELTA_TABLE_NOT_SUPPORTED_IN_OP  \nSQLSTATE: 42809  \nTable is not supported in <operation>. Please use a path instead.  \nDELTA_TABLE_ONLY_OPERATION  \nSQLSTATE: 0AKDD  \n<tableName> is not a Delta table. <operation> is only supported for Delta tables.  \nDELTA_TARGET_TABLE_FINAL_SCHEMA_EMPTY  \nSQLSTATE: 428GU  \nTarget table final schema is empty.  \nDELTA_TIMESTAMP_GREATER_THAN_COMMIT  \nSQLSTATE: 42816  \nThe provided timestamp (<providedTimestamp>) is after the latest version available to this  \ntable (<tableName>). Please use a timestamp before or at <maximumTimestamp>.  \nDELTA_TIMESTAMP_INVALID  \nSQLSTATE: 42816  \nThe provided timestamp (<expr>) cannot be converted to a valid timestamp.  \nDELTA_TIME_TRAVEL_INVALID_BEGIN_VALUE  \nSQLSTATE: 42604  \n<timeTravelKey> needs to be a valid begin value.  \nDELTA_TRUNCATED_TRANSACTION_LOG  \nSQLSTATE: 42K03  \n<path>: Unable to reconstruct state at version <version> as the transaction log has been truncated due to manual deletion or the log retention policy (<logRetentionKey>=<logRetention>) and checkpoint retention policy (<checkpointRetentionKey>=<checkpointRetention>)  \nDELTA_TRUNCATE_TABLE_PARTITION_NOT_SUPPORTED  \nSQLSTATE: 0AKDC  \nOperation not allowed: TRUNCATE TABLE on Delta tables does not support partition predicates; use DELETE to delete specific partitions or rows.  \nDELTA_UDF_IN_GENERATED_COLUMN  \nSQLSTATE: 42621"
    },
    {
        "id": 726,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Operation not allowed: TRUNCATE TABLE on Delta tables does not support partition predicates; use DELETE to delete specific partitions or rows.  \nDELTA_UDF_IN_GENERATED_COLUMN  \nSQLSTATE: 42621  \nFound <udfExpr>. A generated column cannot use a user-defined function  \nDELTA_UNEXPECTED_ACTION_EXPRESSION  \nSQLSTATE: 42601  \nUnexpected action expression <expression>.  \nDELTA_UNEXPECTED_NUM_PARTITION_COLUMNS_FROM_FILE_NAME  \nSQLSTATE: KD009  \nExpecting <expectedColsSize> partition column(s): <expectedCols>, but found <parsedColsSize> partition column(s): <parsedCols> from parsing the file name: <path>  \nDELTA_UNEXPECTED_PARTIAL_SCAN  \nSQLSTATE: KD00A  \nExpect a full scan of Delta sources, but found a partial scan. path:<path>  \nDELTA_UNEXPECTED_PARTITION_COLUMN_FROM_FILE_NAME  \nSQLSTATE: KD009  \nExpecting partition column <expectedCol>, but found partition column <parsedCol> from parsing the file name: <path>  \nDELTA_UNEXPECTED_PARTITION_SCHEMA_FROM_USER  \nSQLSTATE: KD009  \nCONVERT TO DELTA was called with a partition schema different from the partition schema inferred from the catalog, please avoid providing the schema so that the partition schema can be chosen from the catalog.  \ncatalog partition schema:  \n<catalogPartitionSchema>  \nprovided partition schema:  \n<userPartitionSchema>  \nDELTA_UNIFORM_ICEBERG_INGRESS_VIOLATION  \nSQLSTATE: KD00E  \nRead Iceberg with Delta Uniform has failed.  \nFor more details see DELTA_UNIFORM_ICEBERG_INGRESS_VIOLATION  \nDELTA_UNIFORM_INGRESS_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nCreate or Refresh Uniform ingress table is not supported.  \nDELTA_UNIFORM_INGRESS_NOT_SUPPORTED_FORMAT  \nSQLSTATE: 0AKDC  \nFormat <fileFormat> is not supported. Only iceberg as original file format is supported.  \nDELTA_UNIFORM_NOT_SUPPORTED  \nSQLSTATE: 0AKDC  \nUniversal Format is only supported on Unity Catalog tables.  \nDELTA_UNIVERSAL_FORMAT_CONVERSION_FAILED  \nSQLSTATE: KD00E"
    },
    {
        "id": 727,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "DELTA_UNIFORM_NOT_SUPPORTED  \nSQLSTATE: 0AKDC  \nUniversal Format is only supported on Unity Catalog tables.  \nDELTA_UNIVERSAL_FORMAT_CONVERSION_FAILED  \nSQLSTATE: KD00E  \nFailed to convert the table version <version> to the universal format <format>. <message>  \nDELTA_UNIVERSAL_FORMAT_VIOLATION  \nSQLSTATE: KD00E  \nThe validation of Universal Format (<format>) has failed: <violation>  \nDELTA_UNKNOWN_CONFIGURATION  \nSQLSTATE: F0000  \nUnknown configuration was specified: <config>  \nDELTA_UNKNOWN_PRIVILEGE  \nSQLSTATE: 42601  \nUnknown privilege: <privilege>  \nDELTA_UNKNOWN_READ_LIMIT  \nSQLSTATE: 42601  \nUnknown ReadLimit: <limit>  \nDELTA_UNRECOGNIZED_COLUMN_CHANGE  \nSQLSTATE: 42601  \nUnrecognized column change <otherClass>. You may be running an out-of-date Delta Lake version.  \nDELTA_UNRECOGNIZED_INVARIANT  \nSQLSTATE: 56038  \nUnrecognized invariant. Please upgrade your Spark version.  \nDELTA_UNRECOGNIZED_LOGFILE  \nSQLSTATE: KD00B  \nUnrecognized log file <fileName>  \nDELTA_UNSET_NON_EXISTENT_PROPERTY  \nSQLSTATE: 42616  \nAttempted to unset non-existent property \u2018<property>\u2019 in table <tableName>  \nDELTA_UNSUPPORTED_ABS_PATH_ADD_FILE  \nSQLSTATE: 0AKDC  \n<path> does not support adding files with an absolute path  \nDELTA_UNSUPPORTED_ALTER_TABLE_CHANGE_COL_OP  \nSQLSTATE: 0AKDC  \nALTER TABLE CHANGE COLUMN is not supported for changing column <fieldPath> from <oldField> to <newField>  \nDELTA_UNSUPPORTED_ALTER_TABLE_REPLACE_COL_OP  \nSQLSTATE: 0AKDC  \nUnsupported ALTER TABLE REPLACE COLUMNS operation. Reason: <details>  \nFailed to change schema from:  \n<oldSchema>  \nto:  \n<newSchema>  \nDELTA_UNSUPPORTED_CLONE_REPLACE_SAME_TABLE  \nSQLSTATE: 0AKDC  \nYou tried to REPLACE an existing table (<tableName>) with CLONE. This operation is  \nunsupported. Try a different target for CLONE or delete the table at the current target.  \nDELTA_UNSUPPORTED_COLUMN_MAPPING_MODE_CHANGE  \nSQLSTATE: 0AKDC"
    },
    {
        "id": 728,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "unsupported. Try a different target for CLONE or delete the table at the current target.  \nDELTA_UNSUPPORTED_COLUMN_MAPPING_MODE_CHANGE  \nSQLSTATE: 0AKDC  \nChanging column mapping mode from \u2018<oldMode>\u2019 to \u2018<newMode>\u2019 is not supported.  \nDELTA_UNSUPPORTED_COLUMN_MAPPING_PROTOCOL  \nSQLSTATE: KD004  \nYour current table protocol version does not support changing column mapping modes  \nusing <config>.  \nRequired Delta protocol version for column mapping:  \n<requiredVersion>  \nYour table\u2019s current Delta protocol version:  \n<currentVersion>  \n<advice>  \nDELTA_UNSUPPORTED_COLUMN_MAPPING_SCHEMA_CHANGE  \nSQLSTATE: 0AKDC  \nSchema change is detected:  \nold schema:  \n<oldTableSchema>  \nnew schema:  \n<newTableSchema>  \nSchema changes are not allowed during the change of column mapping mode.  \nDELTA_UNSUPPORTED_COLUMN_MAPPING_WRITE  \nSQLSTATE: 0AKDC  \nWriting data with column mapping mode is not supported.  \nDELTA_UNSUPPORTED_COLUMN_TYPE_IN_BLOOM_FILTER  \nSQLSTATE: 0AKDC  \nCreating a bloom filter index on a column with type <dataType> is unsupported: <columnName>  \nDELTA_UNSUPPORTED_COMMENT_MAP_ARRAY  \nSQLSTATE: 0AKDC  \nCan\u2019t add a comment to <fieldPath>. Adding a comment to a map key/value or array element is not supported.  \nDELTA_UNSUPPORTED_DATA_TYPES  \nSQLSTATE: 0AKDC  \nFound columns using unsupported data types: <dataTypeList>. You can set \u2018<config>\u2019 to \u2018false\u2019 to disable the type check. Disabling this type check may allow users to create unsupported Delta tables and should only be used when trying to read/write legacy tables.  \nDELTA_UNSUPPORTED_DATA_TYPE_IN_GENERATED_COLUMN  \nSQLSTATE: 42621  \n<dataType> cannot be the result of a generated column  \nDELTA_UNSUPPORTED_DEEP_CLONE  \nSQLSTATE: 0A000  \nDeep clone is not supported for this Delta version.  \nDELTA_UNSUPPORTED_DESCRIBE_DETAIL_VIEW  \nSQLSTATE: 42809  \n<view> is a view. DESCRIBE DETAIL is only supported for tables.  \nDELTA_UNSUPPORTED_DROP_CLUSTERING_COLUMN"
    },
    {
        "id": 729,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "DELTA_UNSUPPORTED_DESCRIBE_DETAIL_VIEW  \nSQLSTATE: 42809  \n<view> is a view. DESCRIBE DETAIL is only supported for tables.  \nDELTA_UNSUPPORTED_DROP_CLUSTERING_COLUMN  \nSQLSTATE: 0AKDC  \nDropping clustering columns (<columnList>) is not allowed.  \nDELTA_UNSUPPORTED_DROP_COLUMN  \nSQLSTATE: 0AKDC  \nDROP COLUMN is not supported for your Delta table. <advice>  \nDELTA_UNSUPPORTED_DROP_NESTED_COLUMN_FROM_NON_STRUCT_TYPE  \nSQLSTATE: 0AKDC  \nCan only drop nested columns from StructType. Found <struct>  \nDELTA_UNSUPPORTED_DROP_PARTITION_COLUMN  \nSQLSTATE: 0AKDC  \nDropping partition columns (<columnList>) is not allowed.  \nDELTA_UNSUPPORTED_EXPRESSION  \nSQLSTATE: 0A000  \nUnsupported expression type(<expType>) for <causedBy>. The supported types are [<supportedTypes>].  \nDELTA_UNSUPPORTED_EXPRESSION_GENERATED_COLUMN  \nSQLSTATE: 42621  \n<expression> cannot be used in a generated column  \nDELTA_UNSUPPORTED_FEATURES_FOR_READ  \nSQLSTATE: 56038  \nUnsupported Delta read feature: table \u201c<tableNameOrPath>\u201d requires reader table feature(s) that are unsupported by this version of Databricks: <unsupported>. Please refer to <link> for more information on Delta Lake feature compatibility.  \nDELTA_UNSUPPORTED_FEATURES_FOR_WRITE  \nSQLSTATE: 56038  \nUnsupported Delta write feature: table \u201c<tableNameOrPath>\u201d requires writer table feature(s) that are unsupported by this version of Databricks: <unsupported>. Please refer to <link> for more information on Delta Lake feature compatibility.  \nDELTA_UNSUPPORTED_FEATURES_IN_CONFIG  \nSQLSTATE: 56038  \nTable feature(s) configured in the following Spark configs or Delta table properties are not recognized by this version of Databricks: <configs>.  \nDELTA_UNSUPPORTED_FEATURE_STATUS  \nSQLSTATE: 0AKDE  \nExpecting the status for table feature <feature> to be \u201csupported\u201d, but got \u201c<status>\u201d.  \nDELTA_UNSUPPORTED_FIELD_UPDATE_NON_STRUCT"
    },
    {
        "id": 730,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 0AKDE  \nExpecting the status for table feature <feature> to be \u201csupported\u201d, but got \u201c<status>\u201d.  \nDELTA_UNSUPPORTED_FIELD_UPDATE_NON_STRUCT  \nSQLSTATE: 0AKDC  \nUpdating nested fields is only supported for StructType, but you are trying to update a field of <columnName>, which is of type: <dataType>.  \nDELTA_UNSUPPORTED_FSCK_WITH_DELETION_VECTORS  \nSQLSTATE: 0A000  \nThe \u2018FSCK REPAIR TABLE\u2019 command is not supported on table versions with missing deletion vector files.  \nPlease contact support.  \nDELTA_UNSUPPORTED_GENERATE_WITH_DELETION_VECTORS  \nSQLSTATE: 0A000  \nThe \u2018GENERATE symlink_format_manifest\u2019 command is not supported on table versions with deletion vectors.  \nIn order to produce a version of the table without deletion vectors, run \u2018REORG TABLE table APPLY (PURGE)\u2019. Then re-run the \u2018GENERATE\u2019 command.  \nMake sure that no concurrent transactions are adding deletion vectors again between REORG and GENERATE.  \nIf you need to generate manifests regularly, or you cannot prevent concurrent transactions, consider disabling deletion vectors on this table using \u2018ALTER TABLE table SET TBLPROPERTIES (delta.enableDeletionVectors = false)\u2019.  \nDELTA_UNSUPPORTED_INVARIANT_NON_STRUCT  \nSQLSTATE: 0AKDC  \nInvariants on nested fields other than StructTypes are not supported.  \nDELTA_UNSUPPORTED_IN_SUBQUERY  \nSQLSTATE: 0AKDC  \nIn subquery is not supported in the <operation> condition.  \nDELTA_UNSUPPORTED_LIST_KEYS_WITH_PREFIX  \nSQLSTATE: 0A000  \nlistKeywithPrefix not available  \nDELTA_UNSUPPORTED_MANIFEST_GENERATION_WITH_COLUMN_MAPPING  \nSQLSTATE: 0AKDC  \nManifest generation is not supported for tables that leverage column mapping, as external readers cannot read these Delta tables. See Delta documentation for more details.  \nDELTA_UNSUPPORTED_MERGE_SCHEMA_EVOLUTION_WITH_CDC  \nSQLSTATE: 0A000  \nMERGE INTO operations with schema evolution do not currently support writing CDC output.  \nDELTA_UNSUPPORTED_MULTI_COL_IN_PREDICATE  \nSQLSTATE: 0AKDC"
    },
    {
        "id": 731,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 0A000  \nMERGE INTO operations with schema evolution do not currently support writing CDC output.  \nDELTA_UNSUPPORTED_MULTI_COL_IN_PREDICATE  \nSQLSTATE: 0AKDC  \nMulti-column In predicates are not supported in the <operation> condition.  \nDELTA_UNSUPPORTED_NESTED_COLUMN_IN_BLOOM_FILTER  \nSQLSTATE: 0AKDC  \nCreating a bloom filer index on a nested column is currently unsupported: <columnName>  \nDELTA_UNSUPPORTED_NESTED_FIELD_IN_OPERATION  \nSQLSTATE: 0AKDC  \nNested field is not supported in the <operation> (field = <fieldName>).  \nDELTA_UNSUPPORTED_NON_EMPTY_CLONE  \nSQLSTATE: 0AKDC  \nThe clone destination table is non-empty. Please TRUNCATE or DELETE FROM the table before running CLONE.  \nDELTA_UNSUPPORTED_OUTPUT_MODE  \nSQLSTATE: 0AKDC  \nData source <dataSource> does not support <mode> output mode  \nDELTA_UNSUPPORTED_PARTITION_COLUMN_IN_BLOOM_FILTER  \nSQLSTATE: 0AKDC  \nCreating a bloom filter index on a partitioning column is unsupported: <columnName>  \nDELTA_UNSUPPORTED_RENAME_COLUMN  \nSQLSTATE: 0AKDC  \nColumn rename is not supported for your Delta table. <advice>  \nDELTA_UNSUPPORTED_SCHEMA_DURING_READ  \nSQLSTATE: 0AKDC  \nDelta does not support specifying the schema at read time.  \nDELTA_UNSUPPORTED_SORT_ON_BUCKETED_TABLES  \nSQLSTATE: 0A000  \nSORTED BY is not supported for Delta bucketed tables  \nDELTA_UNSUPPORTED_SOURCE  \nSQLSTATE: 0AKDD  \n<operation> destination only supports Delta sources.  \n<plan>  \nDELTA_UNSUPPORTED_STATIC_PARTITIONS  \nSQLSTATE: 0AKDD  \nSpecifying static partitions in the partition spec is currently not supported during inserts  \nDELTA_UNSUPPORTED_STRATEGY_NAME  \nSQLSTATE: 22023  \nUnsupported strategy name: <strategy>  \nDELTA_UNSUPPORTED_SUBQUERY  \nSQLSTATE: 0AKDC  \nSubqueries are not supported in the <operation> (condition = <cond>).  \nDELTA_UNSUPPORTED_SUBQUERY_IN_PARTITION_PREDICATES  \nSQLSTATE: 0AKDC"
    },
    {
        "id": 732,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 0AKDC  \nSubqueries are not supported in the <operation> (condition = <cond>).  \nDELTA_UNSUPPORTED_SUBQUERY_IN_PARTITION_PREDICATES  \nSQLSTATE: 0AKDC  \nSubquery is not supported in partition predicates.  \nDELTA_UNSUPPORTED_TIME_TRAVEL_MULTIPLE_FORMATS  \nSQLSTATE: 42613  \nCannot specify time travel in multiple formats.  \nDELTA_UNSUPPORTED_TIME_TRAVEL_VIEWS  \nSQLSTATE: 0AKDC  \nCannot time travel views, subqueries, streams or change data feed queries.  \nDELTA_UNSUPPORTED_TRUNCATE_SAMPLE_TABLES  \nSQLSTATE: 0A000  \nTruncate sample tables is not supported  \nDELTA_UNSUPPORTED_TYPE_CHANGE_IN_SCHEMA  \nSQLSTATE: 0AKDC  \nUnable to operate on this table because an unsupported type change was applied. Field <fieldName> was changed from <fromType> to <toType>.  \nDELTA_UNSUPPORTED_VACUUM_SPECIFIC_PARTITION  \nSQLSTATE: 0AKDC  \nPlease provide the base path (<baseDeltaPath>) when Vacuuming Delta tables. Vacuuming specific partitions is currently not supported.  \nDELTA_UNSUPPORTED_WRITES_STAGED_TABLE  \nSQLSTATE: 42807  \nTable implementation does not support writes: <tableName>  \nDELTA_UNSUPPORTED_WRITE_SAMPLE_TABLES  \nSQLSTATE: 0A000  \nWrite to sample tables is not supported  \nDELTA_UPDATE_SCHEMA_MISMATCH_EXPRESSION  \nSQLSTATE: 42846  \nCannot cast <fromCatalog> to <toCatalog>. All nested columns must match.  \nDELTA_VACUUM_COPY_INTO_STATE_FAILED  \nSQLSTATE: 22000  \nVACUUM on data files succeeded, but COPY INTO state garbage collection failed.  \nDELTA_VERSIONS_NOT_CONTIGUOUS  \nSQLSTATE: KD00C  \nVersions (<versionList>) are not contiguous.  \nFor more details see DELTA_VERSIONS_NOT_CONTIGUOUS  \nDELTA_VIOLATE_CONSTRAINT_WITH_VALUES  \nSQLSTATE: 23001  \nCHECK constraint <constraintName> <expression> violated by row with values:  \n<values>  \nDELTA_VIOLATE_TABLE_PROPERTY_VALIDATION_FAILED  \nSQLSTATE: 0A000"
    },
    {
        "id": 733,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 23001  \nCHECK constraint <constraintName> <expression> violated by row with values:  \n<values>  \nDELTA_VIOLATE_TABLE_PROPERTY_VALIDATION_FAILED  \nSQLSTATE: 0A000  \nThe validation of the properties of table <table> has been violated:  \nFor more details see DELTA_VIOLATE_TABLE_PROPERTY_VALIDATION_FAILED  \nDELTA_WRITE_INTO_VIEW_NOT_SUPPORTED  \nSQLSTATE: 0A000  \n<viewIdentifier> is a view. You may not write data into a view.  \nDELTA_ZORDERING_COLUMN_DOES_NOT_EXIST  \nSQLSTATE: 42703  \nZ-Ordering column <columnName> does not exist in data schema.  \nDELTA_ZORDERING_ON_COLUMN_WITHOUT_STATS  \nSQLSTATE: KD00D  \nZ-Ordering on <cols> will be  \nineffective, because we currently do not collect stats for these columns. Please refer to  \n<link>  \nfor more information on data skipping and z-ordering. You can disable  \nthis check by setting  \n\u2018%%sql set <zorderColStatKey> = false\u2019  \nDELTA_ZORDERING_ON_PARTITION_COLUMN  \nSQLSTATE: 42P10  \n<colName> is a partition column. Z-Ordering can only be performed on data columns"
    },
    {
        "id": 734,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Autoloader\nCF_ADD_NEW_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nSchema evolution mode <addNewColumnsMode> is not supported when the schema is specified. To use this mode, you can provide the schema through cloudFiles.schemaHints instead.  \nCF_AMBIGUOUS_AUTH_OPTIONS_ERROR  \nSQLSTATE: 42000  \nFound notification-setup authentication options for the (default) directory  \nlisting mode:  \n<options>  \nIf you wish to use the file notification mode, please explicitly set:  \n.option(\u201ccloudFiles.<useNotificationsKey>\u201d, \u201ctrue\u201d)  \nAlternatively, if you want to skip the validation of your options and ignore these  \nauthentication options, you can set:  \n.option(\u201ccloudFiles.ValidateOptionsKey>\u201d, \u201cfalse\u201d)  \nCF_AMBIGUOUS_INCREMENTAL_LISTING_MODE_ERROR  \nSQLSTATE: 42000  \nIncremental listing mode (cloudFiles.<useIncrementalListingKey>)  \nand file notification (cloudFiles.<useNotificationsKey>)  \nhave been enabled at the same time.  \nPlease make sure that you select only one.  \nCF_AZURE_STORAGE_SUFFIXES_REQUIRED  \nSQLSTATE: 42000  \nRequire adlsBlobSuffix and adlsDfsSuffix for Azure  \nCF_BUCKET_MISMATCH  \nSQLSTATE: 22000  \nThe <storeType> in the file event <fileEvent> is different from expected by the source: <source>.  \nCF_CANNOT_EVOLVE_SCHEMA_LOG_EMPTY  \nSQLSTATE: 22000  \nCannot evolve schema when the schema log is empty. Schema log location: <logPath>  \nCF_CANNOT_PARSE_QUEUE_MESSAGE  \nSQLSTATE: 22000  \nCannot parse the following queue message: <message>  \nCF_CANNOT_RESOLVE_CONTAINER_NAME  \nSQLSTATE: 22000  \nCannot resolve container name from path: <path>, Resolved uri: <uri>  \nCF_CANNOT_RUN_DIRECTORY_LISTING  \nSQLSTATE: 22000  \nCannot run directory listing when there is an async backfill thread running  \nCF_CLEAN_SOURCE_ALLOW_OVERWRITES_BOTH_ON  \nSQLSTATE: 42000"
    },
    {
        "id": 735,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 22000  \nCannot run directory listing when there is an async backfill thread running  \nCF_CLEAN_SOURCE_ALLOW_OVERWRITES_BOTH_ON  \nSQLSTATE: 42000  \nCannot turn on cloudFiles.cleanSource and cloudFiles.allowOverwrites at the same time.  \nCF_CLEAN_SOURCE_UNAUTHORIZED_WRITE_PERMISSION  \nSQLSTATE: 42501  \nAuto Loader cannot delete processed files because it does not have write permissions to the source directory.  \n<reason>  \nTo fix you can either:  \nGrant write permissions to the source directory OR  \nSet cleanSource to \u2018OFF\u2019  \nYou could also unblock your stream by setting the SQLConf spark.databricks.cloudFiles.cleanSource.disabledDueToAuthorizationErrors to \u2018true\u2019.  \nCF_DUPLICATE_COLUMN_IN_DATA  \nSQLSTATE: 22000  \nThere was an error when trying to infer the partition schema of your table. You have the same column duplicated in your data and partition paths. To ignore the partition value, please provide your partition columns explicitly by using: .option(\u201ccloudFiles.<partitionColumnsKey>\u201d, \u201c{comma-separated-list}\u201d)  \nCF_EMPTY_DIR_FOR_SCHEMA_INFERENCE  \nSQLSTATE: 42000  \nCannot infer schema when the input path <path> is empty. Please try to start the stream when there are files in the input path, or specify the schema.  \nCF_EVENT_GRID_AUTH_ERROR  \nSQLSTATE: 22000  \nFailed to create an Event Grid subscription. Please make sure that your service  \nprincipal has <permissionType> Event Grid Subscriptions. See more details at:  \n<docLink>  \nCF_EVENT_GRID_CREATION_FAILED  \nSQLSTATE: 22000  \nFailed to create event grid subscription. Please ensure that Microsoft.EventGrid is  \nregistered as resource provider in your subscription. See more details at:  \n<docLink>  \nCF_EVENT_GRID_NOT_FOUND_ERROR  \nSQLSTATE: 22000  \nFailed to create an Event Grid subscription. Please make sure that your storage  \naccount (<storageAccount>) is under your resource group (<resourceGroup>) and that"
    },
    {
        "id": 736,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 22000  \nFailed to create an Event Grid subscription. Please make sure that your storage  \naccount (<storageAccount>) is under your resource group (<resourceGroup>) and that  \nthe storage account is a \u201cStorageV2 (general purpose v2)\u201d account. See more details at:  \n<docLink>  \nCF_EVENT_NOTIFICATION_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nAuto Loader event notification mode is not supported for <cloudStore>.  \nCF_FAILED_TO_CHECK_STREAM_NEW  \nSQLSTATE: 22000  \nFailed to check if the stream is new  \nCF_FAILED_TO_CREATED_PUBSUB_SUBSCRIPTION  \nSQLSTATE: 22000  \nFailed to create subscription: <subscriptionName>. A subscription with the same name already exists and is associated with another topic: <otherTopicName>. The desired topic is <proposedTopicName>. Either delete the existing subscription or create a subscription with a new resource suffix.  \nCF_FAILED_TO_CREATED_PUBSUB_TOPIC  \nSQLSTATE: 22000  \nFailed to create topic: <topicName>. A topic with the same name already exists.<reason> Remove the existing topic or try again with another resource suffix  \nCF_FAILED_TO_DELETE_GCP_NOTIFICATION  \nSQLSTATE: 22000  \nFailed to delete notification with id <notificationId> on bucket <bucketName> for topic <topicName>. Please retry or manually remove the notification through the GCP console.  \nCF_FAILED_TO_DESERIALIZE_PERSISTED_SCHEMA  \nSQLSTATE: 22000  \nFailed to deserialize persisted schema from string: \u2018<jsonSchema>\u2019  \nCF_FAILED_TO_EVOLVE_SCHEMA  \nSQLSTATE: 22000  \nCannot evolve schema without a schema log.  \nCF_FAILED_TO_FIND_PROVIDER  \nSQLSTATE: 42000  \nFailed to find provider for <fileFormatInput>  \nCF_FAILED_TO_INFER_SCHEMA  \nSQLSTATE: 22000  \nFailed to infer schema for format <fileFormatInput> from existing files in input path <path>.  \nFor more details see CF_FAILED_TO_INFER_SCHEMA"
    },
    {
        "id": 737,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 22000  \nFailed to infer schema for format <fileFormatInput> from existing files in input path <path>.  \nFor more details see CF_FAILED_TO_INFER_SCHEMA  \nCF_FAILED_TO_WRITE_TO_SCHEMA_LOG  \nSQLSTATE: 22000  \nFailed to write to the schema log at location <path>.  \nCF_FILE_FORMAT_REQUIRED  \nSQLSTATE: 42000  \nCould not find required option: cloudFiles.format.  \nCF_FOUND_MULTIPLE_AUTOLOADER_PUBSUB_SUBSCRIPTIONS  \nSQLSTATE: 22000  \nFound multiple (<num>) subscriptions with the Auto Loader prefix for topic <topicName>:  \n<subscriptionList>  \nThere should only be one subscription per topic. Please manually ensure that your topic does not have multiple subscriptions.  \nCF_GCP_AUTHENTICATION  \nSQLSTATE: 42000  \nPlease either provide all of the following: <clientEmail>, <client>,  \n<privateKey>, and <privateKeyId> or provide none of them in order to use the default  \nGCP credential provider chain for authenticating with GCP resources.  \nCF_GCP_LABELS_COUNT_EXCEEDED  \nSQLSTATE: 22000  \nReceived too many labels (<num>) for GCP resource. The maximum label count per resource is <maxNum>.  \nCF_GCP_RESOURCE_TAGS_COUNT_EXCEEDED  \nSQLSTATE: 22000  \nReceived too many resource tags (<num>) for GCP resource. The maximum resource tag count per resource is <maxNum>, as resource tags are stored as GCP labels on resources, and Databricks specific tags consume some of this label quota.  \nCF_INCOMPLETE_LOG_FILE_IN_SCHEMA_LOG  \nSQLSTATE: 22000  \nIncomplete log file in the schema log at path <path>  \nCF_INCOMPLETE_METADATA_FILE_IN_CHECKPOINT  \nSQLSTATE: 22000  \nIncomplete metadata file in the Auto Loader checkpoint  \nCF_INCORRECT_SQL_PARAMS  \nSQLSTATE: 42000"
    },
    {
        "id": 738,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "CF_INCOMPLETE_METADATA_FILE_IN_CHECKPOINT  \nSQLSTATE: 22000  \nIncomplete metadata file in the Auto Loader checkpoint  \nCF_INCORRECT_SQL_PARAMS  \nSQLSTATE: 42000  \nThe cloud_files method accepts two required string parameters: the path to load from, and the file format. File reader options must be provided in a string key-value map. e.g. cloud_files(\u201cpath\u201d, \u201cjson\u201d, map(\u201coption1\u201d, \u201cvalue1\u201d)). Received: <params>  \nCF_INTERNAL_ERROR  \nSQLSTATE: 42000  \nInternal error.  \nFor more details see CF_INTERNAL_ERROR  \nCF_INVALID_ARN  \nSQLSTATE: 42000  \nInvalid ARN: <arn>  \nCF_INVALID_AZURE_CERTIFICATE  \nSQLSTATE: 42000  \nThe private key provided with option cloudFiles.certificate cannot be parsed. Please provide a valid public key in PEM format.  \nCF_INVALID_AZURE_CERT_PRIVATE_KEY  \nSQLSTATE: 42000  \nThe private key provided with option cloudFiles.certificatePrivateKey cannot be parsed. Please provide a valid private key in PEM format.  \nCF_INVALID_CHECKPOINT  \nSQLSTATE: 42000  \nThis checkpoint is not a valid CloudFiles source  \nCF_INVALID_CLEAN_SOURCE_MODE  \nSQLSTATE: 42000  \nInvalid mode for clean source option <value>.  \nCF_INVALID_GCP_RESOURCE_TAG_KEY  \nSQLSTATE: 42000  \nInvalid resource tag key for GCP resource: <key>. Keys must start with a lowercase letter, be within 1 to 63 characters long, and contain only lowercase letters, numbers, underscores (_), and hyphens (-).  \nCF_INVALID_GCP_RESOURCE_TAG_VALUE  \nSQLSTATE: 42000  \nInvalid resource tag value for GCP resource: <value>. Values must be within 0 to 63 characters long and must contain only lowercase letters, numbers, underscores (_), and hyphens (-).  \nCF_INVALID_MANAGED_FILE_EVENTS_OPTION_KEYS  \nSQLSTATE: 42000  \nAuto Loader does not support the following options when used with managed file events:  \n<optionList>"
    },
    {
        "id": 739,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "CF_INVALID_MANAGED_FILE_EVENTS_OPTION_KEYS  \nSQLSTATE: 42000  \nAuto Loader does not support the following options when used with managed file events:  \n<optionList>  \nWe recommend that you remove these options and then restart the stream.  \nCF_INVALID_MANAGED_FILE_EVENTS_RESPONSE  \nSQLSTATE: 22000  \nInvalid response from managed file events service. Please contact Databricks support for assistance.  \nFor more details see CF_INVALID_MANAGED_FILE_EVENTS_RESPONSE  \nCF_INVALID_SCHEMA_EVOLUTION_MODE  \nSQLSTATE: 42000  \ncloudFiles.<schemaEvolutionModeKey> must be one of {  \n\u201c<addNewColumns>\u201d  \n\u201c<failOnNewColumns>\u201d  \n\u201c<rescue>\u201d  \n\u201c<noEvolution>\u201d}  \nCF_INVALID_SCHEMA_HINTS_OPTION  \nSQLSTATE: 42000  \nSchema hints can only specify a particular column once.  \nIn this case, redefining column: <columnName>  \nmultiple times in schemaHints:  \n<schemaHints>  \nCF_INVALID_SCHEMA_HINT_COLUMN  \nSQLSTATE: 42000  \nSchema hints can not be used to override maps\u2019 and arrays\u2019 nested types.  \nConflicted column: <columnName>  \nCF_LATEST_OFFSET_READ_LIMIT_REQUIRED  \nSQLSTATE: 22000  \nlatestOffset should be called with a ReadLimit on this source.  \nCF_LOG_FILE_MALFORMED  \nSQLSTATE: 22000  \nLog file was malformed: failed to read correct log version from <fileName>.  \nCF_MANAGED_FILE_EVENTS_BACKFILL_IN_PROGRESS  \nSQLSTATE: 22000  \nYou have requested Auto Loader to ignore existing files in your external location by setting includeExistingFiles to false. However, the managed file events service is still discovering existing files in your external location. Please try again after managed file events has completed discovering all files in your external location.  \nCF_MANAGED_FILE_EVENTS_ENDPOINT_NOT_FOUND  \nSQLSTATE: 42000  \nYou are using Auto Loader with managed file events, but it appears that the external location for your input path \u2018<path>\u2019 does not have file events enabled or the input path is invalid. Please request your Databricks Administrator to enable file events on the external location for your input path."
    },
    {
        "id": 740,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "CF_MANAGED_FILE_EVENTS_ENDPOINT_PERMISSION_DENIED  \nSQLSTATE: 42000  \nYou are using Auto Loader with managed file events, but you do not have access to the external location or volume for input path \u2018<path>\u2019 or the input path is invalid. Please request your Databricks Administrator to grant read permissions for the external location or volume or provide a valid input path within an existing external location or volume.  \nCF_MANAGED_FILE_EVENTS_ONLY_ON_SERVERLESS  \nSQLSTATE: 56038  \nAuto Loader with managed file events is only available on Databricks serverless. To continue, please move this workload to Databricks serverless or turn off the cloudFiles.useManagedFileEvents option.  \nCF_MAX_MUST_BE_POSITIVE  \nSQLSTATE: 42000  \nmax must be positive  \nCF_METADATA_FILE_CONCURRENTLY_USED  \nSQLSTATE: 22000  \nMultiple streaming queries are concurrently using <metadataFile>  \nCF_MISSING_METADATA_FILE_ERROR  \nSQLSTATE: 42000  \nThe metadata file in the streaming source checkpoint directory is missing. This metadata  \nfile contains important default options for the stream, so the stream cannot be restarted  \nright now. Please contact Databricks support for assistance.  \nCF_MISSING_PARTITION_COLUMN_ERROR  \nSQLSTATE: 42000  \nPartition column <columnName> does not exist in the provided schema:  \n<schema>  \nCF_MISSING_SCHEMA_IN_PATHLESS_MODE  \nSQLSTATE: 42000  \nPlease specify a schema using .schema() if a path is not provided to the CloudFiles source while using file notification mode. Alternatively, to have Auto Loader to infer the schema please provide a base path in .load().  \nCF_MULTIPLE_PUBSUB_NOTIFICATIONS_FOR_TOPIC  \nSQLSTATE: 22000  \nFound existing notifications for topic <topicName> on bucket <bucketName>:  \nnotification,id  \n<notificationList>  \nTo avoid polluting the subscriber with unintended events, please delete the above notifications and retry.  \nCF_NEW_PARTITION_ERROR  \nSQLSTATE: 22000"
    },
    {
        "id": 741,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "notification,id  \n<notificationList>  \nTo avoid polluting the subscriber with unintended events, please delete the above notifications and retry.  \nCF_NEW_PARTITION_ERROR  \nSQLSTATE: 22000  \nNew partition columns were inferred from your files: [<filesList>]. Please provide all partition columns in your schema or provide a list of partition columns which you would like to extract values for by using: .option(\u201ccloudFiles.partitionColumns\u201d, \u201c{comma-separated-list|empty-string}\u201d)  \nCF_PARTITON_INFERENCE_ERROR  \nSQLSTATE: 22000  \nThere was an error when trying to infer the partition schema of the current batch of files. Please provide your partition columns explicitly by using: .option(\u201ccloudFiles.<partitionColumnOption>\u201d, \u201c{comma-separated-list}\u201d)  \nCF_PATH_DOES_NOT_EXIST_FOR_READ_FILES  \nSQLSTATE: 42000  \nCannot read files when the input path <path> does not exist. Please make sure the input path exists and re-try.  \nCF_PERIODIC_BACKFILL_NOT_SUPPORTED  \nSQLSTATE: 0A000  \nPeriodic backfill is not supported if asynchronous backfill is disabled. You can enable asynchronous backfill/directory listing by setting spark.databricks.cloudFiles.asyncDirListing to true  \nCF_PREFIX_MISMATCH  \nSQLSTATE: 22000  \nFound mismatched event: key <key> doesn\u2019t have the prefix: <prefix>  \nCF_PROTOCOL_MISMATCH  \nSQLSTATE: 22000  \n<message>  \nIf you don\u2019t need to make any other changes to your code, then please set the SQL  \nconfiguration: \u2018<sourceProtocolVersionKey> = <value>\u2019  \nto resume your stream. Please refer to:  \n<docLink>  \nfor more details.  \nCF_REGION_NOT_FOUND_ERROR  \nSQLSTATE: 42000  \nCould not get default AWS Region. Please specify a region using the cloudFiles.region option.  \nCF_RESOURCE_SUFFIX_EMPTY  \nSQLSTATE: 42000  \nFailed to create notification services: the resource suffix cannot be empty."
    },
    {
        "id": 742,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "CF_RESOURCE_SUFFIX_EMPTY  \nSQLSTATE: 42000  \nFailed to create notification services: the resource suffix cannot be empty.  \nCF_RESOURCE_SUFFIX_INVALID_CHAR_AWS  \nSQLSTATE: 42000  \nFailed to create notification services: the resource suffix can only have alphanumeric characters, hyphens (-) and underscores (_).  \nCF_RESOURCE_SUFFIX_INVALID_CHAR_AZURE  \nSQLSTATE: 42000  \nFailed to create notification services: the resource suffix can only have lowercase letter, number, and dash (-).  \nCF_RESOURCE_SUFFIX_INVALID_CHAR_GCP  \nSQLSTATE: 42000  \nFailed to create notification services: the resource suffix can only have alphanumeric characters, hyphens (-), underscores (_), periods (.), tildes (~) plus signs (+), and percent signs (<percentSign>).  \nCF_RESOURCE_SUFFIX_LIMIT  \nSQLSTATE: 42000  \nFailed to create notification services: the resource suffix cannot have more than <limit> characters.  \nCF_RESOURCE_SUFFIX_LIMIT_GCP  \nSQLSTATE: 42000  \nFailed to create notification services: the resource suffix must be between <lowerLimit> and <upperLimit> characters.  \nCF_RESTRICTED_GCP_RESOURCE_TAG_KEY  \nSQLSTATE: 22000  \nFound restricted GCP resource tag key (<key>). The following GCP resource tag keys are restricted for Auto Loader: [<restrictedKeys>]  \nCF_RETENTION_GREATER_THAN_MAX_FILE_AGE  \nSQLSTATE: 42000  \ncloudFiles.cleanSource.retentionDuration cannot be greater than cloudFiles.maxFileAge.  \nCF_SAME_PUB_SUB_TOPIC_NEW_KEY_PREFIX  \nSQLSTATE: 22000  \nFailed to create notification for topic: <topic> with prefix: <prefix>. There is already a topic with the same name with another prefix: <oldPrefix>. Try using a different resource suffix for setup or delete the existing setup.  \nCF_SOURCE_DIRECTORY_PATH_REQUIRED  \nSQLSTATE: 42000  \nPlease provide the source directory path with option path"
    },
    {
        "id": 743,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "CF_SOURCE_DIRECTORY_PATH_REQUIRED  \nSQLSTATE: 42000  \nPlease provide the source directory path with option path  \nCF_SOURCE_UNSUPPORTED  \nSQLSTATE: 0A000  \nThe cloud files source only supports S3, Azure Blob Storage (wasb/wasbs) and Azure Data Lake Gen1 (adl) and Gen2 (abfs/abfss) paths right now. path: \u2018<path>\u2019, resolved uri: \u2018<uri>\u2019  \nCF_STATE_INCORRECT_SQL_PARAMS  \nSQLSTATE: 42000  \nThe cloud_files_state function accepts a string parameter representing the checkpoint directory of a cloudFiles stream or a multi-part tableName identifying a streaming table, and an optional second integer parameter representing the checkpoint version to load state for. The second parameter may also be \u2018latest\u2019 to read the latest checkpoint. Received: <params>  \nCF_STATE_INVALID_CHECKPOINT_PATH  \nSQLSTATE: 42000  \nThe input checkpoint path <path> is invalid. Either the path does not exist or there are no cloud_files sources found.  \nCF_STATE_INVALID_VERSION  \nSQLSTATE: 42000  \nThe specified version <version> does not exist, or was removed during analysis.  \nCF_THREAD_IS_DEAD  \nSQLSTATE: 22000  \n<threadName> thread is dead.  \nCF_UNABLE_TO_DERIVE_STREAM_CHECKPOINT_LOCATION  \nSQLSTATE: 42000  \nUnable to derive the stream checkpoint location from the source checkpoint location: <checkPointLocation>  \nCF_UNABLE_TO_DETECT_FILE_FORMAT  \nSQLSTATE: 42000  \nUnable to detect the source file format from <fileSize> sampled file(s), found <formats>. Please specify the format.  \nCF_UNABLE_TO_EXTRACT_BUCKET_INFO  \nSQLSTATE: 42000  \nUnable to extract bucket information. Path: \u2018<path>\u2019, resolved uri: \u2018<uri>\u2019.  \nCF_UNABLE_TO_EXTRACT_KEY_INFO  \nSQLSTATE: 42000  \nUnable to extract key information. Path: \u2018<path>\u2019, resolved uri: \u2018<uri>\u2019.  \nCF_UNABLE_TO_EXTRACT_STORAGE_ACCOUNT_INFO  \nSQLSTATE: 42000"
    },
    {
        "id": 744,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Unable to extract key information. Path: \u2018<path>\u2019, resolved uri: \u2018<uri>\u2019.  \nCF_UNABLE_TO_EXTRACT_STORAGE_ACCOUNT_INFO  \nSQLSTATE: 42000  \nUnable to extract storage account information; path: \u2018<path>\u2019, resolved uri: \u2018<uri>\u2019  \nCF_UNABLE_TO_LIST_EFFICIENTLY  \nSQLSTATE: 22000  \nReceived a directory rename event for the path <path>, but we are unable to list this directory efficiently. In order for the stream to continue, set the option \u2018cloudFiles.ignoreDirRenames\u2019 to true, and consider enabling regular backfills with cloudFiles.backfillInterval for this data to be processed.  \nCF_UNEXPECTED_READ_LIMIT  \nSQLSTATE: 22000  \nUnexpected ReadLimit: <readLimit>  \nCF_UNKNOWN_OPTION_KEYS_ERROR  \nSQLSTATE: 42000  \nFound unknown option keys:  \n<optionList>  \nPlease make sure that all provided option keys are correct. If you want to skip the  \nvalidation of your options and ignore these unknown options, you can set:  \n.option(\u201ccloudFiles.<validateOptions>\u201d, \u201cfalse\u201d)  \nCF_UNKNOWN_READ_LIMIT  \nSQLSTATE: 22000  \nUnknown ReadLimit: <readLimit>  \nCF_UNSUPPORTED_CLOUD_FILES_SQL_FUNCTION  \nSQLSTATE: 0A000  \nThe SQL function \u2018cloud_files\u2019 to create an Auto Loader streaming source is supported only in a Delta Live Tables pipeline. See more details at:  \n<docLink>  \nCF_UNSUPPORTED_FORMAT_FOR_SCHEMA_INFERENCE  \nSQLSTATE: 0A000  \nSchema inference is not supported for format: <format>. Please specify the schema.  \nCF_UNSUPPORTED_LOG_VERSION  \nSQLSTATE: 0A000  \nUnsupportedLogVersion: maximum supported log version is v`<maxVersion>, but encountered v<version>`. The log file was produced by a newer version of DBR and cannot be read by this version. Please upgrade.  \nCF_UNSUPPORTED_SCHEMA_EVOLUTION_MODE  \nSQLSTATE: 0A000  \nSchema evolution mode <mode> is not supported for format: <format>. Please set the schema evolution mode to \u2018none\u2019."
    },
    {
        "id": 745,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "CF_UNSUPPORTED_SCHEMA_EVOLUTION_MODE  \nSQLSTATE: 0A000  \nSchema evolution mode <mode> is not supported for format: <format>. Please set the schema evolution mode to \u2018none\u2019.  \nCF_USE_DELTA_FORMAT  \nSQLSTATE: 42000  \nReading from a Delta table is not supported with this syntax. If you would like to consume data from Delta, please refer to the docs: read a Delta table (<deltaDocLink>), or read a Delta table as a stream source (<streamDeltaDocLink>). The streaming source from Delta is already optimized for incremental consumption of data."
    },
    {
        "id": 746,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "Geospatial\nEWKB_PARSE_ERROR  \nSQLSTATE: 22023  \nError parsing EWKB: <parseError> at position <pos>  \nGEOJSON_PARSE_ERROR  \nSQLSTATE: 22023  \nError parsing GeoJSON: <parseError> at position <pos>  \nFor more details see GEOJSON_PARSE_ERROR  \nH3_INVALID_CELL_ID  \nSQLSTATE: 22023  \n<h3Cell> is not a valid H3 cell ID  \nFor more details see H3_INVALID_CELL_ID  \nH3_INVALID_GRID_DISTANCE_VALUE  \nSQLSTATE: 22023  \nH3 grid distance <k> must be non-negative  \nFor more details see H3_INVALID_GRID_DISTANCE_VALUE  \nH3_INVALID_RESOLUTION_VALUE  \nSQLSTATE: 22023  \nH3 resolution <r> must be between <minR> and <maxR>, inclusive  \nFor more details see H3_INVALID_RESOLUTION_VALUE  \nH3_NOT_ENABLED  \nSQLSTATE: 0A000  \n<h3Expression> is disabled or unsupported. Consider enabling Photon or switch to a tier that supports H3 expressions  \nFor more details see H3_NOT_ENABLED  \nH3_PENTAGON_ENCOUNTERED_ERROR  \nSQLSTATE: 22023  \nA pentagon was encountered while computing the hex ring of <h3Cell> with grid distance <k>  \nH3_UNDEFINED_GRID_DISTANCE  \nSQLSTATE: 22023  \nH3 grid distance between <h3Cell1> and <h3Cell2> is undefined  \nST_DIFFERENT_SRID_VALUES  \nSQLSTATE: 22023  \nArguments to \u201c<sqlFunction>\u201d must have the same SRID value. SRID values found: <srid1>, <srid2>  \nST_INVALID_ARGUMENT  \nSQLSTATE: 22023  \n\u201c<sqlFunction>\u201d: <reason>  \nST_INVALID_ARGUMENT_TYPE  \nSQLSTATE: 22023  \nArgument to \u201c<sqlFunction>\u201d must be of type <validTypes>  \nST_INVALID_CRS_TRANSFORMATION_ERROR  \nSQLSTATE: 22023"
    },
    {
        "id": 747,
        "url": "https://docs.databricks.com/en/error-messages/error-classes.html",
        "content": "SQLSTATE: 22023  \nArgument to \u201c<sqlFunction>\u201d must be of type <validTypes>  \nST_INVALID_CRS_TRANSFORMATION_ERROR  \nSQLSTATE: 22023  \n<sqlFunction>: Invalid or unsupported CRS transformation from SRID <srcSrid> to SRID <trgSrid>  \nST_INVALID_ENDIANNESS_VALUE  \nSQLSTATE: 22023  \nEndianness <e> must be be \u2018NDR\u2019 (little-endian) or \u2018XDR\u2019 (big-endian)  \nST_INVALID_GEOHASH_VALUE  \nSQLSTATE: 22023  \n<sqlFunction>: Invalid geohash value: \u2018<geohash>\u2019. Geohash values must be valid lowercase base32 strings as described inhttps://en.wikipedia.org/wiki/Geohash#Textual_representation  \nST_INVALID_PRECISION_VALUE  \nSQLSTATE: 22023  \nPrecision <p> must be between <minP> and <maxP>, inclusive  \nST_INVALID_SRID_VALUE  \nSQLSTATE: 22023  \nInvalid or unsupported SRID <srid>  \nST_NOT_ENABLED  \nSQLSTATE: 0A000  \n<stExpression> is disabled or unsupported. Consider enabling Photon or switch to a tier that supports ST expressions  \nST_UNSUPPORTED_RETURN_TYPE  \nSQLSTATE: 0A000  \nThe GEOGRAPHY and GEOMETRY data types cannot be returned in queries. Use one of the following SQL expressions to convert them to standard interchange formats: <projectionExprs>.  \nWKB_PARSE_ERROR  \nSQLSTATE: 22023  \nError parsing WKB: <parseError> at position <pos>  \nFor more details see WKB_PARSE_ERROR  \nWKT_PARSE_ERROR  \nSQLSTATE: 22023  \nError parsing WKT: <parseError> at position <pos>  \nFor more details see WKT_PARSE_ERROR"
    },
    {
        "id": 748,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html",
        "content": "Use custom metrics with Databricks Lakehouse Monitoring  \nThis page describes how to create a custom metric in Databricks Lakehouse Monitoring. In addition to the analysis and drift statistics that are automatically calculated, you can create custom metrics. For example, you might want to track a weighted mean that captures some aspect of business logic or use a custom model quality score. You can also create custom drift metrics that track changes to the values in the primary table (compared to the baseline or the previous time window).  \nFor more details on how to use the MonitorMetric API, see the API reference.  \nTypes of custom metrics"
    },
    {
        "id": 749,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html",
        "content": "Types of custom metrics\nDatabricks Lakehouse Monitoring includes the following types of custom metrics:  \nAggregate metrics, which are calculated based on columns in the primary table. Aggregate metrics are stored in the profile metrics table.  \nDerived metrics, which are calculated based on previously computed aggregate metrics and do not directly use data from the primary table. Derived metrics are stored in the profile metrics table.  \nDrift metrics, which compare previously computed aggregate or derived metrics from two different time windows, or between the primary table and the baseline table. Drift metrics are stored in the drift metrics table.  \nUsing derived and drift metrics where possible minimizes recomputation over the full primary table. Only aggregate metrics access data from the primary table. Derived and drift metrics can then be computed directly from the aggregate metric values.\n\nCustom metrics parameters"
    },
    {
        "id": 750,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html",
        "content": "Custom metrics parameters\nTo define a custom metric, you create a Jinja template for a SQL column expression. The tables in this section describe the parameters that define the metric, and the parameters that are used in the Jinja template.  \nParameter  \nDescription  \ntype  \nOne of MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE, MonitorMetricType.CUSTOM_METRIC_TYPE_DERIVED, or MonitorMetricType.CUSTOM_METRIC_TYPE_DRIFT.  \nname  \nColumn name for the custom metric in metric tables.  \ninput_columns  \nList of column names in the input table the metric should be computed for. To indicate that more than one column is used in the calculation, use :table. See the examples in this article.  \ndefinition  \nJinja template for a SQL expression that specifies how to compute the metric. See Create definition.  \noutput_data_type  \nSpark datatype of the metric output in a JSON string format.  \nCreate definition  \nThe definition parameter must be a single string expression in the form of a Jinja template. It cannot contain joins or subqueries. To construct complex definitions, you can use Python helper functions.  \nThe following table lists the parameters you can use to create a SQL Jinja Template to specify how to calculate the metric.  \nParameter  \nDescription  \n{{input_column}}  \nColumn used to compute the custom metric.  \n{{prediction_col}}  \nColumn holding ML model predictions. Used with InferenceLog analysis.  \n{{label_col}}  \nColumn holding ML model ground truth labels. Used with InferenceLog analysis.  \n{{current_df}}  \nFor drift compared to the previous time window. Data from the previous time window.  \n{{base_df}}  \nFor drift compared to the baseline table. Baseline data."
    },
    {
        "id": 751,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html",
        "content": "Aggregate metric example\nThe following example computes the average of the square of the values in a column, and is applied to columns f1 and f2. The output is saved as a new column in the profile metrics table and is shown in the analysis rows corresponding to the columns f1 and f2. The applicable column names are substituted for the Jinja parameter {{input_column}}.  \nfrom databricks.sdk.service.catalog import MonitorMetric, MonitorMetricType from pyspark.sql import types as T MonitorMetric( type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE, name=\"squared_avg\", input_columns=[\"f1\", \"f2\"], definition=\"avg(`{{input_column}}`*`{{input_column}}`)\", output_data_type=T.StructField(\"output\", T.DoubleType()).json(), )  \nThe following code defines a custom metric that computes the average of the difference between columns f1 and f2. This example shows the use of [\":table\"] in the input_columns parameter to indicate that more than one column from the table is used in the calculation.  \nfrom databricks.sdk.service.catalog import MonitorMetric, MonitorMetricType from pyspark.sql import types as T MonitorMetric( type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE, name=\"avg_diff_f1_f2\", input_columns=[\":table\"], definition=\"avg(f1 - f2)\", output_data_type=T.StructField(\"output\", T.DoubleType()).json(), )"
    },
    {
        "id": 752,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html",
        "content": "This example computes a weighted model quality score. For observations where the critical column is True, a heavier penalty is assigned when the predicted value for that row does not match the ground truth. Because it\u2019s defined on the raw columns (prediction and label), it\u2019s defined as an aggregate metric. The :table column indicates that this metric is calculated from multiple columns. The Jinja parameters {{prediction_col}} and {{label_col}} are replaced with the name of the prediction and ground truth label columns for the monitor.  \nfrom databricks.sdk.service.catalog import MonitorMetric, MonitorMetricType from pyspark.sql import types as T MonitorMetric( type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE, name=\"weighted_error\", input_columns=[\":table\"], definition=\"\"\"avg(CASE WHEN {{prediction_col}} = {{label_col}} THEN 0 WHEN {{prediction_col}} != {{label_col}} AND critical=TRUE THEN 2 ELSE 1 END)\"\"\", output_data_type=T.StructField(\"output\", T.DoubleType()).json(), )"
    },
    {
        "id": 753,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html",
        "content": "Derived metric example\nDerived metric example\nThe following code defines a custom metric that computes the square root of the squared_avg metric defined earlier in this section. Because this is a derived metric, it does not reference the primary table data and instead is defined in terms of the squared_avg aggregate metric. The output is saved as a new column in the profile metrics table.  \nfrom databricks.sdk.service.catalog import MonitorMetric, MonitorMetricType from pyspark.sql import types as T MonitorMetric( type=MonitorMetricType.CUSTOM_METRIC_TYPE_DERIVED, name=\"root_mean_square\", input_columns=[\"f1\", \"f2\"], definition=\"sqrt(squared_avg)\", output_data_type=T.StructField(\"output\", T.DoubleType()).json(), )\n\nDrift metrics example"
    },
    {
        "id": 754,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html",
        "content": "Drift metrics example\nThe following code defines a drift metric that tracks the change in the weighted_error metric defined earlier in this section. The {{current_df}} and {{base_df}} parameters allow the metric to reference the weighted_error values from the current window and the comparison window. The comparison window can be either the baseline data or the data from the previous time window. Drift metrics are saved in the drift metrics table.  \nfrom databricks.sdk.service.catalog import MonitorMetric, MonitorMetricType from pyspark.sql import types as T MonitorMetric( type=MonitorMetricType.CUSTOM_METRIC_TYPE_DRIFT, name=\"error_rate_delta\", input_columns=[\":table\"], definition=\"{{current_df}}.weighted_error - {{base_df}}.weighted_error\", output_data_type=T.StructField(\"output\", T.DoubleType()).json(), )"
    },
    {
        "id": 755,
        "url": "https://docs.databricks.com/en/jobs/jobs-api-updates.html",
        "content": "Updating from Jobs API 2.0 to 2.1  \nYou can now orchestrate multiple tasks with Databricks jobs. This article details changes to the Jobs API that support jobs with multiple tasks and provides guidance to help you update your existing API clients to work with this new feature.  \nDatabricks recommends Jobs API 2.1 for your API scripts and clients, particularly when using jobs with multiple tasks.  \nThis article refers to jobs defined with a single task as single-task format and jobs defined with multiple tasks as multi-task format.  \nJobs API 2.0 and 2.1 now support the update request. Use the update request to change an existing job instead of the reset request to minimize changes between single-task format jobs and multi-task format jobs.  \nAPI changes"
    },
    {
        "id": 756,
        "url": "https://docs.databricks.com/en/jobs/jobs-api-updates.html",
        "content": "API changes\nThe Jobs API now defines a TaskSettings object to capture settings for each task in a job. For multi-task format jobs, the tasks field, an array of TaskSettings data structures, is included in the JobSettings object. Some fields previously part of JobSettings are now part of the task settings for multi-task format jobs. JobSettings is also updated to include the format field. The format field indicates the format of the job and is a STRING value set to SINGLE_TASK or MULTI_TASK.  \nYou need to update your existing API clients for these changes to JobSettings for multi-task format jobs. See the API client guide for more information on required changes.  \nJobs API 2.1 supports the multi-task format. All API 2.1 requests must conform to the multi-task format and responses are structured in the multi-task format. New features are released for API 2.1 first.  \nJobs API 2.0 is updated with an additional field to support multi-task format jobs. Except where noted, the examples in this document use API 2.0. However, Databricks recommends API 2.1 for new and existing API scripts and clients.  \nAn example JSON document representing a multi-task format job for API 2.0 and 2.1:"
    },
    {
        "id": 757,
        "url": "https://docs.databricks.com/en/jobs/jobs-api-updates.html",
        "content": "An example JSON document representing a multi-task format job for API 2.0 and 2.1:  \n{ \"job_id\": 53, \"settings\": { \"name\": \"A job with multiple tasks\", \"email_notifications\": {}, \"timeout_seconds\": 0, \"max_concurrent_runs\": 1, \"tasks\": [ { \"task_key\": \"clean_data\", \"description\": \"Clean and prepare the data\", \"notebook_task\": { \"notebook_path\": \"/Users/user@databricks.com/clean-data\" }, \"existing_cluster_id\": \"1201-my-cluster\", \"max_retries\": 3, \"min_retry_interval_millis\": 0, \"retry_on_timeout\": true, \"timeout_seconds\": 3600, \"email_notifications\": {} }, { \"task_key\": \"analyze_data\", \"description\": \"Perform an analysis of the data\", \"notebook_task\": { \"notebook_path\": \"/Users/user@databricks.com/analyze-data\" }, \"depends_on\": [ { \"task_key\": \"clean_data\" } ], \"existing_cluster_id\": \"1201-my-cluster\", \"max_retries\": 3, \"min_retry_interval_millis\": 0, \"retry_on_timeout\": true, \"timeout_seconds\": 3600, \"email_notifications\": {} } ], \"format\": \"MULTI_TASK\" }, \"created_time\": 1625841911296, \"creator_user_name\": \"user@databricks.com\", \"run_as_user_name\": \"user@databricks.com\" }  \nJobs API 2.1 supports configuration of task level clusters or one or more shared job clusters:  \nA task level cluster is created and started when a task starts and terminates when the task completes."
    },
    {
        "id": 758,
        "url": "https://docs.databricks.com/en/jobs/jobs-api-updates.html",
        "content": "Jobs API 2.1 supports configuration of task level clusters or one or more shared job clusters:  \nA task level cluster is created and started when a task starts and terminates when the task completes.  \nA shared job cluster allows multiple tasks in the same job to use the cluster. The cluster is created and started when the first task using the cluster starts and terminates after the last task using the cluster completes. A shared job cluster is not terminated when idle but terminates only after all tasks using it are complete. Multiple non-dependent tasks sharing a cluster can start at the same time. If a shared job cluster fails or is terminated before all tasks have finished, a new cluster is created.  \nTo configure shared job clusters, include a JobCluster array in the JobSettings object. You can specify a maximum of 100 clusters per job. The following is an example of an API 2.1 response for a job configured with two shared clusters:  \nNote  \nIf a task has library dependencies, you must configure the libraries in the task field settings; libraries cannot be configured in a shared job cluster configuration. In the following example, the libraries field in the configuration of the ingest_orders task demonstrates specification of a library dependency."
    },
    {
        "id": 759,
        "url": "https://docs.databricks.com/en/jobs/jobs-api-updates.html",
        "content": "{ \"job_id\": 53, \"settings\": { \"name\": \"A job with multiple tasks\", \"email_notifications\": {}, \"timeout_seconds\": 0, \"max_concurrent_runs\": 1, \"job_clusters\": [ { \"job_cluster_key\": \"default_cluster\", \"new_cluster\": { \"spark_version\": \"7.3.x-scala2.12\", \"node_type_id\": \"i3.xlarge\", \"spark_conf\": { \"spark.speculation\": true }, \"aws_attributes\": { \"availability\": \"SPOT\", \"zone_id\": \"us-west-2a\" }, \"autoscale\": { \"min_workers\": 2, \"max_workers\": 8 } } }, { \"job_cluster_key\": \"data_processing_cluster\", \"new_cluster\": { \"spark_version\": \"7.3.x-scala2.12\", \"node_type_id\": \"r4.2xlarge\", \"spark_conf\": { \"spark.speculation\": true }, \"aws_attributes\": { \"availability\": \"SPOT\", \"zone_id\": \"us-west-2a\" }, \"autoscale\": { \"min_workers\": 8, \"max_workers\": 16 } } } ], \"tasks\": [ { \"task_key\": \"ingest_orders\", \"description\": \"Ingest order data\", \"depends_on\": [ ], \"job_cluster_key\": \"auto_scaling_cluster\", \"spark_jar_task\": { \"main_class_name\": \"com.databricks.OrdersIngest\", \"parameters\": [ \"--data\", \"dbfs:/path/to/order-data.json\" ] }, \"libraries\": [ { \"jar\": \"dbfs:/mnt/databricks/OrderIngest.jar\" } ], \"timeout_seconds\": 86400, \"max_retries\":"
    },
    {
        "id": 760,
        "url": "https://docs.databricks.com/en/jobs/jobs-api-updates.html",
        "content": "\"libraries\": [ { \"jar\": \"dbfs:/mnt/databricks/OrderIngest.jar\" } ], \"timeout_seconds\": 86400, \"max_retries\": 3, \"min_retry_interval_millis\": 2000, \"retry_on_timeout\": false }, { \"task_key\": \"clean_orders\", \"description\": \"Clean and prepare the order data\", \"notebook_task\": { \"notebook_path\": \"/Users/user@databricks.com/clean-data\" }, \"job_cluster_key\": \"default_cluster\", \"max_retries\": 3, \"min_retry_interval_millis\": 0, \"retry_on_timeout\": true, \"timeout_seconds\": 3600, \"email_notifications\": {} }, { \"task_key\": \"analyze_orders\", \"description\": \"Perform an analysis of the order data\", \"notebook_task\": { \"notebook_path\": \"/Users/user@databricks.com/analyze-data\" }, \"depends_on\": [ { \"task_key\": \"clean_data\" } ], \"job_cluster_key\": \"data_processing_cluster\", \"max_retries\": 3, \"min_retry_interval_millis\": 0, \"retry_on_timeout\": true, \"timeout_seconds\": 3600, \"email_notifications\": {} } ], \"format\": \"MULTI_TASK\" }, \"created_time\": 1625841911296, \"creator_user_name\": \"user@databricks.com\", \"run_as_user_name\": \"user@databricks.com\" }"
    },
    {
        "id": 761,
        "url": "https://docs.databricks.com/en/jobs/jobs-api-updates.html",
        "content": "For single-task format jobs, the JobSettings data structure remains unchanged except for the addition of the format field. No TaskSettings array is included, and the task settings remain defined at the top level of the JobSettings data structure. You will not need to make changes to your existing API clients to process single-task format jobs.  \nAn example JSON document representing a single-task format job for API 2.0:  \n{ \"job_id\": 27, \"settings\": { \"name\": \"Example notebook\", \"existing_cluster_id\": \"1201-my-cluster\", \"libraries\": [ { \"jar\": \"dbfs:/FileStore/jars/spark_examples.jar\" } ], \"email_notifications\": {}, \"timeout_seconds\": 0, \"schedule\": { \"quartz_cron_expression\": \"0 0 0 * * ?\", \"timezone_id\": \"US/Pacific\", \"pause_status\": \"UNPAUSED\" }, \"notebook_task\": { \"notebook_path\": \"/notebooks/example-notebook\", \"revision_timestamp\": 0 }, \"max_concurrent_runs\": 1, \"format\": \"SINGLE_TASK\" }, \"created_time\": 1504128821443, \"creator_user_name\": \"user@databricks.com\" }"
    },
    {
        "id": 762,
        "url": "https://docs.databricks.com/en/jobs/jobs-api-updates.html",
        "content": "API client guide\nThis section provides guidelines, examples, and required changes for API calls affected by the new multi-task format feature.  \nIn this section:  \nCreate  \nRuns submit  \nUpdate  \nReset  \nList  \nGet  \nRuns get  \nRuns get output  \nRuns list  \nCreate  \nTo create a single-task format job through the Create a new job operation (POST /jobs/create) in the Jobs API, you do not need to change existing clients.  \nTo create a multi-task format job, use the tasks field in JobSettings to specify settings for each task. The following example creates a job with two notebook tasks. This example is for API 2.0 and 2.1:  \nNote  \nA maximum of 100 tasks can be specified per job.  \n{ \"name\": \"Multi-task-job\", \"max_concurrent_runs\": 1, \"tasks\": [ { \"task_key\": \"clean_data\", \"description\": \"Clean and prepare the data\", \"notebook_task\": { \"notebook_path\": \"/Users/user@databricks.com/clean-data\" }, \"existing_cluster_id\": \"1201-my-cluster\", \"timeout_seconds\": 3600, \"max_retries\": 3, \"retry_on_timeout\": true }, { \"task_key\": \"analyze_data\", \"description\": \"Perform an analysis of the data\", \"notebook_task\": { \"notebook_path\": \"/Users/user@databricks.com/analyze-data\" }, \"depends_on\": [ { \"task_key\": \"clean_data\" } ], \"existing_cluster_id\": \"1201-my-cluster\", \"timeout_seconds\": 3600, \"max_retries\": 3, \"retry_on_timeout\": true } ] }  \nRuns submit  \nTo submit a one-time run of a single-task format job with the Create and trigger a one-time run operation (POST /runs/submit) in the Jobs API, you do not need to change existing clients."
    },
    {
        "id": 763,
        "url": "https://docs.databricks.com/en/jobs/jobs-api-updates.html",
        "content": "Runs submit  \nTo submit a one-time run of a single-task format job with the Create and trigger a one-time run operation (POST /runs/submit) in the Jobs API, you do not need to change existing clients.  \nTo submit a one-time run of a multi-task format job, use the tasks field in JobSettings to specify settings for each task, including clusters. Clusters must be set at the task level when submitting a multi-task format job because the runs submit request does not support shared job clusters. See Create for an example JobSettings specifying multiple tasks.  \nUpdate  \nTo update a single-task format job with the Partially update a job operation (POST /jobs/update) in the Jobs API, you do not need to change existing clients.  \nTo update the settings of a multi-task format job, you must use the unique task_key field to identify new task settings. See Create for an example JobSettings specifying multiple tasks.  \nReset  \nTo overwrite the settings of a single-task format job with the Overwrite all settings for a job operation (POST /jobs/reset) in the Jobs API, you do not need to change existing clients.  \nTo overwrite the settings of a multi-task format job, specify a JobSettings data structure with an array of TaskSettings data structures. See Create for an example JobSettings specifying multiple tasks.  \nUse Update to change individual fields without switching from single-task to multi-task format.  \nList  \nFor single-task format jobs, no client changes are required to process the response from the List all jobs operation (GET /jobs/list) in the Jobs API.  \nFor multi-task format jobs, most settings are defined at the task level and not the job level. Cluster configuration may be set at the task or job level. To modify clients to access cluster or task settings for a multi-task format job returned in the Job structure:  \nParse the job_id field for the multi-task format job.  \nPass the job_id to the Get a job operation (GET /jobs/get) in the Jobs API to retrieve job details. See Get for an example response from the Get API call for a multi-task format job.  \nThe following example shows a response containing single-task and multi-task format jobs. This example is for API 2.0:"
    },
    {
        "id": 764,
        "url": "https://docs.databricks.com/en/jobs/jobs-api-updates.html",
        "content": "The following example shows a response containing single-task and multi-task format jobs. This example is for API 2.0:  \n{ \"jobs\": [ { \"job_id\": 36, \"settings\": { \"name\": \"A job with a single task\", \"existing_cluster_id\": \"1201-my-cluster\", \"email_notifications\": {}, \"timeout_seconds\": 0, \"notebook_task\": { \"notebook_path\": \"/Users/user@databricks.com/example-notebook\", \"revision_timestamp\": 0 }, \"max_concurrent_runs\": 1, \"format\": \"SINGLE_TASK\" }, \"created_time\": 1505427148390, \"creator_user_name\": \"user@databricks.com\" }, { \"job_id\": 53, \"settings\": { \"name\": \"A job with multiple tasks\", \"email_notifications\": {}, \"timeout_seconds\": 0, \"max_concurrent_runs\": 1, \"format\": \"MULTI_TASK\" }, \"created_time\": 1625841911296, \"creator_user_name\": \"user@databricks.com\" } ] }  \nGet  \nFor single-task format jobs, no client changes are required to process the response from the Get a job operation (GET /jobs/get) in the Jobs API.  \nMulti-task format jobs return an array of task data structures containing task settings. If you require access to task level details, you need to modify your clients to iterate through the tasks array and extract required fields.  \nThe following shows an example response from the Get API call for a multi-task format job. This example is for API 2.0 and 2.1:"
    },
    {
        "id": 765,
        "url": "https://docs.databricks.com/en/jobs/jobs-api-updates.html",
        "content": "The following shows an example response from the Get API call for a multi-task format job. This example is for API 2.0 and 2.1:  \n{ \"job_id\": 53, \"settings\": { \"name\": \"A job with multiple tasks\", \"email_notifications\": {}, \"timeout_seconds\": 0, \"max_concurrent_runs\": 1, \"tasks\": [ { \"task_key\": \"clean_data\", \"description\": \"Clean and prepare the data\", \"notebook_task\": { \"notebook_path\": \"/Users/user@databricks.com/clean-data\" }, \"existing_cluster_id\": \"1201-my-cluster\", \"max_retries\": 3, \"min_retry_interval_millis\": 0, \"retry_on_timeout\": true, \"timeout_seconds\": 3600, \"email_notifications\": {} }, { \"task_key\": \"analyze_data\", \"description\": \"Perform an analysis of the data\", \"notebook_task\": { \"notebook_path\": \"/Users/user@databricks.com/analyze-data\" }, \"depends_on\": [ { \"task_key\": \"clean_data\" } ], \"existing_cluster_id\": \"1201-my-cluster\", \"max_retries\": 3, \"min_retry_interval_millis\": 0, \"retry_on_timeout\": true, \"timeout_seconds\": 3600, \"email_notifications\": {} } ], \"format\": \"MULTI_TASK\" }, \"created_time\": 1625841911296, \"creator_user_name\": \"user@databricks.com\", \"run_as_user_name\": \"user@databricks.com\" }  \nRuns get  \nFor single-task format jobs, no client changes are required to process the response from the Get a job run operation (GET /jobs/runs/get) in the Jobs API."
    },
    {
        "id": 766,
        "url": "https://docs.databricks.com/en/jobs/jobs-api-updates.html",
        "content": "Runs get  \nFor single-task format jobs, no client changes are required to process the response from the Get a job run operation (GET /jobs/runs/get) in the Jobs API.  \nThe response for a multi-task format job run contains an array of TaskSettings. To retrieve run results for each task:  \nIterate through each of the tasks.  \nParse the run_id for each task.  \nCall the Get the output for a run operation (GET /jobs/runs/get-output) with the run_id to get details on the run for each task. The following is an example response from this request:"
    },
    {
        "id": 767,
        "url": "https://docs.databricks.com/en/jobs/jobs-api-updates.html",
        "content": "{ \"job_id\": 53, \"run_id\": 759600, \"number_in_job\": 7, \"original_attempt_run_id\": 759600, \"state\": { \"life_cycle_state\": \"TERMINATED\", \"result_state\": \"SUCCESS\", \"state_message\": \"\" }, \"cluster_spec\": {}, \"start_time\": 1595943854860, \"setup_duration\": 0, \"execution_duration\": 0, \"cleanup_duration\": 0, \"trigger\": \"ONE_TIME\", \"creator_user_name\": \"user@databricks.com\", \"run_name\": \"Query logs\", \"run_type\": \"JOB_RUN\", \"tasks\": [ { \"run_id\": 759601, \"task_key\": \"query-logs\", \"description\": \"Query session logs\", \"notebook_task\": { \"notebook_path\": \"/Users/user@databricks.com/log-query\" }, \"existing_cluster_id\": \"1201-my-cluster\", \"state\": { \"life_cycle_state\": \"TERMINATED\", \"result_state\": \"SUCCESS\", \"state_message\": \"\" } }, { \"run_id\": 759602, \"task_key\": \"validate_output\", \"description\": \"Validate query output\", \"depends_on\": [ { \"task_key\": \"query-logs\" } ], \"notebook_task\": { \"notebook_path\": \"/Users/user@databricks.com/validate-query-results\" }, \"existing_cluster_id\": \"1201-my-cluster\", \"state\": { \"life_cycle_state\": \"TERMINATED\", \"result_state\": \"SUCCESS\", \"state_message\": \"\" } } ], \"format\": \"MULTI_TASK\" }  \nRuns get output"
    },
    {
        "id": 768,
        "url": "https://docs.databricks.com/en/jobs/jobs-api-updates.html",
        "content": "Runs get output  \nFor single-task format jobs, no client changes are required to process the response from the Get the output for a run operation (GET /jobs/runs/get-output) in the Jobs API.  \nFor multi-task format jobs, calling Runs get output on a parent run results in an error since run output is available only for individual tasks. To get the output and metadata for a multi-task format job:  \nCall the Get the output for a run request.  \nIterate over the child run_id fields in the response.  \nUse the child run_id values to call Runs get output.  \nRuns list  \nFor single-task format jobs, no client changes are required to process the response from the List runs for a job operation (GET /jobs/runs/list).  \nFor multi-task format jobs, an empty tasks array is returned. Pass the run_id to the Get a job run operation (GET /jobs/runs/get) to retrieve the tasks. The following shows an example response from the Runs list API call for a multi-task format job:  \n{ \"runs\": [ { \"job_id\": 53, \"run_id\": 759600, \"number_in_job\": 7, \"original_attempt_run_id\": 759600, \"state\": { \"life_cycle_state\": \"TERMINATED\", \"result_state\": \"SUCCESS\", \"state_message\": \"\" }, \"cluster_spec\": {}, \"start_time\": 1595943854860, \"setup_duration\": 0, \"execution_duration\": 0, \"cleanup_duration\": 0, \"trigger\": \"ONE_TIME\", \"creator_user_name\": \"user@databricks.com\", \"run_name\": \"Query logs\", \"run_type\": \"JOB_RUN\", \"tasks\": [], \"format\": \"MULTI_TASK\" } ], \"has_more\": false }"
    },
    {
        "id": 769,
        "url": "https://docs.databricks.com/en/notebooks/notebook-workflows.html",
        "content": "Run a Databricks notebook from another notebook  \nImportant  \nFor notebook orchestration, use Databricks Jobs. For code modularization scenarios, use workspace files. You should only use the techniques described in this article when your use case cannot be implemented using a Databricks job, such as for looping notebooks over a dynamic set of parameters, or if you do not have access to workspace files. For more information, see Databricks Jobs and share code.  \nComparison of %run and dbutils.notebook.run()"
    },
    {
        "id": 770,
        "url": "https://docs.databricks.com/en/notebooks/notebook-workflows.html",
        "content": "Comparison of %run and dbutils.notebook.run()\nThe %run command allows you to include another notebook within a notebook. You can use %run to modularize your code, for example by putting supporting functions in a separate notebook. You can also use it to concatenate notebooks that implement the steps in an analysis. When you use %run, the called notebook is immediately executed and the functions and variables defined in it become available in the calling notebook.  \nThe dbutils.notebook API is a complement to %run because it lets you pass parameters to and return values from a notebook. This allows you to build complex workflows and pipelines with dependencies. For example, you can get a list of files in a directory and pass the names to another notebook, which is not possible with %run. You can also create if-then-else workflows based on return values or call other notebooks using relative paths.  \nUnlike %run, the dbutils.notebook.run() method starts a new job to run the notebook.  \nThese methods, like all of the dbutils APIs, are available only in Python and Scala. However, you can use dbutils.notebook.run() to invoke an R notebook."
    },
    {
        "id": 771,
        "url": "https://docs.databricks.com/en/notebooks/notebook-workflows.html",
        "content": "Use %run to import a notebook\nIn this example, the first notebook defines a function, reverse, which is available in the second notebook after you use the %run magic to execute shared-code-notebook.  \nBecause both of these notebooks are in the same directory in the workspace, use the prefix ./ in ./shared-code-notebook to indicate that the path should be resolved relative to the currently running notebook. You can organize notebooks into directories, such as %run ./dir/notebook, or use an absolute path like %run /Users/username@organization.com/directory/notebook.  \nNote  \n%run must be in a cell by itself, because it runs the entire notebook inline.  \nYou cannot use %run to run a Python file and import the entities defined in that file into a notebook. To import from a Python file, see Modularize your code using files. Or, package the file into a Python library, create a Databricks library from that Python library, and install the library into the cluster you use to run your notebook.  \nWhen you use %run to run a notebook that contains widgets, by default the specified notebook runs with the widget\u2019s default values. You can also pass in values to widgets; see Use Databricks widgets with %run."
    },
    {
        "id": 772,
        "url": "https://docs.databricks.com/en/notebooks/notebook-workflows.html",
        "content": "dbutils.notebook API\nThe methods available in the dbutils.notebook API are run and exit. Both parameters and return values must be strings.  \nrun(path: String,\u00a0 timeout_seconds: int, arguments: Map): String  \nRun a notebook and return its exit value. The method starts an ephemeral job that runs immediately.  \nThe timeout_seconds parameter controls the timeout of the run (0 means no timeout): the call to run throws an exception if it doesn\u2019t finish within the specified time. If Databricks is down for more than 10 minutes, the notebook run fails regardless of timeout_seconds.  \nThe arguments parameter sets widget values of the target notebook. Specifically, if the notebook you are running has a widget named A, and you pass a key-value pair (\"A\": \"B\") as part of the arguments parameter to the run() call, then retrieving the value of widget A will return \"B\". You can find the instructions for creating and working with widgets in the Databricks widgets article.  \nNote  \nThe arguments parameter accepts only Latin characters (ASCII character set). Using non-ASCII characters returns an error.  \nJobs created using the dbutils.notebook API must complete in 30 days or less.  \nrun Usage  \ndbutils.notebook.run(\"notebook-name\", 60, {\"argument\": \"data\", \"argument2\": \"data2\", ...})  \ndbutils.notebook.run(\"notebook-name\", 60, Map(\"argument\" -> \"data\", \"argument2\" -> \"data2\", ...))  \nrun Example  \nSuppose you have a notebook named workflows with a widget named foo that prints the widget\u2019s value:  \ndbutils.widgets.text(\"foo\", \"fooDefault\", \"fooEmptyLabel\") print(dbutils.widgets.get(\"foo\"))  \nRunning dbutils.notebook.run(\"workflows\", 60, {\"foo\": \"bar\"}) produces the following result:"
    },
    {
        "id": 773,
        "url": "https://docs.databricks.com/en/notebooks/notebook-workflows.html",
        "content": "Running dbutils.notebook.run(\"workflows\", 60, {\"foo\": \"bar\"}) produces the following result:  \nThe widget had the value you passed in using dbutils.notebook.run(), \"bar\", rather than the default.  \nexit(value: String): void Exit a notebook with a value. If you call a notebook using the run method, this is the value returned.  \ndbutils.notebook.exit(\"returnValue\")  \nCalling dbutils.notebook.exit in a job causes the notebook to complete successfully. If you want to cause the job to fail, throw an exception."
    },
    {
        "id": 774,
        "url": "https://docs.databricks.com/en/notebooks/notebook-workflows.html",
        "content": "Example\nExample\nIn the following example, you pass arguments to DataImportNotebook and run different notebooks (DataCleaningNotebook or ErrorHandlingNotebook) based on the result from DataImportNotebook.  \nWhen the code runs, a table appears containing a link to the running notebook:  \nTo view the run details, click the Start time link in the table. If the run is complete, you can also view the run details by clicking the End time link.\n\nPass structured data"
    },
    {
        "id": 775,
        "url": "https://docs.databricks.com/en/notebooks/notebook-workflows.html",
        "content": "# Example 1 - returning data through temporary views. # You can only return one string using dbutils.notebook.exit(), but since called notebooks reside in the same JVM, you can # return a name referencing data stored in a temporary view. ## In callee notebook spark.range(5).toDF(\"value\").createOrReplaceGlobalTempView(\"my_data\") dbutils.notebook.exit(\"my_data\") ## In caller notebook returned_table = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60) global_temp_db = spark.conf.get(\"spark.sql.globalTempDatabase\") display(table(global_temp_db + \".\" + returned_table)) # Example 2 - returning data through DBFS. # For larger datasets, you can write the results to DBFS and then return the DBFS path of the stored data. ## In callee notebook dbutils.fs.rm(\"/tmp/results/my_data\", recurse=True) spark.range(5).toDF(\"value\").write.format(\"parquet\").save(\"dbfs:/tmp/results/my_data\") dbutils.notebook.exit(\"dbfs:/tmp/results/my_data\") ## In caller notebook returned_table = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60) display(spark.read.format(\"parquet\").load(returned_table)) # Example 3 - returning JSON data. # To return multiple values, you can use standard JSON libraries to serialize and deserialize results. ## In callee notebook import json dbutils.notebook.exit(json.dumps({ \"status\": \"OK\", \"table\": \"my_data\" })) ## In caller notebook import json result = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60) print(json.loads(result))"
    },
    {
        "id": 776,
        "url": "https://docs.databricks.com/en/notebooks/notebook-workflows.html",
        "content": "// Example 1 - returning data through temporary views. // You can only return one string using dbutils.notebook.exit(), but since called notebooks reside in the same JVM, you can // return a name referencing data stored in a temporary view. /** In callee notebook */ sc.parallelize(1 to 5).toDF().createOrReplaceGlobalTempView(\"my_data\") dbutils.notebook.exit(\"my_data\") /** In caller notebook */ val returned_table = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60) val global_temp_db = spark.conf.get(\"spark.sql.globalTempDatabase\") display(table(global_temp_db + \".\" + returned_table)) // Example 2 - returning data through DBFS. // For larger datasets, you can write the results to DBFS and then return the DBFS path of the stored data. /** In callee notebook */ dbutils.fs.rm(\"/tmp/results/my_data\", recurse=true) sc.parallelize(1 to 5).toDF().write.format(\"parquet\").save(\"dbfs:/tmp/results/my_data\") dbutils.notebook.exit(\"dbfs:/tmp/results/my_data\") /** In caller notebook */ val returned_table = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60) display(sqlContext.read.format(\"parquet\").load(returned_table)) // Example 3 - returning JSON data. // To return multiple values, you can use standard JSON libraries to serialize and deserialize results. /** In callee notebook */ // Import jackson json libraries import com.fasterxml.jackson.module.scala.DefaultScalaModule import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper import"
    },
    {
        "id": 777,
        "url": "https://docs.databricks.com/en/notebooks/notebook-workflows.html",
        "content": "Import jackson json libraries import com.fasterxml.jackson.module.scala.DefaultScalaModule import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper import com.fasterxml.jackson.databind.ObjectMapper // Create a json serializer val jsonMapper = new ObjectMapper with ScalaObjectMapper jsonMapper.registerModule(DefaultScalaModule) // Exit with json dbutils.notebook.exit(jsonMapper.writeValueAsString(Map(\"status\" -> \"OK\", \"table\" -> \"my_data\"))) /** In caller notebook */ // Import jackson json libraries import com.fasterxml.jackson.module.scala.DefaultScalaModule import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper import com.fasterxml.jackson.databind.ObjectMapper // Create a json serializer val jsonMapper = new ObjectMapper with ScalaObjectMapper jsonMapper.registerModule(DefaultScalaModule) val result = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60) println(jsonMapper.readValue[Map[String, String]](result))"
    },
    {
        "id": 778,
        "url": "https://docs.databricks.com/en/notebooks/notebook-workflows.html",
        "content": "Handle errors\nThis section illustrates how to handle errors.  \n# Errors throw a WorkflowException. def run_with_retry(notebook, timeout, args = {}, max_retries = 3): num_retries = 0 while True: try: return dbutils.notebook.run(notebook, timeout, args) except Exception as e: if num_retries > max_retries: raise e else: print(\"Retrying error\", e) num_retries += 1 run_with_retry(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60, max_retries = 5)  \n// Errors throw a WorkflowException. import com.databricks.WorkflowException // Since dbutils.notebook.run() is just a function call, you can retry failures using standard Scala try-catch // control flow. Here we show an example of retrying a notebook a number of times. def runRetry(notebook: String, timeout: Int, args: Map[String, String] = Map.empty, maxTries: Int = 3): String = { var numTries = 0 while (true) { try { return dbutils.notebook.run(notebook, timeout, args) } catch { case e: WorkflowException if numTries < maxTries => println(\"Error, retrying: \" + e) } numTries += 1 } \"\" // not reached } runRetry(\"LOCATION_OF_CALLEE_NOTEBOOK\", timeout = 60, maxTries = 5)"
    },
    {
        "id": 779,
        "url": "https://docs.databricks.com/en/notebooks/notebook-workflows.html",
        "content": "Run multiple notebooks concurrently\nRun multiple notebooks concurrently\nYou can run multiple notebooks at the same time by using standard Scala and Python constructs such as Threads (Scala, Python) and Futures (Scala, Python). The example notebooks demonstrate how to use these constructs.  \nDownload the following 4 notebooks. The notebooks are written in Scala.  \nImport the notebooks into a single folder in the workspace.  \nRun the Run concurrently notebook.  \nRun concurrently notebook  \nOpen notebook in new tab Copy link for import  \nRun in parallel notebook  \nOpen notebook in new tab Copy link for import  \nTesting notebook  \nOpen notebook in new tab Copy link for import  \nTesting-2 notebook  \nOpen notebook in new tab Copy link for import"
    },
    {
        "id": 780,
        "url": "https://docs.databricks.com/en/mlflow/runs.html",
        "content": "Manage training code with MLflow runs  \nThis article describes MLflow runs for managing machine learning training. It also includes guidance on how to manage and compare runs across experiments.  \nAn MLflow run corresponds to a single execution of model code. Each run records the following information:  \nSource: Name of the notebook that launched the run or the project name and entry point for the run.  \nVersion: Git commit hash if notebook is stored in a Databricks Git folder or run from an MLflow Project. Otherwise, notebook revision.  \nStart & end time: Start and end time of the run.  \nParameters: Model parameters saved as key-value pairs. Both keys and values are strings.  \nMetrics: Model evaluation metrics saved as key-value pairs. The value is numeric. Each metric can be updated throughout the course of the run (for example, to track how your model\u2019s loss function is converging), and MLflow records and lets you visualize the metric\u2019s history.  \nTags: Run metadata saved as key-value pairs. You can update tags during and after a run completes. Both keys and values are strings.  \nArtifacts: Output files in any format. For example, you can record images, models (for example, a pickled scikit-learn model), and data files (for example, a Parquet file) as an artifact.  \nAll MLflow runs are logged to the active experiment. If you have not explicitly set an experiment as the active experiment, runs are logged to the notebook experiment.  \nView runs"
    },
    {
        "id": 781,
        "url": "https://docs.databricks.com/en/mlflow/runs.html",
        "content": "View runs\nYou can access a run either from its parent experiment page or directly from the notebook that created the run.  \nFrom the experiment page, in the runs table, click the start time of a run.  \nFrom the notebook, click next to the date and time of the run in the Experiment Runs sidebar.  \nThe run screen shows the parameters used for the run, the metrics resulting from the run, and any tags or notes. To display Notes, Parameters, Metrics, or Tags for this run, click to the left of the label.  \nYou also access artifacts saved from a run in this screen.  \nCode snippets for prediction  \nIf you log a model from a run, the model appears in the Artifacts section of this page. To display code snippets illustrating how to load and use the model to make predictions on Spark and pandas DataFrames, click the model name.  \nView the notebook or Git project used for a run  \nTo view the version of the notebook that created a run:  \nOn the experiment page, click the link in the Source column.  \nOn the run page, click the link next to Source.  \nFrom the notebook, in the Experiment Runs sidebar, click the Notebook icon in the box for that Experiment Run.  \nThe version of the notebook associated with the run appears in the main window with a highlight bar showing the date and time of the run.  \nIf the run was launched remotely from a Git project, click the link in the Git Commit field to open the specific version of the project used in the run. The link in the Source field opens the main branch of the Git project used in the run."
    },
    {
        "id": 782,
        "url": "https://docs.databricks.com/en/mlflow/runs.html",
        "content": "Add a tag to a run\nAdd a tag to a run\nTags are key-value pairs that you can create and use later to search for runs.  \nFrom the run page, click if it is not already open. The tags table appears.  \nClick in the Name and Value fields and type the key and value for your tag.  \nClick Add.  \nEdit or delete a tag for a run  \nTo edit or delete an existing tag, use the icons in the Actions column.\n\nReproduce the software environment of a run"
    },
    {
        "id": 783,
        "url": "https://docs.databricks.com/en/mlflow/runs.html",
        "content": "Reproduce the software environment of a run\nYou can reproduce the exact software environment for the run by clicking Reproduce Run. The following dialog appears:  \nWith the default settings, when you click Confirm:  \nThe notebook is cloned to the location shown in the dialog.  \nIf the original cluster still exists, the cloned notebook is attached to the original cluster and the cluster is started.  \nIf the original cluster no longer exists, a new cluster with the same configuration, including any installed libraries, is created and started. The notebook is attached to the new cluster.  \nYou can select a different location for the cloned notebook and inspect the cluster configuration and installed libraries:  \nTo select a different folder to save the cloned notebook, click Edit Folder.  \nTo see the cluster spec, click View Spec. To clone only the notebook and not the cluster, uncheck this option.  \nTo see the libraries installed on the original cluster, click View Libraries. If you don\u2019t care about installing the same libraries as on the original cluster, uncheck this option.\n\nManage runs"
    },
    {
        "id": 784,
        "url": "https://docs.databricks.com/en/mlflow/runs.html",
        "content": "Manage runs\nRename run  \nTo rename a run, click at the upper right corner of the run page and select Rename.  \nFilter runs  \nYou can search for runs based on parameter or metric values. You can also search for runs by tag.  \nTo search for runs that match an expression containing parameter and metric values, enter a query in the search field and click Search. Some query syntax examples are:  \nmetrics.r2 > 0.3  \nparams.elasticNetParam = 0.5  \nparams.elasticNetParam = 0.5 AND metrics.avg_areaUnderROC > 0.3  \nTo search for runs by tag, enter tags in the format: tags.<key>=\"<value>\". String values must be enclosed in quotes as shown.  \ntags.estimator_name=\"RandomForestRegressor\"  \ntags.color=\"blue\" AND tags.size=5  \nBoth keys and values can contain spaces. If the key includes spaces, you must enclose it in backticks as shown.  \ntags.`my custom tag` = \"my value\"  \nYou can also filter runs based on their state (Active or Deleted) and based on whether a model version is associated with the run. To do this, make your selections from the State and Time Created drop-down menus respectively.  \nDownload runs  \nSelect one or more runs.  \nClick Download CSV. A CSV file containing the following fields downloads:  \nRun ID,Name,Source Type,Source Name,User,Status,<parameter1>,<parameter2>,...,<metric1>,<metric2>,...  \nDelete runs  \nYou can delete runs using the Databricks Machine Learning UI with the following steps:  \nIn the experiment, select one or more runs by clicking in the checkbox to the left of the run.  \nClick Delete.  \nIf the run is a parent run, decide whether you also want to delete descendant runs. This option is selected by default.  \nClick Delete to confirm. Deleted runs are saved for 30 days. To display deleted runs, select Deleted in the State field.  \nBulk delete runs based on the creation time"
    },
    {
        "id": 785,
        "url": "https://docs.databricks.com/en/mlflow/runs.html",
        "content": "Click Delete to confirm. Deleted runs are saved for 30 days. To display deleted runs, select Deleted in the State field.  \nBulk delete runs based on the creation time  \nYou can use Python to bulk delete runs of an experiment that were created prior to or at a UNIX timestamp. Using Databricks Runtime 14.1 or later, you can call the mlflow.delete_runs API to delete runs and return the number of runs deleted.  \nThe following are the mlflow.delete_runs parameters:  \nexperiment_id: The ID of the experiment containing the runs to delete.  \nmax_timestamp_millis: The maximum creation timestamp in milliseconds since the UNIX epoch for deleting runs. Only runs created prior to or at this timestamp are deleted.  \nmax_runs: Optional. A positive integer that indicates the maximum number of runs to delete. The maximum allowed value for max_runs is 10000. If not specified, max_runs defaults to 10000.  \nimport mlflow # Replace <experiment_id>, <max_timestamp_ms>, and <max_runs> with your values. runs_deleted = mlflow.delete_runs( experiment_id=<experiment_id>, max_timestamp_millis=<max_timestamp_ms>, max_runs=<max_runs> ) # Example: runs_deleted = mlflow.delete_runs( experiment_id=\"4183847697906956\", max_timestamp_millis=1711990504000, max_runs=10 )  \nUsing Databricks Runtime 13.3 LTS or earlier, you can run the following client code in a Databricks Notebook."
    },
    {
        "id": 786,
        "url": "https://docs.databricks.com/en/mlflow/runs.html",
        "content": "Using Databricks Runtime 13.3 LTS or earlier, you can run the following client code in a Databricks Notebook.  \nfrom typing import Optional def delete_runs(experiment_id: str, max_timestamp_millis: int, max_runs: Optional[int] = None) -> int: \"\"\" Bulk delete runs in an experiment that were created prior to or at the specified timestamp. Deletes at most max_runs per request. :param experiment_id: The ID of the experiment containing the runs to delete. :param max_timestamp_millis: The maximum creation timestamp in milliseconds since the UNIX epoch for deleting runs. Only runs created prior to or at this timestamp are deleted. :param max_runs: Optional. A positive integer indicating the maximum number of runs to delete. The maximum allowed value for max_runs is 10000. If not specified, max_runs defaults to 10000. :return: The number of runs deleted. \"\"\" from mlflow.utils.databricks_utils import get_databricks_host_creds from mlflow.utils.request_utils import augmented_raise_for_status from mlflow.utils.rest_utils import http_request json_body = {\"experiment_id\": experiment_id, \"max_timestamp_millis\": max_timestamp_millis} if max_runs is not None: json_body[\"max_runs\"] = max_runs response = http_request( host_creds=get_databricks_host_creds(), endpoint=\"/api/2.0/mlflow/databricks/runs/delete-runs\", method=\"POST\", json=json_body, ) augmented_raise_for_status(response) return response.json()[\"runs_deleted\"]  \nSee the Databricks Experiments API documentation for parameters and return value specifications for deleting runs based on creation time.  \nRestore runs  \nYou can restore previously deleted runs using the Databricks Machine Learning UI.  \nOn the Experiment page, select Deleted in the State field to display deleted runs."
    },
    {
        "id": 787,
        "url": "https://docs.databricks.com/en/mlflow/runs.html",
        "content": "Restore runs  \nYou can restore previously deleted runs using the Databricks Machine Learning UI.  \nOn the Experiment page, select Deleted in the State field to display deleted runs.  \nSelect one or more runs by clicking in the checkbox to the left of the run.  \nClick Restore.  \nClick Restore to confirm. To display the restored runs, select Active in the State field.  \nBulk restore runs based on the deletion time  \nYou can also use Python to bulk restore runs of an experiment that were deleted at or after a UNIX timestamp. Using Databricks Runtime 14.1 or later, you can call the mlflow.restore_runs API to restore runs and return the number of restored runs.  \nThe following are the mlflow.restore_runs parameters:  \nexperiment_id: The ID of the experiment containing the runs to restore.  \nmin_timestamp_millis: The minimum deletion timestamp in milliseconds since the UNIX epoch for restoring runs. Only runs deleted at or after this timestamp are restored.  \nmax_runs: Optional. A positive integer that indicates the maximum number of runs to restore. The maximum allowed value for max_runs is 10000. If not specified, max_runs defaults to 10000.  \nimport mlflow # Replace <experiment_id>, <min_timestamp_ms>, and <max_runs> with your values. runs_restored = mlflow.restore_runs( experiment_id=<experiment_id>, min_timestamp_millis=<min_timestamp_ms>, max_runs=<max_runs> ) # Example: runs_restored = mlflow.restore_runs( experiment_id=\"4183847697906956\", min_timestamp_millis=1711990504000, max_runs=10 )  \nUsing Databricks Runtime 13.3 LTS or earlier, you can run the following client code in a Databricks Notebook."
    },
    {
        "id": 788,
        "url": "https://docs.databricks.com/en/mlflow/runs.html",
        "content": "Using Databricks Runtime 13.3 LTS or earlier, you can run the following client code in a Databricks Notebook.  \nfrom typing import Optional def restore_runs(experiment_id: str, min_timestamp_millis: int, max_runs: Optional[int] = None) -> int: \"\"\" Bulk restore runs in an experiment that were deleted at or after the specified timestamp. Restores at most max_runs per request. :param experiment_id: The ID of the experiment containing the runs to restore. :param min_timestamp_millis: The minimum deletion timestamp in milliseconds since the UNIX epoch for restoring runs. Only runs deleted at or after this timestamp are restored. :param max_runs: Optional. A positive integer indicating the maximum number of runs to restore. The maximum allowed value for max_runs is 10000. If not specified, max_runs defaults to 10000. :return: The number of runs restored. \"\"\" from mlflow.utils.databricks_utils import get_databricks_host_creds from mlflow.utils.request_utils import augmented_raise_for_status from mlflow.utils.rest_utils import http_request json_body = {\"experiment_id\": experiment_id, \"min_timestamp_millis\": min_timestamp_millis} if max_runs is not None: json_body[\"max_runs\"] = max_runs response = http_request( host_creds=get_databricks_host_creds(), endpoint=\"/api/2.0/mlflow/databricks/runs/restore-runs\", method=\"POST\", json=json_body, ) augmented_raise_for_status(response) return response.json()[\"runs_restored\"]  \nSee the Databricks Experiments API documentation for parameters and return value specifications for restoring runs based on deletion time."
    },
    {
        "id": 789,
        "url": "https://docs.databricks.com/en/mlflow/runs.html",
        "content": "Compare runs\nYou can compare runs from a single experiment or from multiple experiments. The Comparing Runs page presents information about the selected runs in graphic and tabular formats. You can also create visualizations of run results and tables of run information, run parameters, and metrics.  \nTo create a visualization:  \nSelect the plot type (Parallel Coordinates Plot, Scatter Plot, or Contour Plot).  \nFor a Parallel Coordinates Plot, select the parameters and metrics to plot. From here, you can identify relationships between the selected parameters and metrics, which helps you better define the hyperparameter tuning space for your models.  \nFor a Scatter Plot or Contour Plot, select the parameter or metric to display on each axis.  \nThe Parameters and Metrics tables display the run parameters and metrics from all selected runs. The columns in these tables are identified by the Run details table immediately above. For simplicity, you can hide parameters and metrics that are identical in all selected runs by toggling .  \nCompare runs from a single experiment  \nOn the experiment page, select two or more runs by clicking in the checkbox to the left of the run, or select all runs by checking the box at the top of the column.  \nClick Compare. The Comparing <N> Runs screen appears.  \nCompare runs from multiple experiments  \nOn the experiments page, select the experiments you want to compare by clicking in the box at the left of the experiment name.  \nClick Compare (n) (n is the number of experiments you selected). A screen appears showing all of the runs from the experiments you selected.  \nSelect two or more runs by clicking in the checkbox to the left of the run, or select all runs by checking the box at the top of the column.  \nClick Compare. The Comparing <N> Runs screen appears."
    },
    {
        "id": 790,
        "url": "https://docs.databricks.com/en/mlflow/runs.html",
        "content": "Copy runs between workspaces\nCopy runs between workspaces\nTo import or export MLflow runs to or from your Databricks workspace, you can use the community-driven open source project MLflow Export-Import."
    },
    {
        "id": 791,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/create-monitor-ui.html",
        "content": "Create a monitor using the Databricks UI  \nThis article demonstrates create a data monitor using the Databricks UI. You can also use the API.  \nTo access the Databricks UI, do the following:  \nIn the workspace left sidebar, click to open Catalog Explorer.  \nNavigate to the table you want to monitor.  \nClick the Quality tab.  \nClick the Get started button.  \nIn Create monitor, choose the options you want to set up the monitor.  \nProfiling"
    },
    {
        "id": 792,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/create-monitor-ui.html",
        "content": "Profiling\nFrom the Profile type menu, select the type of monitor you want to create. The profile types are shown in the table.  \nProfile type  \nDescription  \nTime series profile  \nA table containing values measured over time. This table includes a timestamp column.  \nInference profile  \nA table containing predicted values output by a machine learning classification or regression model. This table includes a timestamp, a model id, model inputs (features), a column containing model predictions, and optional columns containing unique observation IDs and ground truth labels. It can also contain metadata, such as demographic information, that is not used as input to the model but might be useful for fairness and bias investigations or other monitoring.  \nSnapshot profile  \nAny Delta managed table, external table, view, materialized view, or streaming table.  \nIf you select TimeSeries or Inference, additional parameters are required and are described in the following sections.  \nNote  \nWhen you first create a time series or inference profile, the monitor analyzes only data from the 30 days prior to its creation. After the monitor is created, all new data is processed.  \nMonitors defined on materialized views and streaming tables do not support incremental processing.  \nTip  \nFor TimeSeries and Inference profiles, it\u2019s a best practice to enable change data feed (CDF) on your table. When CDF is enabled, only newly appended data is processed, rather than re-processing the entire table every refresh. This makes execution more efficient and reduces costs as you scale monitoring across many tables.  \nTimeSeries profile  \nFor a TimeSeries profile, you must make the following selections:  \nSpecify the Metric granularities that determine how to partition the data in windows across time.  \nSpecify the Timestamp column, the column in the table that contains the timestamp. The timestamp column data type must be either TIMESTAMP or a type that can be converted to timestamps using the to_timestamp PySpark function.  \nInference profile  \nFor a Inference profile, in addition to the granularities and the timestamp, you must make the following selections:  \nSelect the Problem type, either classification or regression.  \nSpecify the Prediction column, the column containing the model\u2019s predicted values.  \nOptionally specify the Label column, the column containing the ground truth for model predictions.  \nSpecify the Model ID column, the column containing the id of the model used for prediction."
    },
    {
        "id": 793,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/create-monitor-ui.html",
        "content": "Schedule\nSchedule\nTo set up a monitor to run on a scheduled basis, select Refresh on schedule and select the frequency and time for the monitor to run. If you do not want the monitor to run automatically, select Refresh manually. If you select Refresh manually, you can later refresh the metrics from the Quality tab.\n\nNotifications\nNotifications\nTo set up email notifications for a monitor, enter the email to be notified and select the notifications to enable. Up to 5 emails are supported per notification event type.\n\nGeneral"
    },
    {
        "id": 794,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/create-monitor-ui.html",
        "content": "General\nIn the General section, you need to specify one required setting and some additional configuration options:  \nYou must specify the Unity Catalog schema where the metric tables created by the monitor are stored. The location must be in the format {catalog}.{schema}.  \nYou can also specify the following settings:  \nAssets directory. Enter the absolute path to the existing directory to store monitoring assets such as the generated dashboard. By default, assets are stored in the default directory: \u201c/Users/{user_name}/databricks_lakehouse_monitoring/{table_name}\u201d. If you enter a different location in this field, assets are created under \u201c/{table_name}\u201d in the directory you specify. This directory can be anywhere in the workspace. For monitors intended to be shared within an organization, you can use a path in the \u201c/Shared/\u201d directory.  \nThis field cannot be left blank.  \nUnity Catalog baseline table name. Name of a table or view that contains baseline data for comparison. For more information about baseline tables, see Primary input table and baseline table.  \nMetric slicing expressions. Slicing expressions let you define subsets of the table to monitor in addition to the table as a whole. To create a slicing expression, click Add expression and enter the expression definition. For example the expression \"col_2 > 10\" generates two slices: one for col_2 > 10 and one for col_2 <= 10. As another example, the expression \"col_1\" will generate one slice for each unique value in col_1. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements."
    },
    {
        "id": 795,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/create-monitor-ui.html",
        "content": "Custom metrics. Custom metrics appear in the metric tables like any built-in metric. For details, see Use custom metrics with Databricks Lakehouse Monitoring. To configure a custom metric, click Add custom metric. - Enter a Name for the custom metric. - Select the custom metric Type, one of Aggregate, Derived, or Drift. For definitions, see Types of custom metrics. - From the drop-down list in Input columns, select the columns to apply the metric to. - In the Output type field, select the Spark data type of the metric. - In the Definition field, enter SQL code that defines the custom metric."
    },
    {
        "id": 796,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/create-monitor-ui.html",
        "content": "Edit monitor settings in the UI\nEdit monitor settings in the UI\nAfter you have created a monitor, you can make changes to the monitor\u2019s settings by clicking the Edit monitor configuration button on the Quality tab.\n\nRefresh and view monitor results in the UI\nRefresh and view monitor results in the UI\nTo run the monitor manually, click Refresh metrics.  \nFor information about the statistics that are stored in monitor metric tables, see Monitor metric tables. Metric tables are Unity Catalog tables. You can query them in notebooks or in the SQL query explorer, and view them in Catalog Explorer.\n\nControl access to monitor outputs\nControl access to monitor outputs\nThe metric tables and dashboard created by a monitor are owned by the user who created the monitor. You can use Unity Catalog privileges to control access to metric tables. To share dashboards within a workspace, click the Share button on the upper-right side of the dashboard.\n\nDelete a monitor from the UI\nDelete a monitor from the UI\nTo delete a monitor from the UI, click the kebab menu next to the Refresh metrics button and select Delete monitor."
    },
    {
        "id": 797,
        "url": "https://docs.databricks.com/en/machine-learning/train-model/xgboost-spark.html",
        "content": "Distributed training of XGBoost models using xgboost.spark  \nPreview  \nThis feature is in Public Preview.  \nThe Python package xgboost>=1.7 contains a new module xgboost.spark. This module includes the xgboost PySpark estimators xgboost.spark.SparkXGBRegressor, xgboost.spark.SparkXGBClassifier, and xgboost.spark.SparkXGBRanker. These new classes support the inclusion of XGBoost estimators in SparkML Pipelines. For API details, see the XGBoost python spark API doc.  \nRequirements\nRequirements\nDatabricks Runtime 12.0 ML and above.\n\nxgboost.spark parameters"
    },
    {
        "id": 798,
        "url": "https://docs.databricks.com/en/machine-learning/train-model/xgboost-spark.html",
        "content": "xgboost.spark parameters\nThe estimators defined in the xgboost.spark module support most of the same parameters and arguments used in standard XGBoost.  \nThe parameters for the class constructor, fit method, and predict method are largely identical to those in the xgboost.sklearn module.  \nNaming, values, and defaults are mostly identical to those described in XGBoost parameters.  \nExceptions are a few unsupported parameters (such as gpu_id, nthread, sample_weight, eval_set), and the pyspark estimator specific parameters that have been added (such as featuresCol, labelCol, use_gpu, validationIndicatorCol). For details, see XGBoost Python Spark API documentation.\n\nDistributed training"
    },
    {
        "id": 799,
        "url": "https://docs.databricks.com/en/machine-learning/train-model/xgboost-spark.html",
        "content": "Distributed training\nPySpark estimators defined in the xgboost.spark module support distributed XGBoost training using the num_workers parameter. To use distributed training, create a classifier or regressor and set num_workers to the number of concurrent running Spark tasks during distributed training. To use the all Spark task slots, set num_workers=sc.defaultParallelism.  \nFor example:  \nfrom xgboost.spark import SparkXGBClassifier classifier = SparkXGBClassifier(num_workers=sc.defaultParallelism)  \nNote  \nYou cannot use mlflow.xgboost.autolog with distributed XGBoost. To log an xgboost Spark model using MLflow, use mlflow.spark.log_model(spark_xgb_model, artifact_path).  \nYou cannot use distributed XGBoost on a cluster that has autoscaling enabled. New worker nodes that start in this elastic scaling paradigm cannot receive new sets of tasks and remain idle. For instructions to disable autoscaling, see Enable autoscaling.\n\nEnable optimization for training on sparse features dataset"
    },
    {
        "id": 800,
        "url": "https://docs.databricks.com/en/machine-learning/train-model/xgboost-spark.html",
        "content": "Enable optimization for training on sparse features dataset\nPySpark Estimators defined in xgboost.spark module support optimization for training on datasets with sparse features. To enable optimization of sparse feature sets, you need to provide a dataset to the fit method that contains a features column consisting of values of type pyspark.ml.linalg.SparseVector and set the estimator parameter enable_sparse_data_optim to True. Additionally, you need to set the missing parameter to 0.0.  \nFor example:  \nfrom xgboost.spark import SparkXGBClassifier classifier = SparkXGBClassifier(enable_sparse_data_optim=True, missing=0.0) classifier.fit(dataset_with_sparse_features_col)\n\nGPU training"
    },
    {
        "id": 801,
        "url": "https://docs.databricks.com/en/machine-learning/train-model/xgboost-spark.html",
        "content": "GPU training\nPySpark estimators defined in the xgboost.spark module support training on GPUs. Set the parameter use_gpu to True to enable GPU training.  \nNote  \nFor each Spark task used in XGBoost distributed training, only one GPU is used in training when the use_gpu argument is set to True. Databricks recommends using the default value of 1 for the Spark cluster configuration spark.task.resource.gpu.amount. Otherwise, the additional GPUs allocated to this Spark task are idle.  \nFor example:  \nfrom xgboost.spark import SparkXGBClassifier classifier = SparkXGBClassifier(num_workers=sc.defaultParallelism, use_gpu=True)\n\nTroubleshooting"
    },
    {
        "id": 802,
        "url": "https://docs.databricks.com/en/machine-learning/train-model/xgboost-spark.html",
        "content": "Troubleshooting\nDuring multi-node training, if you encounter a NCCL failure: remote process exited or there was a network error message, it typically indicates a problem with network communication among GPUs. This issue arises when NCCL (NVIDIA Collective Communications Library) cannot use certain network interfaces for GPU communication.  \nTo resolve, set the cluster\u2019s sparkConf for spark.executorEnv.NCCL_SOCKET_IFNAME to eth. This essentially sets the environment variable NCCL_SOCKET_IFNAME to eth for all of the workers in a node.\n\nExample notebook\nExample notebook\nThis notebook shows the use of the Python package xgboost.spark with Spark MLlib.  \nPySpark-XGBoost notebook  \nOpen notebook in new tab Copy link for import\n\nMigration guide for the deprecated sparkdl.xgboost module"
    },
    {
        "id": 803,
        "url": "https://docs.databricks.com/en/machine-learning/train-model/xgboost-spark.html",
        "content": "Migration guide for the deprecated sparkdl.xgboost module\nReplace from sparkdl.xgboost import XgboostRegressor with from xgboost.spark import SparkXGBRegressor and replace from sparkdl.xgboost import XgboostClassifier with from xgboost.spark import SparkXGBClassifier.  \nChange all parameter names in the estimator constructor from camelCase style to snake_case style. For example, change XgboostRegressor(featuresCol=XXX) to SparkXGBRegressor(features_col=XXX).  \nThe parameters use_external_storage and external_storage_precision have been removed. xgboost.spark estimators use the DMatrix data iteration API to use memory more efficiently. There is no longer a need to use the inefficient external storage mode. For extremely large datasets, Databricks recommends that you increase the num_workers parameter, which makes each training task partition the data into smaller, more manageable data partitions. Consider setting num_workers = sc.defaultParallelism, which sets num_workers to the total number of Spark task slots in the cluster.  \nFor estimators defined in xgboost.spark, setting num_workers=1 executes model training using a single Spark task. This utilizes the number of CPU cores specified by the Spark cluster configuration setting spark.task.cpus, which is 1 by default. To use more CPU cores to train the model, increase num_workers or spark.task.cpus. You cannot set the nthread or n_jobs parameter for estimators defined in xgboost.spark. This behavior is different from the previous behavior of estimators defined in the deprecated sparkdl.xgboost package.  \nConvert sparkdl.xgboost model into xgboost.spark model  \nsparkdl.xgboost models are saved in a different format than xgboost.spark models and have different parameter settings. Use the following utility function to convert the model:"
    },
    {
        "id": 804,
        "url": "https://docs.databricks.com/en/machine-learning/train-model/xgboost-spark.html",
        "content": "def convert_sparkdl_model_to_xgboost_spark_model( xgboost_spark_estimator_cls, sparkdl_xgboost_model, ): \"\"\" :param xgboost_spark_estimator_cls: `xgboost.spark` estimator class, e.g. `xgboost.spark.SparkXGBRegressor` :param sparkdl_xgboost_model: `sparkdl.xgboost` model instance e.g. the instance of `sparkdl.xgboost.XgboostRegressorModel` type. :return A `xgboost.spark` model instance \"\"\" def convert_param_key(key): from xgboost.spark.core import _inverse_pyspark_param_alias_map if key == \"baseMarginCol\": return \"base_margin_col\" if key in _inverse_pyspark_param_alias_map: return _inverse_pyspark_param_alias_map[key] if key in ['use_external_storage', 'external_storage_precision', 'nthread', 'n_jobs', 'base_margin_eval_set']: return None return key xgboost_spark_params_dict = {} for param in sparkdl_xgboost_model.params: if param.name == \"arbitraryParamsDict\": continue if sparkdl_xgboost_model.isDefined(param): xgboost_spark_params_dict[param.name] = sparkdl_xgboost_model.getOrDefault(param) xgboost_spark_params_dict.update(sparkdl_xgboost_model.getOrDefault(\"arbitraryParamsDict\")) xgboost_spark_params_dict = { convert_param_key(k): v for k, v in xgboost_spark_params_dict.items() if convert_param_key(k) is not None } booster ="
    },
    {
        "id": 805,
        "url": "https://docs.databricks.com/en/machine-learning/train-model/xgboost-spark.html",
        "content": "= { convert_param_key(k): v for k, v in xgboost_spark_params_dict.items() if convert_param_key(k) is not None } booster = sparkdl_xgboost_model.get_booster() booster_bytes = booster.save_raw(\"json\") booster_config = booster.save_config() estimator = xgboost_spark_estimator_cls(**xgboost_spark_params_dict) sklearn_model = estimator._convert_to_sklearn_model(booster_bytes, booster_config) return estimator._copyValues(estimator._create_pyspark_model(sklearn_model)) # Example from xgboost.spark import SparkXGBRegressor new_model = convert_sparkdl_model_to_xgboost_spark_model( xgboost_spark_estimator_cls=SparkXGBRegressor, sparkdl_xgboost_model=model, )"
    },
    {
        "id": 806,
        "url": "https://docs.databricks.com/en/machine-learning/train-model/xgboost-spark.html",
        "content": "If you have a pyspark.ml.PipelineModel model containing a sparkdl.xgboost model as the last stage, you can replace the stage of sparkdl.xgboost model with the converted xgboost.spark model.  \npipeline_model.stages[-1] = convert_sparkdl_model_to_xgboost_spark_model( xgboost_spark_estimator_cls=SparkXGBRegressor, sparkdl_xgboost_model=pipeline_model.stages[-1], )"
    },
    {
        "id": 807,
        "url": "https://docs.databricks.com/en/machine-learning/reference-solutions/images-etl-inference.html",
        "content": "Reference solution for image applications  \nLearn how to do distributed image model inference from reference solution notebooks using pandas UDF, PyTorch, and TensorFlow in a common configuration shared by many real-world image applications. This configuration assumes that you store many images in an object store and optionally have continuously arriving new images.  \nWorkflow for image model inferencing\nWorkflow for image model inferencing\nSuppose you have several trained deep learning (DL) models for image classification and object detection\u2014for example, MobileNetV2 for detecting human objects in user-uploaded photos to help protect privacy\u2014and you want to apply these DL models to the stored images.  \nYou might re-train the models and update previously computed predictions. However, it is both I/O-heavy and compute-heavy to load many images and apply DL models. Fortunately, the inference workload is embarrassingly parallel and in theory can be distributed easily. This guide walks you through a practical solution that contains two major stages:  \nETL images into a Delta table using Auto Loader  \nPerform distributed inference using pandas UDF\n\nETL images into a Delta table using Auto Loader"
    },
    {
        "id": 808,
        "url": "https://docs.databricks.com/en/machine-learning/reference-solutions/images-etl-inference.html",
        "content": "ETL images into a Delta table using Auto Loader\nFor image applications, including training and inference tasks, Databricks recommends that you ETL images into a Delta table with the Auto Loader. The Auto Loader helps data management and automatically handles continuously arriving new images.  \nETL image dataset into a Delta table notebook  \nOpen notebook in new tab Copy link for import\n\nPerform distributed inference using pandas UDF\nPerform distributed inference using pandas UDF\nThe following notebooks use PyTorch and TensorFlow tf.Keras to demonstrate the reference solution.  \nDistributed inference via Pytorch and pandas UDF notebook  \nOpen notebook in new tab Copy link for import  \nDistributed inference via Keras and pandas UDF notebook  \nOpen notebook in new tab Copy link for import\n\nLimitations: Image file sizes\nLimitations: Image file sizes\nFor large image files (average image size greater than 100 MB), Databricks recommends using the Delta table only to manage the metadata (list of file names) and loading the images from the object store using their paths when needed."
    },
    {
        "id": 809,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/index.html",
        "content": "Introduction to Databricks Lakehouse Monitoring  \nThis article describes Databricks Lakehouse Monitoring. It covers the benefits of monitoring your data and gives an overview of the components and usage of Databricks Lakehouse Monitoring.  \nDatabricks Lakehouse Monitoring lets you monitor the statistical properties and quality of the data in all of the tables in your account. You can also use it to track the performance of machine learning models and model-serving endpoints by monitoring inference tables that contain model inputs and predictions. The diagram shows the flow of data through data and ML pipelines in Databricks, and how you can use monitoring to continuously track data quality and model performance.  \nWhy use Databricks Lakehouse Monitoring?"
    },
    {
        "id": 810,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/index.html",
        "content": "Why use Databricks Lakehouse Monitoring?\nTo draw useful insights from your data, you must have confidence in the quality of your data. Monitoring your data provides quantitative measures that help you track and confirm the quality and consistency of your data over time. When you detect changes in your table\u2019s data distribution or corresponding model\u2019s performance, the tables created by Databricks Lakehouse Monitoring can capture and alert you to the change and can help you identify the cause.  \nDatabricks Lakehouse Monitoring helps you answer questions like the following:  \nWhat does data integrity look like, and how does it change over time? For example, what is the fraction of null or zero values in the current data, and has it increased?  \nWhat does the statistical distribution of the data look like, and how does it change over time? For example, what is the 90th percentile of a numerical column? Or, what is the distribution of values in a categorical column, and how does it differ from yesterday?  \nIs there drift between the current data and a known baseline, or between successive time windows of the data?  \nWhat does the statistical distribution or drift of a subset or slice of the data look like?  \nHow are ML model inputs and predictions shifting over time?  \nHow is model performance trending over time? Is model version A performing better than version B?  \nIn addition, Databricks Lakehouse Monitoring lets you control the time granularity of observations and set up custom metrics."
    },
    {
        "id": 811,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/index.html",
        "content": "Requirements\nRequirements\nThe following are required to use Databricks Lakehouse Monitoring:  \nYour workspace must be enabled for Unity Catalog and you must have access to Databricks SQL.  \nOnly Delta tables are supported for monitoring, and the table must be one of the following table types: managed tables, external tables, views, materialized views, or streaming tables.  \nMonitors created over materialized views and streaming tables do not support incremental processing.  \nNot all regions are supported. For regional support, see the column Serverless compute for notebooks and workflows in the table Features with limited regional availability.  \nNote  \nDatabricks Lakehouse Monitoring uses serverless compute for jobs. For information about tracking Lakehouse Monitoring expenses, see View Lakehouse Monitoring expenses.\n\nHow Lakehouse Monitoring works on Databricks"
    },
    {
        "id": 812,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/index.html",
        "content": "How Lakehouse Monitoring works on Databricks\nTo monitor a table in Databricks, you create a monitor attached to the table. To monitor the performance of a machine learning model, you attach the monitor to an inference table that holds the model\u2019s inputs and corresponding predictions.  \nDatabricks Lakehouse Monitoring provides the following types of analysis: time series, snapshot, and inference.  \nProfile type  \nDescription  \nTime series  \nUse for tables that contain a time series dataset based on a timestamp column. Monitoring computes data quality metrics across time-based windows of the time series.  \nInference  \nUse for tables that contain the request log for a model. Each row is a request, with columns for the timestamp , the model inputs, the corresponding prediction, and (optional) ground-truth label. Monitoring compares model performance and data quality metrics across time-based windows of the request log.  \nSnapshot  \nUse for all other types of tables. Monitoring calculates data quality metrics over all data in the table. The complete table is processed with every refresh.  \nThis section briefly describes the input tables used by Databricks Lakehouse Monitoring and the metric tables it produces. The diagram shows the relationship between the input tables, the metric tables, the monitor, and the dashboard.  \nPrimary table and baseline table  \nIn addition to the table to be monitored, called the \u201cprimary table\u201d, you can optionally specify a baseline table to use as a reference for measuring drift, or the change in values over time. A baseline table is useful when you have a sample of what you expect your data to look like. The idea is that drift is then computed relative to expected data values and distributions.  \nThe baseline table should contain a dataset that reflects the expected quality of the input data, in terms of statistical distributions, individual column distributions, missing values, and other characteristics. It should match the schema of the monitored table. The exception is the timestamp column for tables used with time series or inference profiles. If columns are missing in either the primary table or the baseline table, monitoring uses best-effort heuristics to compute the output metrics."
    },
    {
        "id": 813,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/index.html",
        "content": "For monitors that use a snapshot profile, the baseline table should contain a snapshot of the data where the distribution represents an acceptable quality standard. For example, on grade distribution data, one might set the baseline to a previous class where grades were distributed evenly.  \nFor monitors that use a time series profile, the baseline table should contain data that represents time window(s) where data distributions represent an acceptable quality standard. For example, on weather data, you might set the baseline to a week, month, or year where the temperature was close to expected normal temperatures.  \nFor monitors that use an inference profile, a good choice for a baseline is the data that was used to train or validate the model being monitored. In this way, users can be alerted when the data has drifted relative to what the model was trained and validated on. This table should contain the same feature columns as the primary table, and additionally should have the same model_id_col that was specified for the primary table\u2019s InferenceLog so that the data is aggregated consistently. Ideally, the test or validation set used to evaluate the model should be used to ensure comparable model quality metrics.  \nMetric tables and dashboard  \nA table monitor creates two metric tables and a dashboard. Metric values are computed for the entire table, and for the time windows and data subsets (or \u201cslices\u201d) that you specify when you create the monitor. In addition, for inference analysis, metrics are computed for each model ID. For more details about the metric tables, see Monitor metric tables.  \nThe profile metric table contains summary statistics. See the profile metrics table schema.  \nThe drift metrics table contains statistics related to the data\u2019s drift over time. If a baseline table is provided, drift is also monitored relative to the baseline values. See the drift metrics table schema.  \nThe metric tables are Delta tables and are stored in a Unity Catalog schema that you specify. You can view these tables using the Databricks UI, query them using Databricks SQL, and create dashboards and alerts based on them.  \nFor each monitor, Databricks automatically creates a dashboard to help you visualize and present the monitor results. The dashboard is fully customizable like any other legacy dashboard."
    },
    {
        "id": 814,
        "url": "https://docs.databricks.com/en/lakehouse-monitoring/index.html",
        "content": "Start using Lakehouse Monitoring on Databricks\nStart using Lakehouse Monitoring on Databricks\nSee the following articles to get started:  \nCreate a monitor using the Databricks UI.  \nCreate a monitor using the API.  \nUnderstand monitor metric tables.  \nWork with the monitor dashboard.  \nCreate SQL alerts based on a monitor.  \nCreate custom metrics.  \nMonitor model serving endpoints.  \nMonitor fairness and bias for classification models.  \nSee the reference material for the Databricks Lakehouse Monitoring API.  \nExample notebooks."
    },
    {
        "id": 815,
        "url": "https://docs.databricks.com/en/notebooks/execution-context.html",
        "content": "Databricks notebook execution contexts  \nWhen you attach a notebook to a cluster, Databricks creates an execution context. An execution context contains the state for a REPL environment for each supported programming language: Python, R, Scala, and SQL. When you run a cell in a notebook, the command is dispatched to the appropriate language REPL environment and run.  \nYou can also use the command execution API to create an execution context and send a command to run in the execution context. Similarly, the command is dispatched to the language REPL environment and run.  \nA cluster has a maximum number of execution contexts (145). Once the number of execution contexts has reached this threshold, you cannot attach a notebook to the cluster or create a new execution context.  \nIdle execution contexts"
    },
    {
        "id": 816,
        "url": "https://docs.databricks.com/en/notebooks/execution-context.html",
        "content": "Idle execution contexts\nAn execution context is considered idle when the last completed execution occurred past a set idle threshold. Last completed execution is the last time the notebook completed execution of commands. The idle threshold is the amount of time that must pass between the last completed execution and any attempt to automatically detach the notebook.  \nWhen a cluster has reached the maximum context limit, Databricks removes (evicts) idle execution contexts (starting with the least recently used) as needed. Even when a context is removed, the notebook using the context is still attached to the cluster and appears in the cluster\u2019s notebook list. Streaming notebooks are considered actively running, and their context is never evicted until their execution has been stopped. If an idle context is evicted, the UI displays a message indicating that the notebook using the context was detached due to being idle.  \nIf you attempt to attach a notebook to cluster that has maximum number of execution contexts and there are no idle contexts (or if auto-eviction is disabled), the UI displays a message saying that the current maximum execution contexts threshold has been reached and the notebook will remain in the detached state.  \nIf you fork a process, an idle execution context is still considered idle once execution of the request that forked the process returns. Forking separate processes is not recommended with Spark."
    },
    {
        "id": 817,
        "url": "https://docs.databricks.com/en/notebooks/execution-context.html",
        "content": "Configure context auto-eviction\nConfigure context auto-eviction\nAuto-eviction is enabled by default. To disable auto-eviction for a cluster, set the Spark property spark.databricks.chauffeur.enableIdleContextTracking false.\n\nDetermine Spark and Databricks Runtime version\nDetermine Spark and Databricks Runtime version\nTo determine the Spark version of the cluster your notebook is attached to, run:  \nspark.version  \nTo determine the Databricks Runtime version of the cluster your notebook is attached to, run:  \nspark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\")  \nNote  \nBoth this sparkVersion tag and the spark_version property required by the endpoints in the Clusters API and Jobs API refer to the Databricks Runtime version, not the Spark version."
    },
    {
        "id": 818,
        "url": "https://docs.databricks.com/en/machine-learning/load-data/index.html",
        "content": "Load data for machine learning and deep learning  \nThis section covers information about loading data specifically for ML and DL applications. For general information about loading data, see Ingest data into a Databricks lakehouse.  \nStore files for data loading and model checkpointing\nStore files for data loading and model checkpointing\nMachine learning applications may need to use shared storage for data loading and model checkpointing. This is particularly important for distributed deep learning.  \nDatabricks provides the Databricks File System (DBFS) for accessing data on a cluster using both Spark and local file APIs.\n\nLoad tabular data\nLoad tabular data\nYou can load tabular machine learning data from tables or files (for example, see Read CSV files). You can convert Apache Spark DataFrames into pandas DataFrames using the PySpark method toPandas(), and then optionally convert to NumPy format using the PySpark method to_numpy().\n\nPrepare data to fine tune large language models\nPrepare data to fine tune large language models\nYou can prepare your data for fine-tuning open source large language models with Hugging Face Transformers and Hugging Face Datasets.  \nPrepare data for fine tuning Hugging Face models"
    },
    {
        "id": 819,
        "url": "https://docs.databricks.com/en/machine-learning/load-data/index.html",
        "content": "Prepare data for distributed training\nPrepare data for distributed training\nThis section covers three methods for preparing data for distributed training: Mosaic Streaming, Petastorm, and TFRecords.  \nPrepare data for distributed training  \nMosaic Streaming (Recommended) Petastorm TFRecord"
    },
    {
        "id": 820,
        "url": "https://docs.databricks.com/en/optimizations/spark-ui-guide/jobs-timeline.html",
        "content": "Jobs timeline  \nThe jobs timeline is a great starting point for understanding your pipeline or query. It gives you an overview of what was running, how long each step took, and if there were any failures along the way.  \nHow to open the jobs timeline\nHow to open the jobs timeline\nIn the Spark UI, click on Jobs and Event Timeline as highlighted in red in the following screenshot. You will see the timeline. This example shows the driver and executor 0 being added:\n\nWhat to look for"
    },
    {
        "id": 821,
        "url": "https://docs.databricks.com/en/optimizations/spark-ui-guide/jobs-timeline.html",
        "content": "What to look for\nThe sections below explain how to read the event timeline to discover the possible cause of your performance or cost issue. If you notice any of these trends in your timeline, the end of each corresponding section contains a link to an article that provides guidance.  \nFailing jobs or failing executors  \nHere\u2019s an example of a failed job and removed executors, indicated by a red status, in the event timeline.  \nIf you see failing jobs or failing executors, see Failing jobs or executors removed.  \nGaps in execution  \nLook for gaps of a minute or more, such as in this example:  \nThis example has several gaps, a few of which are highlighted by the red arrows. If you see gaps in your timeline, are they a minute or more? Short gaps are to be expected as the driver coordinates work. If you do have longer gaps, are they in the middle of a pipeline? Or is this cluster constantly running and so the gaps are explained by pauses in activity? You might be able to determine this based on what time your workload started and ended.  \nIf you see long unexplained gaps in the middle of a pipeline, see Gaps between Spark jobs.  \nLong jobs  \nIs the timeline dominated by one or a few long jobs? These long jobs would be something to investigate. In the following example, the workload has one job that\u2019s much longer than the others. This is a good target for investigation.  \nClick on the longest job to dig in. For information about investigating this long stage, see Diagnosing a long stage in Spark.  \nMany small jobs  \nWhat we\u2019re looking for here is a timeline dominated by tiny jobs. It might look something like this:  \nNotice all the tiny blue lines. Each of those is a small job that took a few seconds or less.  \nIf your timeline is mostly small jobs, see Many small Spark jobs.  \nNone of the above  \nIf your timeline doesn\u2019t look like any of the above, the next step is to identify the longest job. Sort the jobs by duration and click on the link in the description for the longest job:  \nOnce you\u2019re in the page for the longest job, additional information about investigating this long stage is in Diagnosing a long stage in Spark."
    },
    {
        "id": 822,
        "url": "https://docs.databricks.com/en/notebooks/notebook-ui.html",
        "content": "Databricks notebook interface and controls  \nLearn how to use the Databricks notebook toolbar and menus to control the notebook and cell display settings.  \nNotebook cells\nNotebook cells\nNotebooks contain a collection of two types of cells: code cells and Markdown cells. Code cells contain runnable code, while Markdown cells contain Markdown code that is renders text and graphics. Markdown can be used to document or illustrate your code. You can add or remove cells to your notebook to structure your work.  \nYou can run a single cell, a group of cells, or the whole notebook at once. A notebook cell can contain at most 10MB, and its output is limited to 20 MB.\n\nNotebook toolbar icons and buttons"
    },
    {
        "id": 823,
        "url": "https://docs.databricks.com/en/notebooks/notebook-ui.html",
        "content": "Notebook toolbar icons and buttons\nThe toolbar includes menus and icons that you can use to manage and edit the notebook.  \nNext to the notebook name are buttons that let you change the default language of the notebook and, if the notebook is included in a Databricks Git folder, open the Git dialog.  \nTo view previous versions of the notebook, click the \u201cLast edit\u2026\u201d message to the right of the menus.  \nThe icons and buttons at the right of the toolbar are described in the following table:  \nIcon  \nDescription  \nRun all cells or stop execution. The name of this button changes depending on the state of the notebook.  \nOpen compute selector. When the notebook is connected to a cluster or SQL warehouse, this button shows the name of the compute resource.  \nOpen job scheduler.  \nOpen Delta Live Tables. This button appears only if the notebook is part of a Delta Live Tables pipeline.  \nOpen permissions dialog.\n\nRight sidebar actions"
    },
    {
        "id": 824,
        "url": "https://docs.databricks.com/en/notebooks/notebook-ui.html",
        "content": "Right sidebar actions\nSeveral actions are available from the notebook\u2019s right sidebar, as described in the following table:  \nIcon  \nDescription  \nOpen notebook comments.  \nOpen MLflow notebook experiment.  \nOpen notebook version history.  \nOpen variable explorer. (Available for Python variables with Databricks Runtime 12.2 LTS and above.)  \nOpen the Python environment panel. This panel shows all Python libraries available to the notebook, including notebook-scoped libraries, cluster libraries, and libraries included in the Databricks Runtime. Available only when the notebook is attached to a cluster.\n\nCreate cells\nCreate cells\nNotebooks have two types of cells: code and Markdown. The contents of Markdown cells are rendered into HTML. For example, this snippet contains markup for a level-one heading:  \n%md ### Libraries Import the necessary libraries.  \nRenders as shown:  \nTo create a new cell, hover over a cell at the top or bottom. Click Code or Text to create a code or Markdown cell, respectively.\n\nCell actions"
    },
    {
        "id": 825,
        "url": "https://docs.databricks.com/en/notebooks/notebook-ui.html",
        "content": "Cell actions\nThe cell actions menu lets you run, cut, and copy cells, move cells around in the notebook, and hide code or results.  \nRun a cell  \nTo run code cells, click the down arrow at the upper-left of the code cell.  \nAfter a cell has been run, a notice appears to the right of the cell run menu, showing the last time the cell was run and its duration. Hover your cursor over the notice for more details.  \nTo show detailed run information by default, click your profile image at the top right. Then click Settings > Developer > Show detailed command run information.  \nIf you have a tabular result output, you can also access this information by hovering over the \u201cLast refreshed\u201d section of the UI.  \nCell icons  \nThe following screenshot describes the icons that appear at the upper-right of a notebook cell:  \nLanguage selector: Select the language for the cell.  \nDatabricks Assistant: Enable or disable Databricks Assistant for code suggestions in the cell.  \nCell focus: Enlarge the cell to make editing easier.  \nDisplay cell actions menu: Open the cell actions menu. The options in this menu are slightly different for code and Markdown cells.  \nDelete a cell  \nTo delete a cell, click the trash icon to the right of the cell. This icon only appears when you hover your cursor over the cell.  \nMove a cell  \nTo move a cell up or down, click and hold the drag handle icon to the left of the cell.  \nYou can also select Move up or Move down from the cell actions menu.  \nCut, copy, and paste cells  \nThere are several options for cutting and copying cells. However, if you are using the Safari browser, only the keyboard shortcuts are available.  \nFrom the cell actions menu select Cut cell or Copy cell.  \nUse keyboard shortcuts: Command-X or Ctrl-X to cut and Command-C or Ctrl-C to copy.  \nUse the Edit menu at the top of the notebook. Select Cut or Copy.  \nAfter you cut or copy cells, you can paste those cells elsewhere in the notebook, into a different notebook, or a notebook in a different browser tab or window. To paste cells, use the keyboard shortcut Command-V or Ctrl-V. The cells are pasted below the current cell."
    },
    {
        "id": 826,
        "url": "https://docs.databricks.com/en/notebooks/notebook-ui.html",
        "content": "To undo cut or paste actions, you can use the keyboard shortcut Command-Z or Ctrl-Z or the menu options Edit > Undo cut cells or Edit > Undo paste cells.  \nTo select adjacent cells, click in a Markdown cell and then use Shift + Up or Down to select the cells above or below it. Use the edit menu to copy, cut, paste, or delete the selected cells as a group. To select all cells, select Edit > Select all cells or use the command mode shortcut Cmd+A.  \nOpen Databricks Assistant  \nTo open a Databricks Assistant text box in a cell, click the Databricks Assistant icon in the upper-right corner of the cell.  \nYou can use it in a code cell to get help or suggestions for your code. Databricks Assistant must be enabled in your workspace.  \nAdd code comments  \nTo add a comment to code in a cell, highlight the code. To the right of the cell, a comment icon appears. Click the icon to open the comment box.  \nAdd a cell to a dashboard  \nTo add a Markdown cell or a cell that has tabular results to a dashboard, select Add to dashboard from the cell actions menu. For more information, see Dashboards in notebooks.  \nLink to a cell  \nTo get a URL link to a specific command in your notebook, click to open focus mode and copy the URL from the browser address bar. To exit focus mode, click ."
    },
    {
        "id": 827,
        "url": "https://docs.databricks.com/en/notebooks/notebook-ui.html",
        "content": "Notebook table of contents\nNotebook table of contents\nTo display an automatically generated table of contents, click the icon at the upper left of the notebook (between the left sidebar and the topmost cell). The table of contents is generated from the Markdown headings used in the notebook. Cells with titles also appear in the table of contents.\n\nAdd a cell title\nAdd a cell title\nTo add a title to a cell, do one of the following:  \nClick the cell number at the center of the top of the cell and type the title.  \nSelect Add title from the cell actions menu.  \nCells with titles appear in the notebook\u2019s table of contents.  \nPromoted cell titles  \nTo make cell titles more visible in the UI, users can enable Show promoted cell titles.  \nIn the upper-right corner of any page, click your profile photo, then click Settings.  \nClick Developer > Enable Show promoted cell titles.\n\nHide and show cell content"
    },
    {
        "id": 828,
        "url": "https://docs.databricks.com/en/notebooks/notebook-ui.html",
        "content": "Hide and show cell content\nCell content consists of cell code and the results of running the cell. To hide cell code or results, click the kebab menu at the upper-right of the cell and select Hide code or Hide result.  \nYou can also select Collapse cell to display only the first line of a cell. To expand a collapsed cell, select Expand cell.  \nTo show hidden cells, click the show icon: .\n\nCollapsible headings\nCollapsible headings\nCells that appear after cells containing Markdown headings can be collapsed into the heading cell. To expand or collapse cells after cells containing Markdown headings throughout the notebook, select Collapse all headings from the View menu. The rest of this section describes how to expand or collapse a subset of cells.  \nExpand and collapse headings  \nThe image shows a level-two heading MLflow setup with the following two cells collapsed into it.  \nTo expand and collapse headings, hover your cursor over the Markdown cell. Click the arrow that appears to the left of the cell.\n\nCell display options"
    },
    {
        "id": 829,
        "url": "https://docs.databricks.com/en/notebooks/notebook-ui.html",
        "content": "Cell display options\nThere are three display options for notebooks. Use the View menu to change the display option.  \nStandard view: results are displayed immediately after code cells.  \nResults only: only results are displayed.  \nSide-by-side: code and results cells are displayed side by side.  \nActions are available from icons in the cell gutter (the area to the right and left of the cell). For example, use the grip dots to move a cell up or down!move cell icon in the left gutter. To delete a cell, use the trash can icon in the right gutter.  \nFor easier editing, click the focus mode icon to display the cell at full width. To exit focus mode, click . You can also enlarge the displayed width of a cell by turning off View > Centered layout.  \nTo automatically format all cells in the notebook to industry standard line lengths and spacing, select Edit > Format notebook.\n\nRemove cell margins\nRemove cell margins\nYou can expand or minimize margins by clicking View > Notebook layout in the notebook menu.\n\nLine and command numbers"
    },
    {
        "id": 830,
        "url": "https://docs.databricks.com/en/notebooks/notebook-ui.html",
        "content": "Line and command numbers\nTo show or hide line numbers or command numbers, select Line numbers or Command numbers from the View menu. For line numbers, you can also use the keyboard shortcut Control+L. When a cell is in an error state, line numbers are displayed regardless of the selection.  \nIf you enable line or command numbers, Databricks saves your preference and shows them in all of your other notebooks for that browser.\n\nLine wrapping\nLine wrapping\nYou can enable or disable line wrapping in notebook cells, allowing text to either wrap onto multiple lines or remain on a single line with horizontal scrolling.  \nIn the upper-right corner of any page, click your profile photo, then click Settings.  \nClick Developer settings > Wrap lines.\n\nView notebooks in dark mode\nView notebooks in dark mode\nYou can choose to display notebooks in dark mode. To turn dark mode on or off, select View > Theme and select Light theme or Dark theme.  \nble line or command numbers, Databricks saves your preference and shows them in your other notebooks for that browser.\n\nBrowse data"
    },
    {
        "id": 831,
        "url": "https://docs.databricks.com/en/notebooks/notebook-ui.html",
        "content": "Browse data\nPreview  \nThis feature is in Public Preview.  \nTo explore tables and volumes available to use in the notebook, click on the left side of the notebook to open the schema browser. See Browse data for more details.\n\nCompute resources for notebooks"
    },
    {
        "id": 832,
        "url": "https://docs.databricks.com/en/notebooks/notebook-ui.html",
        "content": "Compute resources for notebooks\nThis section covers the options for notebook compute resources. You can run a notebook on a Databricks cluster, or, for SQL commands, you can use a SQL warehouse, a type of compute-optimized for SQL analytics.  \nServerless compute for notebooks  \nServerless compute allows you to quickly connect your notebook to on-demand computing resources.  \nTo attach to the serverless compute, click the Connect drop-down menu in the notebook and select Serverless.  \nSee Serverless compute for notebooks for more information.  \nAttach a notebook to a cluster  \nTo attach a notebook to a cluster, you need the CAN ATTACH TO cluster-level permission.  \nImportant  \nAs long as a notebook is attached to a cluster, any user with the CAN RUN permission on the notebook has implicit permission to access the cluster.  \nTo attach a notebook to a cluster, click the compute selector in the notebook toolbar and select a cluster from the dropdown menu.  \nThe menu shows a selection of clusters you have used recently or are currently running.  \nTo select from all available clusters, click More\u2026. Click the cluster name to display a dropdown menu, and select an existing cluster.  \nYou can also create a new cluster by selecting Create new resource\u2026 from the dropdown menu.  \nImportant  \nAn attached notebook has the following Apache Spark variables defined.  \nClass  \nVariable Name  \nSparkContext  \nsc  \nSQLContext/HiveContext  \nsqlContext  \nSparkSession (Spark 2.x)  \nspark  \nDo not create a SparkSession, SparkContext, or SQLContext. Doing so will lead to inconsistent behavior.  \nUse a notebook with a SQL warehouse  \nWhen a notebook is attached to a SQL warehouse, you can run SQL and Markdown cells. Running a cell in any other language (such as Python or R) throws an error. SQL cells executed on a SQL warehouse appear in the SQL warehouse\u2019s query history. The user who ran a query can view the query profile from the notebook by clicking the elapsed time at the bottom of the output.  \nRunning a notebook requires a Pro or Serverless SQL warehouse. You must have access to the workspace and the SQL warehouse.  \nTo attach a notebook to a SQL warehouse do the following:"
    },
    {
        "id": 833,
        "url": "https://docs.databricks.com/en/notebooks/notebook-ui.html",
        "content": "Running a notebook requires a Pro or Serverless SQL warehouse. You must have access to the workspace and the SQL warehouse.  \nTo attach a notebook to a SQL warehouse do the following:  \nClick the compute selector in the notebook toolbar. The dropdown menu shows compute resources that are currently running or that you have used recently. SQL warehouses are marked with .  \nFrom the menu, select a SQL warehouse.  \nTo see all available SQL warehouses, select More\u2026 from the dropdown menu. A dialog appears showing compute resources available for the notebook. Select SQL Warehouse, choose the warehouse you want to use, and click Attach.  \nYou can also select a SQL warehouse as the compute resource for a SQL notebook when you create a workflow or scheduled job.  \nSQL warehouse limitations  \nSee Known limitations Databricks notebooks for more information.  \nDetach a notebook  \nTo detach a notebook from a compute resource, click the compute selector in the notebook toolbar and hover over the attached cluster or SQL warehouse in the list to display a side menu. From the side menu, select Detach.  \nYou can also detach notebooks from a cluster using the Notebooks tab on the cluster details page.  \nWhen you detach a notebook, the execution context is removed, and all computed variable values are cleared from the notebook.  \nTip  \nDatabricks recommends that you detach unused notebooks from clusters. This frees up memory space on the driver."
    },
    {
        "id": 834,
        "url": "https://docs.databricks.com/en/notebooks/notebook-ui.html",
        "content": "Use web terminal and Databricks CLI\nUse web terminal and Databricks CLI\nTo open the web terminal in a notebook, click at the bottom of the right sidebar.  \nUse Databricks CLI in a web terminal  \nStarting with Databricks Runtime 15.0, you can use the Databricks CLI from the web terminal in the notebook.  \nRequirements  \nThe notebook must be attached to a cluster in Single user or No isolation shared access mode.  \nThe CLI is not available in workspaces enabled for PrivateLink.  \nThe installed CLI is always the latest version. Authentication is based on the current user.  \nYou cannot use the CLI from a notebook cell. In a notebook, commands like %sh databricks ... do not work with Databricks Runtime 15.0 or above."
    },
    {
        "id": 835,
        "url": "https://docs.databricks.com/en/machine-learning/model-inference/resnet-model-inference-keras.html",
        "content": "Model inference using TensorFlow Keras API  \nThe following notebook demonstrates the Databricks recommended deep learning inference workflow. This example illustrates model inference using a ResNet-50 model trained with TensorFlow Keras API and Parquet files as input data.  \nTo understand the example, be familiar with Spark data sources.  \nModel inference TensorFlow Keras API notebook\nModel inference TensorFlow Keras API notebook\nOpen notebook in new tab Copy link for import"
    },
    {
        "id": 836,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/cost-optimization/index.html",
        "content": "Cost optimization for the data lakehouse  \nThis article covers architectural principles of the cost optimization pillar, aimed at enabling cost management in a way that maximizes the value delivered. Given a budget, cost efficiency is driven by business objectives and return on investment. Cost optimization principles can help achieve both business objectives and cost justification.  \nPrinciples of cost optimization"
    },
    {
        "id": 837,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/cost-optimization/index.html",
        "content": "Principles of cost optimization\nChoose optimal resources  \nChoose the right resources that align with business goals and can handle workload performance. When onboarding new workloads, explore the different deployment options and choose the one with the best price/performance ratio.  \nDynamically allocate resources  \nDynamically allocate and release resources to match performance requirements. Identify unused or underutilized resources and reconfigure, consolidate, or turn them off.  \nMonitor and control cost  \nThe cost of your workloads depends on the amount of resources consumed and the rates charged for those resources. To understand the cost of these workloads, monitor them for each resource involved. This provides a baseline for controlling consumption and costs.  \nIn addtion, the lakehouse makes it easy to identify workload usage and costs accurately. This enables the transparent allocation of costs to individual workload owners. They can then measure return on investment and optimize their resources to reduce costs if necessary.  \nDesign cost-effective workloads  \nA key advantage of the lakehouse is its ability to scale dynamically. As a starting point, usage and performance metrics are analyzed to determine the initial number of instances. With auto-scaling, additional costs can be saved by choosing smaller instances for a highly variable workload, or by scaling out rather than up to achieve the required level of performance."
    },
    {
        "id": 838,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/cost-optimization/index.html",
        "content": "Next: Best practices for cost optimization\nNext: Best practices for cost optimization\nSee Best practices for cost optimization."
    },
    {
        "id": 839,
        "url": "https://docs.databricks.com/en/mlflow/build-dashboards.html",
        "content": "Build dashboards with the MLflow Search API  \nYou can pull aggregate metrics on your MLflow runs using the mlflow.search_runs API and display them in a dashboard. Regularly such reviewing metrics can provide insight into your progress and productivity. For example, you can track improvement of a goal metric like revenue or accuracy over time, across many runs and/or experiments.  \nThis notebook demonstrates how to build the following custom dashboard using the mlflow.search_runs API:  \nYou can either run the notebook on your own experiments or against autogenerated mock experiment data.  \nDashboard comparing MLflow runs notebook\nDashboard comparing MLflow runs notebook\nOpen notebook in new tab Copy link for import"
    },
    {
        "id": 840,
        "url": "https://docs.databricks.com/en/machine-learning/ray/start-ray.html",
        "content": "Start a Ray cluster on Databricks  \nDatabricks simplifies the process of starting a Ray cluster by handling cluster and job configuration the same way it does with any Apache Spark job. This is because the Ray cluster is actually started on top of the managed Apache Spark cluster.  \nRun Ray on a local machine\nRun Ray on a local machine\nimport ray ray.init()\n\nRun Ray on Databricks"
    },
    {
        "id": 841,
        "url": "https://docs.databricks.com/en/machine-learning/ray/start-ray.html",
        "content": "Run Ray on Databricks\nfrom ray.util.spark import setup_ray_cluster import ray # If the cluster has four workers with 8 CPUs each as an example setup_ray_cluster(num_worker_nodes=4, num_cpus_per_worker=8) # Pass any custom configuration to ray.init ray.init(ignore_reinit_error=True)  \nThis approach works at any cluster scale, from a few to hundreds of nodes. Ray clusters on Databricks also support autoscaling.  \nAfter creating the Ray cluster, you can run any Ray application code in a Databricks notebook.  \nImportant  \nDatabricks recommends installing any necessary libraries for your application with %pip install <your-library-dependency> to ensure they are available to your Ray cluster and application accordingly. Specifying dependencies in the Ray init function call installs the dependencies in a location inaccessible to the Apache Spark worker nodes, which results in version incompatibilities and import errors.  \nFor example, you can run a simple Ray application in a Databricks notebook as follows:  \nimport ray import random import time from fractions import Fraction ray.init() @ray.remote def pi4_sample(sample_count): \"\"\"pi4_sample runs sample_count experiments, and returns the fraction of time it was inside the circle. \"\"\" in_count = 0 for i in range(sample_count): x = random.random() y = random.random() if x*x + y*y <= 1: in_count += 1 return Fraction(in_count, sample_count) SAMPLE_COUNT = 1000 * 1000 start = time.time() future = pi4_sample.remote(sample_count=SAMPLE_COUNT) pi4 = ray.get(future) end = time.time() dur = end - start print(f'Running {SAMPLE_COUNT} tests took {dur} seconds') pi = pi4 * 4 print(float(pi))"
    },
    {
        "id": 842,
        "url": "https://docs.databricks.com/en/machine-learning/ray/start-ray.html",
        "content": "Shut down a Ray cluster\nShut down a Ray cluster\nRay clusters automatically shut down under the following circumstances:  \nYou detach your interactive notebook from your Databricks cluster.  \nYour Databricks job is completed.  \nYour Databricks cluster is restarted or terminated.  \nThere\u2019s no activity for the specified idle time.  \nTo shut down a Ray cluster running on Databricks, you can call the ray.utils.spark.shutdown_ray_cluster API.  \nfrom ray.utils.spark import shutdown_ray_cluster import ray shutdown_ray_cluster() ray.shutdown()\n\nNext steps\nNext steps\nScale Ray clusters on Databricks"
    },
    {
        "id": 843,
        "url": "https://docs.databricks.com/en/machine-learning/train-model/tensorflow.html",
        "content": "TensorFlow  \nTensorFlow is an open-source framework for machine learning created by Google. It supports deep-learning and general numerical computations on CPUs, GPUs, and clusters of GPUs. It is subject to the terms and conditions of the Apache License 2.0.  \nDatabricks Runtime ML includes TensorFlow and TensorBoard, so you can use these libraries without installing any packages. For the version of TensorFlow installed in the Databricks Runtime ML version that you are using, see the release notes.  \nNote  \nThis guide is not a comprehensive guide on TensorFlow. See the TensorFlow website.  \nSingle node and distributed training\nSingle node and distributed training\nTo test and migrate single-machine workflows, use a Single Node cluster.  \nFor distributed training options for deep learning, see Distributed training.\n\nTensorflow example notebook\nTensorflow example notebook\nThe following notebook shows how you can run TensorFlow (1.x and 2.x) with TensorBoard monitoring on a Single Node cluster.  \nTensorFlow 1.15/2.x notebook  \nOpen notebook in new tab Copy link for import\n\nTensorFlow Keras example notebook"
    },
    {
        "id": 844,
        "url": "https://docs.databricks.com/en/machine-learning/train-model/tensorflow.html",
        "content": "TensorFlow Keras example notebook\nTensorFlow Keras is a deep learning API written in Python that runs on top of the machine learning platform TensorFlow. The 10-minute tutorial notebook shows an example of training machine learning models on tabular data with TensorFlow Keras, including using inline TensorBoard.  \nGet started with TensorFlow Keras notebook  \nOpen notebook in new tab Copy link for import"
    },
    {
        "id": 845,
        "url": "https://docs.databricks.com/en/optimizations/dynamic-file-pruning.html",
        "content": "Dynamic file pruning  \nDynamic file pruning, can significantly improve the performance of many queries on Delta Lake tables. Dynamic file pruning triggers for queries that contain filter statements or WHERE clauses. You must use Photon-enabled compute to use dynamic file pruning in MERGE, UPDATE, and DELETE statements. Only SELECT statements leverage dynamic file pruning when Photon is not used.  \nDynamic file pruning is especially efficient for non-partitioned tables, or for joins on non-partitioned columns. The performance impact of dynamic file pruning is often correlated to the clustering of data so consider using Z-Ordering to maximize the benefit.  \nFor background and use cases for dynamic file pruning, see Faster SQL queries on Delta Lake with dynamic file pruning.  \nConfiguration"
    },
    {
        "id": 846,
        "url": "https://docs.databricks.com/en/optimizations/dynamic-file-pruning.html",
        "content": "Configuration\nDynamic file pruning is controlled by the following Apache Spark configuration options:  \nspark.databricks.optimizer.dynamicFilePruning (default is true): The main flag that directs the optimizer to push down filters. When set to false, dynamic file pruning will not be in effect.  \nspark.databricks.optimizer.deltaTableSizeThreshold (default is 10,000,000,000 bytes (10 GB)): Represents the minimum size (in bytes) of the Delta table on the probe side of the join required to trigger dynamic file pruning. If the probe side is not very large, it is probably not worthwhile to push down the filters and we can just simply scan the whole table. You can find the size of a Delta table by running the DESCRIBE DETAIL table_name command and then looking at the sizeInBytes column.  \nspark.databricks.optimizer.deltaTableFilesThreshold (default is 10): Represents the number of files of the Delta table on the probe side of the join required to trigger dynamic file pruning. When the probe side table contains fewer files than the threshold value, dynamic file pruning is not triggered. If a table has only a few files, it is probably not worthwhile to enable dynamic file pruning. You can find the size of a Delta table by running the DESCRIBE DETAIL table_name command and then looking at the numFiles column."
    },
    {
        "id": 847,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/scope.html",
        "content": "The scope of the lakehouse platform  \nA modern data and AI platform framework\nA modern data and AI platform framework\nTo discuss the scope of the Databricks Data intelligence Platform, it is helpful to first define a basic framework for the modern data and AI platform:\n\nOverview of the lakehouse scope\nOverview of the lakehouse scope\nThe Databricks Data Intelligence Platform covers the complete modern data platform framework. It is built on the lakehouse architecture and powered by a data intelligence engine that understands the unique qualities of your data. It is an open and unified foundation for ETL, ML/AI, and DWH/BI workloads, and has Unity Catalog as the central data and AI governance solution.\n\nPersonas of the platform framework"
    },
    {
        "id": 848,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/scope.html",
        "content": "Personas of the platform framework\nThe framework covers the primary data team members (personas) working with the applications in the framework:  \nData engineers provide data scientists and business analysts with accurate and reproducible data for timely decision-making and real-time insights. They implement highly consistent and reliable ETL processes to increase user confidence and trust in data. They ensure that data is well integrated with the various pillars of the business and typically follow software engineering best practices.  \nData scientists blend analytical expertise and business understanding to transform data into strategic insights and predictive models. They are adept at translating business challenges into data-driven solutions, be that through retrospective analytical insights or forward-looking predictive modeling. Leveraging data modeling and machine learning techniques, they design, develop, and deploy models that unveil patterns, trends, and forecasts from data. They act as a bridge, converting complex data narratives into comprehensible stories, ensuring business stakeholders not only understand but can also act upon the data-driven recommendations, in turn driving a data-centric approach to problem-solving within an organization.  \nML engineers (machine learning engineers) lead the practical application of data science in products and solutions by building, deploying, and maintaining machine learning models. Their primary focus pivots towards the engineering aspect of model development and deployment. ML Engineers ensure the robustness, reliability, and scalability of machine learning systems in live environments, addressing challenges related to data quality, infrastructure, and performance. By integrating AI and ML models into operational business processes and user-facing products, they facilitate the utilization of data science in solving business challenges, ensuring models don\u2019t just stay in research but drive tangible business value.  \nBusiness analysts empower stakeholders and business teams with actionable data. They often interpret data and create reports or other documentation for leadership using standard BI tools. They are typically the go-to point of contact for non-technical business and operations colleagues for quick analysis questions.  \nBusiness partners are important stakeholders in an increasingly networked business world. They are defined as a company or individuals with whom a business has a formal relationship to achieve a common goal, and can include vendors, suppliers, distributors, and other third-party partners. Data sharing is an important aspect of business partnerships, as it enables the transfer and exchange of data to enhance collaboration and data-driven decision-making."
    },
    {
        "id": 849,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/scope.html",
        "content": "Domains of the platform framework\nDomains of the platform framework\nThe platform consists of multiple domains:  \nStorage: In the cloud, data is mainly stored in scalable, efficient, and resilient object storage on cloud providers.  \nGovernance: Capabilities around data governance, such as access control, auditing, metadata management, lineage tracking, and monitoring for all data and AI assets.  \nAI engine: The AI engine provides generative AI capabilities for the whole platform.  \nIngest & transform: The capabilities for ETL workloads.  \nAdvanced analytics, ML, and AI: All capabilities around machine learning, AI, Generative AI, and also streaming analytics.  \nData warehouse: The domain supporting DWH and BI use cases.  \nOrchestration: Central workflow management of data processing, machine learning, and analytics pipelines.  \nETL & DS tools: The front-end tools that data engineers, data scientists and ML engineers primarily use for work.  \nBI tools: The front-end tools that BI analysts primarily use for work.  \nCollaboration: Capabilities for data sharing between two or more parties.\n\nThe scope of the Databricks Platform"
    },
    {
        "id": 850,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/scope.html",
        "content": "The scope of the Databricks Platform\nThe Databricks Data Intelligence Platform and its components can be mapped to the framework in the following way:  \nDownload: Scope of the lakehouse - Databricks components  \nData workloads on Databricks  \nMost importantly, the Databricks Data Intelligence Platform covers all relevant workloads for the data domain in one platform, with Apache Spark/Photon as the engine:  \nIngest & transform  \nFor data ingestion, Auto Loader incrementally and automatically processes files landing in cloud storage in scheduled or continuous jobs - without the need to manage state information. Once ingested, raw data needs to be transformed so it\u2019s ready for BI and ML/AI. Databricks provides powerful ETL capabilities for data engineers, data scientists, and analysts.  \nDelta Live Tables (DLT) allows ETL jobs to be written in a declarative way, simplifying the entire implementation process. Data quality can be improved by defining data expectations.  \nAdvanced analytics, ML, and AI  \nThe platform includes Databricks Mosaic AI, a set of fully integrated machine learning and AI tools for classic machine and deep learning as well as generative AI and large language models (LLMs). It covers the entire workflow from preparing data to building machine learning and deep learning models, to Mosaic AI Model Serving.  \nSpark Structured Streaming and DLT enable real-time analytics.  \nData warehouse  \nThe Databricks Data Intelligence Platform also has a complete data warehouse solution with Databricks SQL, centrally governed by Unity Catalog with fine-grained access control.  \nOutline of Databricks feature areas  \nThis is a mapping of the Databricks Data Intelligence Platform features to the other layers of the framework, from bottom to top:  \nCloud storage  \nAll data for the lakehouse is stored in the cloud provider\u2019s object storage. Databricks supports three cloud providers: AWS, Azure, and GCP. Files in various structured and semi-structured formats (for example, Parquet, CSV, JSON, and Avro) as well as unstructured formats (such as images and documents) are ingested and transformed using either batch or streaming processes.  \nDelta Lake is the recommended data format for the lakehouse (file transactions, reliability, consistency, updates, and so on) and is completely open source to avoid lock-in. And Delta Universal Format (UniForm) allows you to read Delta tables with Iceberg reader clients."
    },
    {
        "id": 851,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/scope.html",
        "content": "No proprietary data formats are used in the Databricks Data Intelligence Platform.  \nData governance  \nOn top of the storage layer, Unity Catalog offers a wide range of data governance capabilities, including metadata management in the metastore, access control, auditing, data discovery, data lineage.  \nLakehouse monitoring provides out-of-the-box quality metrics for data and AI assets, and auto-generated dashboards to visualize these metrics.  \nExternal SQL sources can be integrated into the lakehouse and Unity Catalog through lakehouse federation.  \nAI engine  \nThe Data Intelligence Platform is built on the lakehouse architecture and enhanced by the data intelligence engine DatabricksIQ. DatabricksIQ combines generative AI with the unification benefits of the lakehouse architecture to understand the unique semantics of your data. Intelligent Search and the Databricks Assistant are examples of AI powered services that simplify working with the platform for every user.  \nOrchestration  \nDatabricks Jobs enable you to run diverse workloads for the full data and AI lifecycle on any cloud. They allow you to orchestrate jobs as well as Delta Live Tables for SQL, Spark, notebooks, DBT, ML models, and more.  \nETL & DS tools  \nAt the consumption layer, data engineers and ML engineers typically work with the platform using IDEs. Data scientists often prefer notebooks and use the ML & AI runtimes, and the machine learning workflow system MLflow to track experiments and manage the model lifecycle.  \nBI tools  \nBusiness analysts typically use their preferred BI tool to access the Databricks data warehouse. Databricks SQL can be queried by different Analysis and BI tools, see BI and visualization  \nIn addition, the platform offers query and analysis tools out of the box:  \nDashboards to drag-and-drop data visualizations and share insights.  \nSQL editor for SQL analysts to analyze data.  \nCollaboration  \nDelta Sharing is an open protocol developed by Databricks for secure data sharing with other organizations regardless of the computing platforms they use.  \nDatabricks Marketplace is an open forum for exchanging data products. It takes advantage of Delta Sharing to give data providers the tools to share data products securely and data consumers the power to explore and expand their access to the data and data services they need."
    },
    {
        "id": 852,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/data-governance/best-practices.html",
        "content": "Best practices for data and AI governance  \nThis article covers best practices of data and AI governance, organized by architectural principles listed in the following sections.  \n1. Unify data and AI management"
    },
    {
        "id": 853,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/data-governance/best-practices.html",
        "content": "1. Unify data and AI management\nEstablish a data and AI governance process  \nData and AI governance is the management of the availability, usability, integrity, and security of an organization\u2019s data and AI assets. By strengthening data and AI governance, organizations can ensure the quality of the assets that are critical for accurate analytics and decision-making, help to identify new opportunities, improve customer satisfaction, and ultimately increase revenue. It helps organizations comply with data and AI privacy regulations and improve security measures, reducing the risk of data breaches and penalties. Effective data governance also eliminates redundancies and streamlines data management, resulting in cost savings and increased operational efficiency.  \nAn organization might want to choose which governance model suits them best:  \nIn the centralized governance model, your governance administrators are owners of the metastore and can take ownership of any object and grant and revoke permissions.  \nIn a distributed governance model, the catalog or a set of catalogs is the data domain. The owner of that catalog can create and own all assets and manage governance within that domain. The owners of any given domain can operate independently of the owners of other domains.  \nThe data and AI governance solution Unity Catalog is integrated into the Databricks Data Intelligence Platform. It supports both governance models and helps to seamlessly manage structured and unstructured data, ML models, notebooks, dashboards, and files on any cloud or platform. The Unity Catalog best practices help to implement data and AI governance.  \nManage metadata for all data and AI assets in one place  \nThe benefits of managing metadata for all assets in one place are similar to the benefits of maintaining a single source of truth for all your data. These include reduced data redundancy, increased data integrity, and the elimination of misunderstandings due to different definitions or taxonomies. It\u2019s also easier to implement global policies, standards, and rules with a single source."
    },
    {
        "id": 854,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/data-governance/best-practices.html",
        "content": "As a best practice, run the lakehouse in a single account with a Unity Catalog. The Unity Catalog can manage data and volumes (arbitrary files), as well as AI assets such as features and AI models. The top-level container of objects in the Unity Catalog is a metastore. It stores data assets (such as tables and views) and the permissions that govern access to them. Use a single metastore per cloud region and do not access metastores across regions to avoid latency issues.  \nThe metastore provides a three-level namespace to structure data, volumes and AI assets:  \nCatalog  \nSchema  \nTable/view.  \nDatabricks recommends using catalogs to provide segregation across your organization\u2019s information architecture. Often this means that catalogs can correspond to software development environment scope, team, or business unit.  \nTrack data and AI lineage to drive visibility of the data  \nData lineage is a powerful tool that helps data leaders gain greater visibility and understanding of the data in their organizations. Data lineage describes the transformation and refinement of data from source to insight. It includes the capture of all relevant metadata and events associated with the data throughout its lifecycle, including the source of the data set, what other data sets were used to create it, who created it and when, what transformations were performed, what other data sets use it, and many other events and attributes.  \nIn addition, when you train a model on a table in Unity Catalog, you can track the model\u2019s lineage to the upstream dataset(s) on which it was trained and evaluated.  \nLineage can be used for many data-related use cases:  \nCompliance and audit readiness: Data lineage helps organizations trace the source of tables and fields. This is important for meeting the requirements of many compliance regulations, such as General Data Protection Regulation (GDPR), California Consumer Privacy Act (CCPA), Health Insurance Portability and Accountability Act (HIPAA), Basel Committee on Banking Supervision (BCBS) 239, and Sarbanes-Oxley Act (SOX).  \nImpact analysis/change management: Data undergoes multiple transformations from the source to the final business-ready table. Understanding the potential impact of data changes on downstream users becomes important from a risk management perspective. This impact can be easily determined using the data lineage captured by the Unity Catalog."
    },
    {
        "id": 855,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/data-governance/best-practices.html",
        "content": "Data quality assurance: Understanding where a data set came from and what transformations have been applied provides much better context for data scientists and analysts, enabling them to gain better and more accurate insights.  \nDebugging and diagnostics: In the event of an unexpected result, data lineage helps data teams perform root cause analysis by tracing the error back to its source. This dramatically reduces troubleshooting time.  \nUnity Catalog captures runtime data lineage across queries running on Databricks and also model lineage. Lineage is supported for all languages and is captured down to the column level. Lineage data includes notebooks, jobs, and dashboards related to the query. Lineage can be visualized in near real-time in the Catalog Explorer and accessed using the Databricks\u2019 Data Lineage REST API.  \nAdd consistent descriptions to your metadata  \nDescriptions provide essential context for data. They help users understand the purpose and content of data tables and columns. This clarity allows them to more easily discover, identify, and filter the data they need, which is critical for effective data analysis and decision making. Descriptions can include data sensitivity and compliance information. This helps organizations meet legal and regulatory requirements for data privacy and security. Descriptions should also include information about the source, accuracy, and relevance of data. This helps ensure data integrity and promotes better collaboration across teams.  \nTwo main features in Unity Catalog support describing tables and columns. The Unity Catalog allows to  \nadd comments to tables and columns in the form of comments.  \nYou can also add an AI-generated comment for any table or table column managed by Unity Catalog to speed up the process. However, AI models are not always accurate and comments must be reviewed before saving. Databricks strongly recommends human review of AI-generated comments to check for inaccuracies.  \nadd tags to any securable in Unity Catalog. Tags are attributes with keys and optional values that you can apply to different securable objects in Unity Catalog. Tagging is useful for organizing and categorizing different securable objects within a metastore. Using tags also makes it easier to search and discover your data assets.  \nAllow easy data discovery for data consumers  \nEasy data discovery enables data scientists, data analysts, and data engineers to quickly discover and reference relevant data and accelerate time to value."
    },
    {
        "id": 856,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/data-governance/best-practices.html",
        "content": "Allow easy data discovery for data consumers  \nEasy data discovery enables data scientists, data analysts, and data engineers to quickly discover and reference relevant data and accelerate time to value.  \nDatabricks Catalog Explorer provides a user interface for exploring and managing data, schemas (databases), tables, and permissions, data owners, external locations, and credentials. In addition, you can use the Insights tab in Catalog Explorer to view the most frequent recent queries and users of any table registered in Unity Catalog.  \nGovern AI assets together with data  \nThe relationship between data governance and artificial intelligence (AI) has become critical to success. How organizations manage, secure, and use data directly impacts the outcomes and considerations of AI implementations: you can\u2019t have AI without quality data, and you can\u2019t have quality data without data governance.  \nGoverning data and AI together improves AI performance by ensuring seamless access to high-quality, up-to-date data, leading to improved accuracy and better decision-making. Breaking down silos increases efficiency by enabling better collaboration and streamlining workflows, resulting in increased productivity and reduced costs.  \nImproved data security is another benefit, as a unified governance approach establishes consistent data handling practices, reducing vulnerabilities and improving an organization\u2019s ability to protect sensitive information. Compliance with data privacy regulations is easier to maintain when data and AI governance are integrated, as data handling and AI processes are aligned with regulatory requirements.  \nOverall, a unified governance approach fosters trust among stakeholders and ensures transparency in AI decision-making processes by establishing clear policies and procedures for both data and AI.  \nIn the Databricks Data Intelligence Platform, the Unity Catalog is the central component for governing both data and AI assets:  \nFeature in Unity Catalog  \nIn Unity Catalog enabled workspaces, data scientists can create feature tables in Unity Catalog. These feature tables are Delta tables or Delta Live Tables managed by Unity Catalog.  \nModels in Unity Catalog  \nModels in Unity Catalog extends the benefits of Unity Catalog to ML models, including centralized access control, auditing, lineage, and model discovery across workspaces. Key features of models in Unity Catalog include governance for models, chronological model lineage, model versioning, and model deployment via aliases."
    },
    {
        "id": 857,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/data-governance/best-practices.html",
        "content": "2. Unify data and AI security\nCentralize access control for all data and AI assets  \nCentralizing access control for all data assets is important because it simplifies the security and governance of your data and AI assets by providing a central place to administer and audit access to these assets. This approach helps in managing data and AI object access more efficiently, ensuring that operational requirements around segregation of duty are enforced, which is crucial for regulatory compliance and risk avoidance.  \nThe Databricks Data Intelligence Platform provides data access control methods that describe which groups or individuals can access which data. These are policy statements that can be extremely granular and specific, down to the definition of each record that each individual has access to. Or they can be very expressive and broad, such as all financial users can see all financial data.  \nThe Unity Catalog centralizes access controls for all supported securable objects such as tables, files, models, and many more. Every securable object in Unity Catalog has an owner. The owner of an object has all privileges on the object, as well as the ability to grant privileges on the securable object to other principals. The Unity Catalog allows you to manage privileges, and to configure access control by using SQL DDL statements.  \nThe Unity Catalog uses row filters and column masks for fine-grained access control. Row filters allow you to apply a filter to a table so that subsequent queries return only rows for which the filter predicate evaluates to true. Column masks allow you apply a masking function to a table column. The masking function gets evaluated at query runtime, substituting each reference to the target column with the results of the masking function.  \nFor further information see Security, compliance & privacy - Manage identity and access using least privilege.  \nConfigure audit logging  \nAudit logging is important because it provides a detailed account of system activities (user actions, changes to settings, and so on) that could affect the integrity of the system. While standard system logs are designed to help developers troubleshoot problems, audit logs provide a historical record of activity for compliance and other business policy enforcement purposes. Maintaining robust audit logs can help identify and ensure preparedness in the face of threats, breaches, fraud, and other system issues."
    },
    {
        "id": 858,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/data-governance/best-practices.html",
        "content": "Databricks provides access to audit logs of activities performed by Databricks users, allowing your organization to monitor detailed Databricks usage patterns. There are two types of logs, Workspace-level audit logs with workspace-level events and account-level audit logs with account-level events.  \nYou can also enable verbose audit logs are additional audit logs recorded whenever a query or command is run in your workspace.  \nAudit data platform events  \nAudit logging is important because it provides a detailed account of system activities. The Data Intelligence Platform has audit logs for the metadata access (hence data access) and for data sharing:  \nUnity Catalog captures an audit log of actions performed against the metastore. This enables admins to access fine-grained details about who accessed a given dataset and what actions they performed.  \nFor secure sharing with Delta Sharing, Databricks provides audit logs to monitor Delta Sharing events, including:  \nWhen someone creates, modifies, updates, or deletes a share or a recipient.  \nWhen a recipient accesses an activation link and downloads the credential.  \nWhen a recipient accesses shares or data in shared tables.  \nWhen a recipient\u2019s credential is rotated or expires."
    },
    {
        "id": 859,
        "url": "https://docs.databricks.com/en/lakehouse-architecture/data-governance/best-practices.html",
        "content": "3. Establish data quality standards\nThe Databricks Data Intelligence Platform provides robust data quality management with built-in quality controls, testing, monitoring, and enforcement to ensure accurate and useful data is available for downstream BI, analytics, and machine learning workloads.  \nImplementation details can be seen in Reliability - Manage data quality.  \nDefine clear data quality standards  \nDefining clear and actionable data quality standards is crucial, because it helps ensure that data used for analysis, reporting, and decision-making is reliable and trustworthy. Documenting these standards helps ensure that they are upheld. Data quality standards should be based on the specific needs of the business and should address dimensions of data quality such as accuracy, completeness, consistency, timeliness, and reliability:  \nAccuracy: Ensure data accurately reflects real-world values.  \nCompleteness: All necessary data should be captured and no critical data should be missing.  \nConsistency: Data across all systems should be consistent and not contradict other data.  \nTimeliness: Data should be updated and available in a timely manner.  \nReliability: Data should be sourced and processed in a way that ensures its dependability.  \nUse data quality tools for profiling, cleansing, validating, and monitoring data  \nLeverage data quality tools for profiling, cleansing, validating, and monitoring data. These tools help in automating the processes of detecting and correcting data quality issues, which is vital for scaling data quality initiatives across large datasets typical in data lakes  \nFor teams using DLT, you can use expectations to define data quality constraints on the contents of a dataset. Expectations allow you to guarantee data arriving in tables meets data quality requirements and provide insights into data quality for each pipeline update.  \nImplement and enforce standardized data formats and definitions  \nStandardized data formats and definitions help achieve a consistent representation of data across all systems to facilitate data integration and analysis, reduce costs, and improve decision making by enhancing communication and collaboration across teams and departments. It also helps provide a structure for creating and maintaining data quality.  \nDevelop and enforce a standard data dictionary that includes definitions, formats, and acceptable values for all data elements used across the organization.  \nUse consistent naming conventions, date formats, and measurement units across all databases and applications to prevent discrepancies and confusion."
    },
    {
        "id": 860,
        "url": "https://docs.databricks.com/en/jobs/how-to/index.html",
        "content": "Implement data processing and analysis workflows with Jobs  \nYou can use a Databricks job to orchestrate your data processing, machine learning, or data analytics pipelines on the Databricks platform. Databricks Jobs support a number of workload types, including notebooks, scripts, Delta Live Tables pipelines, Databricks SQL queries, and dbt projects. The following articles guide you in using the features and options of Databricks Jobs to implement your data pipelines.  \nTransform, analyze, and visualize your data with a Databricks job\nTransform, analyze, and visualize your data with a Databricks job\nYou can use a job to create a data pipeline that ingests, transforms, analyzes, and visualizes data. The example in Use Databricks SQL in a Databricks job builds a pipeline that:  \nUses a Python script to fetch data using a REST API.  \nUses Delta Live Tables to ingest and transform the fetched data and save the transformed data to Delta Lake.  \nUses the Jobs integration with Databricks SQL to analyze the transformed data and create graphs to visualize the results.\n\nUse dbt transformations in a job"
    },
    {
        "id": 861,
        "url": "https://docs.databricks.com/en/jobs/how-to/index.html",
        "content": "Use dbt transformations in a job\nUse the dbt task type if you are doing data transformation with a dbt core project and want to integrate that project into a Databricks job, or you want to create new dbt transformations and run those transformations in a job. See Use dbt transformations in a Databricks job.\n\nUse a Python package in a job\nUse a Python package in a job\nPython wheel files are a standard way to package and distribute the files required to run a Python application. You can easily create a job that uses Python code packaged as a Python wheel file with the Python wheel task type. See Use a Python wheel file in a Databricks job.\n\nUse code packaged in a JAR\nUse code packaged in a JAR\nLibraries and applications implemented in a JVM language such as Java and Scala are commonly packaged in a Java archive (JAR) file. Databricks Jobs supports code packaged in a JAR with the JAR task type. See Use a JAR in a Databricks job.\n\nUse notebooks or Python code maintained in a central repository"
    },
    {
        "id": 862,
        "url": "https://docs.databricks.com/en/jobs/how-to/index.html",
        "content": "Use notebooks or Python code maintained in a central repository\nA common way to manage version control and collaboration for production artifacts is to use a central repository such as GitHub. Databricks Jobs supports creating and running jobs using notebooks or Python code imported from a repository, including GitHub or Databricks Git folders. See Use version-controlled source code in a Databricks job.\n\nOrchestrate your jobs with Apache Airflow\nOrchestrate your jobs with Apache Airflow\nDatabricks recommends using Databricks Jobs to orchestrate your workflows. However, Apache Airflow is commonly used as a workflow orchestration system and provides native support for Databricks Jobs. While Databricks Jobs provides a visual UI to create your workflows, Airflow uses Python files to define and deploy your data pipelines. For an example of creating and running a job with Airflow, see Orchestrate Databricks jobs with Apache Airflow."
    },
    {
        "id": 863,
        "url": "https://docs.databricks.com/en/migration/etl.html",
        "content": "Migrate ETL pipelines to Databricks  \nThis article provides an overview of options for migrating extract, transform, load (ETL) pipelines running on other data systems to Databricks. If you are migrating Apache Spark code, see Adapt your exisiting Apache Spark code for Databricks.  \nFor general information about moving from an enterprise data warehouse to a lakehouse, see Migrate your data warehouse to the Databricks lakehouse. For information about moving from Parquet to Delta Lake, see Migrate a Parquet data lake to Delta Lake.  \nCan you run Hive pipelines on Databricks?"
    },
    {
        "id": 864,
        "url": "https://docs.databricks.com/en/migration/etl.html",
        "content": "Can you run Hive pipelines on Databricks?\nMost Hive workloads can run on Databricks with minimal refactoring. The version of Spark SQL supported by Databricks Runtime allows many HiveQL constructs. See Apache Hive compatibility. Databricks includes a Hive metastore by default. Most Hive migrations need to address a few primary concerns:  \nHive SerDe need to be updated to use Databricks-native file codecs. (Change DDL from STORED AS to USING to use Databricks SerDe.)  \nHive UDFs must either be installed to clusters as libraries or refactored to native Spark. Because Hive UDFs are already in the JVM, they might provide sufficient performance for many workloads. See Which UDFs are most efficient?.  \nThe directory structure for tables should be altered, as Databricks uses partitions differently than Hive. See When to partition tables on Databricks.  \nIf you choose to update your tables to Delta Lake during your initial migration, a number of DDL and DML statements are unsupported. These include:  \nROWFORMAT  \nSERDE  \nOUTPUTFORMAT  \nINPUTFORMAT  \nCOMPRESSION  \nSTORED AS  \nANALYZE TABLE PARTITION  \nALTER TABLE [ADD|DROP] PARTITION  \nALTER TABLE RECOVER PARTITIONS  \nALTER TABLE SET SERDEPROPERTIES  \nCREATE TABLE LIKE  \nINSERT OVERWRITE DIRECTORY  \nLOAD DATA  \nSpecifying target partitions using PARTITION (part_spec) in TRUNCATE TABLE"
    },
    {
        "id": 865,
        "url": "https://docs.databricks.com/en/migration/etl.html",
        "content": "Can you run SQL ETL pipelines on Databricks?\nCan you run SQL ETL pipelines on Databricks?\nMigrating SQL workloads from other systems to Databricks usually requires very little refactoring, depending on the extent to which system specific protocols were used in the source code. Databricks uses Delta Lake as the default table format, so tables are created with transactional guarantees by default.  \nSpark SQL is mostly ANSI-compliant, but some differences in behavior might exist. See How is the Databricks Data Intelligence Platform different than an enterprise data warehouse?.  \nBecause data systems tend to configure access to external data differently, much of the work refactoring SQL ETL pipelines might be configuring access to these data sources and then updating your logic to use these new connections. Databricks provides options for connecting to many data sources for ingestion.\n\nCan you run dbt ETL pipelines on Databricks?"
    },
    {
        "id": 866,
        "url": "https://docs.databricks.com/en/migration/etl.html",
        "content": "Can you run dbt ETL pipelines on Databricks?\nDatabricks provides a native integration with dbt, allowing you to leverage existing dbt scripts with very little refactoring.  \nDelta Live Tables provides an optimized Databricks-native declarative SQL syntax for creating, testing, and deploying pipelines. While you can leverage dbt on Databricks, a light refactor of code to Delta Live Tables might lower your total cost to operate your pipelines on Databricks. See What is Delta Live Tables?.\n\nCan you migrate serverless cloud functions to Databricks?"
    },
    {
        "id": 867,
        "url": "https://docs.databricks.com/en/migration/etl.html",
        "content": "Can you migrate serverless cloud functions to Databricks?\nThe extensibility and versatility of custom serverless cloud functions makes it difficult to provide a common recommendation, but one of the most common use cases for these functions is waiting for files or data to appear in a location or message queue and then performing some action as a result. While Databricks does not support complex logic for triggering workloads based on cloud conditions, you can use Structured Streaming in conjunction with Jobs to process data incrementally.  \nUse Auto Loader for optimized data ingestion from cloud object storage. Structured Streaming can process data from streaming sources in near-real time.\n\nCan you run syntax from other data systems on Databricks?\nCan you run syntax from other data systems on Databricks?\nETL pipelines defined in languages other than SQL, Apache Spark, or Hive might need to be heavily refactored before running on Databricks. Databricks has experience helping customers migrate from most of the data systems in use today, and might have resources available to jumpstart your migration efforts."
    },
    {
        "id": 868,
        "url": "https://docs.databricks.com/en/marketplace/index.html",
        "content": "What is Databricks Marketplace?  \nThis article introduces Databricks Marketplace, an open forum for exchanging data products. Databricks Marketplace takes advantage of Delta Sharing to give data providers the tools to share data products securely and data consumers the power to explore and expand their access to the data and data services they need.  \nWhat kinds of data assets are shared on Databricks Marketplace?\nWhat kinds of data assets are shared on Databricks Marketplace?\nMarketplace assets include datasets, Databricks notebooks, Databricks Solution Accelerators, and machine learning (AI) models. Datasets are typically made available as catalogs of tabular data, although non-tabular data, in the form of Databricks volumes, is also supported. Solution Accelerators are available as clonable Git repos.\n\nHow do consumers get access to data in Databricks Marketplace?"
    },
    {
        "id": 869,
        "url": "https://docs.databricks.com/en/marketplace/index.html",
        "content": "How do consumers get access to data in Databricks Marketplace?\nTo find a data product you want on the Databricks Marketplace, simply browse or search provider listings.  \nYou can browse:  \nThe Open Marketplace, which does not require access to a Databricks workspace.  \nThe Databricks Marketplace on your Databricks workspace. Just click Marketplace.  \nTo request access to data products in the Marketplace, you must use the Marketplace on a Databricks workspace. You do not need a Databricks workspace to access and work with data once it is shared, although using a Databricks workspace with Unity Catalog enabled lets you take advantage of the deep integration of Unity Catalog with Delta Sharing.  \nSome data products are available to everyone in the public marketplace, and others are available as part of a private exchange, in which a provider shares their listings only with member consumers. Whether public or private, some data products are available instantly, as soon as you request them and agree to the terms. Others might require provider approval and transaction completion using provider interfaces. In either case, the Delta Sharing protocol that powers the Marketplace ensures that you can access shared data securely.  \nGet started accessing data products  \nTo learn how to get started as a data consumer:  \nUsing a Databricks workspace that is enabled for Unity Catalog, see Access data products in Databricks Marketplace (Unity Catalog-enabled workspaces).  \nUsing third-party platforms like Power BI, pandas, or Apache Spark, along with Databricks workspaces that are not enabled for Unity Catalog, see Access data products in Databricks Marketplace using external platforms."
    },
    {
        "id": 870,
        "url": "https://docs.databricks.com/en/marketplace/index.html",
        "content": "How do providers list data products in Databricks Marketplace?\nDatabricks Marketplace gives data providers a secure platform for sharing data products that data scientists and analysts can use to help their organizations succeed. Databricks Marketplace uses Delta Sharing to provide security and control over your shared data. You can share public data, free sample data, and commercialized data offerings. You can share data products in public listings or as part of private exchanges that you create, making listings discoverable only by member consumers. In addition to datasets, you can also share Databricks notebooks and other content to demonstrate use cases and show customers how to take full advantage of your data products.  \nGet started listing data products  \nTo list your data products on Databricks Marketplace, you must:  \nHave a Databricks account and premium workspace that is enabled for Unity Catalog. You do not need to enable all of your workspaces for Unity Catalog. You can create one specifically for managing Marketplace listings.  \nApply to be a provider through the Databricks Data Partner Program. Alternatively, if you only want to share data through private exchanges, you can use the self-service signup flow in the provider console. See Sign up to be a Databricks Marketplace provider.  \nReview the Marketplace provider policies.  \nTo learn how to get started, see List your data product in Databricks Marketplace."
    },
    {
        "id": 871,
        "url": "https://docs.databricks.com/en/marketplace/index.html",
        "content": "View a demo \u00a0\nView a demo\nThis video introduces Databricks Marketplace, shows how consumers access listings, and demonstrates how providers create them."
    },
    {
        "id": 872,
        "url": "https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html",
        "content": "Create custom model serving endpoints  \nThis article describes how to create model serving endpoints that serve custom models using Databricks Model Serving.  \nModel Serving provides the following options for serving endpoint creation:  \nThe Serving UI  \nREST API  \nMLflow Deployments SDK  \nFor creating endpoints that serve generative AI models, see Create generative AI model serving endpoints.  \nRequirements\nRequirements\nYour workspace must be in a supported region.  \nIf you use custom libraries or libraries from a private mirror server with your model, see Use custom Python libraries with Model Serving before you create the model endpoint.  \nFor creating endpoints using the MLflow Deployments SDK, you must install the MLflow Deployment client. To install it, run:  \nimport mlflow.deployments client = mlflow.deployments.get_deploy_client(\"databricks\")\n\nAccess control\nAccess control\nTo understand access control options for model serving endpoints for endpoint management, see Manage permissions on your model serving endpoint.  \nYou can also:  \nAdd an instance profile to a model serving endpoint  \nConfigure access to resources from model serving endpoints\n\nCreate an endpoint"
    },
    {
        "id": 873,
        "url": "https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html",
        "content": "Create an endpoint\nYou can create an endpoint for model serving with the Serving UI.  \nClick Serving in the sidebar to display the Serving UI.  \nClick Create serving endpoint.  \nFor models registered in the Workspace model registry or models in Unity Catalog:  \nIn the Name field provide a name for your endpoint.  \nIn the Served entities section  \nClick into the Entity field to open the Select served entity form.  \nSelect the type of model you want to serve. The form dynamically updates based on your selection.  \nSelect which model and model version you want to serve.  \nSelect the percentage of traffic to route to your served model.  \nSelect what size compute to use. You can use CPU or GPU computes for your workloads. Support for model serving on GPU is in Public Preview. See GPU workload types for more information on available GPU computes.  \nUnder Compute Scale-out, select the size of the compute scale out that corresponds with the number of requests this served model can process at the same time. This number should be roughly equal to QPS x model run time.  \nAvailable sizes are Small for 0-4 requests, Medium 8-16 requests, and Large for 16-64 requests.  \nSpecify if the endpoint should scale to zero when not in use.  \nUnder Advanced configuration, you can add an instance profile to connect to AWS resources from your endpoint.  \nClick Create. The Serving endpoints page appears with Serving endpoint state shown as Not Ready.  \nYou can create endpoints using the REST API. See POST /api/2.0/serving-endpoints for endpoint configuration parameters.  \nThe following example creates an endpoint that serves the first version of the ads1 model that is registered in the model registry. To specify a model from Unity Catalog, provide the full model name including parent catalog and schema such as, catalog.schema.example-model."
    },
    {
        "id": 874,
        "url": "https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html",
        "content": "POST /api/2.0/serving-endpoints { \"name\": \"workspace-model-endpoint\", \"config\":{ \"served_entities\": [ { \"name\": \"ads-entity\" \"entity_name\": \"my-ads-model\", \"entity_version\": \"3\", \"workload_size\": \"Small\", \"scale_to_zero_enabled\": true }, { \"entity_name\": \"my-ads-model\", \"entity_version\": \"4\", \"workload_size\": \"Small\", \"scale_to_zero_enabled\": true } ], \"traffic_config\":{ \"routes\": [ { \"served_model_name\": \"my-ads-model-3\", \"traffic_percentage\": 100 }, { \"served_model_name\": \"my-ads-model-4\", \"traffic_percentage\": 20 } ] } }, \"tags\": [ { \"key\": \"team\", \"value\": \"data science\" } ] }  \nThe following is an example response. The endpoint\u2019s config_update state is NOT_UPDATING and the served model is in a READY state."
    },
    {
        "id": 875,
        "url": "https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html",
        "content": "The following is an example response. The endpoint\u2019s config_update state is NOT_UPDATING and the served model is in a READY state.  \n{ \"name\": \"workspace-model-endpoint\", \"creator\": \"user@email.com\", \"creation_timestamp\": 1700089637000, \"last_updated_timestamp\": 1700089760000, \"state\": { \"ready\": \"READY\", \"config_update\": \"NOT_UPDATING\" }, \"config\": { \"served_entities\": [ { \"name\": \"ads-entity\", \"entity_name\": \"my-ads-model-3\", \"entity_version\": \"3\", \"workload_size\": \"Small\", \"scale_to_zero_enabled\": true, \"workload_type\": \"CPU\", \"state\": { \"deployment\": \"DEPLOYMENT_READY\", \"deployment_state_message\": \"\" }, \"creator\": \"user@email.com\", \"creation_timestamp\": 1700089760000 } ], \"traffic_config\": { \"routes\": [ { \"served_model_name\": \"my-ads-model-3\", \"traffic_percentage\": 100 } ] }, \"config_version\": 1 }, \"tags\": [ { \"key\": \"team\", \"value\": \"data science\" } ], \"id\": \"e3bd3e471d6045d6b75f384279e4b6ab\", \"permission_level\": \"CAN_MANAGE\", \"route_optimized\": false }  \nMLflow Deployments provides an API for create, update and deletion tasks. The APIs for these tasks accept the same parameters as the REST API for serving endpoints. See POST /api/2.0/serving-endpoints for endpoint configuration parameters."
    },
    {
        "id": 876,
        "url": "https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html",
        "content": "from mlflow.deployments import get_deploy_client client = get_deploy_client(\"databricks\") endpoint = client.create_endpoint( name=\"workspace-model-endpoint\", config={ \"served_entities\": [ { \"name\": \"ads-entity\" \"entity_name\": \"my-ads-model\", \"entity_version\": \"3\", \"workload_size\": \"Small\", \"scale_to_zero_enabled\": true } ], \"traffic_config\": { \"routes\": [ { \"served_model_name\": \"my-ads-model-3\", \"traffic_percentage\": 100 } ] } } )  \nYou can also:  \nConfigure your endpoint to serve multiple models.  \nConfigure your endpoint for route optimization.  \nConfigure your endpoint to access external resources using Databricks Secrets.  \nAdd an instance profile to your model serving endpoint to access AWS resources.  \nEnable inference tables to automatically capture incoming requests and outgoing responses to your model serving endpoints.  \nGPU workload types  \nGPU deployment is compatible with the following package versions:  \nPytorch 1.13.0 - 2.0.1  \nTensorFlow 2.5.0 - 2.13.0  \nMLflow 2.4.0 and above  \nTo deploy your models using GPUs include the workload_type field in your endpoint configuration during endpoint creation or as an endpoint configuration update using the API. To configure your endpoint for GPU workloads with the Serving UI, select the desired GPU type from the Compute Type dropdown.  \n{ \"served_entities\": [{ \"name\": \"ads1\", \"entity_version\": \"2\", \"workload_type\": \"GPU_MEDIUM\", \"workload_size\": \"Small\", \"scale_to_zero_enabled\": false, }] }  \nThe following table summarizes the available GPU workload types supported.  \nGPU workload type  \nGPU instance  \nGPU memory  \nGPU_SMALL  \n1xT4  \n16GB  \nGPU_MEDIUM  \n1xA10G  \n24GB  \nMULTIGPU_MEDIUM  \n4xA10G  \n96GB  \nGPU_MEDIUM_8  \n8xA10G  \n192GB  \nGPU_LARGE_8"
    },
    {
        "id": 877,
        "url": "https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html",
        "content": "1xT4  \n16GB  \nGPU_MEDIUM  \n1xA10G  \n24GB  \nMULTIGPU_MEDIUM  \n4xA10G  \n96GB  \nGPU_MEDIUM_8  \n8xA10G  \n192GB  \nGPU_LARGE_8  \n8xA100-80GB  \n320GB"
    },
    {
        "id": 878,
        "url": "https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html",
        "content": "Modify a custom model endpoint\nAfter enabling a custom model endpoint, you can update the compute configuration as desired. This configuration is particularly helpful if you need additional resources for your model. Workload size and compute configuration play a key role in what resources are allocated for serving your model.  \nUntil the new configuration is ready, the old configuration keeps serving prediction traffic. While there is an update in progress, another update cannot be made. However, you can cancel an in progress update from the Serving UI.  \nAfter you enable a model endpoint, select Edit endpoint to modify the compute configuration of your endpoint.  \nYou can do the following:  \nChoose from a few workload sizes, and autoscaling is automatically configured within the workload size.  \nSpecify if your endpoint should scale down to zero when not in use.  \nModify the percent of traffic to route to your served model.  \nYou can cancel an in progress configuration update by selecting Cancel update on the top right of the endpoint\u2019s details page. This functionality is only available in the Serving UI.  \nThe following is an endpoint configuration update example using the REST API. See PUT /api/2.0/serving-endpoints/{name}/config.  \nPUT /api/2.0/serving-endpoints/{name}/config { \"name\": \"workspace-model-endpoint\", \"config\":{ \"served_entities\": [ { \"name\": \"ads-entity\" \"entity_name\": \"my-ads-model\", \"entity_version\": \"5\", \"workload_size\": \"Small\", \"scale_to_zero_enabled\": true } ], \"traffic_config\":{ \"routes\": [ { \"served_model_name\": \"my-ads-model-5\", \"traffic_percentage\": 100 } ] } } }  \nThe MLflow Deployments SDK uses the same parameters as the REST API, see PUT /api/2.0/serving-endpoints/{name}/config for request and response schema details.  \nThe following code sample uses a model from the Unity Catalog model registry:"
    },
    {
        "id": 879,
        "url": "https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html",
        "content": "The following code sample uses a model from the Unity Catalog model registry:  \nimport mlflow from mlflow.deployments import get_deploy_client mlflow.set_registry_uri(\"databricks-uc\") client = get_deploy_client(\"databricks\") endpoint = client.create_endpoint( name=f\"{endpointname}\", config={ \"served_entities\": [ { \"entity_name\": f\"{catalog}.{schema}.{model_name}\", \"entity_version\": \"1\", \"workload_size\": \"Small\", \"scale_to_zero_enabled\": True } ], \"traffic_config\": { \"routes\": [ { \"served_model_name\": f\"{model_name}-1\", \"traffic_percentage\": 100 } ] } } )"
    },
    {
        "id": 880,
        "url": "https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html",
        "content": "Scoring a model endpoint\nScoring a model endpoint\nTo score your model, send requests to the model serving endpoint.  \nSee Query serving endpoints for custom models.  \nSee Query foundation models and external models.\n\nAdditional resources\nAdditional resources\nManage model serving endpoints.  \nQuery serving endpoints for custom models.  \nQuery foundation models and external models.  \nExternal models in Mosaic AI Model Serving.  \nInference tables for monitoring and debugging models.  \nIf you prefer to use Python, you can use the Databricks real-time serving Python SDK.\n\nNotebook examples\nNotebook examples\nThe following notebooks include different Databricks registered models that you can use to get up and running with model serving endpoints.  \nThe model examples can be imported into the workspace by following the directions in Import a notebook. After you choose and create a model from one of the examples, register it in the MLflow Model Registry, and then follow the UI workflow steps for model serving.  \nTrain and register a scikit-learn model for model serving notebook  \nOpen notebook in new tab Copy link for import  \nTrain and register a HuggingFace model for model serving notebook  \nOpen notebook in new tab Copy link for import"
    },
    {
        "id": 881,
        "url": "https://docs.databricks.com/en/machine-learning/feature-store/rag.html",
        "content": "Feature engineering example: structured RAG application  \nRetrieval-augmented generation, or RAG, is one of the most common approaches to building generative AI applications. Feature engineering in Unity Catalog supports structured RAG applications using online tables. You create an online table for the structured data that the RAG application needs and host it on a feature serving endpoint. The RAG application uses the feature serving endpoint to look up relevant data from the online table.  \nThe typical steps are as follows:  \nCreate a feature serving endpoint.  \nCreate a LangChainTool that uses the endpoint to look up relevant data.  \nUse the tool in the LangChain agent to retrieve relevant data.  \nCreate a model serving endpoint to host the LangChain application.  \nThe following notebook illustrates how to use Databricks online tables and feature serving endpoints for retrieval augmented generation (RAG) applications.  \nOnline tables with RAG applications demo notebook\nOnline tables with RAG applications demo notebook\nOpen notebook in new tab Copy link for import"
    },
    {
        "id": 882,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "bamboolib  \nPreview  \nThis feature is in Public Preview.  \nNote  \nbamboolib is supported in Databricks Runtime 11.3 LTS and above.  \nbamboolib is a user interface component that allows no-code data analysis and transformations from within a Databricks notebook. bamboolib helps users more easily work with their data and speeds up common data wrangling, exploration, and visualization tasks. As users complete these kinds of tasks with their data, bamboolib automatically generates Python code in the background. Users can share this code with others, who can run this code in their own notebooks to quickly reproduce those original tasks. They can also use bamboolib to extend those original tasks with additional data tasks, all without needing to know how to code. Those who are experienced with coding can extend this code to create even more sophisticated results.  \nBehind the scenes, bamboolib uses ipywidgets, which is an interactive HTML widget framework for the IPython kernel. ipywidgets runs inside of the IPython kernel.  \nContents  \nRequirements  \nQuickstart  \nWalkthroughs  \nKey tasks  \nLimitations  \nAdditional resources  \nRequirements"
    },
    {
        "id": 883,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "Requirements\nA Databricks notebook, which is attached to a Databricks cluster with Databricks Runtime 11.0 or above.  \nThe bamboolib library must be available to the notebook.  \nTo install the library from PyPI only on a specific cluster, see Cluster libraries.  \nTo use the %pip command to make the library available only to a specific notebook, see Notebook-scoped Python libraries.\n\nQuickstart\nQuickstart\nCreate a Python notebook.  \nAttach the notebook to a cluster that meets the requirements.  \nIn the notebook\u2019s first cell, enter the following code, and then run the cell. This step can be skipped if bamboolib is already installed in the workspace or cluster.  \n%pip install bamboolib  \nIn the notebook\u2019s second cell, enter the following code, and then run the cell.  \nimport bamboolib as bam  \nIn the notebook\u2019s third cell, enter the following code, and then run the cell.  \nbam  \nNote  \nAlternatively, you can print an existing pandas DataFrame to display bamboolib for use with that specific DataFrame.  \nContinue with key tasks.\n\nWalkthroughs"
    },
    {
        "id": 884,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "Walkthroughs\nYou can use bamboolib by itself or with an existing pandas DataFrame.  \nUse bamboolib by itself  \nIn this walkthrough, you use bamboolib to display in your notebook the contents of an example sales data set. You then experiment with some of the related notebook code that bamboolib automatically generates for you. You finish by querying and sorting a copy of the sales data set\u2019s contents.  \nCreate a Python notebook.  \nAttach the notebook to a cluster that meets the requirements.  \nIn the notebook\u2019s first cell, enter the following code, and then run the cell. This step can be skipped if bamboolib is already installed in the workspace or cluster.  \n%pip install bamboolib  \nIn the notebook\u2019s second cell, enter the following code, and then run the cell.  \nimport bamboolib as bam  \nIn the notebook\u2019s third cell, enter the following code, and then run the cell.  \nbam  \nClick Load dummy data.  \nIn the Load dummy data pane, for Load a dummy data set for testing bamboolib, select Sales dataset.  \nClick Execute.  \nDisplay all of the rows where item_type is Baby Food:  \nIn the Search actions list, select Filter rows.  \nIn the Filter rows pane, in the Choose list (above where), select Select rows.  \nIn the list below where, select item_type.  \nIn the Choose list next to item_type, select has value(s).  \nIn the Choose value(s) box next to has value(s), select Baby Food.  \nClick Execute.  \nCopy the automatically generated Python code for this query:  \nCick Copy Code below the data preview.  \nPaste and modify the code:  \nIn the notebook\u2019s fourth cell, paste the code that you copied. It should look like this:  \nimport pandas as pd df = pd.read_csv(bam.sales_csv) # Step: Keep rows where item_type is one of: Baby Food df = df.loc[df['item_type'].isin(['Baby Food'])]  \nAdd to this code so that it displays only those rows where order_prio is C, and then run the cell:"
    },
    {
        "id": 885,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "Add to this code so that it displays only those rows where order_prio is C, and then run the cell:  \nimport pandas as pd df = pd.read_csv(bam.sales_csv) # Step: Keep rows where item_type is one of: Baby Food df = df.loc[df['item_type'].isin(['Baby Food'])] # Add the following code. # Step: Keep rows where order_prio is one of: C df = df.loc[df['order_prio'].isin(['C'])] df  \nTip  \nInstead of writing this code, you can also do the same thing by just using bamboolib in the third cell to display only those rows where order_prio is C. This step is an example of extending the code that bamboolib automatically generated earlier.  \nSort the rows by region in ascending order:  \nIn the widget within the fourth cell, in the Search actions list, select Sort rows.  \nIn the Sort column(s) pane, in the Choose column list, select region.  \nIn the list next to region, select ascending (A-Z).  \nClick Execute.  \nNote  \nThis is equivalent to writing the following code yourself:  \ndf = df.sort_values(by=['region'], ascending=[True]) df  \nYou could have also just used bamboolib in the third cell to sort the rows by region in ascending order. This step demonstrates how you can use bamboolib to extend the code that you write. As you use bamboolib, it automatically generates the additional code for you in the background, so that you can further extend your already-extended code!  \nContinue with key tasks.  \nUse bamboolib with an existing DataFrame  \nIn this walkthrough, you use bamboolib to display in your notebook the contents of a pandas DataFrame. This DataFrame contains a copy of an example sales data set. You then experiment with some of the related notebook code that bamboolib automatically generates for you. You finish by querying and sorting some of the DataFrame\u2019s contents.  \nCreate a Python notebook.  \nAttach the notebook to a cluster that meets the requirements."
    },
    {
        "id": 886,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "Create a Python notebook.  \nAttach the notebook to a cluster that meets the requirements.  \nIn the notebook\u2019s first cell, enter the following code, and then run the cell. This step can be skipped if bamboolib is already installed in the workspace or cluster.  \n%pip install bamboolib  \nIn the notebook\u2019s second cell, enter the following code, and then run the cell.  \nimport bamboolib as bam  \nIn the notebook\u2019s third cell, enter the following code, and then run the cell.  \nimport pandas as pd df = pd.read_csv(bam.sales_csv) df  \nNote that bamboolib only supports pandas DataFrames. To convert a PySpark DataFrame to a pandas DataFrame, call toPandas on the PySpark DataFrame. To convert a Pandas API on Spark DataFrame to a pandas DataFrame, call to_pandas on the Pandas API on Spark DataFrame.  \nClick Show bamboolib UI.  \nDisplay all of the rows where item_type is Baby Food:  \nIn the Search actions list, select Filter rows.  \nIn the Filter rows pane, in the Choose list (above where), select Select rows.  \nIn the list below where, select item_type.  \nIn the Choose list next to item_type, select has value(s).  \nIn the Choose value(s) box next to has value(s), select Baby Food.  \nClick Execute.  \nCopy the automatically generated Python code for this query. To do this, click Copy Code below the data preview.  \nPaste and modify the code:  \nIn the notebook\u2019s fourth cell, paste the code that you copied. It should look like this:  \n# Step: Keep rows where item_type is one of: Baby Food df = df.loc[df['item_type'].isin(['Baby Food'])]  \nAdd to this code so that it displays only those rows where order_prio is C, and then run the cell:"
    },
    {
        "id": 887,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "Add to this code so that it displays only those rows where order_prio is C, and then run the cell:  \n# Step: Keep rows where item_type is one of: Baby Food df = df.loc[df['item_type'].isin(['Baby Food'])] # Add the following code. # Step: Keep rows where order_prio is one of: C df = df.loc[df['order_prio'].isin(['C'])] df  \nTip  \nInstead of writing this code, you can also do the same thing by just using bamboolib in the third cell to display only those rows where order_prio is C. This step is an example of extending the code that bamboolib automatically generated earlier.  \nSort the rows by region in ascending order:  \na. In the widget within the fourth cell, click Sort rows.  \nIn the Sort column(s) pane, in the Choose column list, select region.  \nIn the list next to region, select ascending (A-Z).  \nClick Execute.  \nNote  \nThis is equivalent to writing the following code yourself:  \ndf = df.sort_values(by=['region'], ascending=[True]) df  \nYou could have also just used bamboolib in the third cell to sort the rows by region in ascending order. This step demonstrates how you can use bamboolib to extend the code that you write. As you use bamboolib, it automatically generates the additional code for you in the background, so that you can further extend your already-extended code!  \nContinue with key tasks."
    },
    {
        "id": 888,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "Key tasks\nIn this section:  \nAdd the widget to a cell  \nClear the widget  \nData loading tasks  \nData action tasks  \nData action history tasks  \nGet code to programmatically recreate the widget\u2019s current state as a DataFrame  \nAdd the widget to a cell  \nScenario: You want the bamboolib widget to display in a cell.  \nMake sure the notebook meets the requirements for bamboolib.  \nIf bamboolib is not already installed in the workspace or cluster run the following code in a cell in the notebook, preferably in the first cell:  \n%pip install bamboolib  \nRun the following code in the notebook, preferably in the notebook\u2019s first or second cell:  \nimport bamboolib as bam  \nOption 1: In the cell where you want the widget to appear, add the following code, and then run the cell:  \nbam  \nThe widget appears in the cell below the code.  \nOr:  \nOption 2: In a cell that contains a reference to a pandas DataFrame, print the DataFrame. For example, given the following DataFrame definition, run the cell:  \nimport pandas as pd from datetime import datetime, date df = pd.DataFrame({ 'a': [ 1, 2, 3 ], 'b': [ 2., 3., 4. ], 'c': [ 'string1', 'string2', 'string3' ], 'd': [ date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1) ], 'e': [ datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0) ] }) df  \nThe widget appears in the cell below the code.  \nNote that bamboolib only supports pandas DataFrames. To convert a PySpark DataFrame to a pandas DataFrame, call toPandas on the PySpark DataFrame. To convert a Pandas API on Spark DataFrame to a pandas DataFrame, call to_pandas on the Pandas API on Spark DataFrame.  \nClear the widget"
    },
    {
        "id": 889,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "Clear the widget  \nScenario: You want to clear the contents of a widget and then read new data into the existing widget.  \nOption 1: Run the following code within the cell that contains the target widget:  \nbam  \nThe widget clears and then redisplays the Databricks: Read CSV file from DBFS, Databricks: Load database table, and Load dummy data buttons.  \nNote  \nIf the error name 'bam' is not defined appears, run the following code in the notebook (preferably in the notebook\u2019s first cell), and then try again:  \nimport bamboolib as bam  \nOption 2: In a cell that contains a reference to a pandas DataFrame, print the DataFrame again by running the cell again. The widget clears and then displays the new data.  \nData loading tasks  \nIn this section:  \nRead an example dataset\u2019s contents into the widget  \nRead a CSV file\u2019s contents into the widget  \nRead a database table\u2019s contents into the widget  \nRead an example dataset\u2019s contents into the widget  \nScenario: You want to read some example data into the widget, for example some pretend sales data, so that you can test out the widget\u2019s functionality.  \nClick Load dummy data.  \nNote  \nIf Load dummy data is not visible, clear the widget with Option 1 and try again.  \nIn the Load dummy data pane, for Load a dummy data set for testing bamboolib, select the name of the dataset that you want to load.  \nFor Dataframe name, enter a name for the programmatic identifier of the table\u2019s contents as a DataFrame, or leave df as the default programmatic identifier.  \nClick Execute.  \nThe widget displays the contents of the dataset.  \nTip  \nYou can switch the current widget to display the contents of a different example dataset:  \nIn the current widget, click the Load dummy data tab.  \nFollow the preceding steps to read the other example dataset\u2019s contents into the widget.  \nRead a CSV file\u2019s contents into the widget  \nScenario: You want to read the contents of a CSV file within your Databricks workspace into the widget.  \nClick Databricks: Read CSV file from DBFS.  \nNote"
    },
    {
        "id": 890,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "Scenario: You want to read the contents of a CSV file within your Databricks workspace into the widget.  \nClick Databricks: Read CSV file from DBFS.  \nNote  \nIf Databricks: Read CSV file from DBFS is not visible, clear the widget with Option 1 and try again.  \nIn the Read CSV from DBFS pane, browse to the location that contains the target CSV file.  \nSelect the target CSV file.  \nFor Dataframe name, enter a name for the programmatic identifier of the CSV file\u2019s contents as a DataFrame, or leave df as the default programmatic identifier.  \nFor CSV value separator, enter the character that separates values in the CSV file, or leave the , (comma) character as the default value separator.  \nFor Decimal separator, enter the character that separates decimals in the CSV file, or leave the . (dot) character as the default value separator.  \nFor Row limit: read the first N rows - leave empty for no limit, enter the maximum number of rows to read into the widget, or leave 100000 as the default number of rows, or leave this box empty to specify no row limit.  \nClick Open CSV file.  \nThe widget displays the contents of the CSV file, based on the settings that you specified.  \nTip  \nYou can switch the current widget to display the contents of a different CSV file:  \nIn the current widget, click the Read CSV from DBFS tab.  \nFollow the preceding steps to read the other CSV file\u2019s contents into the widget.  \nRead a database table\u2019s contents into the widget  \nScenario: You want to read the contents of a database table within your Databricks workspace into the widget.  \nClick Databricks: Load database table.  \nNote  \nIf Databricks: Load database table is not visible, clear the widget with Option 1 and try again.  \nIn the Databricks: Load database table pane, for Database - leave empty for default database, enter the name of the database in which the target table is located, or leave this box empty to specify the default database.  \nFor Table, enter the name of the target table."
    },
    {
        "id": 891,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "For Table, enter the name of the target table.  \nFor Row limit: read the first N rows - leave empty for no limit, enter the maximum number of rows to read into the widget, or leave 100000 as the default number of rows, or leave this box empty to specify no row limit.  \nFor Dataframe name, enter a name for the programmatic identifier of the table\u2019s contents as a DataFrame, or leave df as the default programmatic identifier.  \nClick Execute.  \nThe widget displays the contents of the table, based on the settings that you specified.  \nTip  \nYou can switch the current widget to display the contents of a different table:  \nIn the current widget, click the Databricks: Load database table tab.  \nFollow the preceding steps to read the other table\u2019s contents into the widget.  \nData action tasks  \nbamboolib offers over 50 data actions. Following are some of the more common getting-started data action tasks.  \nIn this section:  \nSelect columns  \nDrop columns  \nFilter rows  \nSort rows  \nGrouping rows and columns tasks  \nRemove rows with missing values  \nRemove duplicated rows  \nFind and replace missing values  \nCreate a column formula  \nSelect columns  \nScenario: You want to show only specific table columns by name, by data type, or that match some regular expression. For example, in the dummy Sales dataset, you want to show only the item_type and sales_channel columns, or you want to show only the columns that contain the string _date in their column names.  \nOn the Data tab, in the Search actions drop-down list, do one of the following:  \nType select, and then select Select or drop columns.  \nSelect Select or drop columns.  \nIn the Select or drop columns pane, in the Choose drop-down list, select Select.  \nSelect the target column names or inclusion criterion.  \nFor Dataframe name, enter a name for the programmatic identifier of the table\u2019s contents as a DataFrame, or leave df as the default programmatic identifier.  \nClick Execute.  \nDrop columns  \nScenario: You want to hide specific table columns by name, by data type, or that match some regular expression. For example, in the dummy Sales dataset, you want to hide the order_prio, order_date, and ship_date columns, or you want to hide all columns that contain only date-time values."
    },
    {
        "id": 892,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "On the Data tab, in the Search actions drop-down list, do one of the following:  \nType drop, and then select Select or drop columns.  \nSelect Select or drop columns.  \nIn the Select or drop columns pane, in the Choose drop-down list, select Drop.  \nSelect the target column names or inclusion criterion.  \nFor Dataframe name, enter a name for the programmatic identifier of the table\u2019s contents as a DataFrame, or leave df as the default programmatic identifier.  \nClick Execute.  \nFilter rows  \nScenario: You want to show or hide specific table rows based on criteria such as specific column values that are matching or missing. For example, in the dummy Sales dataset, you want to show only those rows where the item_type column\u2019s value is set to Baby Food.  \nOn the Data tab, in the Search actions drop-down list, do one of the following:  \nType filter, and then select Filter rows.  \nSelect Filter rows.  \nIn the Filter rows pane, in the Choose drop-down list above where, select Select rows or Drop rows.  \nSpecify the first filter criterion.  \nTo add another filter criterion, click add condition, and specify the next filter criterion. Repeat as desired.  \nFor Dataframe name, enter a name for the programmatic identifier of the table\u2019s contents as a DataFrame, or leave df as the default programmatic identifier.  \nClick Execute.  \nSort rows  \nScenario: You want to sort table rows based on the values within one or more columns. For example, in the dummy Sales dataset, you want to show the rows by the region column\u2019s values in alphabetical order from A to Z.  \nOn the Data tab, in the Search actions drop-down list, do one of the following:  \nType sort, and then select Sort rows.  \nSelect Sort rows.  \nIn the Sort column(s) pane, choose the first column to sort by and the sort order.  \nTo add another sort criterion, click add column, and specify the next sort criterion. Repeat as desired.  \nFor Dataframe name, enter a name for the programmatic identifier of the table\u2019s contents as a DataFrame, or leave df as the default programmatic identifier.  \nClick Execute.  \nGrouping rows and columns tasks  \nIn this section:"
    },
    {
        "id": 893,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "Click Execute.  \nGrouping rows and columns tasks  \nIn this section:  \nGroup rows and columns by a single aggregate function  \nGroup rows and columns by multiple aggregate functions  \nGroup rows and columns by a single aggregate function  \nScenario: You want to show row and column results by calculated groupings, and you want to assign custom names to those groupings. For example, in the dummy Sales dataset, you want to group the rows by the country column\u2019s values, showing the numbers of rows containing the same country value, and giving the list of calculated counts the name country_count.  \nOn the Data tab, in the Search actions drop-down list, do one of the following:  \nType group, and then select Group by and aggregate (with renaming).  \nSelect Group by and aggregate (with renaming).  \nIn the Group by with column rename pane, select the columns to group by, the first calculation, and optionally specify a name for the calculated column.  \nTo add another calculation, click add calculation, and specify the next calculation and column name. Repeat as desired.  \nSpecify where to store the result.  \nFor Dataframe name, enter a name for the programmatic identifier of the table\u2019s contents as a DataFrame, or leave df as the default programmatic identifier.  \nClick Execute.  \nGroup rows and columns by multiple aggregate functions  \nScenario: You want to show row and column results by calculated groupings. For example, in the dummy Sales dataset, you want to group the rows by the region, country, and sales_channel columns\u2019 values, showing the numbers of rows containing the same region and country value by sales_channel, as well as the total_revenue by unique combination of region, country, and sales_channel.  \nOn the Data tab, in the Search actions drop-down list, do one of the following:  \nType group, and then select Group by and aggregate (default).  \nSelect Group by and aggregate (default).  \nIn the Group by with column rename pane, select the columns to group by and the first calculation.  \nTo add another calculation, click add calculation, and specify the next calculation. Repeat as desired.  \nSpecify where to store the result."
    },
    {
        "id": 894,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "In the Group by with column rename pane, select the columns to group by and the first calculation.  \nTo add another calculation, click add calculation, and specify the next calculation. Repeat as desired.  \nSpecify where to store the result.  \nFor Dataframe name, enter a name for the programmatic identifier of the table\u2019s contents as a DataFrame, or leave df as the default programmatic identifier.  \nClick Execute.  \nRemove rows with missing values  \nScenario: You want to remove any row that has a missing value for the specified columns. For example, in the dummy Sales dataset, you want to remove any rows that have a missing item_type value.  \nOn the Data tab, in the Search actions drop-down list, do one of the following:  \nType drop or remove, and then select Drop missing values.  \nSelect Drop missing values.  \nIn the Drop missing values pane, select the columns to remove any row that has a missing value for that column.  \nFor Dataframe name, enter a name for the programmatic identifier of the table\u2019s contents as a DataFrame, or leave df as the default programmatic identifier.  \nClick Execute.  \nRemove duplicated rows  \nScenario: You want to to remove any row that has a duplicated value for the specified columns. For example, in the dummy Sales dataset, you want to remove any rows that are exact duplicates of each other.  \nOn the Data tab, in the Search actions drop-down list, do one of the following:  \nType drop or remove, and then select Drop/Remove duplicates.  \nSelect Drop/Remove duplicates.  \nIn the Remove Duplicates pane, select the columns to remove any row that has a duplicated value for those columns, and then select whether to keep the first or last row that has the duplicated value.  \nFor Dataframe name, enter a name for the programmatic identifier of the table\u2019s contents as a DataFrame, or leave df as the default programmatic identifier.  \nClick Execute.  \nFind and replace missing values  \nScenario: You want to replace the missing value with a replacement value for any row with the specified columns. For example, in the dummy Sales dataset, you want to replace any row with a missing value in the item_type column with the value Unknown Item Type."
    },
    {
        "id": 895,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "On the Data tab, in the Search actions drop-down list, do one of the following:  \nType find or replace, and then select Find and replace missing values.  \nSelect Find and replace missing values.  \nIn the Replace missing values pane, select the columns to replace missing values for, and then specify the replacement value.  \nClick Execute.  \nCreate a column formula  \nScenario: You want to create a column that uses a unique formula. For example, in the dummy Sales dataset, you want to create a column named profit_per_unit that displays the result of dividing the total_profit column value by the units_sold column value for each row.  \nOn the Data tab, in the Search actions drop-down list, do one of the following:  \nType formula, and then select New column formula.  \nSelect New column formula.  \nIn the Replace missing values pane, select the columns to replace missing values for, and then specify the replacement value.  \nClick Execute.  \nData action history tasks  \nIn this section:  \nView the list of actions taken in the widget  \nUndo the most recent action taken in the widget  \nRedo the most recent action taken in the widget  \nChange the most recent action taken in the widget  \nView the list of actions taken in the widget  \nScenario: You want to see a list of all of the changes that were made in the widget, starting with the most recent change.  \nClick History. The list of actions appears in the Transformations history pane.  \nUndo the most recent action taken in the widget  \nScenario: You want to revert the most recent change that was made in the widget.  \nDo one of the following:  \nClick the counterclockwise arrow icon.  \nClick History, and in the Transformations history pane, click Undo last step.  \nRedo the most recent action taken in the widget  \nScenario: You want to revert the most recent revert that was made in the widget.  \nDo one of the following:  \nClick the clockwise arrow icon.  \nClick History, and in the Transformations history pane, click Recover last step.  \nChange the most recent action taken in the widget  \nScenario: You want to change the most recent change that was taken in the widget.  \nDo one of the following:  \nClick the pencil icon.  \nClick History, and in the Transformations history pane, click Edit last step.  \nMake the desired change, and then click Execute.  \nGet code to programmatically recreate the widget\u2019s current state as a DataFrame"
    },
    {
        "id": 896,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "Click History, and in the Transformations history pane, click Edit last step.  \nMake the desired change, and then click Execute.  \nGet code to programmatically recreate the widget\u2019s current state as a DataFrame  \nScenario: You want to get Python code that programmatically recreates the current widget\u2019s state, represented as a pandas DataFrame. You want to run this code in a different cell in this workbook or a different workbook altogether.  \nClick Get Code.  \nIn the Export code pane, click Copy code. The code is copied to your system\u2019s clipboard.  \nPaste the code into a different cell in this workbook or into a different workbook.  \nWrite additional code to work with this pandas DataFrame programmatically, and then run the cell. For example, to display the DataFrame\u2019s contents, assuming that your DataFrame is represented programmatically by df:  \n# Your pasted code here, followed by... df"
    },
    {
        "id": 897,
        "url": "https://docs.databricks.com/en/notebooks/bamboolib.html",
        "content": "Limitations\nLimitations\nSee Known limitations Databricks notebooks for more more information.\n\nAdditional resources\nAdditional resources\nbamboolib plugins"
    },
    {
        "id": 898,
        "url": "https://docs.databricks.com/en/machine-learning/model-serving/add-model-serving-instance-profile.html",
        "content": "Add an instance profile to a model serving endpoint  \nThis article demonstrates how to attach an instance profile to a model serving endpoint. Doing so allows customers to access any AWS resources from the model permissible by the instance profile. Learn more about instance profiles.  \nRequirements\nRequirements\nCreate an instance profile.  \nAdd an instance profile to Databricks.  \nIf you have an instance profile already configured for serverless SQL, be sure to change the access policies so that your models have the right access policy to your resources.\n\nAdd an instance profile during endpoint creation"
    },
    {
        "id": 899,
        "url": "https://docs.databricks.com/en/machine-learning/model-serving/add-model-serving-instance-profile.html",
        "content": "Add an instance profile during endpoint creation\nWhen you create a model serving endpoint you can add an instance profile to the endpoint configuration.  \nNote  \nThe endpoint creator\u2019s permission to an instance profile is validated at endpoint creation time.  \nFrom the Serving UI, you can add an instance profile in Advanced configurations:  \nFor programmatic workflows, use the instance_profile_arn field when you create an endpoint to add an instance profile.  \nPOST /api/2.0/serving-endpoints { \"name\": \"feed-ads\", \"config\":{ \"served_entities\": [{ \"entity_name\": \"ads1\", \"entity_version\": \"1\", \"workload_size\": \"Small\", \"scale_to_zero_enabled\": true, \"instance_profile_arn\": \"arn:aws:iam::<aws-account-id>:instance-profile/<instance-profile-name-1>\" }] } }\n\nUpdate an existing endpoint with an instance profile"
    },
    {
        "id": 900,
        "url": "https://docs.databricks.com/en/machine-learning/model-serving/add-model-serving-instance-profile.html",
        "content": "Update an existing endpoint with an instance profile\nYou can also update an existing model serving endpoint configuration with an instance profile with the instance_profile_arn field.  \nPUT /api/2.0/serving-endpoints/{name}/config { \"served_entities\": [{ \"entity_name\": \"ads1\", \"entity_version\": \"2\", \"workload_size\": \"Small\", \"scale_to_zero_enabled\": true, \"instance_profile_arn\": \"arn:aws:iam::<aws-account-id>:instance-profile/<instance-profile-name-2>\" }] }\n\nLimitations"
    },
    {
        "id": 901,
        "url": "https://docs.databricks.com/en/machine-learning/model-serving/add-model-serving-instance-profile.html",
        "content": "Limitations\nThe following limitations apply:  \nSTS temporary security credentials are used to authenticate data access. It can\u2019t bypass any network restriction.  \nIf customers edit the instance profile IAM role from the Settings of the Databricks UI, endpoints running with the instance profile continue to use the old IAM role until the endpoint updates.  \nIf customers delete an instance profile from the Settings of the Databricks UI and that profile is used in running endpoints, the running endpoint is not impacted.  \nFor general model serving endpoint limitations, see Model Serving limits and regions.\n\nAdditional resources\nAdditional resources\nLook up features using the same instance profile that you added to the serving endpoint.  \nConfigure access to resources from model serving endpoints."
    },
    {
        "id": 902,
        "url": "https://docs.databricks.com/en/optimizations/predictive-optimization.html",
        "content": "Predictive optimization for Unity Catalog managed tables  \nPredictive optimization removes the need to manually manage maintenance operations for Unity Catalog managed tables on Databricks.  \nWith predictive optimization enabled, Databricks automatically identifies tables that would benefit from maintenance operations and runs them for the user. Maintenance operations are only run as necessary, eliminating both unnecessary runs for maintenance operations and the burden associated with tracking and troubleshooting performance.  \nWhat operations does predictive optimization run?"
    },
    {
        "id": 903,
        "url": "https://docs.databricks.com/en/optimizations/predictive-optimization.html",
        "content": "What operations does predictive optimization run?\nPredictive optimization runs the following operations automatically for enabled tables:  \nOperation  \nDescription  \nOPTIMIZE  \nTriggers incremental clustering for enabled tables. See Use liquid clustering for Delta tables.  \nImproves query performance by optimizing file sizes. See Optimize data file layout.  \nVACUUM  \nReduces storage costs by deleting data files no longer referenced by the table. See Remove unused data files with vacuum.  \nNote  \nOPTIMIZE does not run ZORDER when executed with predictive optimization.  \nWarning  \nThe retention window for the VACUUM command is determined by the delta.deletedFileRetentionDuration table property, which defaults to 7 days. This means VACUUM removes data files that are no longer referenced by a Delta table version in the last 7 days. If you\u2019d like to retain data for longer (such as to support time travel for longer durations), you must set this table property appropriately before you enable predictive optimization, as in the following example:  \nALTER TABLE table_name SET TBLPROPERTIES ('delta.deletedFileRetentionDuration' = '30 days');\n\nWhere does predictive optimization run?"
    },
    {
        "id": 904,
        "url": "https://docs.databricks.com/en/optimizations/predictive-optimization.html",
        "content": "Where does predictive optimization run?\nPredictive optimization identifies tables that would benefit from OPTIMIZE and VACUUM operations and queues them to run using serverless compute for jobs. Your account is billed for compute associated with these workloads using a SKU specific to Databricks Managed Services. See pricing for Databricks managed services. Databricks provides system tables for observability into predictive optimization operations, costs, and impact. See Use system tables to track predictive optimization.\n\nPrerequisites for predictive optimization\nPrerequisites for predictive optimization\nYou must fulfill the following requirements to enable predictive optimization:  \nYour Databricks workspace must be on the Premium plan or above in a region that supports predictive optimization. See Databricks clouds and regions.  \nYou must use SQL warehouses or Databricks Runtime 12.2 LTS or above when you enable predictive optimization.  \nOnly Unity Catalog managed tables are supported.\n\nEnable predictive optimization"
    },
    {
        "id": 905,
        "url": "https://docs.databricks.com/en/optimizations/predictive-optimization.html",
        "content": "Enable predictive optimization\nYou must enable predictive optimization at the account level.  \nYou must have the following privileges to enable or disable predictive optimization at the specified level:  \nUnity Catalog object  \nPrivilege  \nAccount  \nAccount admin  \nCatalog  \nCatalog owner  \nSchema  \nSchema owner  \nNote  \nWhen you enable predictive optimization for the first time, Databricks automatically creates a service principal in your Databricks account. Databricks uses this service principal to perform the requested maintenance operations. See Manage service principals.  \nEnable predictive optimization for your account  \nAn account admin must complete the following steps to enable predictive optimization for all metastores in an account:  \nAccess the accounts console.  \nNavigate to Settings, then Feature enablement.  \nSelect Enabled next to Predictive optimization.  \nNote  \nMetastores in regions that don\u2019t support predictive optimization aren\u2019t enabled.  \nEnable or disable predictive optimization for a catalog or schema  \nPredictive optimization uses an inheritance model. When enabled for a catalog, schemas inherit the property. Tables within an enabled schema inherit predictive optimization. To override this inheritance behavior, you can explicitly disable predictive optimization for a catalog or schema.  \nNote  \nYou can disable predictive optimization at the catalog or schema level before enabling it at the account level. If predictive optimization is later enabled on the account, it is blocked for tables in these objects.  \nUse the following syntax to enable or disable predictive optimization:  \nALTER CATALOG [catalog_name] {ENABLE | DISABLE} PREDICTIVE OPTIMIZATION; ALTER {SCHEMA | DATABASE} schema_name {ENABLE | DISABLE} PREDICTIVE OPTIMIZATION;"
    },
    {
        "id": 906,
        "url": "https://docs.databricks.com/en/optimizations/predictive-optimization.html",
        "content": "Check whether predictive optimization is enabled\nCheck whether predictive optimization is enabled\nThe Predictive Optimization field is a Unity Catalog property that details if predictive optimization is enabled. If predictive optimization is inherited from a parent object, this is indicated in the field value.  \nImportant  \nYou must enable predictive optimization at the account level to view this field.  \nUse the following syntax to see if predictive optimization is enabled:  \nDESCRIBE (CATALOG | SCHEMA | TABLE) EXTENDED name\n\nUse system tables to track predictive optimization\nUse system tables to track predictive optimization\nDatabricks provides a system table to track the history of predictive optimization operations. See Predictive optimization system table reference.\n\nLimitations"
    },
    {
        "id": 907,
        "url": "https://docs.databricks.com/en/optimizations/predictive-optimization.html",
        "content": "Limitations\nPredictive optimization is not available in all regions. See Databricks clouds and regions.  \nPredictive optimization does not run OPTIMIZE commands on tables that use Z-order.  \nPredictive optimization does not run VACUUM operations on tables with a file retention window configured below the default of 7 days. See Configure data retention for time travel queries.  \nPredictive optimization does not perform maintenance operations on the following tables:  \nTables loaded to a workspace as Delta Sharing recipients.  \nExternal tables.  \nMaterialized views. See Use materialized views in Databricks SQL.  \nStreaming tables. See Load data using streaming tables in Databricks SQL."
    },
    {
        "id": 908,
        "url": "https://docs.databricks.com/en/machine-learning/train-model/xgboost.html",
        "content": "Use XGBoost on Databricks  \nThis article provides examples of training machine learning models using XGBoost in Databricks. Databricks Runtime for Machine Learning includes XGBoost libraries for both Python and Scala. You can train XGBoost models on an individual machine or in a distributed fashion.  \nTrain XGBoost models on a single node\nTrain XGBoost models on a single node\nYou can train models using the Python xgboost package. This package supports only single node workloads. To train a PySpark ML pipeline and take advantage of distributed training, see Distributed training of XGBoost models.  \nXGBoost Python notebook  \nOpen notebook in new tab Copy link for import\n\nDistributed training of XGBoost models"
    },
    {
        "id": 909,
        "url": "https://docs.databricks.com/en/machine-learning/train-model/xgboost.html",
        "content": "Distributed training of XGBoost models\nFor distributed training of XGBoost models, Databricks includes PySpark estimators based on the xgboost package. Databricks also includes the Scala package xgboost-4j. For details and example notebooks, see the following:  \nDistributed training of XGBoost models using xgboost.spark (Databricks Runtime 12.0 ML and above)  \nDistributed training of XGBoost models using sparkdl.xgboost (deprecated starting with Databricks Runtime 12.0 ML)  \nDistributed training of XGBoost models using Scala\n\nInstall XGBoost on Databricks"
    },
    {
        "id": 910,
        "url": "https://docs.databricks.com/en/machine-learning/train-model/xgboost.html",
        "content": "Install XGBoost on Databricks\nIf you need to install XGBoost on Databricks Runtime or use a different version than the one pre-installed with Databricks Runtime ML, follow these instructions.  \nInstall XGBoost on Databricks Runtime ML  \nXGBoost is included in Databricks Runtime ML. You can use these libraries in Databricks Runtime ML without installing any packages.  \nFor the version of XGBoost installed in the Databricks Runtime ML version you are using, see the release notes. To install other Python versions in Databricks Runtime ML, install XGBoost as a Databricks PyPI library. Specify it as the following and replace <xgboost version> with the desired version.  \nxgboost==<xgboost version>  \nInstall XGBoost on Databricks Runtime  \nPython package: Execute the following command in a notebook cell:  \n%pip install xgboost  \nTo install a specific version, replace <xgboost version> with the desired version:  \n%pip install xgboost==<xgboost version>  \nScala/Java packages: Install as a Databricks library with the Spark Package name xgboost-linux64."
    },
    {
        "id": 911,
        "url": "https://docs.databricks.com/en/machine-learning/mlops/llmops.html",
        "content": "LLMOps workflows on Databricks  \nThis article complements MLOps workflows on Databricks by adding information specific to LLMOps workflows. For more details, see The Big Book of MLOps.  \nHow does the MLOps workflow change for LLMs?"
    },
    {
        "id": 912,
        "url": "https://docs.databricks.com/en/machine-learning/mlops/llmops.html",
        "content": "How does the MLOps workflow change for LLMs?\nLLMs are a class of natural language processing (NLP) models that have significantly surpassed their predecessors in size and performance across a variety of tasks, such as open-ended question answering, summarization, and execution of instructions.  \nDevelopment and evaluation of LLMs differs in some important ways from traditional ML models. This section briefly summarizes some of the key properties of LLMs and the implications for MLOps.  \nKey properties of LLMs  \nImplications for MLOps  \nLLMs are available in many forms.  \nGeneral proprietary and OSS models that are accessed using paid APIs.  \nOff-the-shelf open source models that vary from general to specific applications.  \nCustom models that have been fine-tuned for specific applications.  \nCustom pre-trained applications.  \nDevelopment process: Projects often develop incrementally, starting from existing, third-party or open source models and ending with custom fine-tuned models.  \nMany LLMs take general natural language queries and instructions as input. Those queries can contain carefully engineered prompts to elicit the desired responses.  \nDevelopment process: Designing text templates for querying LLMs is often an important part of developing new LLM pipelines.  \nPackaging ML artifacts: Many LLM pipelines use existing LLMs or LLM serving endpoints. The ML logic developed for those pipelines might focus on prompt templates, agents, or chains instead of the model itself. The ML artifacts packaged and promoted to production might be these pipelines, rather than models.  \nMany LLMs can be given prompts with examples, context, or other information to help answer the query.  \nServing infrastructure: When augmenting LLM queries with context, you might use additional tools such as vector databases to search for relevant context.  \nThird-party APIs provide proprietary and open-source models.  \nAPI governance: Using centralized API governance provides the ability to easily switch between API providers.  \nLLMs are very large deep learning models, often ranging from gigabytes to hundreds of gigabytes.  \nServing infrastructure: LLMs might require GPUs for real-time model serving, and fast storage for models that need to be loaded dynamically.  \nCost/performance tradeoffs: Because larger models require more computation and are more expensive to serve, techniques for reducing model size and computation might be required."
    },
    {
        "id": 913,
        "url": "https://docs.databricks.com/en/machine-learning/mlops/llmops.html",
        "content": "Cost/performance tradeoffs: Because larger models require more computation and are more expensive to serve, techniques for reducing model size and computation might be required.  \nLLMs are hard to evaluate using traditional ML metrics since there is often no single \u201cright\u201d answer.  \nHuman feedback: Human feedback is essential for evaluating and testing LLMs. You should incorporate user feedback directly into the MLOps process, including for testing, monitoring, and future fine-tuning."
    },
    {
        "id": 914,
        "url": "https://docs.databricks.com/en/machine-learning/mlops/llmops.html",
        "content": "Commonalities between MLOps and LLMOps\nCommonalities between MLOps and LLMOps\nMany aspects of MLOps processes do not change for LLMs. For example, the following guidelines also apply to LLMs:  \nUse separate environments for development, staging, and production.  \nUse Git for version control.  \nManage model development with MLflow, and use Models in Unity Catalog to manage the model lifecycle.  \nStore data in a lakehouse architecture using Delta tables.  \nYour existing CI/CD infrastructure should not require any changes.  \nThe modular structure of MLOps remains the same, with pipelines for featurization, model training, model inference, and so on.\n\nReference architecture diagrams"
    },
    {
        "id": 915,
        "url": "https://docs.databricks.com/en/machine-learning/mlops/llmops.html",
        "content": "Reference architecture diagrams\nThis section uses two LLM-based applications to illustrate some of the adjustments to the reference architecture of traditional MLOps. The diagrams show the production architecture for 1) a retrieval-augmented generation (RAG) application using a third-party API, and 2) a RAG application using a self-hosted fine-tuned model. Both diagrams show an optional vector database \u2014 this item can be replaced by directly querying the LLM through the Model Serving endpoint.  \nRAG with a third-party LLM API  \nThe diagram shows a production architecture for a RAG application that connects to a third-party LLM API using Databricks External Models.  \nRAG with a fine-tuned open source model  \nThe diagram shows a production architecture for a RAG application that fine-tunes an open source model.\n\nLLMOps changes to MLOps production architecture"
    },
    {
        "id": 916,
        "url": "https://docs.databricks.com/en/machine-learning/mlops/llmops.html",
        "content": "LLMOps changes to MLOps production architecture\nThis section highlights the major changes to the MLOps reference architecture for LLMOps applications.  \nModel hub  \nLLM applications often use existing, pretrained models selected from an internal or external model hub. The model can be used as-is or fine-tuned.  \nDatabricks includes a selection of high-quality, pre-trained foundation models in Unity Catalog and in Databricks Marketplace. You can use these pre-trained models to access state-of-the-art AI capabilities, saving you the time and expense of building your own custom models. For details, see Pre-trained models in Unity Catalog and Marketplace.  \nVector database  \nSome LLM applications use vector databases for fast similarity searches, for example to provide context or domain knowledge in LLM queries. Databricks provides an integrated vector search functionality that lets you use any Delta table in Unity Catalog as a vector database. The vector search index automatically syncs with the Delta table. For details, see Vector Search.  \nYou can create a model artifact that encapsulates the logic to retrieve information from a vector database and provides the returned data as context to the LLM. You can then log the model using the MLflow LangChain or PyFunc model flavor.  \nFine-tune LLM  \nBecause LLM models are expensive and time-consuming to create from scratch, LLM applications often fine-tune an existing model to improve its performance in a particular scenario. In the reference architecture, fine-tuning and model deployment are represented as distinct Databricks Jobs. Validating a fine-tuned model before deploying is often a manual process.  \nDatabricks provides Mosaic AI Model Training, which lets you use your own data to customize an existing LLM to optimize its performance for your specific application. For details, see Mosaic AI Model Training.  \nModel serving  \nIn the RAG using a third-party API scenario, an important architectural change is that the LLM pipeline makes external API calls, from the Model Serving endpoint to internal or third-party LLM APIs. This adds complexity, potential latency, and additional credential management.  \nDatabricks provides Mosaic AI Model Serving, which provides a unified interface to deploy, govern, and query AI models. For details, see Mosaic AI Model Serving.  \nHuman feedback in monitoring and evaluation"
    },
    {
        "id": 917,
        "url": "https://docs.databricks.com/en/machine-learning/mlops/llmops.html",
        "content": "Databricks provides Mosaic AI Model Serving, which provides a unified interface to deploy, govern, and query AI models. For details, see Mosaic AI Model Serving.  \nHuman feedback in monitoring and evaluation  \nHuman feedback loops are essential in most LLM applications. Human feedback should be managed like other data, ideally incorporated into monitoring based on near real-time streaming.  \nThe Mosaic AI Agent Framework review app helps you gather feedback from human reviewers. For details, see Get feedback about the quality of an agentic application."
    },
    {
        "id": 918,
        "url": "https://docs.databricks.com/en/migration/parquet-to-delta-lake.html",
        "content": "Migrate a Parquet data lake to Delta Lake  \nThis article provides recommendations for converting an existing Parquet data lake to Delta Lake. Delta Lake is the underlying format in the Databricks lakehouse. See What is Delta Lake?.  \nConsiderations before converting to Delta Lake\nConsiderations before converting to Delta Lake\nYour Parquet data lake likely has a partitioning strategy that has been optimized for your existing workloads and systems. While you can convert to Delta Lake and maintain this partitioning structure, over-partitioned tables are one of the main culprits that cause slow workloads on Delta Lake. See When to partition tables on Databricks and guidelines for adapting Spark code to Databricks.  \nYou also need to consider whether or not the data being converted is still growing, as well as how frequently data is currently being queried. You might choose different approaches for different Parquet tables in your data lake.\n\nApproaches to Delta Lake conversion"
    },
    {
        "id": 919,
        "url": "https://docs.databricks.com/en/migration/parquet-to-delta-lake.html",
        "content": "Approaches to Delta Lake conversion\nThe following matrix outlines the four main approaches to converting a Parquet data lake to Delta Lake and some of the trade-offs. To clarify each column:  \nIncremental: Denotes functionality that supports converting additional data appended to the conversion source after conversion has begun.  \nDuplicates data: Indicates whether data is written to a new location or modified in place.  \nMaintains data structure: Indicates whether the partitioning strategy is maintained during conversion.  \nBackfill data: Denotes functionality that supports backfilling data that has been added to the conversion source after conversion has begun.  \nEase of use: Indicates the level of user effort to configure and run the data conversion.  \nMethod  \nIncremental  \nDuplicates data  \nMaintains data structure  \nBackfill data  \nEase of use  \nDeep CLONE Parquet  \nYes  \nYes  \nYes  \nYes  \nEasy  \nShallow CLONE Parquet  \nYes  \nNo  \nYes  \nYes  \nEasy  \nCONVERT TO DELTA  \nNo  \nNo  \nYes  \nNo  \nEasy  \nAuto Loader  \nYes  \nYes  \nNo  \nOptional  \nSome configuration  \nBatch Spark job  \nCustom logic  \nYes  \nNo  \nCustom logic  \nCustom logic  \nThe following sections discuss each of these options in greater depth.\n\nMigrate Parquet data with CLONE Parquet"
    },
    {
        "id": 920,
        "url": "https://docs.databricks.com/en/migration/parquet-to-delta-lake.html",
        "content": "Migrate Parquet data with CLONE Parquet\nYou can use CLONE Parquet to incrementally copy data from a Parquet data lake to Delta Lake. Shallow clones create pointers to existing Parquet files, maintaining your Parquet table in its original location and format while providing optimized access through collected file statistics. You can write to the table created by a shallow clone without impacting the original data source.  \nDeep clone copies all data files from the source to a new location while converting to Delta Lake. Deep clone allows you to incrementally detect new files, including backfill operations, on subsequent execution of the logic. See Incrementally clone Parquet and Iceberg tables to Delta Lake.  \nThe following example demonstrates using CLONE:  \nCREATE OR REPLACE TABLE <target-table-name> [SHALLOW] CLONE parquet.`/path/to/data`;\n\nMigrate Parquet data with CONVERT TO DELTA"
    },
    {
        "id": 921,
        "url": "https://docs.databricks.com/en/migration/parquet-to-delta-lake.html",
        "content": "Migrate Parquet data with CONVERT TO DELTA\nYou can use CONVERT TO DELTA to transform a directory of Parquet files into a Delta table with a single command. Once you have converted a table to Delta Lake, you should stop reading and writing from the table using Parquet logic. Data written to the target directory after conversion has started might not be reflected in the resultant Delta table. See Convert to Delta Lake.  \nThe follow example demonstrates using CONVERT TO DELTA:  \nCONVERT TO DELTA parquet.`s3://my-bucket/parquet-data`;\n\nMigrate Parquet data with Auto Loader"
    },
    {
        "id": 922,
        "url": "https://docs.databricks.com/en/migration/parquet-to-delta-lake.html",
        "content": "Migrate Parquet data with Auto Loader\nWhile Auto Loader is a product designed for incremental data ingestion from cloud object storage, you can leverage it to implement a pattern that incrementally copies all data from a given directory to a target table. See What is Auto Loader?.  \nThe following code example includes configurations that:  \nProcess all existing files in the source directory.  \nTrigger an automatic weekly backfill job to capture files that might have been missed.  \nAllow Apache Spark to use many Spark jobs to avoid spill and out-of-memory errors associated with large data partitions.  \nProvide end-to-end exactly-once processing guarantees.  \n(spark.readStream .format(\"cloudFiles\") .option(\"cloudFiles.format\", \"parquet\") .option(\"cloudFiles.includeExistingFiles\", \"true\") .option(\"cloudFiles.backfillInterval\", \"1 week\") .option(\"cloudFiles.schemaLocation\", checkpoint_path) .load(file_path) .writeStream .option(\"checkpointLocation\", checkpoint_path) .trigger(availableNow=True) .toTable(table_name) )  \nYou can use Auto Loader in Delta Live Tables with either Python or SQL:  \nLoad data using streaming tables (Python/SQL notebook)  \nLoad data using streaming tables in Databricks SQL"
    },
    {
        "id": 923,
        "url": "https://docs.databricks.com/en/migration/parquet-to-delta-lake.html",
        "content": "Migrate Parquet data with custom Apache Spark batch logic\nMigrate Parquet data with custom Apache Spark batch logic\nWriting custom Apache Spark logic provides great flexibility in controlling how and when different data from your source system is migrated, but might require extensive configuration to provide capabilities built into other approaches.  \nAt the heart of this approach is a simple Apache Spark read and write operation, such as the following:  \nspark.read.format(\"parquet\").load(file_path).write.mode(\"append\").saveAsTable(table_name)  \nTo perform backfills or incremental migration, you might be able to rely on the partitioning structure of your data source, but might also need to write custom logic to track which files have been added since you last loaded data from the source. While you can use Delta Lake merge capabilities to avoid writing duplicate records, comparing all records from a large Parquet source table to the contents of a large Delta table is a computationally expensive task.\n\nWhen shouldn\u2019t you convert to Delta Lake?"
    },
    {
        "id": 924,
        "url": "https://docs.databricks.com/en/migration/parquet-to-delta-lake.html",
        "content": "When shouldn\u2019t you convert to Delta Lake?\nBefore converting all your existing Parquet data to Delta Lake, you are likely to consider potential trade-offs.  \nDatabricks designs many optimized features of the lakehouse around Delta Lake, and Delta Lake provides a rich open source ecosystem with native connectors for many languages and enterprise data systems. Delta Sharing extends the ability to share data stored with Delta Lake to other clients.  \nDelta Lake is built on top of Parquet, and as such, Databricks also has optimized readers and writers for interacting with Parquet files.  \nDatabricks recommends using Delta Lake for all tables that receive regular updates or queries from Databricks. You might choose to maintain data in Parquet format in some cases, such as the following:  \nAn upstream system that writes data to Parquet does not support native writing to Delta Lake.  \nA downstream system that reads Parquet data cannot read Delta Lake.  \nIn both of these cases, you might want to replicate your tables to Delta Lake to leverage performance benefits while reading, writing, updating, and deleting records in the table.  \nNote  \nSimultaneously modifying data in the same Delta table stored in S3 from multiple workspaces or data systems is not recommended. See Delta Lake limitations on S3."
    },
    {
        "id": 925,
        "url": "https://docs.databricks.com/en/mlflow/experiments.html",
        "content": "Organize training runs with MLflow experiments  \nExperiments are units of organization for your model training runs. There are two types of experiments: workspace and notebook.  \nYou can create a workspace experiment from the Databricks Machine Learning UI or the MLflow API. Workspace experiments are not associated with any notebook, and any notebook can log a run to these experiments by using the experiment ID or the experiment name.  \nA notebook experiment is associated with a specific notebook. Databricks automatically creates a notebook experiment if there is no active experiment when you start a run using mlflow.start_run().  \nTo see all of the experiments in a workspace that you have access to, select Machine Learning > Experiments in the sidebar.  \nCreate workspace experiment"
    },
    {
        "id": 926,
        "url": "https://docs.databricks.com/en/mlflow/experiments.html",
        "content": "Create workspace experiment\nThis section describes how to create a workspace experiment using the Databricks UI. You can create a workspace experiment directly from the workspace or from the Experiments page.  \nYou can also use the MLflow API, or the Databricks Terraform provider with databricks_mlflow_experiment.  \nFor instructions on logging runs to workspace experiments, see Logging example notebook.  \nClick Workspace in the sidebar.  \nNavigate to the folder in which you want to create the experiment.  \nRight-click on the folder and select Create > MLflow experiment.  \nIn the Create MLflow Experiment dialog, enter a name for the experiment and an optional artifact location. If you do not specify an artifact location, artifacts are stored in dbfs:/databricks/mlflow-tracking/<experiment-id>.  \nDatabricks supports DBFS, S3, and Azure Blob storage artifact locations.  \nTo store artifacts in S3, specify a URI of the form s3://<bucket>/<path>. MLflow obtains credentials to access S3 from your clusters\u2019s instance profile. Artifacts stored in S3 do not appear in the MLflow UI; you must download them using an object storage client.  \nNote  \nFor MLflow version 2.3.0 and above, the maximum size for an MLflow artifact uploaded to DBFS on AWS is up to 5 TiB. For MLflow version 2.3.0 and lower, the maximum size for an MLflow artifact uploaded to DBFS on AWS is 5 GiB.  \nNote  \nWhen you store an artifact in a location other than DBFS, the artifact does not appear in the MLflow UI. Models stored in locations other than DBFS cannot be registered in Model Registry.  \nClick Create. An empty experiment appears.  \nYou can also create a new workspace experiment from the Experiments page. To create a new experiment, use the drop-down menu. From the drop-down menu, you can select either an AutoML experiment or a blank (empty) experiment.  \nAutoML experiment. The Configure AutoML experiment page appears. For information about using AutoML, see Train ML models with the Databricks AutoML UI."
    },
    {
        "id": 927,
        "url": "https://docs.databricks.com/en/mlflow/experiments.html",
        "content": "AutoML experiment. The Configure AutoML experiment page appears. For information about using AutoML, see Train ML models with the Databricks AutoML UI.  \nBlank experiment. The Create MLflow Experiment dialog appears. Enter a name and optional artifact location in the dialog to create a new workspace experiment. The default artifact location is dbfs:/databricks/mlflow-tracking/<experiment-id>.  \nTo log runs to this experiment, call mlflow.set_experiment() with the experiment path. The experiment path appears at the top of the experiment page. See Logging example notebook for details and an example notebook."
    },
    {
        "id": 928,
        "url": "https://docs.databricks.com/en/mlflow/experiments.html",
        "content": "Create notebook experiment\nCreate notebook experiment\nWhen you use the mlflow.start_run() command in a notebook, the run logs metrics and parameters to the active experiment. If no experiment is active, Databricks creates a notebook experiment. A notebook experiment shares the same name and ID as its corresponding notebook. The notebook ID is the numerical identifier at the end of a Notebook URL and ID.  \nAlternatively, you can pass a Databricks workspace path to an existing notebook in mlflow.set_experiment() to create a notebook experiment for it.  \nFor instructions on logging runs to notebook experiments, see Logging example notebook.  \nNote  \nIf you delete a notebook experiment using the API (for example, MlflowClient.tracking.delete_experiment() in Python), the notebook itself is moved into the Trash folder.\n\nView experiments"
    },
    {
        "id": 929,
        "url": "https://docs.databricks.com/en/mlflow/experiments.html",
        "content": "View experiments\nEach experiment that you have access to appears on the experiments page. From this page, you can view any experiment. Click on an experiment name to display the experiment page.  \nAdditional ways to access the experiment page:  \nYou can access the experiment page for a workspace experiment from the workspace menu.  \nYou can access the experiment page for a notebook experiment from the notebook.  \nTo search for experiments, type text in the Filter experiments field and press Enter or click the magnifying glass icon. The experiment list changes to show only those experiments that contain the search text in the Name, Created by, Location, or Description column.  \nClick the name of any experiment in the table to display its experiment page:  \nThe experiment page lists all runs associated with the experiment. From the table, you can open the run page for any run associated with the experiment by clicking its Run Name. The Source column gives you access to the notebook version that created the run. You can also search and filter runs by metrics or parameter settings.  \nView workspace experiment  \nClick Workspace in the sidebar.  \nGo to the folder containing the experiment.  \nClick the experiment name.  \nView notebook experiment  \nIn the notebook\u2019s right sidebar, click the Experiment icon .  \nThe Experiment Runs sidebar appears and shows a summary of each run associated with the notebook experiment, including run parameters and metrics. At the top of the sidebar is the name of the experiment that the notebook most recently logged runs to (either a notebook experiment or a workspace experiment).  \nFrom the sidebar, you can navigate to the experiment page or directly to a run.  \nTo view the experiment, click at the far right, next to Experiment Runs.  \nTo display a run, click the name of the run."
    },
    {
        "id": 930,
        "url": "https://docs.databricks.com/en/mlflow/experiments.html",
        "content": "Manage experiments\nYou can rename, delete, or manage permissions for an experiment you own from the experiments page, the experiment page, or the workspace menu.  \nNote  \nYou cannot directly rename, delete, or manage permissions on an MLflow experiment that was created by a notebook in a Databricks Git folder. You must perform these actions at the Git folder level.  \nRename experiment from the experiments page or the experiment page  \nPreview  \nThis feature is in Public Preview.  \nTo rename an experiment from the experiments page or the experiment page, click and select Rename.  \nRename experiment from the workspace menu  \nClick Workspace in the sidebar.  \nGo to the folder containing the experiment.  \nRight-click on the experiment name and select Rename.  \nCopy experiment name  \nTo copy the experiment name, click at the top of the experiment page. You can use this name in the MLflow command set_experiment to set the active MLflow experiment.  \nYou can also copy the experiment name from the experiment sidebar in a notebook.  \nDelete notebook experiment  \nNotebook experiments are part of the notebook and cannot be deleted separately. When you delete a notebook, the associated notebook experiment is deleted. When you delete a notebook experiment using the UI, the notebook is also deleted.  \nTo delete notebook experiments using the API, use the Workspace API to ensure both the notebook and experiment are deleted from the workspace.  \nDelete workspace experiment from the workspace menu  \nClick Workspace in the sidebar.  \nGo to the folder containing the experiment.  \nRight-click on the experiment name and select Move to Trash.  \nDelete workspace or notebook experiment from the experiments page or the experiment page  \nPreview  \nThis feature is in Public Preview.  \nTo delete an experiment from the experiments page or the experiment page, click and select Delete.  \nWhen you delete a notebook experiment, the notebook is also deleted.  \nChange permissions for experiment  \nTo change permissions for an experiment from the experiment page, click Share.  \nYou can change permissions for an experiment that you own from the experiments page. Click in the Actions column and select Permission.  \nFor information on experiment permission levels, see MLflow experiment ACLs."
    },
    {
        "id": 931,
        "url": "https://docs.databricks.com/en/mlflow/experiments.html",
        "content": "Copy experiments between workspaces\nCopy experiments between workspaces\nTo migrate MLflow experiments between workspaces, you can use the community-driven open source project MLflow Export-Import.  \nWith these tools, you can:  \nShare and collaborate with other data scientists in the same or another tracking server. For example, you can clone an experiment from another user into your workspace.  \nCopy MLflow experiments and runs from your local tracking server to your Databricks workspace.  \nBack up mission critical experiments and models to another Databricks workspace."
    },
    {
        "id": 932,
        "url": "https://docs.databricks.com/en/libraries/workspace-files-libraries.html",
        "content": "Install libraries from workspace files  \nThis article walks you through the steps required to upload package or requirements.txt files to workspace files and install them onto clusters in Databricks. You can install libraries onto all-purpose compute or job compute.  \nImportant  \nThis article describes storing libraries as workspace files. This is different than workspace libraries which are deprecated.  \nFor more information about workspace files, see Navigate the workspace.  \nFor full library compatibility details, see Libraries.  \nLoad libraries to workspace files\nLoad libraries to workspace files\nYou can load libraries to workspace files the same way you load other files.  \nTo load a library to workspace files:  \nClick Workspace in the left sidebar.  \nNavigate to the location in the workspace where you want to upload the library.  \nClick the in the upper right and choose Import.  \nThe Import dialog appears. For Import from: choose File or URL. Drag and drop or browse to the file(s) you want to upload, or provide the URL path to the file.  \nClick Import.\n\nInstall libraries from workspace files onto a cluster"
    },
    {
        "id": 933,
        "url": "https://docs.databricks.com/en/libraries/workspace-files-libraries.html",
        "content": "Install libraries from workspace files onto a cluster\nWhen you install a library onto a cluster, all notebooks running on that cluster have access to the library.  \nTo install a library from workspace files onto a cluster:  \nClick Compute in the left sidebar.  \nClick the name of the cluster in the cluster list.  \nClick the Libraries tab.  \nClick Install new. The Install library dialog appears.  \nFor Library Source, select Workspace.  \nUpload the library or requirements.txt file, browse to the library or requirements.txt in the workspace, or enter its workspace location in the Workspace File Path field, such as the following: /Workspace/Users/someone@example.com/<path-to-library>/<library-name>.<ext>  \nClick Install.\n\nAdd dependent libraries to workflow tasks from workspace files"
    },
    {
        "id": 934,
        "url": "https://docs.databricks.com/en/libraries/workspace-files-libraries.html",
        "content": "Add dependent libraries to workflow tasks from workspace files\nYou can add dependent libraries to tasks from workspace files. See Configure dependent libraries.  \nTo configure a workflow task with a dependent library from workspace files:  \nSelect an existing task in a workflow or create a new task.  \nNext to Dependent libraries, click + Add.  \nIn the Add dependent library dialog, select Workspace for Library Source.  \nUpload the library or requirements.txt file, browse to the library or requirements.txt file in the workspace, or enter its workspace location in the Workspace File Path field, such as the following: /Workspace/Users/someone@example.com/<path-to-library>/<library-name>.<ext>  \nClick Install.\n\nInstall libraries from workspace files to a notebook"
    },
    {
        "id": 935,
        "url": "https://docs.databricks.com/en/libraries/workspace-files-libraries.html",
        "content": "Install libraries from workspace files to a notebook\nYou can install Python libraries directly to a notebook to create custom environments that are specific to the notebook. For example, you can use a specific version of a library in a notebook, without affecting other users on the cluster who may need a different version of the same library. For more information, see notebook-scoped libraries.  \nWhen you install a library to a notebook, only the current notebook and any jobs associated with that notebook have access to that library. Other notebooks attached to the same cluster are not affected."
    },
    {
        "id": 936,
        "url": "https://docs.databricks.com/en/release-notes/delta-live-tables/2024/29/index.html",
        "content": "Delta Live Tables release 2024.29  \nJuly 22 - 29, 2024  \nThese features and improvements were released with the 2024.29 release of Delta Live Tables.  \nDatabricks Runtime versions used by this release\nDatabricks Runtime versions used by this release\nChannel:  \nCURRENT (default): Databricks Runtime 14.1  \nPREVIEW: Databricks Runtime 14.3 or 15.2  \nNote  \nBecause Delta Live Tables channel releases follow a rolling upgrade process, channel upgrades are deployed to different regions at different times. Your release, including Databricks Runtime versions, might not be updated until a week or more after the initial release date. To find the Databricks Runtime version for a pipeline, see Runtime information.\n\nNew features and improvements in this release"
    },
    {
        "id": 937,
        "url": "https://docs.databricks.com/en/release-notes/delta-live-tables/2024/29/index.html",
        "content": "New features and improvements in this release\nThis release includes a change that improves the performance of regular and validate-only updates for pipelines with source code defined in Python notebooks. Because this change reduces the time to process each notebook, pipelines with many Python notebooks will see larger performance improvements.  \nThe Catalog Explorer UI is updated to show additional details for materialized views and streaming tables. You can now view the current refresh status of a materialized view or streaming table and a user-friendly display of the refresh schedule when a schedule is defined on the object. To learn about using Catalog Explorer to view materialized view and streaming table details, see What is Catalog Explorer?.\n\nBug fixes in this release\nBug fixes in this release\nThis release fixes a bug that could occur when a CREATE MATERIALIZED VIEW or CREATE STREAMING_TABLE statement failed. This bug caused the creation of unnecessary compute resources that performed duplicate work."
    },
    {
        "id": 938,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "Databricks Runtime 11.3 LTS  \nThe following release notes provide information about Databricks Runtime 11.3 LTS, powered by Apache Spark 3.3.0. Databricks released these images in October 2022.  \nNote  \nLTS means this version is under long-term support. See Databricks Runtime LTS version lifecycle.  \nBehavioral changes\nBehavioral changes\n[Breaking change] New Python version requires updating Databricks Connect V1 Python clients  \nTo apply required security patches, the Python version in Databricks Runtime 11.3 LTS is upgraded from 3.9.5 to 3.9.19. Because these changes might cause errors in clients that use specific PySpark functions, any clients that use Databricks Connect V1 for Python with Databricks Runtime 11.3 LTS must be updated to Python 3.9.7 or later.\n\nNew features and improvements"
    },
    {
        "id": 939,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "New features and improvements\nStructured Streaming trigger once is deprecated  \nChange source path for Auto Loader  \nDatabricks Kinesis connector now supports reading from Kinesis Data streams in EFO mode  \nNew H3 geospatial functions and added Photon support for all H3 functions  \nNew features for Predictive I/O  \nIncreasing initial partitions to scan for selective queries  \nNew AQE plan versions visualization  \nNew asynchronous progress tracking and log purging modes  \nStructured Streaming on Unity Catalog now supports display()  \nPipeline events are now logged in JSON format  \nArbitrary Stateful Processing in Structured Streaming with Python  \nDate inference in CSV files  \nClone support for Apache Parquet and Apache Iceberg tables (Public Preview)  \nUse SQL to specify schema- and catalog-level storage locations for Unity Catalog managed tables  \nStructured Streaming trigger once is deprecated  \nThe Trigger.Once setting has been deprecated. Databricks recommends you use Trigger.AvailableNow. See Configure Structured Streaming trigger intervals.  \nChange source path for Auto Loader  \nYou can now change the directory input path for Auto Loader configured with directory listing mode without having to choose a new checkpoint directory. See Change source path for Auto Loader.  \nDatabricks Kinesis connector now supports reading from Kinesis Data streams in EFO mode  \nYou can now use the Databricks Kinesis structured streaming source in Databricks Runtime 11.3 LTS to run queries that read from Kinesis Data streams in enhanced fan-out mode. This allows dedicated throughput per shard, per consumer and record delivery in push mode.  \nNew H3 geospatial functions and added Photon support for all H3 functions  \nIntroducing 4 new H3 functions, h3_maxchild, h3_minchild, h3_pointash3, and h3_pointash3string. These functions are available in SQL, Scala, and Python. All H3 expressions are now supported in Photon. See H3 geospatial functions.  \nNew features for Predictive I/O  \nPhoton supports range mode for running frames, using RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW. Photon also supports range mode for growing frames, using RANGE BETWEEN UNBOUNDED PRECEDING AND offset_stop { PRECEDING | FOLLOWING }.  \nIncreasing initial partitions to scan for selective queries"
    },
    {
        "id": 940,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "Increasing initial partitions to scan for selective queries  \nThe value of initial partitions to scan has been increased to 10 for selective query with take/tail/limit in Photon-enabled clusters and LIMIT in Databricks SQL. With 10 partitions, you can avoid the overhead of launching multiple small jobs and a slow scale-up. You can also configure this through spark.sql.limit.selectiveInitialNumPartitions.  \nNew AQE plan versions visualization  \nIntroducing AQE plan versions that allows you to visualize your runtime plan updates from adaptive query execution (AQE).  \nNew asynchronous progress tracking and log purging modes  \nIntroducing Structured Streaming modes called asynchronous progress tracking and asynchronous log purging. Asynchronous log purging mode lowers the latency of streaming queries by removing logs used for progress tracking in the background.  \nStructured Streaming on Unity Catalog now supports display()  \nYou can now use display() when you use Structured Streaming to work with tables registered in Unity Catalog.  \nPipeline events are now logged in JSON format  \nDatabricks now writes pipeline events to the driver log in JSON format. While each event will be JSON-parseable, large events may not contain all fields, or the fields may be truncated. Each event is logged in a single line with the prefix Event received: . The following is an example event.  \nEvent received: {\"id\":\"some-event-id\",\"origin\":{\"pipeline_id\":\"some-pipeline-id\",\"cluster_id\":\"some-cluster id\"},\"message\":\"simple [truncated] message\",\"level\":\"WARN\"}  \nArbitrary Stateful Processing in Structured Streaming with Python  \nIntroducing the applyInPandasWithState function that can be used to perform arbitrary stateful processing in PySpark. This is equivalent to the flatMapGroupsWithState function in the Java API.  \nDate inference in CSV files"
    },
    {
        "id": 941,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "Date inference in CSV files  \nIntroducing improved inference of date type columns in CSV files. When the date format is consistent across the records for a column, those columns can be inferred as DateType. You can also have a combination of date formats across different columns. Databricks can automatically infer the date format for each column. Date columns in CSV files prior to Databricks Runtime 11.3 LTS are left as StringType.  \nClone support for Apache Parquet and Apache Iceberg tables (Public Preview)  \nClone can now be used to create and incrementally update Delta tables that mirror Apache Parquet and Apache Iceberg tables. You can update your source Parquet table and incrementally apply the changes to their cloned Delta table with the clone command. See Incrementally clone Parquet and Iceberg tables to Delta Lake.  \nUse SQL to specify schema- and catalog-level storage locations for Unity Catalog managed tables  \nYou can now use the MANAGED LOCATION SQL command to specify a cloud storage location for managed tables at the catalog and schema levels. See CREATE CATALOG and CREATE SCHEMA."
    },
    {
        "id": 942,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "Behavior changes\nDatabricks Connect 11.3.2  \nDatabricks Connect client update 11.3.2 is now supported. See Databricks Connect and Databricks Connect release notes.  \nUpdated Databricks Snowflake connector  \nThe Databricks Snowflake connector has been updated to the latest version of code from the open-source repository, Snowflake Data Source for Apache Spark. It is now fully compatible with Databricks Runtime 11.3 LTS, including predicate pushdown and internal query plan pushdown while maintaining all of the features of the open-source version.  \nHadoop cache for S3A is now disabled  \nThe Hadoop cache (FileSystem Apache Hadoop Main 3.3.4 API) for S3A is now disabled. This is to align with other cloud storage connectors. For workloads that rely on file system caching, make sure that newly created file systems are supplied with the correct Hadoop configurations, including credential providers.  \nDelta Lake stats collection schema now matches column order in table schema definition  \nThis change addresses a bug in the Delta Lake protocol where stats were not collected for columns due to a mismatch in DataFrame and table column ordering. In some cases, you might encounter write performance degradation due to stats collection on previously untracked fields. See Data skipping for Delta Lake.  \napplyInPandasWithState throws an error if the query has a shuffle after the operator  \nThe operator applyInPandasWithState throws an error if the query has shuffle after the operator. This happens when either the user adds shuffle after the operation, or the optimizer or sink adds shuffle implicitly."
    },
    {
        "id": 943,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "Library upgrades\nUpgraded Python libraries:  \ndistlib from 0.3.5 to 0.3.6  \nUpgraded R libraries:  \nbroom from 1.0.0 to 1.0.1  \ncallr from 3.7.1 to 3.7.2  \ndplyr from 1.0.9 to 1.0.10  \ndtplyr from 1.2.1 to 1.2.2  \nforcats from 0.5.1 to 0.5.2  \nfuture from 1.27.0 to 1.28.0  \nfuture.apply from 1.9.0 to 1.9.1  \ngert from 1.7.0 to 1.8.0  \nglobals from 0.16.0 to 0.16.1  \ngtable from 0.3.0 to 0.3.1  \nhaven from 2.5.0 to 2.5.1  \nhms from 1.1.1 to 1.1.2  \nhttr from 1.4.3 to 1.4.4  \nknitr from 1.39 to 1.40  \nmodelr from 0.1.8 to 0.1.9  \npillar from 1.8.0 to 1.8.1  \nprogressr from 0.10.1 to 0.11.0  \nreadxl from 1.4.0 to 1.4.1  \nreprex from 2.0.1 to 2.0.2  \nrlang from 1.0.4 to 1.0.5  \nrmarkdown from 2.14 to 2.16  \nRSQLite from 2.2.15 to 2.2.16  \nrstudioapi from 0.13 to 0.14  \nrversions from 2.1.1 to 2.1.2  \nrvest from 1.0.2 to 1.0.3  \nscales from 1.2.0 to 1.2.1  \nsparklyr from 1.7.7 to 1.7.8  \nstringr from 1.4.0 to 1.4.1  \nsurvival from 3.2-13 to 3.4-0  \ntinytex from 0.40 to 0.41  \nviridisLite from 0.4.0 to 0.4.1  \nUpgraded Java libraries:  \ncom.fasterxml.jackson.core.jackson-annotations from 2.13.3 to 2.13.4"
    },
    {
        "id": 944,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "viridisLite from 0.4.0 to 0.4.1  \nUpgraded Java libraries:  \ncom.fasterxml.jackson.core.jackson-annotations from 2.13.3 to 2.13.4  \ncom.fasterxml.jackson.core.jackson-core from 2.13.3 to 2.13.4  \ncom.fasterxml.jackson.core.jackson-databind from 2.13.3 to 2.13.4  \ncom.fasterxml.jackson.dataformat.jackson-dataformat-cbor from 2.13.3 to 2.13.4  \ncom.fasterxml.jackson.datatype.jackson-datatype-joda from 2.13.3 to 2.13.4  \ncom.fasterxml.jackson.datatype.jackson-datatype-jsr310 from 2.13.3 to 2.13.4  \ncom.fasterxml.jackson.module.jackson-module-paranamer from 2.13.3 to 2.13.4  \ncom.fasterxml.jackson.module.jackson-module-scala_2.12 from 2.13.3 to 2.13.4  \norg.apache.hadoop.hadoop-client-api from 3.3.2-databricks to 3.3.4-databricks  \norg.apache.hadoop.hadoop-client-runtime from 3.3.2 to 3.3.4  \norg.apache.orc.orc-core from 1.7.5 to 1.7.6  \norg.apache.orc.orc-mapreduce from 1.7.5 to 1.7.6  \norg.apache.orc.orc-shims from 1.7.5 to 1.7.6  \norg.apache.parquet.parquet-column from 1.12.0-databricks-0004 to 1.12.0-databricks-0007  \norg.apache.parquet.parquet-common from 1.12.0-databricks-0004 to 1.12.0-databricks-0007"
    },
    {
        "id": 945,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "org.apache.parquet.parquet-common from 1.12.0-databricks-0004 to 1.12.0-databricks-0007  \norg.apache.parquet.parquet-encoding from 1.12.0-databricks-0004 to 1.12.0-databricks-0007  \norg.apache.parquet.parquet-format-structures from 1.12.0-databricks-0004 to 1.12.0-databricks-0007  \norg.apache.parquet.parquet-hadoop from 1.12.0-databricks-0004 to 1.12.0-databricks-0007  \norg.apache.parquet.parquet-jackson from 1.12.0-databricks-0004 to 1.12.0-databricks-0007  \norg.glassfish.jersey.containers.jersey-container-servlet from 2.34 to 2.36  \norg.glassfish.jersey.containers.jersey-container-servlet-core from 2.34 to 2.36  \norg.glassfish.jersey.core.jersey-client from 2.34 to 2.36  \norg.glassfish.jersey.core.jersey-common from 2.34 to 2.36  \norg.glassfish.jersey.core.jersey-server from 2.34 to 2.36  \norg.glassfish.jersey.inject.jersey-hk2 from 2.34 to 2.36"
    },
    {
        "id": 946,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "Apache Spark\nDatabricks Runtime 11.3 LTS includes Apache Spark 3.3.0. This release includes all Spark fixes and improvements included in Databricks Runtime 11.2 (EoS), as well as the following additional bug fixes and improvements made to Spark:  \n[SPARK-39957] [WARMFIX][SC-111425][CORE] Delay onDisconnected to enable Driver receives ExecutorExitCode  \n[SPARK-39955] [WARMFIX][SC-111424][CORE] Improve LaunchTask process to avoid Stage failures caused by fail-to-send LaunchTask messages  \n[SPARK-40474] [SC-106248][Cherry-Pick] Correct CSV schema inference behavior for datetime columns and introduce auto detection for Date fields  \n[SPARK-40535] [SC-111243][SQL] Fix bug the buffer of AggregatingAccumulator will not be created if the input rows is empty  \n[SPARK-40434] [SC-111125][SC-111144][SC-111138][SPARK-40435][11.3][SS][PYTHON] Implement applyInPandasWithState in PySpark  \n[SPARK-40460] [SC-110832][SS] Fix streaming metrics when selecting _metadata  \n[SPARK-40324] [SC-109943][SQL] Provide a query context of ParseException  \n[SPARK-40466] [SC-110899][SS] Improve the error message when DSv2 is disabled while DSv1 is not available  \n[SPARK-40456] [SC-110848][SQL] PartitionIterator.hasNext should be cheap to call repeatedly  \n[SPARK-40169] [SC-110772][SQL] Don\u2019t pushdown Parquet filters with no reference to data schema  \n[SPARK-40467] [SC-110759][SS] Split FlatMapGroupsWithState down to multiple test suites  \n[SPARK-40468] [SC-110813][SQL] Fix column pruning in CSV when corruptrecord is selected"
    },
    {
        "id": 947,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "[SPARK-40468] [SC-110813][SQL] Fix column pruning in CSV when corruptrecord is selected  \n[SPARK-40291] [SC-110085][SQL] Improve the message for column not in group by clause error  \n[SPARK-40398] [SC-110762][CORE][SQL] Use Loop instead of Arrays.stream api  \n[SPARK-40433] [SC-110684][SS][PYTHON] Add toJVMRow in PythonSQLUtils to convert pickled PySpark Row to JVM Row  \n[SPARK-40414] [SC-110568][SQL][PYTHON] More generic type on PythonArrowInput and PythonArrowOutput  \n[SPARK-40352] [SC-109945][SQL] Add function aliases: len, datepart, dateadd, date_diff and curdate  \n[SPARK-40470] [SC-110761][SQL] Handle GetArrayStructFields and GetMapValue in \u201carrays_zip\u201d function  \n[SPARK-40387] [SC-110685][SQL] Improve the implementation of Spark Decimal  \n[SPARK-40429] [SC-110675][SQL] Only set KeyGroupedPartitioning when the referenced column is in the output  \n[SPARK-40432] [SC-110716][SS][PYTHON] Introduce GroupStateImpl and GroupStateTimeout in PySpark  \n[SPARK-39915] [SC-110496][SQL] Ensure the output partitioning is user-specified in AQE  \n[SPARK-29260] [SQL] Support ALTER DATABASE SET LOCATION if HMS supports  \n[SPARK-40185] [SC-110056][SQL] Remove column suggestion when the candidate list is empty  \n[SPARK-40362] [SC-110401][SQL] Fix BinaryComparison canonicalization"
    },
    {
        "id": 948,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "[SPARK-40362] [SC-110401][SQL] Fix BinaryComparison canonicalization  \n[SPARK-40411] [SC-110381][SS] Refactor FlatMapGroupsWithStateExec to have a parent trait  \n[SPARK-40293] [SC-110084][SQL] Make the V2 table error message more meaningful  \n[SPARK-38734] [SC-110383][SQL] Remove the error class INDEX_OUT_OF_BOUNDS  \n[SPARK-40292] [SC-110300][SQL] Fix column names in \u201carrays_zip\u201d function when arrays are referenced from nested structs  \n[SPARK-40276] [SC-109674][CORE] Reduce the result size of RDD.takeOrdered  \n[SPARK-40197] [SC-109176][SQL] Replace query plan with context for MULTI_VALUE_SUBQUERY_ERROR  \n[SPARK-40300] [SC-109942][SQL] Migrate onto the DATATYPE_MISMATCH error class  \n[SPARK-40149] [SC-110055][SQL] Propagate metadata columns through Project  \n[SPARK-40280] [SC-110146][SQL] Add support for parquet push down for annotated int and long  \n[SPARK-40220] [SC-110143][SC-109175][SQL] Don\u2019t output the empty map of error message parameters  \n[SPARK-40295] [SC-110070][SQL] Allow v2 functions with literal args in write distribution/ordering  \n[SPARK-40156] [SC-109264][SQL] url_decode() should the return an error class  \n[SPARK-39195] [SQL] Spark OutputCommitCoordinator should abort stage when committed file not consistent with task status  \n[SPARK-40260] [SC-109424][SQL] Use error classes in the compilation errors of GROUP BY a position"
    },
    {
        "id": 949,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "[SPARK-40260] [SC-109424][SQL] Use error classes in the compilation errors of GROUP BY a position  \n[SPARK-40205] [SC-110144][SC-109082][SQL] Provide a query context of ELEMENT_AT_BY_INDEX_ZERO  \n[SPARK-40112] [SC-109676][SQL] Improve the TO_BINARY() function  \n[SPARK-40209] [SC-109081][SQL] Don\u2019t change the interval value of Decimal in changePrecision() on errors  \n[SPARK-40319] [SC-109873][SQL] Remove duplicated query execution error method for PARSE_DATETIME_BY_NEW_PARSER  \n[SPARK-40222] [SC-109209][SQL] Numeric try_add/try_divide/try_subtract/try_multiply should throw error from their children  \n[SPARK-40183] [SC-108907][SQL] Use error class NUMERIC_VALUE_OUT_OF_RANGE for overflow in decimal conversion  \n[SPARK-40180] [SC-109069][SQL] Format error messages by spark-sql  \n[SPARK-40153] [SC-109165][SQL] Unify resolve functions and table-valued functions  \n[SPARK-40308] [SC-109880][SQL] Allow non-foldable delimiter arguments to str_to_map function  \n[SPARK-40219] [SC-110052][SC-109663][SQL] Resolved view logical plan should hold the schema to avoid redundant lookup  \n[SPARK-40098] [SC-109939][SC-108693][SQL] Format error messages in the Thrift Server  \n[SPARK-39917] [SC-109038][SQL] Use different error classes for numeric/interval arithmetic overflow"
    },
    {
        "id": 950,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "[SPARK-39917] [SC-109038][SQL] Use different error classes for numeric/interval arithmetic overflow  \n[SPARK-40033] [SC-109875][SQL] Nested schema pruning support through element_at  \n[SPARK-40194] [SC-109660][SQL] SPLIT function on empty regex should truncate trailing empty string.  \n[SPARK-40228] [SC-109835][SQL] Do not simplify multiLike if child is not a cheap expression  \n[SPARK-40039] [SC-109896][SC-109260][SS] Introducing a streaming checkpoint file manager based on Hadoop\u2019s Abortable interface  \n[SPARK-40285] [SC-109679][SQL] Simplify the roundTo[Numeric] for Spark Decimal  \n[SPARK-39896] [SC-109658][SQL] UnwrapCastInBinaryComparison should work when the literal of In/InSet downcast failed  \n[SPARK-40040] [SC-109662][SQL] Push local limit to both sides if join condition is empty  \n[SPARK-40055] [SC-109075][SQL] listCatalogs should also return spark_catalog even when spark_catalog implementation is defaultSessionCatalog  \n[SPARK-39915] [SC-109391][SQL] Dataset.repartition(N) may not create N partitions Non-AQE part  \n[SPARK-40207] [SC-109401][SQL] Specify the column name when the data type is not supported by datasource  \n[SPARK-40245] [SC-109295][SQL] Fix FileScan equality check when partition or data filter columns are not read  \n[SPARK-40113] [SC-109405][SQL] Reactor ParquetScanBuilder DataSourceV2 interface implementations"
    },
    {
        "id": 951,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "[SPARK-40113] [SC-109405][SQL] Reactor ParquetScanBuilder DataSourceV2 interface implementations  \n[SPARK-40211] [SC-109226][CORE][SQL] Allow customize initial partitions number in take() behavior  \n[SPARK-40252] [SC-109379][SQL] Replace Stream.collect(Collectors.joining) with StringJoiner Api  \n[SPARK-40247] [SC-109272][SQL] Fix BitSet equality check  \n[SPARK-40067] [SQL] Use Table#name() instead of Scan#name() to populate the table name in the BatchScan node in SparkUI  \n[SPARK-39966] [SQL] Use V2 Filter in SupportsDelete  \n[SPARK-39607] [SC-109268][SQL][DSV2] Distribution and ordering support V2 function in writing  \n[SPARK-40224] [SC-109271][SQL] Make ObjectHashAggregateExec release memory eagerly when fallback to sort-based  \n[SPARK-40013] [SQL] DS V2 expressions should have the default toString  \n[SPARK-40214] [SC-109079][PYTHON][SQL] add \u2018get\u2019 to functions  \n[SPARK-40192] [SC-109089][SQL][ML] Remove redundant groupby  \n[SPARK-40146] [SC-108694][SQL] Simply the codegen of getting map value  \n[SPARK-40109] [SQL] New SQL function: get()  \n[SPARK-39929] [SQL] DS V2 supports push down string functions(non ANSI)  \n[SPARK-39819] [SQL] DS V2 aggregate push down can work with Top N or Paging (Sort with expressions)  \n[SPARK-40213] [SC-109077][SQL] Support ASCII value conversion for Latin-1 characters"
    },
    {
        "id": 952,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "[SPARK-40213] [SC-109077][SQL] Support ASCII value conversion for Latin-1 characters  \n[SPARK-39887] [SQL] RemoveRedundantAliases should keep aliases that make the output of projection nodes unique  \n[SPARK-39764] [SQL] Make PhysicalOperation the same as ScanOperation  \n[SPARK-39964] [SQL] DS V2 pushdown should unify the translate path  \n[SPARK-39528] [SQL] Use V2 Filter in SupportsRuntimeFiltering  \n[SPARK-40066] [SQL] ANSI mode: always return null on invalid access to map column  \n[SPARK-39912] [SPARK-39828][SQL] Refine CatalogImpl  \n[SPARK-39833] [SC-108736][SQL] Disable Parquet column index in DSv1 to fix a correctness issue in the case of overlapping partition and data columns  \n[SPARK-39880] [SQL] V2 SHOW FUNCTIONS command should print qualified function name like v1  \n[SPARK-39767] [SQL] Remove UnresolvedDBObjectName and add UnresolvedIdentifier  \n[SPARK-40163] [SC-108740][SQL] feat: SparkSession.config(Map)  \n[SPARK-40136] [SQL] Fix the fragment of SQL query contexts  \n[SPARK-40107] [SC-108689][SQL] Pull out empty2null conversion from FileFormatWriter  \n[SPARK-40121] [PYTHON][SQL] Initialize projection used for Python UDF  \n[SPARK-40128] [SQL] Make the VectorizedColumnReader recognize DELTA_LENGTH_BYTE_ARRAY as a standalone column encoding  \n[SPARK-40132] [ML] Restore rawPredictionCol to MultilayerPerceptronClassifier.setParams  \n[SPARK-40050] [SC-108696][SQL] Enhance EliminateSorts to support removing sorts via LocalLimit"
    },
    {
        "id": 953,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "[SPARK-40050] [SC-108696][SQL] Enhance EliminateSorts to support removing sorts via LocalLimit  \n[SPARK-39629] [SQL] Support v2 SHOW FUNCTIONS  \n[SPARK-39925] [SC-108734][SQL] Add array_sort(column, comparator) overload to DataFrame operations  \n[SPARK-40117] [PYTHON][SQL] Convert condition to java in DataFrameWriterV2.overwrite  \n[SPARK-40105] [SQL] Improve repartition in ReplaceCTERefWithRepartition  \n[SPARK-39503] [SQL] Add session catalog name for v1 database table and function  \n[SPARK-39889] [SQL] Use different error classes for numeric/interval divided by 0  \n[SPARK-39741] [SQL] Support url encode/decode as built-in function and tidy up url-related functions  \n[SPARK-40102] [SQL] Use SparkException instead of IllegalStateException in SparkPlan  \n[SPARK-40014] [SQL] Support cast of decimals to ANSI intervals  \n[SPARK-39776] [SQL][FOLLOW] Update UT of PlanStabilitySuite in ANSI mode  \n[SPARK-39963] [SQL] Simplify SimplifyCasts.isWiderCast"
    },
    {
        "id": 954,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "System environment\nOperating System: Ubuntu 20.04.5 LTS  \nJava: Zulu 8.56.0.21-CA-linux64  \nScala: 2.12.14  \nPython: 3.9.19  \nR: 4.1.3  \nDelta Lake: 2.1.0  \nInstalled Python libraries  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nargon2-cffi  \n20.1.0  \nasync-generator  \n1.10  \nattrs  \n21.2.0  \nbackcall  \n0.2.0  \nbackports.entry-points-selectable  \n1.1.1  \nblack  \n22.3.0  \nbleach  \n4.0.0  \nboto3  \n1.21.18  \nbotocore  \n1.24.18  \ncertifi  \n2021.10.8  \ncffi  \n1.14.6  \nchardet  \n4.0.0  \ncharset-normalizer  \n2.0.4  \nclick  \n8.0.3  \ncryptography  \n3.4.8  \ncycler  \n0.10.0  \nCython  \n0.29.24  \ndbus-python  \n1.2.16  \ndebugpy  \n1.4.1  \ndecorator  \n5.1.0  \ndefusedxml  \n0.7.1  \ndistlib  \n0.3.6  \nentrypoints  \n0.3  \nfacets-overview  \n1.0.0  \nfilelock  \n3.8.0  \nidna  \n3.2  \nipykernel  \n6.12.1  \nipython  \n7.32.0  \nipython-genutils  \n0.2.0  \nipywidgets  \n7.7.0  \njedi  \n0.18.0  \nJinja2  \n2.11.3  \njmespath  \n0.10.0  \njoblib  \n1.0.1  \njsonschema  \n3.2.0  \njupyter-client  \n6.1.12  \njupyter-core  \n4.8.1  \njupyterlab-pygments  \n0.1.2  \njupyterlab-widgets  \n1.0.0  \nkiwisolver  \n1.3.1  \nMarkupSafe  \n2.0.1  \nmatplotlib  \n3.4.3  \nmatplotlib-inline  \n0.1.2  \nmistune  \n0.8.4  \nmypy-extensions  \n0.4.3  \nnbclient  \n0.5.3  \nnbconvert  \n6.1.0  \nnbformat  \n5.1.3  \nnest-asyncio  \n1.5.1  \nnotebook  \n6.4.5"
    },
    {
        "id": 955,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "mypy-extensions  \n0.4.3  \nnbclient  \n0.5.3  \nnbconvert  \n6.1.0  \nnbformat  \n5.1.3  \nnest-asyncio  \n1.5.1  \nnotebook  \n6.4.5  \nnumpy  \n1.20.3  \npackaging  \n21.0  \npandas  \n1.3.4  \npandocfilters  \n1.4.3  \nparso  \n0.8.2  \npathspec  \n0.9.0  \npatsy  \n0.5.2  \npexpect  \n4.8.0  \npickleshare  \n0.7.5  \nPillow  \n8.4.0  \npip  \n21.2.4  \nplatformdirs  \n2.5.2  \nplotly  \n5.9.0  \nprometheus-client  \n0.11.0  \nprompt-toolkit  \n3.0.20  \nprotobuf  \n4.21.5  \npsutil  \n5.8.0  \npsycopg2  \n2.9.3  \nptyprocess  \n0.7.0  \npyarrow  \n7.0.0  \npycparser  \n2.20  \nPygments  \n2.10.0  \nPyGObject  \n3.36.0  \npyodbc  \n4.0.31  \npyparsing  \n3.0.4  \npyrsistent  \n0.18.0  \npython-dateutil  \n2.8.2  \npytz  \n2021.3  \npyzmq  \n22.2.1  \nrequests  \n2.26.0  \nrequests-unixsocket  \n0.2.0  \ns3transfer  \n0.5.2  \nscikit-learn  \n0.24.2  \nscipy  \n1.7.1  \nseaborn  \n0.11.2  \nSend2Trash  \n1.8.0  \nsetuptools  \n58.0.4  \nsix  \n1.16.0  \nssh-import-id  \n5.10  \nstatsmodels  \n0.12.2  \ntenacity  \n8.0.1  \nterminado  \n0.9.4  \ntestpath  \n0.5.0  \nthreadpoolctl  \n2.2.0  \ntokenize-rt  \n4.2.1  \ntomli  \n2.0.1  \ntornado  \n6.1  \ntraitlets  \n5.1.0  \ntyping-extensions  \n3.10.0.2  \nunattended-upgrades  \n0.1  \nurllib3  \n1.26.7  \nvirtualenv  \n20.8.0  \nwcwidth  \n0.2.5  \nwebencodings  \n0.5.1  \nwheel  \n0.37.0  \nwidgetsnbextension  \n3.6.0  \nInstalled R libraries"
    },
    {
        "id": 956,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "virtualenv  \n20.8.0  \nwcwidth  \n0.2.5  \nwebencodings  \n0.5.1  \nwheel  \n0.37.0  \nwidgetsnbextension  \n3.6.0  \nInstalled R libraries  \nR libraries are installed from the Microsoft CRAN snapshot on 2022-09-08.  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \naskpass  \n1.1  \nassertthat  \n0.2.1  \nbackports  \n1.4.1  \nbase  \n4.1.3  \nbase64enc  \n0.1-3  \nbit  \n4.0.4  \nbit64  \n4.0.5  \nblob  \n1.2.3  \nboot  \n1.3-28  \nbrew  \n1.0-7  \nbrio  \n1.1.3  \nbroom  \n1.0.1  \nbslib  \n0.4.0  \ncachem  \n1.0.6  \ncallr  \n3.7.2  \ncaret  \n6.0-93  \ncellranger  \n1.1.0  \nchron  \n2.3-57  \nclass  \n7.3-20  \ncli  \n3.3.0  \nclipr  \n0.8.0  \ncluster  \n2.1.3  \ncodetools  \n0.2-18  \ncolorspace  \n2.0-3  \ncommonmark  \n1.8.0  \ncompiler  \n4.1.3  \nconfig  \n0.3.1  \ncpp11  \n0.4.2  \ncrayon  \n1.5.1  \ncredentials  \n1.3.2  \ncurl  \n4.3.2  \ndata.table  \n1.14.2  \ndatasets  \n4.1.3  \nDBI  \n1.1.3  \ndbplyr  \n2.2.1  \ndesc  \n1.4.1  \ndevtools  \n2.4.4  \ndiffobj  \n0.3.5  \ndigest  \n0.6.29  \ndownlit  \n0.4.2  \ndplyr  \n1.0.10  \ndtplyr  \n1.2.2  \ne1071  \n1.7-11  \nellipsis  \n0.3.2  \nevaluate  \n0.16  \nfansi  \n1.0.3  \nfarver  \n2.1.1  \nfastmap  \n1.1.0  \nfontawesome  \n0.3.0  \nforcats  \n0.5.2  \nforeach  \n1.5.2  \nforeign  \n0.8-82  \nforge  \n0.2.0  \nfs  \n1.5.2  \nfuture  \n1.28.0  \nfuture.apply  \n1.9.1  \ngargle  \n1.2.0  \ngenerics  \n0.1.3  \ngert  \n1.8.0  \nggplot2  \n3.3.6  \ngh  \n1.3.0"
    },
    {
        "id": 957,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "future  \n1.28.0  \nfuture.apply  \n1.9.1  \ngargle  \n1.2.0  \ngenerics  \n0.1.3  \ngert  \n1.8.0  \nggplot2  \n3.3.6  \ngh  \n1.3.0  \ngitcreds  \n0.1.1  \nglmnet  \n4.1-4  \nglobals  \n0.16.1  \nglue  \n1.6.2  \ngoogledrive  \n2.0.0  \ngooglesheets4  \n1.0.1  \ngower  \n1.0.0  \ngraphics  \n4.1.3  \ngrDevices  \n4.1.3  \ngrid  \n4.1.3  \ngridExtra  \n2.3  \ngsubfn  \n0.7  \ngtable  \n0.3.1  \nhardhat  \n1.2.0  \nhaven  \n2.5.1  \nhighr  \n0.9  \nhms  \n1.1.2  \nhtmltools  \n0.5.3  \nhtmlwidgets  \n1.5.4  \nhttpuv  \n1.6.5  \nhttr  \n1.4.4  \nids  \n1.0.1  \nini  \n0.3.1  \nipred  \n0.9-13  \nisoband  \n0.2.5  \niterators  \n1.0.14  \njquerylib  \n0.1.4  \njsonlite  \n1.8.0  \nKernSmooth  \n2.23-20  \nknitr  \n1.40  \nlabeling  \n0.4.2  \nlater  \n1.3.0  \nlattice  \n0.20-45  \nlava  \n1.6.10  \nlifecycle  \n1.0.1  \nlistenv  \n0.8.0  \nlubridate  \n1.8.0  \nmagrittr  \n2.0.3  \nmarkdown  \n1.1  \nMASS  \n7.3-56  \nMatrix  \n1.4-1  \nmemoise  \n2.0.1  \nmethods  \n4.1.3  \nmgcv  \n1.8-40  \nmime  \n0.12  \nminiUI  \n0.1.1.1  \nModelMetrics  \n1.2.2.2  \nmodelr  \n0.1.9  \nmunsell  \n0.5.0  \nnlme  \n3.1-157  \nnnet  \n7.3-17  \nnumDeriv  \n2016.8-1.1  \nopenssl  \n2.0.2  \nparallel  \n4.1.3  \nparallelly  \n1.32.1  \npillar  \n1.8.1  \npkgbuild  \n1.3.1  \npkgconfig  \n2.0.3  \npkgdown  \n2.0.6  \npkgload  \n1.3.0  \nplogr"
    },
    {
        "id": 958,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "pillar  \n1.8.1  \npkgbuild  \n1.3.1  \npkgconfig  \n2.0.3  \npkgdown  \n2.0.6  \npkgload  \n1.3.0  \nplogr  \n0.2.0  \nplyr  \n1.8.7  \npraise  \n1.0.0  \nprettyunits  \n1.1.1  \npROC  \n1.18.0  \nprocessx  \n3.7.0  \nprodlim  \n2019.11.13  \nprofvis  \n0.3.7  \nprogress  \n1.2.2  \nprogressr  \n0.11.0  \npromises  \n1.2.0.1  \nproto  \n1.0.0  \nproxy  \n0.4-27  \nps  \n1.7.1  \npurrr  \n0.3.4  \nr2d3  \n0.2.6  \nR6  \n2.5.1  \nragg  \n1.2.2  \nrandomForest  \n4.7-1.1  \nrappdirs  \n0.3.3  \nrcmdcheck  \n1.4.0  \nRColorBrewer  \n1.1-3  \nRcpp  \n1.0.9  \nRcppEigen  \n0.3.3.9.2  \nreadr  \n2.1.2  \nreadxl  \n1.4.1  \nrecipes  \n1.0.1  \nrematch  \n1.0.1  \nrematch2  \n2.1.2  \nremotes  \n2.4.2  \nreprex  \n2.0.2  \nreshape2  \n1.4.4  \nrlang  \n1.0.5  \nrmarkdown  \n2.16  \nRODBC  \n1.3-19  \nroxygen2  \n7.2.1  \nrpart  \n4.1.16  \nrprojroot  \n2.0.3  \nRserve  \n1.8-11  \nRSQLite  \n2.2.16  \nrstudioapi  \n0.14  \nrversions  \n2.1.2  \nrvest  \n1.0.3  \nsass  \n0.4.2  \nscales  \n1.2.1  \nselectr  \n0.4-2  \nsessioninfo  \n1.2.2  \nshape  \n1.4.6  \nshiny  \n1.7.2  \nsourcetools  \n0.1.7  \nsparklyr  \n1.7.8  \nSparkR  \n3.3.0  \nspatial  \n7.3-11  \nsplines  \n4.1.3  \nsqldf  \n0.4-11  \nSQUAREM  \n2021.1  \nstats  \n4.1.3  \nstats4  \n4.1.3  \nstringi  \n1.7.8  \nstringr  \n1.4.1  \nsurvival"
    },
    {
        "id": 959,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "sqldf  \n0.4-11  \nSQUAREM  \n2021.1  \nstats  \n4.1.3  \nstats4  \n4.1.3  \nstringi  \n1.7.8  \nstringr  \n1.4.1  \nsurvival  \n3.4-0  \nsys  \n3.4  \nsystemfonts  \n1.0.4  \ntcltk  \n4.1.3  \ntestthat  \n3.1.4  \ntextshaping  \n0.3.6  \ntibble  \n3.1.8  \ntidyr  \n1.2.0  \ntidyselect  \n1.1.2  \ntidyverse  \n1.3.2  \ntimeDate  \n4021.104  \ntinytex  \n0.41  \ntools  \n4.1.3  \ntzdb  \n0.3.0  \nurlchecker  \n1.0.1  \nusethis  \n2.1.6  \nutf8  \n1.2.2  \nutils  \n4.1.3  \nuuid  \n1.1-0  \nvctrs  \n0.4.1  \nviridisLite  \n0.4.1  \nvroom  \n1.5.7  \nwaldo  \n0.4.0  \nwhisker  \n0.4  \nwithr  \n2.5.0  \nxfun  \n0.32  \nxml2  \n1.3.3  \nxopen  \n1.0.0  \nxtable  \n1.8-4  \nyaml  \n2.3.5  \nzip  \n2.2.0  \nInstalled Java and Scala libraries (Scala 2.12 cluster version)  \nGroup ID  \nArtifact ID  \nVersion  \nantlr  \nantlr  \n2.7.7  \ncom.amazonaws  \namazon-kinesis-client  \n1.12.0  \ncom.amazonaws  \naws-java-sdk-autoscaling  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudformation  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudfront  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudhsm  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudsearch  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudtrail  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudwatch  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudwatchmetrics"
    },
    {
        "id": 960,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudwatch  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudwatchmetrics  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-codedeploy  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cognitoidentity  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cognitosync  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-config  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-core  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-datapipeline  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-directconnect  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-directory  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-dynamodb  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-ec2  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-ecs  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-efs  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-elasticache  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-elasticbeanstalk  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-elasticloadbalancing  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-elastictranscoder  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-emr  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-glacier  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-glue"
    },
    {
        "id": 961,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "1.12.189  \ncom.amazonaws  \naws-java-sdk-glacier  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-glue  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-iam  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-importexport  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-kinesis  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-kms  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-lambda  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-logs  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-machinelearning  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-opsworks  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-rds  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-redshift  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-route53  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-s3  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-ses  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-simpledb  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-simpleworkflow  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-sns  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-sqs  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-ssm  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-storagegateway  \n1.12.189"
    },
    {
        "id": 962,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "com.amazonaws  \naws-java-sdk-ssm  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-storagegateway  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-sts  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-support  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-swf-libraries  \n1.11.22  \ncom.amazonaws  \naws-java-sdk-workspaces  \n1.12.189  \ncom.amazonaws  \njmespath-java  \n1.12.189  \ncom.chuusai  \nshapeless_2.12  \n2.3.3  \ncom.clearspring.analytics  \nstream  \n2.9.6  \ncom.databricks  \nRserve  \n1.8-3  \ncom.databricks  \njets3t  \n0.7.1-0  \ncom.databricks.scalapb  \ncompilerplugin_2.12  \n0.4.15-10  \ncom.databricks.scalapb  \nscalapb-runtime_2.12  \n0.4.15-10  \ncom.esotericsoftware  \nkryo-shaded  \n4.0.2  \ncom.esotericsoftware  \nminlog  \n1.3.0  \ncom.fasterxml  \nclassmate  \n1.3.4  \ncom.fasterxml.jackson.core  \njackson-annotations  \n2.13.4  \ncom.fasterxml.jackson.core  \njackson-core  \n2.13.4  \ncom.fasterxml.jackson.core  \njackson-databind  \n2.13.4  \ncom.fasterxml.jackson.dataformat  \njackson-dataformat-cbor  \n2.13.4  \ncom.fasterxml.jackson.datatype  \njackson-datatype-joda  \n2.13.4  \ncom.fasterxml.jackson.datatype  \njackson-datatype-jsr310  \n2.13.4  \ncom.fasterxml.jackson.module  \njackson-module-paranamer  \n2.13.4  \ncom.fasterxml.jackson.module  \njackson-module-scala_2.12  \n2.13.4  \ncom.github.ben-manes.caffeine"
    },
    {
        "id": 963,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "jackson-module-paranamer  \n2.13.4  \ncom.fasterxml.jackson.module  \njackson-module-scala_2.12  \n2.13.4  \ncom.github.ben-manes.caffeine  \ncaffeine  \n2.3.4  \ncom.github.fommil  \njniloader  \n1.1  \ncom.github.fommil.netlib  \ncore  \n1.1.2  \ncom.github.fommil.netlib  \nnative_ref-java  \n1.1  \ncom.github.fommil.netlib  \nnative_ref-java-natives  \n1.1  \ncom.github.fommil.netlib  \nnative_system-java  \n1.1  \ncom.github.fommil.netlib  \nnative_system-java-natives  \n1.1  \ncom.github.fommil.netlib  \nnetlib-native_ref-linux-x86_64-natives  \n1.1  \ncom.github.fommil.netlib  \nnetlib-native_system-linux-x86_64-natives  \n1.1  \ncom.github.luben  \nzstd-jni  \n1.5.2-1  \ncom.github.wendykierp  \nJTransforms  \n3.1  \ncom.google.code.findbugs  \njsr305  \n3.0.0  \ncom.google.code.gson  \ngson  \n2.8.6  \ncom.google.crypto.tink  \ntink  \n1.6.1  \ncom.google.flatbuffers  \nflatbuffers-java  \n1.12.0  \ncom.google.guava  \nguava  \n15.0  \ncom.google.protobuf  \nprotobuf-java  \n2.6.1  \ncom.h2database  \nh2  \n2.0.204  \ncom.helger  \nprofiler  \n1.1.1  \ncom.jcraft  \njsch  \n0.1.50  \ncom.jolbox  \nbonecp  \n0.8.0.RELEASE  \ncom.lihaoyi  \nsourcecode_2.12  \n0.1.9  \ncom.microsoft.azure  \nazure-data-lake-store-sdk  \n2.3.9  \ncom.microsoft.sqlserver  \nmssql-jdbc  \n9.2.1.jre8  \ncom.ning  \ncompress-lzf  \n1.1"
    },
    {
        "id": 964,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "2.3.9  \ncom.microsoft.sqlserver  \nmssql-jdbc  \n9.2.1.jre8  \ncom.ning  \ncompress-lzf  \n1.1  \ncom.sun.mail  \njavax.mail  \n1.5.2  \ncom.tdunning  \njson  \n1.8  \ncom.thoughtworks.paranamer  \nparanamer  \n2.8  \ncom.trueaccord.lenses  \nlenses_2.12  \n0.4.12  \ncom.twitter  \nchill-java  \n0.10.0  \ncom.twitter  \nchill_2.12  \n0.10.0  \ncom.twitter  \nutil-app_2.12  \n7.1.0  \ncom.twitter  \nutil-core_2.12  \n7.1.0  \ncom.twitter  \nutil-function_2.12  \n7.1.0  \ncom.twitter  \nutil-jvm_2.12  \n7.1.0  \ncom.twitter  \nutil-lint_2.12  \n7.1.0  \ncom.twitter  \nutil-registry_2.12  \n7.1.0  \ncom.twitter  \nutil-stats_2.12  \n7.1.0  \ncom.typesafe  \nconfig  \n1.2.1  \ncom.typesafe.scala-logging  \nscala-logging_2.12  \n3.7.2  \ncom.uber  \nh3  \n3.7.0  \ncom.univocity  \nunivocity-parsers  \n2.9.1  \ncom.zaxxer  \nHikariCP  \n4.0.3  \ncommons-cli  \ncommons-cli  \n1.5.0  \ncommons-codec  \ncommons-codec  \n1.15  \ncommons-collections  \ncommons-collections  \n3.2.2  \ncommons-dbcp  \ncommons-dbcp  \n1.4  \ncommons-fileupload  \ncommons-fileupload  \n1.3.3  \ncommons-httpclient  \ncommons-httpclient  \n3.1  \ncommons-io  \ncommons-io  \n2.11.0  \ncommons-lang  \ncommons-lang  \n2.6  \ncommons-logging  \ncommons-logging  \n1.1.3  \ncommons-pool  \ncommons-pool  \n1.5.4  \ndev.ludovic.netlib  \narpack  \n2.2.1  \ndev.ludovic.netlib  \nblas  \n2.2.1  \ndev.ludovic.netlib  \nlapack  \n2.2.1  \ninfo.ganglia.gmetric4j  \ngmetric4j"
    },
    {
        "id": 965,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "dev.ludovic.netlib  \nblas  \n2.2.1  \ndev.ludovic.netlib  \nlapack  \n2.2.1  \ninfo.ganglia.gmetric4j  \ngmetric4j  \n1.0.10  \nio.airlift  \naircompressor  \n0.21  \nio.delta  \ndelta-sharing-spark_2.12  \n0.5.1  \nio.dropwizard.metrics  \nmetrics-core  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-graphite  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-healthchecks  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jetty9  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jmx  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-json  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jvm  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-servlets  \n4.1.1  \nio.netty  \nnetty-all  \n4.1.74.Final  \nio.netty  \nnetty-buffer  \n4.1.74.Final  \nio.netty  \nnetty-codec  \n4.1.74.Final  \nio.netty  \nnetty-common  \n4.1.74.Final  \nio.netty  \nnetty-handler  \n4.1.74.Final  \nio.netty  \nnetty-resolver  \n4.1.74.Final  \nio.netty  \nnetty-tcnative-classes  \n2.0.48.Final  \nio.netty  \nnetty-transport  \n4.1.74.Final  \nio.netty  \nnetty-transport-classes-epoll  \n4.1.74.Final  \nio.netty  \nnetty-transport-classes-kqueue  \n4.1.74.Final  \nio.netty  \nnetty-transport-native-epoll-linux-aarch_64  \n4.1.74.Final  \nio.netty  \nnetty-transport-native-epoll-linux-x86_64  \n4.1.74.Final  \nio.netty  \nnetty-transport-native-kqueue-osx-aarch_64  \n4.1.74.Final  \nio.netty"
    },
    {
        "id": 966,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "4.1.74.Final  \nio.netty  \nnetty-transport-native-kqueue-osx-aarch_64  \n4.1.74.Final  \nio.netty  \nnetty-transport-native-kqueue-osx-x86_64  \n4.1.74.Final  \nio.netty  \nnetty-transport-native-unix-common  \n4.1.74.Final  \nio.prometheus  \nsimpleclient  \n0.7.0  \nio.prometheus  \nsimpleclient_common  \n0.7.0  \nio.prometheus  \nsimpleclient_dropwizard  \n0.7.0  \nio.prometheus  \nsimpleclient_pushgateway  \n0.7.0  \nio.prometheus  \nsimpleclient_servlet  \n0.7.0  \nio.prometheus.jmx  \ncollector  \n0.12.0  \njakarta.annotation  \njakarta.annotation-api  \n1.3.5  \njakarta.servlet  \njakarta.servlet-api  \n4.0.3  \njakarta.validation  \njakarta.validation-api  \n2.0.2  \njakarta.ws.rs  \njakarta.ws.rs-api  \n2.1.6  \njavax.activation  \nactivation  \n1.1.1  \njavax.annotation  \njavax.annotation-api  \n1.3.2  \njavax.el  \njavax.el-api  \n2.2.4  \njavax.jdo  \njdo-api  \n3.0.1  \njavax.transaction  \njta  \n1.1  \njavax.transaction  \ntransaction-api  \n1.1  \njavax.xml.bind  \njaxb-api  \n2.2.11  \njavolution  \njavolution  \n5.5.1  \njline  \njline  \n2.14.6  \njoda-time  \njoda-time  \n2.10.13  \nnet.java.dev.jna  \njna  \n5.8.0  \nnet.razorvine  \npickle  \n1.2  \nnet.sf.jpam  \njpam  \n1.1  \nnet.sf.opencsv  \nopencsv  \n2.3  \nnet.sf.supercsv  \nsuper-csv  \n2.2.0  \nnet.snowflake  \nsnowflake-ingest-sdk  \n0.9.6  \nnet.snowflake  \nsnowflake-jdbc  \n3.13.14  \nnet.sourceforge.f2j  \narpack_combined_all  \n0.1"
    },
    {
        "id": 967,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "net.snowflake  \nsnowflake-ingest-sdk  \n0.9.6  \nnet.snowflake  \nsnowflake-jdbc  \n3.13.14  \nnet.sourceforge.f2j  \narpack_combined_all  \n0.1  \norg.acplt.remotetea  \nremotetea-oncrpc  \n1.1.2  \norg.antlr  \nST4  \n4.0.4  \norg.antlr  \nantlr-runtime  \n3.5.2  \norg.antlr  \nantlr4-runtime  \n4.8  \norg.antlr  \nstringtemplate  \n3.2.1  \norg.apache.ant  \nant  \n1.9.2  \norg.apache.ant  \nant-jsch  \n1.9.2  \norg.apache.ant  \nant-launcher  \n1.9.2  \norg.apache.arrow  \narrow-format  \n7.0.0  \norg.apache.arrow  \narrow-memory-core  \n7.0.0  \norg.apache.arrow  \narrow-memory-netty  \n7.0.0  \norg.apache.arrow  \narrow-vector  \n7.0.0  \norg.apache.avro  \navro  \n1.11.0  \norg.apache.avro  \navro-ipc  \n1.11.0  \norg.apache.avro  \navro-mapred  \n1.11.0  \norg.apache.commons  \ncommons-collections4  \n4.4  \norg.apache.commons  \ncommons-compress  \n1.21  \norg.apache.commons  \ncommons-crypto  \n1.1.0  \norg.apache.commons  \ncommons-lang3  \n3.12.0  \norg.apache.commons  \ncommons-math3  \n3.6.1  \norg.apache.commons  \ncommons-text  \n1.9  \norg.apache.curator  \ncurator-client  \n2.13.0  \norg.apache.curator  \ncurator-framework  \n2.13.0  \norg.apache.curator  \ncurator-recipes  \n2.13.0  \norg.apache.derby  \nderby  \n10.14.2.0  \norg.apache.hadoop  \nhadoop-client-api  \n3.3.4-databricks  \norg.apache.hadoop  \nhadoop-client-runtime  \n3.3.4  \norg.apache.hive  \nhive-beeline  \n2.3.9  \norg.apache.hive  \nhive-cli  \n2.3.9  \norg.apache.hive  \nhive-jdbc  \n2.3.9  \norg.apache.hive  \nhive-llap-client  \n2.3.9"
    },
    {
        "id": 968,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "2.3.9  \norg.apache.hive  \nhive-cli  \n2.3.9  \norg.apache.hive  \nhive-jdbc  \n2.3.9  \norg.apache.hive  \nhive-llap-client  \n2.3.9  \norg.apache.hive  \nhive-llap-common  \n2.3.9  \norg.apache.hive  \nhive-serde  \n2.3.9  \norg.apache.hive  \nhive-shims  \n2.3.9  \norg.apache.hive  \nhive-storage-api  \n2.7.2  \norg.apache.hive.shims  \nhive-shims-0.23  \n2.3.9  \norg.apache.hive.shims  \nhive-shims-common  \n2.3.9  \norg.apache.hive.shims  \nhive-shims-scheduler  \n2.3.9  \norg.apache.httpcomponents  \nhttpclient  \n4.5.13  \norg.apache.httpcomponents  \nhttpcore  \n4.4.14  \norg.apache.ivy  \nivy  \n2.5.0  \norg.apache.logging.log4j  \nlog4j-1.2-api  \n2.18.0  \norg.apache.logging.log4j  \nlog4j-api  \n2.18.0  \norg.apache.logging.log4j  \nlog4j-core  \n2.18.0  \norg.apache.logging.log4j  \nlog4j-slf4j-impl  \n2.18.0  \norg.apache.mesos  \nmesos-shaded-protobuf  \n1.4.0  \norg.apache.orc  \norc-core  \n1.7.6  \norg.apache.orc  \norc-mapreduce  \n1.7.6  \norg.apache.orc  \norc-shims  \n1.7.6  \norg.apache.parquet  \nparquet-column  \n1.12.0-databricks-0007  \norg.apache.parquet  \nparquet-common  \n1.12.0-databricks-0007  \norg.apache.parquet  \nparquet-encoding  \n1.12.0-databricks-0007  \norg.apache.parquet  \nparquet-format-structures  \n1.12.0-databricks-0007  \norg.apache.parquet  \nparquet-hadoop  \n1.12.0-databricks-0007  \norg.apache.parquet  \nparquet-jackson  \n1.12.0-databricks-0007"
    },
    {
        "id": 969,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "org.apache.parquet  \nparquet-hadoop  \n1.12.0-databricks-0007  \norg.apache.parquet  \nparquet-jackson  \n1.12.0-databricks-0007  \norg.apache.thrift  \nlibfb303  \n0.9.3  \norg.apache.thrift  \nlibthrift  \n0.12.0  \norg.apache.xbean  \nxbean-asm9-shaded  \n4.20  \norg.apache.yetus  \naudience-annotations  \n0.5.0  \norg.apache.zookeeper  \nzookeeper  \n3.6.2  \norg.apache.zookeeper  \nzookeeper-jute  \n3.6.2  \norg.checkerframework  \nchecker-qual  \n3.5.0  \norg.codehaus.jackson  \njackson-core-asl  \n1.9.13  \norg.codehaus.jackson  \njackson-mapper-asl  \n1.9.13  \norg.codehaus.janino  \ncommons-compiler  \n3.0.16  \norg.codehaus.janino  \njanino  \n3.0.16  \norg.datanucleus  \ndatanucleus-api-jdo  \n4.2.4  \norg.datanucleus  \ndatanucleus-core  \n4.1.17  \norg.datanucleus  \ndatanucleus-rdbms  \n4.1.19  \norg.datanucleus  \njavax.jdo  \n3.2.0-m3  \norg.eclipse.jetty  \njetty-client  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-continuation  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-http  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-io  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-jndi  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-plus  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-proxy  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-security  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-server  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-servlet"
    },
    {
        "id": 970,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "org.eclipse.jetty  \njetty-security  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-server  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-servlet  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-servlets  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-util  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-util-ajax  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-webapp  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-xml  \n9.4.46.v20220331  \norg.eclipse.jetty.websocket  \nwebsocket-api  \n9.4.46.v20220331  \norg.eclipse.jetty.websocket  \nwebsocket-client  \n9.4.46.v20220331  \norg.eclipse.jetty.websocket  \nwebsocket-common  \n9.4.46.v20220331  \norg.eclipse.jetty.websocket  \nwebsocket-server  \n9.4.46.v20220331  \norg.eclipse.jetty.websocket  \nwebsocket-servlet  \n9.4.46.v20220331  \norg.fusesource.leveldbjni  \nleveldbjni-all  \n1.8  \norg.glassfish.hk2  \nhk2-api  \n2.6.1  \norg.glassfish.hk2  \nhk2-locator  \n2.6.1  \norg.glassfish.hk2  \nhk2-utils  \n2.6.1  \norg.glassfish.hk2  \nosgi-resource-locator  \n1.0.3  \norg.glassfish.hk2.external  \naopalliance-repackaged  \n2.6.1  \norg.glassfish.hk2.external  \njakarta.inject  \n2.6.1  \norg.glassfish.jersey.containers  \njersey-container-servlet  \n2.36  \norg.glassfish.jersey.containers  \njersey-container-servlet-core  \n2.36  \norg.glassfish.jersey.core  \njersey-client  \n2.36  \norg.glassfish.jersey.core  \njersey-common  \n2.36"
    },
    {
        "id": 971,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "2.36  \norg.glassfish.jersey.containers  \njersey-container-servlet-core  \n2.36  \norg.glassfish.jersey.core  \njersey-client  \n2.36  \norg.glassfish.jersey.core  \njersey-common  \n2.36  \norg.glassfish.jersey.core  \njersey-server  \n2.36  \norg.glassfish.jersey.inject  \njersey-hk2  \n2.36  \norg.hibernate.validator  \nhibernate-validator  \n6.1.0.Final  \norg.javassist  \njavassist  \n3.25.0-GA  \norg.jboss.logging  \njboss-logging  \n3.3.2.Final  \norg.jdbi  \njdbi  \n2.63.1  \norg.jetbrains  \nannotations  \n17.0.0  \norg.joda  \njoda-convert  \n1.7  \norg.jodd  \njodd-core  \n3.5.2  \norg.json4s  \njson4s-ast_2.12  \n3.7.0-M11  \norg.json4s  \njson4s-core_2.12  \n3.7.0-M11  \norg.json4s  \njson4s-jackson_2.12  \n3.7.0-M11  \norg.json4s  \njson4s-scalap_2.12  \n3.7.0-M11  \norg.lz4  \nlz4-java  \n1.8.0  \norg.mariadb.jdbc  \nmariadb-java-client  \n2.7.4  \norg.mlflow  \nmlflow-spark  \n1.27.0  \norg.objenesis  \nobjenesis  \n2.5.1  \norg.postgresql  \npostgresql  \n42.3.3  \norg.roaringbitmap  \nRoaringBitmap  \n0.9.25  \norg.roaringbitmap  \nshims  \n0.9.25  \norg.rocksdb  \nrocksdbjni  \n6.24.2  \norg.rosuda.REngine  \nREngine  \n2.1.0  \norg.scala-lang  \nscala-compiler_2.12  \n2.12.14  \norg.scala-lang  \nscala-library_2.12  \n2.12.14  \norg.scala-lang  \nscala-reflect_2.12  \n2.12.14  \norg.scala-lang.modules  \nscala-collection-compat_2.12"
    },
    {
        "id": 972,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "2.12.14  \norg.scala-lang  \nscala-reflect_2.12  \n2.12.14  \norg.scala-lang.modules  \nscala-collection-compat_2.12  \n2.4.3  \norg.scala-lang.modules  \nscala-parser-combinators_2.12  \n1.1.2  \norg.scala-lang.modules  \nscala-xml_2.12  \n1.2.0  \norg.scala-sbt  \ntest-interface  \n1.0  \norg.scalacheck  \nscalacheck_2.12  \n1.14.2  \norg.scalactic  \nscalactic_2.12  \n3.0.8  \norg.scalanlp  \nbreeze-macros_2.12  \n1.2  \norg.scalanlp  \nbreeze_2.12  \n1.2  \norg.scalatest  \nscalatest_2.12  \n3.0.8  \norg.slf4j  \njcl-over-slf4j  \n1.7.36  \norg.slf4j  \njul-to-slf4j  \n1.7.36  \norg.slf4j  \nslf4j-api  \n1.7.36  \norg.spark-project.spark  \nunused  \n1.0.0  \norg.threeten  \nthreeten-extra  \n1.5.0  \norg.tukaani  \nxz  \n1.8  \norg.typelevel  \nalgebra_2.12  \n2.0.1  \norg.typelevel  \ncats-kernel_2.12  \n2.1.1  \norg.typelevel  \nmacro-compat_2.12  \n1.1.1  \norg.typelevel  \nspire-macros_2.12  \n0.17.0  \norg.typelevel  \nspire-platform_2.12  \n0.17.0  \norg.typelevel  \nspire-util_2.12  \n0.17.0  \norg.typelevel  \nspire_2.12  \n0.17.0  \norg.wildfly.openssl  \nwildfly-openssl  \n1.0.7.Final  \norg.xerial  \nsqlite-jdbc  \n3.8.11.2  \norg.xerial.snappy  \nsnappy-java  \n1.1.8.4  \norg.yaml  \nsnakeyaml  \n1.24  \noro  \noro  \n2.0.8  \npl.edu.icm"
    },
    {
        "id": 973,
        "url": "https://docs.databricks.com/en/release-notes/runtime/11.3lts.html",
        "content": "org.xerial.snappy  \nsnappy-java  \n1.1.8.4  \norg.yaml  \nsnakeyaml  \n1.24  \noro  \noro  \n2.0.8  \npl.edu.icm  \nJLargeArrays  \n1.5  \nsoftware.amazon.ion  \nion-java  \n1.0.2  \nstax  \nstax-api  \n1.0.1"
    },
    {
        "id": 974,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "IP addresses and domains for Databricks services and assets  \nThis article lists IP addresses and domains for Databricks services and assets.  \nYou may need the following information if:  \nYou create your Databricks workspaces in your own VPC, a feature known as customer-managed VPC.  \nSee Configure a customer-managed VPC.  \nYou use AWS PrivateLink within your Databricks network environment.  \nSee Enable private connectivity using AWS PrivateLink.  \nDatabricks control plane addresses\nDatabricks control plane addresses\nThe following tables list the IP addresses or domain names the Databricks control plane uses for each supported region. Port 443 is used for all addresses except for the SCC relay for PrivateLink, which uses Port 6666.\n\nInbound IPs to Databricks control plane"
    },
    {
        "id": 975,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "Inbound IPs to Databricks control plane\nDatabricks Region  \nService  \nPublic IP or domain name  \nap-northeast-1  \nControl plane services, including webapp  \ntokyo.cloud.databricks.com, 35.72.28.0/28  \nSCC relay  \ntunnel.ap-northeast-1.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.ap-northeast-1.cloud.databricks.com  \nap-northeast-2  \nControl plane services, including webapp  \nseoul.cloud.databricks.com, 3.38.156.176/28  \nSCC relay  \ntunnel.ap-northeast-2.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.ap-northeast-2.cloud.databricks.com  \nap-south-1  \nControl plane services, including webapp  \nmumbai.cloud.databricks.com, 65.0.37.64/28  \nSCC relay  \ntunnel.ap-south-1.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.ap-south-1.cloud.databricks.com  \nap-southeast-1  \nControl plane services, including webapp  \nsingapore.cloud.databricks.com, 13.214.1.96/28  \nSCC relay  \ntunnel.ap-southeast-1.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.ap-southeast-1.cloud.databricks.com  \nap-southeast-2  \nControl plane services, including webapp  \nsydney.cloud.databricks.com, 3.26.4.0/28  \nSCC relay  \ntunnel.ap-southeast-2.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.ap-southeast-2.cloud.databricks.com  \nca-central-1  \nControl plane services, including webapp  \ncanada.cloud.databricks.com, 3.96.84.208/28  \nSCC relay  \ntunnel.ca-central-1.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.ca-central-1.cloud.databricks.com"
    },
    {
        "id": 976,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "SCC relay  \ntunnel.ca-central-1.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.ca-central-1.cloud.databricks.com  \neu-central-1  \nControl plane services, including webapp  \nfrankfurt.cloud.databricks.com, 18.159.44.32/28  \nSCC relay  \ntunnel.eu-central-1.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.eu-central-1.cloud.databricks.com  \neu-west-1  \nControl plane services, including webapp  \nireland.cloud.databricks.com, 3.250.244.112/28  \nSCC relay  \ntunnel.eu-west-1.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.eu-west-1.cloud.databricks.com  \neu-west-2  \nControl plane services, including webapp  \nlondon.cloud.databricks.com, 18.134.65.240/28  \nSCC relay  \ntunnel.eu-west-2.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.eu-west-2.cloud.databricks.com  \neu-west-3  \nControl plane services, including webapp  \nparis.cloud.databricks.com, 13.39.141.128/28  \nSCC relay  \ntunnel.eu-west-3.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.eu-west-3.cloud.databricks.com  \nsa-east-1  \nControl plane services, including webapp  \nsaopaulo.cloud.databricks.com, 15.229.120.16/28  \nSCC relay  \ntunnel.sa-east-1.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.sa-east-1.cloud.databricks.com  \nus-east-1  \nControl plane services, including webapp  \nnvirginia.cloud.databricks.com, 3.237.73.224/28  \nSCC relay"
    },
    {
        "id": 977,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "us-east-1  \nControl plane services, including webapp  \nnvirginia.cloud.databricks.com, 3.237.73.224/28  \nSCC relay  \ntunnel.us-east-1.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.us-east-1.cloud.databricks.com  \nus-east-2  \nControl plane services, including webapp  \nohio.cloud.databricks.com, 3.128.237.208/28  \nSCC relay  \ntunnel.us-east-2.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.us-east-2.cloud.databricks.com  \nus-gov-west-1  \nControl plane services, including webapp  \npendleton.cloud.databricks.us, 3.30.186.128/28  \nSCC relay  \ntunnel.us-gov-west-1.cloud.databricks.us  \nSCC relay for PrivateLink  \ntunnel.privatelink.us-gov-west-1.cloud.databricks.us  \nus-west-1  \nControl plane services, including webapp  \noregon.cloud.databricks.com, 44.234.192.32/28  \nSCC relay  \ntunnel.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.cloud.databricks.com  \nus-west-2  \nControl plane services, including webapp  \noregon.cloud.databricks.com, 44.234.192.32/28  \nSCC relay  \ntunnel.cloud.databricks.com  \nSCC relay for PrivateLink  \ntunnel.privatelink.cloud.databricks.com"
    },
    {
        "id": 978,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "Outbound IPs from Databricks control plane\nThe following table list the outbound IP addresses or domain names the Databricks control plane uses for each supported region. Port 443 is used for all addresses except for the SCC relay for PrivateLink, which uses Port 6666.  \nDatabricks Region  \nService  \nPublic IP or domain name  \nap-northeast-1  \nControl plane NAT IPs  \n35.72.28.0/28, 18.177.16.95  \nVPC ID  \nvpc-082c211a3fdc5876e, vpc-0a9dd2f9a99283178  \nap-northeast-2  \nControl plane NAT IPs  \n3.38.156.176/28, 54.180.50.119  \nVPC ID  \nvpc-04e703ba94a49a3ac, vpc-097fd29f3acba52e8  \nap-south-1  \nControl plane NAT IPs  \n65.0.37.64/28, 13.232.248.161  \nVPC ID  \nvpc-042818d612f90f994, vpc-0a0c6fbd8a2890714  \nap-southeast-1  \nControl plane NAT IPs  \n13.214.1.96/28, 13.213.212.4  \nVPC ID  \nvpc-01dcc0ded03337911`, vpc-0e0baa0188c149eae  \nap-southeast-2  \nControl plane NAT IPs  \n3.26.4.0/28, 13.237.96.217  \nVPC ID  \nvpc-0c2c00de7182adc20, vpc-0a5b21c86d3fc89fa  \nca-central-1  \nControl plane NAT IPs  \n3.96.84.208/28, 35.183.59.105  \nVPC ID  \nvpc-0a2b384708459134f, vpc-09e67f3a27be71c9c  \neu-central-1  \nControl plane NAT IPs"
    },
    {
        "id": 979,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "vpc-0a2b384708459134f, vpc-09e67f3a27be71c9c  \neu-central-1  \nControl plane NAT IPs  \n18.159.44.32/28, 18.159.32.64  \nVPC ID  \nvpc-0fac49d340642f67b, vpc-0b6768aacb36c9425  \neu-west-1  \nControl plane NAT IPs  \n3.250.244.112/28, 46.137.47.49  \nVPC ID  \nvpc-0e362545addfa9470, vpc-0004e2691850f29b3  \neu-west-2  \nControl plane NAT IPs  \n18.134.65.240/28,3.10.112.150  \nVPC ID  \nvpc-07eb6b6a2cb9e77eb, vpc-0f0a9d76e15ca7eff  \neu-west-3  \nControl plane NAT IPs  \n13.39.141.128/28, 15.236.174.74  \nVPC ID  \nvpc-0b6f443f7cefdcda2, vpc-01ed1436ea79be8f4  \nsa-east-1  \nControl plane NAT IPs  \n15.229.120.16/28, 177.71.254.47  \nVPC ID  \nvpc-0b13bcae0aa721cbc, vpc-0717a4601f05c79d9  \nus-east-1  \nControl plane NAT IPs  \n3.237.73.224/28, 54.156.226.103  \nVPC ID  \nvpc-0e8e0ec90d0f40c06, vpc-08fd12c62a6e0b1df  \nus-east-2  \nControl plane NAT IPs  \n3.128.237.208/28, 18.221.200.169  \nVPC ID"
    },
    {
        "id": 980,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "us-east-2  \nControl plane NAT IPs  \n3.128.237.208/28, 18.221.200.169  \nVPC ID  \nvpc-0865fc77cf45f52b7, vpc-0ea50fe31af7760e4  \nus-gov-west-1  \nControl plane NAT IPs  \n3.30.186.128/28, 3.30.245.130  \nVPC ID  \nvpc-0ab77b8381fdd5416, vpc-0b394a016af6d42ab  \nus-west-1  \nControl plane NAT IPs  \n44.234.192.32/28, 52.27.216.188  \nVPC ID  \nN/A  \nus-west-2  \nControl plane NAT IPs  \n44.234.192.32/28, 52.27.216.188  \nVPC ID  \nvpc-dc8086b9, vpc-0c51983cc62e5ce0b"
    },
    {
        "id": 981,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "Addresses for artifact storage, log storage, system tables, and shared datasets buckets\nDatabricks Region  \nService  \nPublic IP or domain name  \nap-northeast-1  \nArtifact storage bucket  \ndatabricks-prod-artifacts-ap-northeast-1  \nLog storage bucket  \ndatabricks-prod-storage-tokyo  \nSystem tables bucket  \nsystem-tables-prod-ap-northeast-1-uc-metastore-bucket  \nShared datasets bucket  \ndatabricks-datasets-tokyo  \nap-northeast-2  \nArtifact storage bucket  \ndatabricks-prod-artifacts-ap-northeast-2  \nLog storage bucket  \ndatabricks-prod-storage-seoul  \nSystem tables bucket  \nsystem-tables-prod-ap-northeast-2-uc-metastore-bucket  \nShared datasets bucket  \ndatabricks-datasets-seoul  \nap-south-1  \nArtifact storage bucket  \ndatabricks-prod-artifacts-ap-south-1  \nLog storage bucket  \ndatabricks-prod-storage-mumbai  \nSystem tables bucket  \nsystem-tables-prod-ap-south-1-uc-metastore-bucket  \nShared datasets bucket  \ndatabricks-datasets-mumbai  \nap-southeast-1  \nArtifact storage bucket  \ndatabricks-prod-artifacts-ap-southeast-1  \nLog storage bucket  \ndatabricks-prod-storage-singapore  \nSystem tables bucket  \nsystem-tables-prod-ap-southeast-1-uc-metastore-bucket  \nShared datasets bucket  \ndatabricks-datasets-singapore  \nap-southeast-2  \nArtifact storage bucket  \ndatabricks-prod-artifacts-ap-southeast-2  \nLog storage bucket  \ndatabricks-prod-storage-sydney  \nSystem tables bucket  \nsystem-tables-prod-ap-southeast-2-uc-metastore-bucket  \nShared datasets bucket  \ndatabricks-datasets-sydney  \nca-central-1  \nArtifact storage bucket  \ndatabricks-prod-artifacts-ca-central-1  \nLog storage bucket  \ndatabricks-prod-storage-montreal  \nSystem tables bucket  \nsystem-tables-prod-ca-central-1-uc-metastore-bucket  \nShared datasets bucket  \ndatabricks-datasets-montreal  \neu-central-1  \nArtifact storage bucket  \ndatabricks-prod-artifacts-eu-central-1  \nLog storage bucket  \ndatabricks-prod-storage-frankfurt  \nSystem tables bucket"
    },
    {
        "id": 982,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "databricks-datasets-montreal  \neu-central-1  \nArtifact storage bucket  \ndatabricks-prod-artifacts-eu-central-1  \nLog storage bucket  \ndatabricks-prod-storage-frankfurt  \nSystem tables bucket  \nsystem-tables-prod-eu-central-1-uc-metastore-bucket  \nShared datasets bucket  \ndatabricks-datasets-frankfurt  \neu-west-1  \nArtifact storage bucket  \ndatabricks-prod-artifacts-eu-west-1  \nLog storage bucket  \ndatabricks-prod-storage-ireland  \nSystem tables bucket  \nsystem-tables-prod-eu-west-1-uc-metastore-bucket  \nShared datasets bucket  \ndatabricks-datasets-ireland  \neu-west-2  \nArtifact storage bucket  \ndatabricks-prod-artifacts-eu-west-2  \nLog storage bucket  \ndatabricks-prod-storage-london  \nSystem tables bucket  \nsystem-tables-prod-eu-west-2-uc-metastore-bucket  \nShared datasets bucket  \ndatabricks-datasets-london  \neu-west-3  \nArtifact storage bucket  \ndatabricks-prod-artifacts-eu-west-3  \nLog storage bucket  \ndatabricks-prod-storage-paris  \nSystem tables bucket  \nsystem-tables-prod-eu-west-3-uc-metastore-bucket  \nShared datasets bucket  \ndatabricks-datasets-paris  \nsa-east-1  \nArtifact storage bucket  \ndatabricks-prod-artifacts-sa-east-1  \nLog storage bucket  \ndatabricks-prod-storage-saopaulo  \nSystem tables bucket  \nsystem-tables-prod-sa-east-1-uc-metastore-bucket  \nShared datasets bucket  \ndatabricks-datasets-saopaulo  \nus-east-1  \nArtifact storage bucket  \ndatabricks-prod-artifacts-us-east-1  \nLog storage bucket  \ndatabricks-prod-storage-virginia  \nSystem tables bucket  \nsystem-tables-prod-us-east-1-uc-metastore-bucket  \nShared datasets bucket  \ndatabricks-datasets-virginia  \nus-east-2  \nArtifact storage bucket  \ndatabricks-prod-artifacts-us-east-2  \nLog storage bucket  \ndatabricks-prod-storage-ohio  \nSystem tables bucket  \nsystem-tables-prod-us-east-2-uc-metastore-bucket"
    },
    {
        "id": 983,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "Artifact storage bucket  \ndatabricks-prod-artifacts-us-east-2  \nLog storage bucket  \ndatabricks-prod-storage-ohio  \nSystem tables bucket  \nsystem-tables-prod-us-east-2-uc-metastore-bucket  \nShared datasets bucket  \ndatabricks-datasets-ohio  \nus-gov-west-1  \nArtifact storage bucket  \ndatabricks-prod-artifacts-us-gov-west-1  \nLog storage bucket  \ndatabricks-prod-storage-pendleton  \nSystem tables bucket  \nN/A  \nShared datasets bucket  \ndatabricks-datasets-pendleton  \nus-west-1  \nArtifact storage bucket  \ndatabricks-prod-artifacts-us-west-2  \nLog storage bucket  \ndatabricks-prod-storage-oregon  \nSystem tables bucket  \nsystem-tables-prod-us-west-1-uc-metastore-bucket  \nShared datasets bucket  \ndatabricks-datasets-oregon  \nus-west-2  \nArtifact storage bucket  \ndatabricks-prod-artifacts-us-west-2, databricks-update-oregon  \nLog storage bucket  \ndatabricks-prod-storage-oregon  \nSystem tables bucket  \nsystem-tables-prod-us-west-2-uc-metastore-bucket  \nShared datasets bucket  \ndatabricks-datasets-oregon"
    },
    {
        "id": 984,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "S3 addresses\nS3 addresses\nTo add the global S3 bucket service to a route or allow list, use the following address and port, regardless of region: s3.amazonaws.com:443  \nFor regional S3\u00a0buckets, AWS provides an address and port for a regional endpoint (s3.<region-name>.amazonaws.com:443). Databricks recommends that you use a VPC endpoint instead. Databricks uses VPC IDs for accessing S3 buckets in the same region as the Databricks control plane, and NAT IPs for accessing S3 buckets in different regions from the control plane. See (Recommended) Configure regional endpoints.\n\nSTS addresses"
    },
    {
        "id": 985,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "STS addresses\nTo add the global STS (AWS Security Token Service) to a route or allow list, use the following address and port, regardless of region: sts.amazonaws.com:443  \nFor regional STS, AWS provides an address and port for a regional endpoint (sts.<region-name>.amazonaws.com:443), but Databricks recommends that you use a VPC endpoint instead. See (Recommended) Configure regional endpoints.\n\nKinesis addresses\nKinesis addresses\nFor the Kinesis service, AWS provides addresses and ports for regional endpoints as shown in the table below. However, Databricks recommends that you use a VPC endpoint instead. See (Recommended) Configure regional endpoints.  \nVPC region  \nAddress  \nPort  \nus-west-1  \nkinesis-fips.us-west-2.amazonaws.com  \n443  \nAll other regions  \nkinesis.<region-name>.amazonaws.com  \n443\n\nRDS addresses for legacy Hive metastore"
    },
    {
        "id": 986,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "RDS addresses for legacy Hive metastore\nTo add the Amazon RDS services used by Databricks to a route or allow list, use the following addresses.  \nVPC region  \nAddress  \nPort  \nap-northeast-1  \nmddx5a4bpbpm05.cfrfsun7mryq.ap-northeast-1.rds.amazonaws.com  \n3306  \nap-northeast-2  \nmd1915a81ruxky5.cfomhrbro6gt.ap-northeast-2.rds.amazonaws.com  \n3306  \nap-south-1  \nmdjanpojt83v6j.c5jml0fhgver.ap-south-1.rds.amazonaws.com  \n3306  \nap-southeast-1  \nmd1n4trqmokgnhr.csnrqwqko4ho.ap-southeast-1.rds.amazonaws.com  \n3306  \nap-southeast-2  \nmdnrak3rme5y1c.c5f38tyb1fdu.ap-southeast-2.rds.amazonaws.com  \n3306  \nca-central-1  \nmd1w81rjeh9i4n5.co1tih5pqdrl.ca-central-1.rds.amazonaws.com  \n3306  \neu-central-1  \nmdv2llxgl8lou0.ceptxxgorjrc.eu-central-1.rds.amazonaws.com  \n3306  \neu-west-1  \nmd15cf9e1wmjgny.cxg30ia2wqgj.eu-west-1.rds.amazonaws.com  \n3306  \neu-west-2  \nmdio2468d9025m.c6fvhwk6cqca.eu-west-2.rds.amazonaws.com  \n3306  \neu-west-3"
    },
    {
        "id": 987,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "3306  \neu-west-3  \nmetastorerds-dbconsolidationmetastore-asda4em2u6eg.c2ybp3dss6ua.eu-west-3.rds.amazonaws.com  \n3306  \nsa-east-1  \nmetastorerds-dbconsolidationmetastore-fqekf3pck8yw.cog1aduyg4im.sa-east-1.rds.amazonaws.com  \n3306  \nus-east-1  \nmdb7sywh50xhpr.chkweekm4xjq.us-east-1.rds.amazonaws.com  \n3306  \nus-east-2  \nmd7wf1g369xf22.cluz8hwxjhb6.us-east-2.rds.amazonaws.com  \n3306  \nus-west-1  \nmdzsbtnvk0rnce.c13weuwubexq.us-west-1.rds.amazonaws.com  \n3306  \nus-west-2  \nmdpartyyphlhsp.caj77bnxuhme.us-west-2.rds.amazonaws.com  \n3306"
    },
    {
        "id": 988,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "PrivateLink VPC endpoint services\nTo configure your workspace to use AWS PrivateLink, use the following table to determine your region\u2019s VPC endpoint service domains. You can use any availability zone in your region.  \nThe endpoint service identified as Workspace (including REST API) is used for both the front-end connection (user-to-workspace for web application and REST APIs) and the back-end connection (to connect to REST APIs). If you are implementing both front-end and back-end connections, use this same workspace VPC endpoint service for both use cases.  \nFor more information, see Enable private connectivity using AWS PrivateLink.  \nRegion  \nCreate VPC endpoints to these regional VPC endpoint services  \nap-northeast-1  \nWorkspace (including REST API): com.amazonaws.vpce.ap-northeast-1.vpce-svc-02691fd610d24fd64  \nSecure cluster connectivity relay: com.amazonaws.vpce.ap-northeast-1.vpce-svc-02aa633bda3edbec0  \nap-northeast-2  \nWorkspace (including REST API): com.amazonaws.vpce.ap-northeast-2.vpce-svc-0babb9bde64f34d7e  \nSecure cluster connectivity relay: com.amazonaws.vpce.ap-northeast-2.vpce-svc-0dc0e98a5800db5c4  \nap-south-1  \nWorkspace (including REST API): com.amazonaws.vpce.ap-south-1.vpce-svc-0dbfe5d9ee18d6411  \nSecure cluster connectivity relay: com.amazonaws.vpce.ap-south-1.vpce-svc-03fd4d9b61414f3de  \nap-southeast-1"
    },
    {
        "id": 989,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "Secure cluster connectivity relay: com.amazonaws.vpce.ap-south-1.vpce-svc-03fd4d9b61414f3de  \nap-southeast-1  \nWorkspace (including REST API): com.amazonaws.vpce.ap-southeast-1.vpce-svc-02535b257fc253ff4  \nSecure cluster connectivity relay: com.amazonaws.vpce.ap-southeast-1.vpce-svc-0557367c6fc1a0c5c  \nap-southeast-2  \nWorkspace (including REST API): com.amazonaws.vpce.ap-southeast-2.vpce-svc-0b87155ddd6954974  \nSecure cluster connectivity relay: com.amazonaws.vpce.ap-southeast-2.vpce-svc-0b4a72e8f825495f6  \nca-central-1  \nWorkspace (including REST API): com.amazonaws.vpce.ca-central-1.vpce-svc-0205f197ec0e28d65  \nSecure cluster connectivity relay: com.amazonaws.vpce.ca-central-1.vpce-svc-0c4e25bdbcbfbb684  \neu-central-1  \nWorkspace (including REST API): com.amazonaws.vpce.eu-central-1.vpce-svc-081f78503812597f7  \nSecure cluster connectivity relay: com.amazonaws.vpce.eu-central-1.vpce-svc-08e5dfca9572c85c4  \neu-west-1  \nWorkspace (including REST API): com.amazonaws.vpce.eu-west-1.vpce-svc-0da6ebf1461278016"
    },
    {
        "id": 990,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "Workspace (including REST API): com.amazonaws.vpce.eu-west-1.vpce-svc-0da6ebf1461278016  \nSecure cluster connectivity relay: com.amazonaws.vpce.eu-west-1.vpce-svc-09b4eb2bc775f4e8c  \neu-west-2  \nWorkspace (including REST API): com.amazonaws.vpce.eu-west-2.vpce-svc-01148c7cdc1d1326c  \nSecure cluster connectivity relay: com.amazonaws.vpce.eu-west-2.vpce-svc-05279412bf5353a45  \neu-west-3  \nWorkspace (including REST API): com.amazonaws.vpce.eu-west-3.vpce-svc-008b9368d1d011f37  \nSecure cluster connectivity relay: com.amazonaws.vpce.eu-west-3.vpce-svc-005b039dd0b5f857d  \nsa-east-1  \nWorkspace (including REST API): com.amazonaws.vpce.sa-east-1.vpce-svc-0bafcea8cdfe11b66  \nSecure cluster connectivity relay: com.amazonaws.vpce.sa-east-1.vpce-svc-0e61564963be1b43f  \nus-east-1  \nWorkspace (including REST API): com.amazonaws.vpce.us-east-1.vpce-svc-09143d1e626de2f04  \nSecure cluster connectivity relay: com.amazonaws.vpce.us-east-1.vpce-svc-00018a8c3ff62ffdf  \nus-east-2"
    },
    {
        "id": 991,
        "url": "https://docs.databricks.com/en/resources/ip-domain-region.html",
        "content": "Secure cluster connectivity relay: com.amazonaws.vpce.us-east-1.vpce-svc-00018a8c3ff62ffdf  \nus-east-2  \nWorkspace (including REST API): com.amazonaws.vpce.us-east-2.vpce-svc-041dc2b4d7796b8d3  \nSecure cluster connectivity relay: com.amazonaws.vpce.us-east-2.vpce-svc-090a8fab0d73e39a6  \nus-gov-west-1  \nWorkspace (including REST API): com.amazonaws.vpce.us-gov-west-1.vpce-svc-0f25e28401cbc9418  \nSecure cluster connectivity relay: com.amazonaws.vpce.us-gov-west-1.vpce-svc-05f27abef1a1a3faa  \nus-west-1  \nPrivateLink connectivity is not supported for this region.  \nus-west-2  \nWorkspace (including REST API): com.amazonaws.vpce.us-west-2.vpce-svc-0129f463fcfbc46c5  \nSecure cluster connectivity relay: com.amazonaws.vpce.us-west-2.vpce-svc-0158114c0c730c3bb"
    },
    {
        "id": 992,
        "url": "https://docs.databricks.com/en/security/auth/email-login.html",
        "content": "Sign-in with email or external accounts  \nThis article shows how to allow users to sign in to Databricks using one-time passcodes via email or common external accounts, such as Google or Microsoft. To configure single sign-on to authenticate using your organization\u2019s identity provider, see Configure SSO in Databricks.  \nConfigure sign-in with email or external accounts"
    },
    {
        "id": 993,
        "url": "https://docs.databricks.com/en/security/auth/email-login.html",
        "content": "Configure sign-in with email or external accounts\nThe following demo walks you through configuring sign-in with email or external accounts: Configure sign-in with Google or Microsoft.  \nAs an account admin, log in to the account console and click the Settings icon in the sidebar.  \nClick the Single sign-on tab.  \nNext to Authentication, click Manage.  \nChoose Sign-in with email or external providers.  \nIn Choose sign-in options, select one or more methods that users can choose to log in:  \nOne-time passcode: Users will receive codes via email that can be used to sign in to Databricks.  \nGoogle Sign-in: Users can sign in to Databricks with their existing Google accounts.  \nMicrosoft Entra ID: Users can sign in to Databricks with their existing Microsoft Entra ID accounts.  \nClick update.\n\nLog in using email or external accounts"
    },
    {
        "id": 994,
        "url": "https://docs.databricks.com/en/security/auth/email-login.html",
        "content": "Log in using email or external accounts\nWhen single sign-on is not configured, users can log in to Databricks using their email or one of the account providers selected in the account console. When a user logins with a one-time passcode, Databricks sends a unique code to the user\u2019s email address. The user must then retrieve this code from their email and enter it on the login page to verify their identity. This enhances security by ensuring that only individuals with access to the registered email can log in."
    },
    {
        "id": 995,
        "url": "https://docs.databricks.com/en/partners/bi/workbenchj.html",
        "content": "Connect to SQL Workbench/J  \nThis article describes how to use SQL Workbench/J with Databricks.  \nNote  \nThis article covers SQL Workbench/J, which is neither provided nor supported by Databricks. To contact the provider, see use the SQL Workbench/J support forum in Google Groups..  \nRequirements"
    },
    {
        "id": 996,
        "url": "https://docs.databricks.com/en/partners/bi/workbenchj.html",
        "content": "Requirements\nSQL Workbench/J.  \nThe Databricks JDBC Driver. Download the Databricks JDBC Driver onto your local development machine, extracting the DatabricksJDBC42.jar file from the downloaded DatabricksJDBC42-<version>.zip file.  \nNote  \nThis article was tested with macOS, SQL Workbench/J Build 130, Zulu OpenJDK 21.0.1, and Databricks JDBC Driver 2.6.36.  \nFor Databricks authentication, if you are not using Databricks personal access token authentication, you can skip generating a personal access token later in these requirements. For more information about available Databricks authentication types, see Authentication settings for the Databricks JDBC Driver.  \nA cluster or SQL warehouse in your Databricks workspace.  \nCompute configuration reference.  \nCreate a SQL warehouse.  \nThe connection details for your cluster or SQL warehouse, specifically the Server Hostname, Port, and HTTP Path values.  \nGet connection details for a Databricks compute resource.  \nA Databricks personal access token. To create a personal access token, do the following:  \nIn your Databricks workspace, click your Databricks username in the top bar, and then select Settings from the drop down.  \nClick Developer.  \nNext to Access tokens, click Manage.  \nClick Generate new token.  \n(Optional) Enter a comment that helps you to identify this token in the future, and change the token\u2019s default lifetime of 90 days. To create a token with no lifetime (not recommended), leave the Lifetime (days) box empty (blank).  \nClick Generate.  \nCopy the displayed token to a secure location, and then click Done.  \nNote  \nBe sure to save the copied token in a secure location. Do not share your copied token with others. If you lose the copied token, you cannot regenerate that exact same token. Instead, you must repeat this procedure to create a new token. If you lose the copied token, or you believe that the token has been compromised, Databricks strongly recommends that you immediately delete that token from your workspace by clicking the trash can (Revoke) icon next to the token on the Access tokens page."
    },
    {
        "id": 997,
        "url": "https://docs.databricks.com/en/partners/bi/workbenchj.html",
        "content": "If you are not able to create or use tokens in your workspace, this might be because your workspace administrator has disabled tokens or has not given you permission to create or use tokens. See your workspace administrator or the following:  \nEnable or disable personal access token authentication for the workspace  \nPersonal access token permissions  \nNote  \nAs a security best practice when you authenticate with automated tools, systems, scripts, and apps, Databricks recommends that you use OAuth tokens.  \nIf you use personal access token authentication, Databricks recommends using personal access tokens belonging to service principals instead of workspace users. To create tokens for service principals, see Manage tokens for a service principal."
    },
    {
        "id": 998,
        "url": "https://docs.databricks.com/en/partners/bi/workbenchj.html",
        "content": "Steps to connect to Workbench/J\nSteps to connect to Workbench/J\nTo connect to Workbench/J, do the following:  \nLaunch SQL Workbench/J.  \nSelect File > Connect window.  \nIn the Select Connection Profile dialog, click Manage Drivers.  \nIn the Name field, type Databricks.  \nIn the Library field, click the Select the JAR file(s) icon. Browse to the directory where you extracted the DatabricksJDBC42.jar file from the downloaded DatabricksJDBC42-<version>.zip file, and select the JAR file. Then click Choose.  \nVerify that the Classname field is populated with com.databricks.client.jdbc.Driver.  \nClick OK.  \nClick the Create a new connection profile icon.  \nType a name for the profile.  \nIn the Driver field, select Databricks (com.databricks.client.jdbc.Driver).  \nIn the URL field, enter the JDBC URL for your Databricks resource. For the URL field syntax for JDBC URLs, see Authentication settings for the Databricks JDBC Driver.  \nClick Test.  \nClick OK twice.\n\nAdditional resources"
    },
    {
        "id": 999,
        "url": "https://docs.databricks.com/en/release-notes/product/2020/february.html",
        "content": "February 2020  \nThese features and Databricks platform improvements were released in February 2020.  \nNote  \nReleases are staged. Your Databricks account may not be updated until up to a week after the initial release date.  \nDatabricks Runtime 6.4 for Genomics GA\nDatabricks Runtime 6.4 for Genomics GA\nFebruary 26, 2020  \nDatabricks Runtime 6.4 for Genomics is built on top of Databricks Runtime 6.4. It includes many improvements and upgrades from Databricks Runtime 6.3 for Genomics.  \nThe key features are:  \nYou can now customize DNASeq Pipeline users can selectively disable any legitimate combination of the read alignment, variant calling, and variant annotation stages. Users can also perform single-end read alignment.  \nThe version of Glow included in Databricks Runtime 6.4 for Genomics now provides Python and Scala APIs for functions previously exposed only via SQL expressions. These functions are available for DataFrame operations, providing improved compile-time safety.\n\nDatabricks Runtime 6.4 ML GA"
    },
    {
        "id": 1000,
        "url": "https://docs.databricks.com/en/release-notes/product/2020/february.html",
        "content": "Databricks Runtime 6.4 ML GA\nFebruary 26, 2020  \nDatabricks Runtime 6.4 ML GA brings library upgrades, including:  \nPyTorch: 1.3.1 to 1.4.0  \nHorovod: 0.18.2 to 1.19.0  \nFor details, see the complete Databricks Runtime 6.4 for ML (EoS) release notes.\n\nDatabricks Runtime 6.4 GA"
    },
    {
        "id": 1001,
        "url": "https://docs.databricks.com/en/release-notes/product/2020/february.html",
        "content": "Databricks Runtime 6.4 GA\nFebruary 26, 2020  \nDatabricks Runtime 6.4 GA brings new features, improvements, and many bug fixes.  \nProcess new data files incrementally with Auto Loader (Public Preview). Auto Loader gives you a more efficient way to process new data files incrementally as they arrive on a cloud blob store during ETL. This is an improvement over file-based structured streaming, which identifies new files by repeatedly listing the cloud directory and tracking the files that have been seen, and can be very inefficient as the directory grows.  \nLoad data into Delta Lake with idempotent retries (Public Preview). The COPY INTO SQL command lets you load data into Delta Lake with idempotent retries (Public Preview). To load data into Delta Lake today you have to use Apache Spark DataFrame APIs. If there are failures during loads, you have to handle them effectively.  \nOperation metrics for all writes, updates, and deletes on a Delta table now shown in table history.  \nInline Matplotlib figures now enabled by default in Databricks notebooks (Public Preview).  \nFor details, see the complete Databricks Runtime 6.4 (EoS) release notes."
    },
    {
        "id": 1002,
        "url": "https://docs.databricks.com/en/release-notes/product/2020/february.html",
        "content": "The Clusters and Jobs UIs now reflect new cluster terminology and cluster pricing\nThe Clusters and Jobs UIs now reflect new cluster terminology and cluster pricing\nFeb 25 - March 3, 2019: Version 3.14  \nTo identify the type of cluster you are using for a job and the pricing for each type of cluster, the Clusters page and the Configure Cluster UI for jobs have been updated.  \nOn the Clusters page, the list headings have been renamed:  \nInteractive Clusters has been renamed to All-Purpose Clusters to reflect that you can use such clusters for any type of workload.  \nAutomated Clusters has been renamed to Job Clusters to reflect that you can use these clusters only for jobs.  \nWhen you configure a cluster for a job, the Cluster Type options have been renamed:  \nNew Automated Cluster has been renamed to New Job Cluster to reflect the fact that if you choose a new cluster it is charged at the job rate.  \nExisting Interactive Cluster has been renamed to Existing All-Purpose Cluster to reflect the fact that if you choose an existing cluster it is charged at the all-purpose rate.\n\nNew interactive charts offer rich client-side interactions"
    },
    {
        "id": 1003,
        "url": "https://docs.databricks.com/en/release-notes/product/2020/february.html",
        "content": "New interactive charts offer rich client-side interactions\nFeb 25 - March 3, 2019: Version 3.14  \nThis release introduces two new interactive chart types that replace the bar chart and line chart implementations. In addition to existing chart functionality, the line chart has a few new custom plot options: setting a Y-axis range, showing or hiding markers, and applying log scale to the Y-axis. Both charts have a built-in toolbar that supports a rich set of client-side interactions.  \nIf you want to use the existing chart implementations, you can select them from the Legacy Charts drop-down menu. Existing charts will continue to use the previously available implementations.\n\nNew data ingestion network adds partner integrations with Delta Lake (Public Preview)"
    },
    {
        "id": 1004,
        "url": "https://docs.databricks.com/en/release-notes/product/2020/february.html",
        "content": "New data ingestion network adds partner integrations with Delta Lake (Public Preview)\nFebruary 24, 2020  \nNow you can easily populate your \u201clakehouse\u201d\u2014your data lake empowered by the kinds of data structures and data management features you typically get with a data warehouse\u2014from hundreds of data sources into Delta Lake. At the heart of this network is the new Partner Integrations gallery, accessible from your workspace and providing access to a huge network of data sources via our partners Fivetran, Qlik, Infoworks, StreamSets, and Syncsort.  \nFor an overview, see our blog. For details, see Technology partners.\n\nFlags to manage workspace security and notebook features now available\nFlags to manage workspace security and notebook features now available\nFebruary 4-11, 2020: Version 3.12  \nThis release introduces new flags for managing the security headers that are sent to prevent attacks on your workspace, as well as access to notebook results downloads and Git versioning. All of these administrative options are enabled by default."
    },
    {
        "id": 1005,
        "url": "https://docs.databricks.com/en/release-notes/product/2018/january.html",
        "content": "January 2018  \nReleases are staged. Your Databricks account may not be updated until a week after the initial release date.  \nNote  \nWe are now providing Databricks Runtime deprecation notices in the Databricks Runtime release notes versions and compatibility.  \nMount points for Azure Blob storage containers and Data Lake Stores"
    },
    {
        "id": 1006,
        "url": "https://docs.databricks.com/en/release-notes/product/2018/january.html",
        "content": "Mount points for Azure Blob storage containers and Data Lake Stores\nJan 16-23, 2018: Version 2.63  \nWe have provided instructions for mounting Azure Blob storage containers and Data Lake Stores through the Databricks File System (DBFS). This gives all users in the same workspace the ability to access the Blob storage container or Data Lake Store (or folder inside the container or store) through the mount point. DBFS manages the credentials used to access a mounted Blob storage container or Data Lake Store and automatically handles the authentication with Azure Blob storage or Data Lake Store in the background.  \nMounting Blob storage containers and Data Lake Stores requires Databricks Runtime 4.0 and above. Once a container or store is mounted, you can use Runtime 3.4 or above to access the mount point.  \nSee Connect to Azure Data Lake Storage Gen2 and Blob Storage and Accessing Azure Data Lake Storage Gen1 from Databricks for more information.\n\nTable Access Control for SQL and Python (Private Preview)"
    },
    {
        "id": 1007,
        "url": "https://docs.databricks.com/en/release-notes/product/2018/january.html",
        "content": "Table Access Control for SQL and Python (Private Preview)\nJan 4-11, 2018: Version 2.62  \nNote  \nThis feature is in private preview. Please contact your account manager to request access. This feature also requires Databricks Runtime 3.5+.  \nLast year, we introduced data object access control for SQL users. Today we are excited to announce the private preview of Table Access Control (ACL) for both SQL and Python users. With Table Access Control, you can restrict access to securable objects like tables, databases, views, or functions. You can also provide fine-grained access control (to rows and columns matching specific conditions, for example) by setting permissions on derived views containing arbitrary queries.  \nSee Hive metastore privileges and securable objects (legacy) for more information.\n\nExporting notebook job run results via API"
    },
    {
        "id": 1008,
        "url": "https://docs.databricks.com/en/release-notes/product/2018/january.html",
        "content": "Exporting notebook job run results via API\nJan 4-11, 2018: Version 2.62  \nTo improve your ability to share and collaborate on the results of jobs, we now have a new Jobs API endpoint, jobs/runs/export that lets you retrieve the static HTML representation of a notebook job\u2019s run results in both code and dashboard view.  \nSee Runs export for more information.\n\nApache Airflow 1.9.0 includes Databricks integration\nApache Airflow 1.9.0 includes Databricks integration\nJan 2, 2018  \nLast year, we released a preview feature in Airflow\u2014a popular solution for managing ETL scheduling\u2014that allows customers to natively create tasks that trigger Databricks runs in an Airflow DAG. We\u2019re pleased to announce that these integrations have been released publicly in the 1.9.0 release of Airflow.  \nSee Orchestrate Databricks jobs with Apache Airflow for more information."
    },
    {
        "id": 1009,
        "url": "https://docs.databricks.com/en/security/privacy/gov-cloud.html",
        "content": "Databricks on AWS GovCloud  \nThis article describes the Databricks on AWS GovCloud offering and its compliance controls.  \nAWS GovCloud overview"
    },
    {
        "id": 1010,
        "url": "https://docs.databricks.com/en/security/privacy/gov-cloud.html",
        "content": "AWS GovCloud overview\nAWS GovCloud gives United States government customers and their partners the flexibility to architect secure cloud solutions that comply with the FedRAMP High baseline and other compliance regimes, including United States International Traffic in Arms Regulations (ITAR) and Export Administration Regulations (EAR). For details, see AWS GovCloud.  \nDatabricks on AWS GovCloud provides the Databricks platform deployed in AWS GovCloud with compliance and security controls. Databricks on AWS GovCloud is operated exclusively by US citizens on US soil.  \nNote  \nThe Databricks GovCloud Help Center is where you submit and manage support cases. Go to https://help.databricks.us/s/. Do not share any export-controlled data regarding support cases through channels other than the Databricks GovCloud Help Center. For more information on support, see Support.  \nWhen a Databricks on AWS GovCloud account is provisioned, the account owner receives an email with a short-lived login URL.  \nCompliance security profile  \nThe compliance security profile is enabled on all Databricks on AWS GovCloud workspaces by default. The compliance security profile has additional monitoring, enforced instance types for inter-node encryption, a hardened compute image, and other features that help meet the requirements of FedRAMP High compliance. Automatic cluster update and enhanced securing monitoring are also automatically enabled.  \nThe compliance security profile enforces the use of AWS Nitro instance types that provide both hardware-implemented network encryption between cluster nodes and encryption at rest for local disks in cluster and Databricks SQL SQL warehouses. Fleet instances are not available in AWS Gov Cloud. The supported instance types are:  \nGeneral purpose: M5dn, M5n, M5zn, M6i, M7i, M6id, M6in, M6idn, M6a, M7a  \nCompute optimized: C5a, C5ad, C5n, C6i, C6id, C7i, C6in, C6a, C7a  \nMemory optimized: R6i, R7i, R7iz, R6id, R6in, R6idn, R6a, R7a"
    },
    {
        "id": 1011,
        "url": "https://docs.databricks.com/en/security/privacy/gov-cloud.html",
        "content": "Memory optimized: R6i, R7i, R7iz, R6id, R6in, R6idn, R6a, R7a  \nStorage optimized: D3, D3en, P3dn, R5dn, R5n, I4i, I3en  \nAccelerated computing: G4dn, G5, P4d, P4de, P5  \nFor more information on the compliance security profile, see Compliance security profile."
    },
    {
        "id": 1012,
        "url": "https://docs.databricks.com/en/security/privacy/gov-cloud.html",
        "content": "FedRAMP High compliance\nFedRAMP High compliance\nThe FedRAMP High authorization status of Databricks on AWS GovCloud is currently In Process.  \nCustomers are responsible for implementing and operating applicable FedRAMP HIGH compliance controls as documented in the Control Implementation Summary / Customer Responsibility Matrix in SSP Appendix J of the Databricks FedRAMP authorization documentation package. US Government agencies can obtain access to the Databricks FedRAMP High authorization documentation through the FedRAMP package access request form. Follow the instructions on the Databricks FedRAMP Marketplace listing (package ID: FR2324740262).  \nYou must configure the following on Databricks on AWS GovCloud workspaces:  \nSingle sign-on authentication, see Configure SSO in Databricks  \nPrivateLink for both back-end and front-end connections, see Enable private connectivity using AWS PrivateLink.\n\nDatabricks for AWS GovCloud region and URLs"
    },
    {
        "id": 1013,
        "url": "https://docs.databricks.com/en/security/privacy/gov-cloud.html",
        "content": "Databricks for AWS GovCloud region and URLs\nThe Databricks AWS account ID for Databricks on AWS GovCloud is 044793339203. This account ID is required to create and configure a cross-account IAM role for Databricks workspace deployment. See Create an IAM role for workspace deployment.  \nDatabricks on AWS GovCloud workspaces are in the us-gov-west-1 region. For region information, see Databricks clouds and regions.  \nDatabricks on AWS GovCloud URLs differ from Databricks URLs on the commercial offering. Use the following URLs for Databricks on AWS GovCloud:  \nDatabricks account console URL: https://accounts.cloud.databricks.us  \nBase URL for account-level REST APIs: https://accounts.cloud.databricks.us/  \nDatabricks workspace URL: https://<deployment-name>.cloud.databricks.us  \nFor example, if the deployment name you specified during workspace creation is ABCSales, your workspace URL is https://abcsales.cloud.databricks.com.us.  \nBase URL for workspace-level REST APIs: https://<deployment-name>.cloud.databricks.us/"
    },
    {
        "id": 1014,
        "url": "https://docs.databricks.com/en/security/privacy/gov-cloud.html",
        "content": "Feature availability\nFeature availability\nNotable features that are supported:  \nUnity Catalog  \nDatabricks Runtime latest versions and LTS versions  \nDatabricks SQL  \nDashboards  \nMLflow experiments  \nOAuth authentication  \nFeatures that are not supported:  \nServerless compute  \nModel serving  \nIn-product messaging  \nDatabricks Marketplace  \nPartner Connect  \nSystem tables  \nCompute metrics  \nIn-product support ticket submission"
    },
    {
        "id": 1015,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "Databricks Runtime 15.3  \nThe following release notes provide information about Databricks Runtime 15.3, powered by Apache Spark 3.5.0.  \nDatabricks released these images in June 2024.  \nNew features and improvements"
    },
    {
        "id": 1016,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "New features and improvements\nPySpark DataSources now support stream reading and writing  \nPySpark DataSources, which enable reading from custom data sources and writing to custom data sinks, now support stream reading and writing. See PySpark custom data sources.  \nDisable column mapping with drop feature  \nYou can now use DROP FEATURE to disable column mapping on Delta tables and downgrade the table protocol. See Disable column mapping.  \nVariant type syntax and functions in Public Preview  \nNative Apache Spark support for working with semi-structured data as VARIANT type is now available in Spark DataFrames and SQL. See Query variant data.  \nVariant type support for Delta Lake in Public Preview  \nYou can now use VARIANT to store semi-structured data in tables backed by Delta Lake. See Variant support in Delta Lake.  \nSupport for different modes of schema evolution in views  \nCREATE VIEW and ALTER VIEW now allows you to set a schema binding mode, enhancing how views handle schema changes in underlying objects. This feature enables views to either tolerate or adapt to schema changes in the underlying objects. It addresses changes in the query schema resulting from modifications to object definitions.  \nSupport for observe() methods added to the Spark Connect Scala client  \nThe Spark Connect Scala client now supports the Dataset.observe() methods. Support for these methods allows you to define metrics to observe on a Dataset object using the ds.observe(\"name\", metrics...) or ds.observe(observationObject, metrics...) methods.  \nImproved stream restart latency for Auto Loader  \nThis release includes a change that improves the stream restart latency for Auto Loader. This improvement is implemented by making the loading of state by the RocksDB state store asynchronous. With this change, you should see an improvement in start times for streams with large states, for example, streams with a large number of already ingested files.  \nReturn a DataFrame as a pyarrow.Table  \nTo support writing a DataFrame directly to an Apache Arrow pyarrow.Table object, this release includes the DataFrame.toArrow() function. To learn more about using Arrow in PySpark, see Apache Arrow in PySpark.  \nPerformance improvement for some window functions"
    },
    {
        "id": 1017,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "Performance improvement for some window functions  \nThis release includes a change that improves the performance of some Spark window functions, specifically functions that do not include an ORDER BY clause or a window_frame parameter. In these cases, the system can rewrite the query to run it using an aggregate function. This change allows the query to run faster by using partial aggregation and avoiding the overhead of running window functions. The Spark configuration parameter spark.databricks.optimizer.replaceWindowsWithAggregates.enabled controls this optimization and is set to true by default. To turn this optimization off, set spark.databricks.optimizer.replaceWindowsWithAggregates.enabled to false.  \nSupport for the try_mod function added  \nThis release adds support for the PySpark try_mod() function. This function supports the ANSI SQL-compatible calculation of the integer remainder from dividing two numeric values. If the divisor argument is 0, the try_mod() function returns null instead of throwing an exception. You can use the try_mod() function instead of mod or %, which throw an exception if the divisor argument is 0 and ANSI SQL is enabled.  \nSupport for an optional scope parameter in the list_secrets()  \nThis release allows you to limit the results of the list_secrets() table function to a specific scope."
    },
    {
        "id": 1018,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "Bug fixes\nWriteIntoDeltaCommand metrics in the Spark UI now display correctly  \nThis release includes a fix to the metrics displayed in the SQL tab of the Spark UI for the Execute WriteIntoDeltaCommand node. Previously, the metrics shown for this node were all zero.  \nThe groupby() function in the Pandas API ignored the as_index=False argument  \nThis release includes a fix for an issue with the groupby() function in the Pandas API on Spark. Before this fix, a groupby() with relabeling of aggregate columns and the as_index=False argument did not include group keys in the resulting DataFrame.  \nParsing error with multiple window functions that reference output from the other functions  \nThis release fixes an issue that might occur when a Spark query has multiple consecutive window functions, where the window functions reference output from the other window functions. In rare cases, the system might drop one of the references, leading to errors in evaluating the query. This fix projects these references to ensure query evaluation. This change is controlled by the Spark configuration parameter spark.databricks.optimizer.collapseWindows.projectReferences, which is set to true by default. To turn off this change, set spark.databricks.optimizer.collapseWindows.projectReferences to false."
    },
    {
        "id": 1019,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "Library upgrades\nLibrary upgrades\nUpgraded Python libraries:  \nfilelock from 3.13.1 to 3.13.4  \nUpgraded R libraries:  \nUpgraded Java libraries:  \norg.rocksdb.rocksdbjni from 8.3.2 to 8.11.4\n\nApache Spark"
    },
    {
        "id": 1020,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "Apache Spark\nDatabricks Runtime 15.3 includes Apache Spark 3.5.0. This release includes all Spark fixes and improvements included in Databricks Runtime 15.2, as well as the following additional bug fixes and improvements made to Spark:  \n[SPARK-48288] [SC-166141] Add source data type for connector cast expression  \n[SPARK-48175] [SC-166341][SQL][PYTHON] Store collation information in metadata and not in type for SER/DE  \n[SPARK-48172] [SC-166078][SQL] Fix escaping issues in JDBCDialects  \n[SPARK-48186] [SC-165544][SQL] Add support for AbstractMapType  \n[SPARK-48031] [SC-166523] Grandfather legacy views to SCHEMA BINDING  \n[SPARK-48369] [SC-166566][SQL][PYTHON][CONNECT] Add function timestamp_add  \n[SPARK-48330] [SC-166458][SS][PYTHON] Fix the python streaming data source timeout issue for large trigger interval  \n[SPARK-48369] Revert \u201c[SC-166494][SQL][PYTHON][CONNECT] Add function timestamp_add\u201d  \n[SPARK-48336] [SC-166495][PS][CONNECT] Implement ps.sql in Spark Connect  \n[SPARK-48369] [SC-166494][SQL][PYTHON][CONNECT] Add function timestamp_add  \n[SPARK-47354] [SC-165546][SQL] Add collation support for variant expressions  \n[SPARK-48161] [SC-165325][SQL] Fix reverted PR - Add collation support for JSON expressions  \n[SPARK-48031] [SC-166391] Decompose viewSchemaMode config, add SHOW CREATE TABLE support  \n[SPARK-48268] [SC-166217][CORE] Add a configuration for SparkContext.setCheckpointDir"
    },
    {
        "id": 1021,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "[SPARK-48268] [SC-166217][CORE] Add a configuration for SparkContext.setCheckpointDir  \n[SPARK-48312] [ES-1076198][SQL] Improve Alias.removeNonInheritableMetadata performance  \n[SPARK-48294] [SC-166205][SQL] Handle lowercase in nestedTypeMissingElementTypeError  \n[SPARK-47254] [SC-158847][SQL] Assign names to the error classes LEGACYERROR_TEMP_325[1-9]  \n[SPARK-47255] [SC-158749][SQL] Assign names to the error classes LEGACYERROR_TEMP_323[6-7] and LEGACYERROR_TEMP_324[7-9]  \n[SPARK-48310] [SC-166239][PYTHON][CONNECT] Cached properties must return copies  \n[SPARK-48308] [SC-166152][Core] Unify getting data schema without partition columns in FileSourceStrategy  \n[SPARK-48301] [SC-166143][SQL] Rename CREATE_FUNC_WITH_IF_NOT_EXISTS_AND_REPLACE to CREATE_ROUTINE_WITH_IF_NOT_EXISTS_AND_REPLACE  \n[SPARK-48287] [SC-166139][PS][CONNECT] Apply the builtin timestamp_diff method  \n[SPARK-47607] [SC-166129] Add documentation for Structured logging framework  \n[SPARK-48297] [SC-166136][SQL] Fix a regression TRANSFORM clause with char/varchar  \n[SPARK-47045] [SC-156808][SQL] Replace IllegalArgumentException by SparkIllegalArgumentException in sql/api  \n[SPARK-46991] [SC-156399][SQL] Replace IllegalArgumentException by SparkIllegalArgumentException in catalyst"
    },
    {
        "id": 1022,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "[SPARK-46991] [SC-156399][SQL] Replace IllegalArgumentException by SparkIllegalArgumentException in catalyst  \n[SPARK-48031] [SC-165805][SQL] Support view schema evolution  \n[SPARK-48276] [SC-166116][PYTHON][CONNECT] Add the missing __repr__ method for SQLExpression  \n[SPARK-48278] [SC-166071][PYTHON][CONNECT] Refine the string representation of Cast  \n[SPARK-48272] [SC-166069][SQL][PYTHON][CONNECT] Add function timestamp_diff  \n[SPARK-47798] [SC-162492][SQL] Enrich the error message for the reading failures of decimal values  \n[SPARK-47158] [SC-158078][SQL] Assign proper name and sqlState to _LEGACY_ERROR_TEMP_(2134|2231)  \n[SPARK-48218] [SC-166082][CORE] TransportClientFactory.createClient may NPE cause FetchFailedException  \n[SPARK-48277] [SC-166066] Improve error message for ErrorClassesJsonReader.getErrorMessage  \n[SPARK-47545] [SC-165444][CONNECT] Dataset observe support for the Scala client  \n[SPARK-48954] [SC-171672][SQL] Add try_mod function  \n[SPARK-43258] [SC-158151][SQL] Assign names to error LEGACYERROR_TEMP_202[3,5]  \n[SPARK-48267] [SC-165903][SS] Regression e2e test with SPARK-47305  \n[SPARK-48263] [SC-165925] Collate function support for non UTF8_BINARY strings"
    },
    {
        "id": 1023,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "[SPARK-48263] [SC-165925] Collate function support for non UTF8_BINARY strings  \n[SPARK-48260] [SC-165883][SQL] Disable output committer coordination in one test of ParquetIOSuite  \n[SPARK-47834] [SC-165645][SQL][CONNECT] Mark deprecated functions with @deprecated in SQLImplicits  \n[SPARK-48011] [SC-164061][Core] Store LogKey name as a value to avoid generating new string instances  \n[SPARK-48074] [SC-164647][Core] Improve the readability of JSON loggings  \n[SPARK-44953] [SC-165866][CORE] Log a warning when shuffle tracking is enabled along side another DA supported mechanism  \n[SPARK-48266] [SC-165897][CONNECT] Move package object org.apache.spark.sql.connect.dsl to test directory  \n[SPARK-47847] [SC-165651][CORE] Deprecate spark.network.remoteReadNioBufferConversion  \n[SPARK-48171] [SC-165178][CORE] Clean up the use of deprecated constructors of o.rocksdb.Logger  \n[SPARK-48235] [SC-165669][SQL] Directly pass join instead of all arguments to getBroadcastBuildSide and getShuffleHashJoinBuildSide  \n[SPARK-48224] [SC-165648][SQL] Disallow map keys from being of variant type  \n[SPARK-48248] [SC-165791][PYTHON] Fix nested array to respect legacy conf of inferArrayTypeFromFirstElement  \n[SPARK-47409] [SC-165568][SQL] Add support for collation for StringTrim type of functions/expressions (for UTF8_BINARY & LCASE)"
    },
    {
        "id": 1024,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "[SPARK-47409] [SC-165568][SQL] Add support for collation for StringTrim type of functions/expressions (for UTF8_BINARY & LCASE)  \n[SPARK-48143] [ES-1095638][SQL] Use lightweight exceptions for control-flow between UnivocityParser and FailureSafeParser  \n[SPARK-48146] [SC-165668][SQL] Fix aggregate function in With expression child assertion  \n[SPARK-48161] Revert \u201c[SC-165325][SQL] Add collation support for JSON expressions\u201d  \n[SPARK-47963] [CORE] Make the external Spark ecosystem can use structured logging mechanisms  \n[SPARK-48180] [SC-165632][SQL] Improve error when UDTF call with TABLE arg forgets parentheses around multiple PARTITION/ORDER BY exprs  \n[SPARK-48002] [SC-164132][PYTHON][SS] Add test for observed metrics in PySpark StreamingQueryListener  \n[SPARK-47421] [SC-165548][SQL] Add collation support for URL expressions  \n[SPARK-48161] [SC-165325][SQL] Add collation support for JSON expressions  \n[SPARK-47359] [SC-164328][SQL] Support TRANSLATE function to work with collated strings  \n[SPARK-47365] [SC-165643][PYTHON] Add toArrow() DataFrame method to PySpark  \n[SPARK-48228] [SC-165649][PYTHON][CONNECT] Implement the missing function validation in ApplyInXXX  \n[SPARK-47986] [SC-165549][CONNECT][PYTHON] Unable to create a new session when the default session is closed by the server  \n[SPARK-47963] Revert \u201c[CORE] Make the external Spark ecosystem can use structured logging mechanisms \u201c  \n[SPARK-47963] [CORE] Make the external Spark ecosystem can use structured logging mechanisms"
    },
    {
        "id": 1025,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "[SPARK-47963] Revert \u201c[CORE] Make the external Spark ecosystem can use structured logging mechanisms \u201c  \n[SPARK-47963] [CORE] Make the external Spark ecosystem can use structured logging mechanisms  \n[SPARK-48208] [SC-165547][SS] Skip providing memory usage metrics from RocksDB if bounded memory usage is enabled  \n[SPARK-47566] [SC-164314][SQL] Support SubstringIndex function to work with collated strings  \n[SPARK-47589] [SC-162127][SQL]Hive-Thriftserver: Migrate logError with variables to structured logging framework  \n[SPARK-48205] [SC-165501][PYTHON] Remove the private[sql] modifier for Python data sources  \n[SPARK-48173] [SC-165181][SQL] CheckAnalysis should see the entire query plan  \n[SPARK-48197] [SC-165520][SQL] Avoid assert error for invalid lambda function  \n[SPARK-47318] [SC-162573][CORE] Adds HKDF round to AuthEngine key derivation to follow standard KEX practices  \n[SPARK-48184] [SC-165335][PYTHON][CONNECT] Always set the seed of Dataframe.sample in Client side  \n[SPARK-48128] [SC-165004][SQL] For BitwiseCount / bit_count expression, fix codegen syntax error for boolean type inputs  \n[SPARK-48147] [SC-165144][SS][CONNECT] Remove client side listeners when local Spark session is deleted  \n[SPARK-48045] [SC-165299][PYTHON] Pandas API groupby with multi-agg-relabel ignores as_index=False  \n[SPARK-48191] [SC-165323][SQL] Support UTF-32 for string encode and decode  \n[SPARK-45352] [SC-137627][SQL] Eliminate foldable window partitions"
    },
    {
        "id": 1026,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "[SPARK-45352] [SC-137627][SQL] Eliminate foldable window partitions  \n[SPARK-47297] [SC-165179][SQL] Add collation support for format expressions  \n[SPARK-47267] [SC-165170][SQL] Add collation support for hash expressions  \n[SPARK-47965] [SC-163822][CORE] Avoid orNull in TypedConfigBuilder and OptionalConfigEntry  \n[SPARK-48166] [SQL] Avoid using BadRecordException as user-facing error in VariantExpressionEvalUtils  \n[SPARK-48105] [SC-165132][SS] Fix the race condition between state store unloading and snapshotting  \n[SPARK-47922] [SC-163997][SQL] Implement the try_parse_json expression  \n[SPARK-47719] [SC-161909][SQL] Change spark.sql.legacy.timeParse\u2026  \n[SPARK-48035] [SC-165136][SQL] Fix try_add/try_multiply being semantic equal to add/multiply  \n[SPARK-48107] [SC-164878][PYTHON] Exclude tests from Python distribution  \n[SPARK-46894] [SC-164447][PYTHON] Move PySpark error conditions into standalone JSON file  \n[SPARK-47583] [SC-163825][CORE] SQL core: Migrate logError with variables to structured logging framework  \n[SPARK-48064] Revert \u201c[SC-164697][SQL] Update error messages for routine related error classes\u201d  \n[SPARK-48048] [SC-164846][CONNECT][SS] Added client side listener support for Scala  \n[SPARK-48064] [SC-164697][SQL] Update error messages for routine related error classes"
    },
    {
        "id": 1027,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "[SPARK-48064] [SC-164697][SQL] Update error messages for routine related error classes  \n[SPARK-48056] [SC-164715][CONNECT][PYTHON] Re-execute plan if a SESSION_NOT_FOUND error is raised and no partial response was received  \n[SPARK-48124] [SC-165009][CORE] Disable structured logging for Connect-Repl by default  \n[SPARK-47922] Revert \u201c[SC-163997][SQL] Implement the try_parse_json expression\u201d  \n[SPARK-48065] [SC-164817][SQL] SPJ: allowJoinKeysSubsetOfPartitionKeys is too strict  \n[SPARK-48114] [SC-164956][CORE] Precompile template regex to avoid unnecessary work  \n[SPARK-48067] [SC-164773][SQL] Fix variant default columns  \n[SPARK-47922] [SC-163997][SQL] Implement the try_parse_json expression  \n[SPARK-48075] [SC-164709] [SS] Add type checking for PySpark avro functions  \n[SPARK-47793] [SC-164324][SS][PYTHON] Implement SimpleDataSourceStreamReader for python streaming data source  \n[SPARK-48081] [SC-164719] Fix ClassCastException in NTile.checkInputDataTypes() when argument is non-foldable or of wrong type  \n[SPARK-48102] [SC-164853][SS] Track duration for acquiring source/sink metrics while reporting streaming query progress  \n[SPARK-47671] [SC-164854][Core] Enable structured logging in log4j2.properties.template and update docs  \n[SPARK-47904] [SC-163409][SQL] Preserve case in Avro schema when using enableStableIdentifiersForUnionType"
    },
    {
        "id": 1028,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "[SPARK-47904] [SC-163409][SQL] Preserve case in Avro schema when using enableStableIdentifiersForUnionType  \n[SPARK-48058] [SC-164695][SPARK-43727][PYTHON][CONNECT] UserDefinedFunction.returnType parse the DDL string  \n[SPARK-46922] [SC-156102][CORE][SQL] Do not wrap runtime user-facing errors  \n[SPARK-48047] [SC-164576][SQL] Reduce memory pressure of empty TreeNode tags  \n[SPARK-47934] [SC-164648] [CORE] Ensure trailing slashes in HistoryServer URL redirections  \n[SPARK-47921] [SC-163826][CONNECT] Fix ExecuteJobTag creation in ExecuteHolder  \n[SPARK-48014] [SC-164150][SQL] Change the makeFromJava error in EvaluatePython to a user-facing error  \n[SPARK-47903] [SC-163424][PYTHON] Add support for remaining scalar types in the PySpark Variant library  \n[SPARK-48068] [SC-164579][PYTHON] mypy should have --python-executable parameter  \n[SPARK-47846] [SC-163074][SQL] Add support for Variant type in from_json expression  \n[SPARK-48050] [SC-164322][SS] Log logical plan at query start  \n[SPARK-47810] [SC-163076][SQL] Replace equivalent expression to <=> in join condition  \n[SPARK-47741] [SC-164290] Added stack overflow handling in parser  \n[SPARK-48053] [SC-164309][PYTHON][CONNECT] SparkSession.createDataFrame should warn for unsupported options"
    },
    {
        "id": 1029,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "[SPARK-48053] [SC-164309][PYTHON][CONNECT] SparkSession.createDataFrame should warn for unsupported options  \n[SPARK-47939] [SC-164171][SQL] Implement a new Analyzer rule to move ParameterizedQuery inside ExplainCommand and DescribeQueryCommand  \n[SPARK-48063] [SC-164432][CORE] Enable spark.stage.ignoreDecommissionFetchFailure by default  \n[SPARK-48016] [SC-164280][SQL] Fix a bug in try_divide function when with decimals  \n[SPARK-48033] [SC-164291][SQL] Fix RuntimeReplaceable expressions being used in default columns  \n[SPARK-48010] [SC-164051][SQL][ES-1109148] Avoid repeated calls to conf.resolver in resolveExpression  \n[SPARK-48003] [SC-164323][SQL] Add collation support for hll sketch aggregate  \n[SPARK-47994] [SC-164175][SQL] Fix bug with CASE WHEN column filter push down in SQLServer  \n[SPARK-47984] [SC-163837][ML][SQL] Change MetricsAggregate/V2Aggregator#serialize/deserialize to call SparkSerDeUtils#serialize/deserialize  \n[SPARK-48042] [SC-164202][SQL] Use a timestamp formatter with timezone at class level instead of making copies at method level  \n[SPARK-47596] [SPARK-47600][SPARK-47604][SPARK-47804] Structured log migrations  \n[SPARK-47594] [SPARK-47590][SPARK-47588][SPARK-47584] Structured log migrations  \n[SPARK-47567] [SC-164172][SQL] Support LOCATE function to work with collated strings"
    },
    {
        "id": 1030,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "[SPARK-47567] [SC-164172][SQL] Support LOCATE function to work with collated strings  \n[SPARK-47764] [SC-163820][CORE][SQL] Cleanup shuffle dependencies based on ShuffleCleanupMode  \n[SPARK-48039] [SC-164174][PYTHON][CONNECT] Update the error class for group.apply  \n[SPARK-47414] [SC-163831][SQL] Lowercase collation support for regexp expressions  \n[SPARK-47418] [SC-163818][SQL] Add hand-crafted implementations for lowercase unicode-aware contains, startsWith and endsWith and optimize UTF8_BINARY_LCASE  \n[SPARK-47580] [SC-163834][SQL] SQL catalyst: eliminate unnamed variables in error logs  \n[SPARK-47351] [SC-164008][SQL] Add collation support for StringToMap & Mask string expressions  \n[SPARK-47292] [SC-164151][SS] safeMapToJValue should consider null typed values  \n[SPARK-47476] [SC-164014][SQL] Support REPLACE function to work with collated strings  \n[SPARK-47408] [SC-164104][SQL] Fix mathExpressions that use StringType  \n[SPARK-47350] [SC-164006][SQL] Add collation support for SplitPart string expression  \n[SPARK-47692] [SC-163819][SQL] Fix default StringType meaning in implicit casting  \n[SPARK-47094] [SC-161957][SQL] SPJ : Dynamically rebalance number of buckets when they are not equal  \n[SPARK-48001] [SC-164000][CORE] Remove unused private implicit def arrayToArrayWritable from SparkContext"
    },
    {
        "id": 1031,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "[SPARK-48001] [SC-164000][CORE] Remove unused private implicit def arrayToArrayWritable from SparkContext  \n[SPARK-47986] [SC-163999][CONNECT][PYTHON] Unable to create a new session when the default session is closed by the server  \n[SPARK-48019] [SC-164121] Fix incorrect behavior in ColumnVector/ColumnarArray with dictionary and nulls  \n[SPARK-47743] [SPARK-47592][SPARK-47589][SPARK-47581][SPARK-47586][SPARK-47593][SPARK-47595][SPARK-47587][SPARK-47603] Structured log migrations  \n[SPARK-47999] [SC-163998][SS] Improve logging around snapshot creation and adding/removing entries from state cache map in HDFS backed state store provider  \n[SPARK-47417] [SC-162872][SQL] Collation support: Ascii, Chr, Base64, UnBase64, Decode, StringDecode, Encode, ToBinary, FormatNumber, Sentences  \n[SPARK-47712] [SC-161871][CONNECT] Allow connect plugins to create and process Datasets  \n[SPARK-47411] [SC-163351][SQL] Support StringInstr & FindInSet functions to work with collated strings  \n[SPARK-47360] [SC-163014][SQL] Collation support: Overlay, FormatString, Length, BitLength, OctetLength, SoundEx, Luhncheck  \n[SPARK-47413] [SC-163323][SQL] - add support to substr/left/right for collations  \n[SPARK-47983] [SC-163835][SQL] Demote spark.sql.pyspark.legacy.inferArrayTypeFromFirstElement.enabled to internal"
    },
    {
        "id": 1032,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "[SPARK-47983] [SC-163835][SQL] Demote spark.sql.pyspark.legacy.inferArrayTypeFromFirstElement.enabled to internal  \n[SPARK-47964] [SC-163816][PYTHON][CONNECT] Hide SQLContext and HiveContext in pyspark-connect  \n[SPARK-47985] [SC-163841][PYTHON] Simplify functions with lit  \n[SPARK-47633] [SC-163453][SQL] Include right-side plan output in LateralJoin#allAttributes for more consistent canonicalization  \n[SPARK-47909] Revert \u201c[PYTHON][CONNECT] Parent DataFrame class f\u2026  \n[SPARK-47767] [SC-163096][SQL] Show offset value in TakeOrderedAndProjectExec  \n[SPARK-47822] [SC-162968][SQL] Prohibit Hash Expressions from hashing the Variant Data Type  \n[SPARK-47490] [SC-160494][SS] Fix RocksDB Logger constructor use to avoid deprecation warning  \n[SPARK-47417] Revert \u201c[SC-162872][SQL] Collation support: Ascii, Chr, Base64, UnBase64, Decode, StringDecode, Encode, ToBinary, FormatNumber, Sentences\u201d  \n[SPARK-47352] [SC-163460][SQL] Fix Upper, Lower, InitCap collation awareness  \n[SPARK-47909] [PYTHON][CONNECT] Parent DataFrame class for Spark Connect and Spark Classic  \n[SPARK-47417] [SC-162872][SQL] Collation support: Ascii, Chr, Base64, UnBase64, Decode, StringDecode, Encode, ToBinary, FormatNumber, Sentences"
    },
    {
        "id": 1033,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "Databricks ODBC/JDBC driver support\nDatabricks ODBC/JDBC driver support\nDatabricks supports ODBC/JDBC drivers released in the past 2 years. Please download the recently released drivers and upgrade (download ODBC, download JDBC).\n\nSystem environment"
    },
    {
        "id": 1034,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "System environment\nOperating System: Ubuntu 22.04.4 LTS  \nJava: Zulu 8.78.0.19-CA-linux64  \nScala: 2.12.18  \nPython: 3.11.0  \nR: 4.3.2  \nDelta Lake: 3.2.0  \nInstalled Python libraries  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nasttokens  \n2.0.5  \nastunparse  \n1.6.3  \nazure-core  \n1.30.1  \nazure-storage-blob  \n12.19.1  \nazure-storage-file-datalake  \n12.14.0  \nbackcall  \n0.2.0  \nblack  \n23.3.0  \nblinker  \n1.4  \nboto3  \n1.34.39  \nbotocore  \n1.34.39  \ncachetools  \n5.3.3  \ncertifi  \n2023.7.22  \ncffi  \n1.15.1  \nchardet  \n4.0.0  \ncharset-normalizer  \n2.0.4  \nclick  \n8.0.4  \ncloudpickle  \n2.2.1  \ncomm  \n0.1.2  \ncontourpy  \n1.0.5  \ncryptography  \n41.0.3  \ncycler  \n0.11.0  \nCython  \n0.29.32  \ndatabricks-sdk  \n0.20.0  \ndbus-python  \n1.2.18  \ndebugpy  \n1.6.7  \ndecorator  \n5.1.1  \ndistlib  \n0.3.8  \nentrypoints  \n0.4  \nexecuting  \n0.8.3  \nfacets-overview  \n1.1.1  \nfilelock  \n3.13.4  \nfonttools  \n4.25.0  \ngitdb  \n4.0.11  \nGitPython  \n3.1.43  \ngoogle-api-core  \n2.18.0  \ngoogle-auth  \n2.29.0  \ngoogle-cloud-core  \n2.4.1  \ngoogle-cloud-storage  \n2.16.0  \ngoogle-crc32c  \n1.5.0  \ngoogle-resumable-media  \n2.7.0  \ngoogleapis-common-protos  \n1.63.0  \ngrpcio  \n1.60.0  \ngrpcio-status  \n1.60.0  \nhttplib2  \n0.20.2  \nidna  \n3.4  \nimportlib-metadata  \n6.0.0  \nipyflow-core  \n0.0.198  \nipykernel  \n6.25.1  \nipython  \n8.15.0  \nipython-genutils"
    },
    {
        "id": 1035,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "idna  \n3.4  \nimportlib-metadata  \n6.0.0  \nipyflow-core  \n0.0.198  \nipykernel  \n6.25.1  \nipython  \n8.15.0  \nipython-genutils  \n0.2.0  \nipywidgets  \n7.7.2  \nisodate  \n0.6.1  \njedi  \n0.18.1  \njeepney  \n0.7.1  \njmespath  \n0.10.0  \njoblib  \n1.2.0  \njupyter_client  \n7.4.9  \njupyter_core  \n5.3.0  \nkeyring  \n23.5.0  \nkiwisolver  \n1.4.4  \nlaunchpadlib  \n1.10.16  \nlazr.restfulclient  \n0.14.4  \nlazr.uri  \n1.0.6  \nmatplotlib  \n3.7.2  \nmatplotlib-inline  \n0.1.6  \nmlflow-skinny  \n2.11.3  \nmore-itertools  \n8.10.0  \nmypy-extensions  \n0.4.3  \nnest-asyncio  \n1.5.6  \nnumpy  \n1.23.5  \noauthlib  \n3.2.0  \npackaging  \n23.2  \npandas  \n1.5.3  \nparso  \n0.8.3  \npathspec  \n0.10.3  \npatsy  \n0.5.3  \npexpect  \n4.8.0  \npickleshare  \n0.7.5  \nPillow  \n9.4.0  \npip  \n23.2.1  \nplatformdirs  \n3.10.0  \nplotly  \n5.9.0  \nprompt-toolkit  \n3.0.36  \nproto-plus  \n1.23.0  \nprotobuf  \n4.24.1  \npsutil  \n5.9.0  \npsycopg2  \n2.9.3  \nptyprocess  \n0.7.0  \npure-eval  \n0.2.2  \npyarrow  \n14.0.1  \npyasn1  \n0.4.8  \npyasn1-modules  \n0.2.8  \npyccolo  \n0.0.52  \npycparser  \n2.21  \npydantic  \n1.10.6  \nPygments  \n2.15.1  \nPyGObject  \n3.42.1  \nPyJWT  \n2.3.0  \npyodbc  \n4.0.38  \npyparsing  \n3.0.9  \npython-dateutil  \n2.8.2  \npython-lsp-jsonrpc  \n1.1.1  \npytz  \n2022.7"
    },
    {
        "id": 1036,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "pyodbc  \n4.0.38  \npyparsing  \n3.0.9  \npython-dateutil  \n2.8.2  \npython-lsp-jsonrpc  \n1.1.1  \npytz  \n2022.7  \nPyYAML  \n6.0  \npyzmq  \n23.2.0  \nrequests  \n2.31.0  \nrsa  \n4.9  \ns3transfer  \n0.10.1  \nscikit-learn  \n1.3.0  \nscipy  \n1.11.1  \nseaborn  \n0.12.2  \nSecretStorage  \n3.3.1  \nsetuptools  \n68.0.0  \nsix  \n1.16.0  \nsmmap  \n5.0.1  \nsqlparse  \n0.5.0  \nssh-import-id  \n5.11  \nstack-data  \n0.2.0  \nstatsmodels  \n0.14.0  \ntenacity  \n8.2.2  \nthreadpoolctl  \n2.2.0  \ntokenize-rt  \n4.2.1  \ntornado  \n6.3.2  \ntraitlets  \n5.7.1  \ntyping_extensions  \n4.10.0  \ntzdata  \n2022.1  \nujson  \n5.4.0  \nunattended-upgrades  \n0.1  \nurllib3  \n1.26.16  \nvirtualenv  \n20.24.2  \nwadllib  \n1.3.6  \nwcwidth  \n0.2.5  \nwheel  \n0.38.4  \nzipp  \n3.11.0  \nInstalled R libraries  \nR libraries are installed from the Posit Package Manager CRAN snapshot.  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \narrow  \n14.0.0.2  \naskpass  \n1.2.0  \nassertthat  \n0.2.1  \nbackports  \n1.4.1  \nbase  \n4.3.2  \nbase64enc  \n0.1-3  \nbigD  \n0.2.0  \nbit  \n4.0.5  \nbit64  \n4.0.5  \nbitops  \n1.0-7  \nblob  \n1.2.4  \nboot  \n1.3-28  \nbrew  \n1.0-10  \nbrio  \n1.1.4  \nbroom  \n1.0.5  \nbslib  \n0.6.1  \ncachem  \n1.0.8  \ncallr  \n3.7.3  \ncaret  \n6.0-94  \ncellranger  \n1.1.0  \nchron  \n2.3-61  \nclass  \n7.3-22  \ncli  \n3.6.2  \nclipr  \n0.8.0  \nclock  \n0.7.0  \ncluster  \n2.1.4  \ncodetools  \n0.2-19  \ncolorspace"
    },
    {
        "id": 1037,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "chron  \n2.3-61  \nclass  \n7.3-22  \ncli  \n3.6.2  \nclipr  \n0.8.0  \nclock  \n0.7.0  \ncluster  \n2.1.4  \ncodetools  \n0.2-19  \ncolorspace  \n2.1-0  \ncommonmark  \n1.9.1  \ncompiler  \n4.3.2  \nconfig  \n0.3.2  \nconflicted  \n1.2.0  \ncpp11  \n0.4.7  \ncrayon  \n1.5.2  \ncredentials  \n2.0.1  \ncurl  \n5.2.0  \ndata.table  \n1.15.0  \ndatasets  \n4.3.2  \nDBI  \n1.2.1  \ndbplyr  \n2.4.0  \ndesc  \n1.4.3  \ndevtools  \n2.4.5  \ndiagram  \n1.6.5  \ndiffobj  \n0.3.5  \ndigest  \n0.6.34  \ndownlit  \n0.4.3  \ndplyr  \n1.1.4  \ndtplyr  \n1.3.1  \ne1071  \n1.7-14  \nellipsis  \n0.3.2  \nevaluate  \n0.23  \nfansi  \n1.0.6  \nfarver  \n2.1.1  \nfastmap  \n1.1.1  \nfontawesome  \n0.5.2  \nforcats  \n1.0.0  \nforeach  \n1.5.2  \nforeign  \n0.8-85  \nforge  \n0.2.0  \nfs  \n1.6.3  \nfuture  \n1.33.1  \nfuture.apply  \n1.11.1  \ngargle  \n1.5.2  \ngenerics  \n0.1.3  \ngert  \n2.0.1  \nggplot2  \n3.4.4  \ngh  \n1.4.0  \ngit2r  \n0.33.0  \ngitcreds  \n0.1.2  \nglmnet  \n4.1-8  \nglobals  \n0.16.2  \nglue  \n1.7.0  \ngoogledrive  \n2.1.1  \ngooglesheets4  \n1.1.1  \ngower  \n1.0.1  \ngraphics  \n4.3.2  \ngrDevices  \n4.3.2  \ngrid  \n4.3.2  \ngridExtra  \n2.3  \ngsubfn  \n0.7  \ngt  \n0.10.1  \ngtable  \n0.3.4  \nhardhat  \n1.3.1  \nhaven  \n2.5.4  \nhighr  \n0.10  \nhms  \n1.1.3  \nhtmltools  \n0.5.7  \nhtmlwidgets  \n1.6.4  \nhttpuv  \n1.6.14  \nhttr  \n1.4.7  \nhttr2"
    },
    {
        "id": 1038,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "highr  \n0.10  \nhms  \n1.1.3  \nhtmltools  \n0.5.7  \nhtmlwidgets  \n1.6.4  \nhttpuv  \n1.6.14  \nhttr  \n1.4.7  \nhttr2  \n1.0.0  \nids  \n1.0.1  \nini  \n0.3.1  \nipred  \n0.9-14  \nisoband  \n0.2.7  \niterators  \n1.0.14  \njquerylib  \n0.1.4  \njsonlite  \n1.8.8  \njuicyjuice  \n0.1.0  \nKernSmooth  \n2.23-21  \nknitr  \n1.45  \nlabeling  \n0.4.3  \nlater  \n1.3.2  \nlattice  \n0.21-8  \nlava  \n1.7.3  \nlifecycle  \n1.0.4  \nlistenv  \n0.9.1  \nlubridate  \n1.9.3  \nmagrittr  \n2.0.3  \nmarkdown  \n1.12  \nMASS  \n7.3-60  \nMatrix  \n1.5-4.1  \nmemoise  \n2.0.1  \nmethods  \n4.3.2  \nmgcv  \n1.8-42  \nmime  \n0.12  \nminiUI  \n0.1.1.1  \nmlflow  \n2.10.0  \nModelMetrics  \n1.2.2.2  \nmodelr  \n0.1.11  \nmunsell  \n0.5.0  \nnlme  \n3.1-163  \nnnet  \n7.3-19  \nnumDeriv  \n2016.8-1.1  \nopenssl  \n2.1.1  \nparallel  \n4.3.2  \nparallelly  \n1.36.0  \npillar  \n1.9.0  \npkgbuild  \n1.4.3  \npkgconfig  \n2.0.3  \npkgdown  \n2.0.7  \npkgload  \n1.3.4  \nplogr  \n0.2.0  \nplyr  \n1.8.9  \npraise  \n1.0.0  \nprettyunits  \n1.2.0  \npROC  \n1.18.5  \nprocessx  \n3.8.3  \nprodlim  \n2023.08.28  \nprofvis  \n0.3.8  \nprogress  \n1.2.3  \nprogressr  \n0.14.0  \npromises  \n1.2.1  \nproto  \n1.0.0  \nproxy  \n0.4-27  \nps  \n1.7.6  \npurrr  \n1.0.2  \nR6  \n2.5.1  \nragg  \n1.2.7  \nrandomForest  \n4.7-1.1"
    },
    {
        "id": 1039,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "proto  \n1.0.0  \nproxy  \n0.4-27  \nps  \n1.7.6  \npurrr  \n1.0.2  \nR6  \n2.5.1  \nragg  \n1.2.7  \nrandomForest  \n4.7-1.1  \nrappdirs  \n0.3.3  \nrcmdcheck  \n1.4.0  \nRColorBrewer  \n1.1-3  \nRcpp  \n1.0.12  \nRcppEigen  \n0.3.3.9.4  \nreactable  \n0.4.4  \nreactR  \n0.5.0  \nreadr  \n2.1.5  \nreadxl  \n1.4.3  \nrecipes  \n1.0.9  \nrematch  \n2.0.0  \nrematch2  \n2.1.2  \nremotes  \n2.4.2.1  \nreprex  \n2.1.0  \nreshape2  \n1.4.4  \nrlang  \n1.1.3  \nrmarkdown  \n2.25  \nRODBC  \n1.3-23  \nroxygen2  \n7.3.1  \nrpart  \n4.1.21  \nrprojroot  \n2.0.4  \nRserve  \n1.8-13  \nRSQLite  \n2.3.5  \nrstudioapi  \n0.15.0  \nrversions  \n2.1.2  \nrvest  \n1.0.3  \nsass  \n0.4.8  \nscales  \n1.3.0  \nselectr  \n0.4-2  \nsessioninfo  \n1.2.2  \nshape  \n1.4.6  \nshiny  \n1.8.0  \nsourcetools  \n0.1.7-1  \nsparklyr  \n1.8.4  \nspatial  \n7.3-15  \nsplines  \n4.3.2  \nsqldf  \n0.4-11  \nSQUAREM  \n2021.1  \nstats  \n4.3.2  \nstats4  \n4.3.2  \nstringi  \n1.8.3  \nstringr  \n1.5.1  \nsurvival  \n3.5-5  \nswagger  \n3.33.1  \nsys  \n3.4.2  \nsystemfonts  \n1.0.5  \ntcltk  \n4.3.2  \ntestthat  \n3.2.1  \ntextshaping  \n0.3.7  \ntibble  \n3.2.1  \ntidyr  \n1.3.1  \ntidyselect  \n1.2.0  \ntidyverse  \n2.0.0  \ntimechange  \n0.3.0  \ntimeDate  \n4032.109  \ntinytex  \n0.49  \ntools  \n4.3.2  \ntzdb  \n0.4.0  \nurlchecker  \n1.0.1"
    },
    {
        "id": 1040,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "2.0.0  \ntimechange  \n0.3.0  \ntimeDate  \n4032.109  \ntinytex  \n0.49  \ntools  \n4.3.2  \ntzdb  \n0.4.0  \nurlchecker  \n1.0.1  \nusethis  \n2.2.2  \nutf8  \n1.2.4  \nutils  \n4.3.2  \nuuid  \n1.2-0  \nV8  \n4.4.1  \nvctrs  \n0.6.5  \nviridisLite  \n0.4.2  \nvroom  \n1.6.5  \nwaldo  \n0.5.2  \nwhisker  \n0.4.1  \nwithr  \n3.0.0  \nxfun  \n0.41  \nxml2  \n1.3.6  \nxopen  \n1.0.0  \nxtable  \n1.8-4  \nyaml  \n2.3.8  \nzeallot  \n0.1.0  \nzip  \n2.3.1  \nInstalled Java and Scala libraries (Scala 2.12 cluster version)  \nGroup ID  \nArtifact ID  \nVersion  \nantlr  \nantlr  \n2.7.7  \ncom.amazonaws  \namazon-kinesis-client  \n1.12.0  \ncom.amazonaws  \naws-java-sdk-autoscaling  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-cloudformation  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-cloudfront  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-cloudhsm  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-cloudsearch  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-cloudtrail  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-cloudwatch  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-cloudwatchmetrics  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-codedeploy  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-cognitoidentity  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-cognitosync  \n1.12.610  \ncom.amazonaws"
    },
    {
        "id": 1041,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "aws-java-sdk-cognitoidentity  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-cognitosync  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-config  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-core  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-datapipeline  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-directconnect  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-directory  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-dynamodb  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-ec2  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-ecs  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-efs  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-elasticache  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-elasticbeanstalk  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-elasticloadbalancing  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-elastictranscoder  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-emr  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-glacier  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-glue  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-iam  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-importexport  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-kinesis  \n1.12.610  \ncom.amazonaws"
    },
    {
        "id": 1042,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "aws-java-sdk-importexport  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-kinesis  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-kms  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-lambda  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-logs  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-machinelearning  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-opsworks  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-rds  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-redshift  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-route53  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-s3  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-ses  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-simpledb  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-simpleworkflow  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-sns  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-sqs  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-ssm  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-storagegateway  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-sts  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-support  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-swf-libraries  \n1.11.22  \ncom.amazonaws"
    },
    {
        "id": 1043,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "aws-java-sdk-support  \n1.12.610  \ncom.amazonaws  \naws-java-sdk-swf-libraries  \n1.11.22  \ncom.amazonaws  \naws-java-sdk-workspaces  \n1.12.610  \ncom.amazonaws  \njmespath-java  \n1.12.610  \ncom.clearspring.analytics  \nstream  \n2.9.6  \ncom.databricks  \nRserve  \n1.8-3  \ncom.databricks  \ndatabricks-sdk-java  \n0.17.1  \ncom.databricks  \njets3t  \n0.7.1-0  \ncom.databricks.scalapb  \ncompilerplugin_2.12  \n0.4.15-10  \ncom.databricks.scalapb  \nscalapb-runtime_2.12  \n0.4.15-10  \ncom.esotericsoftware  \nkryo-shaded  \n4.0.2  \ncom.esotericsoftware  \nminlog  \n1.3.0  \ncom.fasterxml  \nclassmate  \n1.3.4  \ncom.fasterxml.jackson.core  \njackson-annotations  \n2.15.2  \ncom.fasterxml.jackson.core  \njackson-core  \n2.15.2  \ncom.fasterxml.jackson.core  \njackson-databind  \n2.15.2  \ncom.fasterxml.jackson.dataformat  \njackson-dataformat-cbor  \n2.15.2  \ncom.fasterxml.jackson.dataformat  \njackson-dataformat-yaml  \n2.15.2  \ncom.fasterxml.jackson.datatype  \njackson-datatype-joda  \n2.15.2  \ncom.fasterxml.jackson.datatype  \njackson-datatype-jsr310  \n2.16.0  \ncom.fasterxml.jackson.module  \njackson-module-paranamer  \n2.15.2  \ncom.fasterxml.jackson.module  \njackson-module-scala_2.12  \n2.15.2  \ncom.github.ben-manes.caffeine  \ncaffeine  \n2.9.3  \ncom.github.fommil  \njniloader  \n1.1  \ncom.github.fommil.netlib  \nnative_ref-java  \n1.1"
    },
    {
        "id": 1044,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "caffeine  \n2.9.3  \ncom.github.fommil  \njniloader  \n1.1  \ncom.github.fommil.netlib  \nnative_ref-java  \n1.1  \ncom.github.fommil.netlib  \nnative_ref-java  \n1.1-natives  \ncom.github.fommil.netlib  \nnative_system-java  \n1.1  \ncom.github.fommil.netlib  \nnative_system-java  \n1.1-natives  \ncom.github.fommil.netlib  \nnetlib-native_ref-linux-x86_64  \n1.1-natives  \ncom.github.fommil.netlib  \nnetlib-native_system-linux-x86_64  \n1.1-natives  \ncom.github.luben  \nzstd-jni  \n1.5.5-4  \ncom.github.wendykierp  \nJTransforms  \n3.1  \ncom.google.code.findbugs  \njsr305  \n3.0.0  \ncom.google.code.gson  \ngson  \n2.10.1  \ncom.google.crypto.tink  \ntink  \n1.9.0  \ncom.google.errorprone  \nerror_prone_annotations  \n2.10.0  \ncom.google.flatbuffers  \nflatbuffers-java  \n23.5.26  \ncom.google.guava  \nguava  \n15.0  \ncom.google.protobuf  \nprotobuf-java  \n2.6.1  \ncom.helger  \nprofiler  \n1.1.1  \ncom.ibm.icu  \nicu4j  \n72.1  \ncom.jcraft  \njsch  \n0.1.55  \ncom.jolbox  \nbonecp  \n0.8.0.RELEASE  \ncom.lihaoyi  \nsourcecode_2.12  \n0.1.9  \ncom.microsoft.azure  \nazure-data-lake-store-sdk  \n2.3.9  \ncom.microsoft.sqlserver  \nmssql-jdbc  \n11.2.2.jre8  \ncom.ning  \ncompress-lzf  \n1.1.2  \ncom.sun.mail  \njavax.mail  \n1.5.2  \ncom.sun.xml.bind  \njaxb-core  \n2.2.11  \ncom.sun.xml.bind  \njaxb-impl"
    },
    {
        "id": 1045,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "1.1.2  \ncom.sun.mail  \njavax.mail  \n1.5.2  \ncom.sun.xml.bind  \njaxb-core  \n2.2.11  \ncom.sun.xml.bind  \njaxb-impl  \n2.2.11  \ncom.tdunning  \njson  \n1.8  \ncom.thoughtworks.paranamer  \nparanamer  \n2.8  \ncom.trueaccord.lenses  \nlenses_2.12  \n0.4.12  \ncom.twitter  \nchill-java  \n0.10.0  \ncom.twitter  \nchill_2.12  \n0.10.0  \ncom.twitter  \nutil-app_2.12  \n7.1.0  \ncom.twitter  \nutil-core_2.12  \n7.1.0  \ncom.twitter  \nutil-function_2.12  \n7.1.0  \ncom.twitter  \nutil-jvm_2.12  \n7.1.0  \ncom.twitter  \nutil-lint_2.12  \n7.1.0  \ncom.twitter  \nutil-registry_2.12  \n7.1.0  \ncom.twitter  \nutil-stats_2.12  \n7.1.0  \ncom.typesafe  \nconfig  \n1.4.3  \ncom.typesafe.scala-logging  \nscala-logging_2.12  \n3.7.2  \ncom.uber  \nh3  \n3.7.3  \ncom.univocity  \nunivocity-parsers  \n2.9.1  \ncom.zaxxer  \nHikariCP  \n4.0.3  \ncommons-cli  \ncommons-cli  \n1.5.0  \ncommons-codec  \ncommons-codec  \n1.16.0  \ncommons-collections  \ncommons-collections  \n3.2.2  \ncommons-dbcp  \ncommons-dbcp  \n1.4  \ncommons-fileupload  \ncommons-fileupload  \n1.5  \ncommons-httpclient  \ncommons-httpclient  \n3.1  \ncommons-io  \ncommons-io  \n2.13.0  \ncommons-lang  \ncommons-lang  \n2.6  \ncommons-logging  \ncommons-logging  \n1.1.3  \ncommons-pool  \ncommons-pool  \n1.5.4  \ndev.ludovic.netlib  \narpack  \n3.0.3  \ndev.ludovic.netlib  \nblas  \n3.0.3  \ndev.ludovic.netlib  \nlapack  \n3.0.3  \ninfo.ganglia.gmetric4j  \ngmetric4j  \n1.0.10"
    },
    {
        "id": 1046,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "blas  \n3.0.3  \ndev.ludovic.netlib  \nlapack  \n3.0.3  \ninfo.ganglia.gmetric4j  \ngmetric4j  \n1.0.10  \nio.airlift  \naircompressor  \n0.25  \nio.delta  \ndelta-sharing-client_2.12  \n1.0.5  \nio.dropwizard.metrics  \nmetrics-annotation  \n4.2.19  \nio.dropwizard.metrics  \nmetrics-core  \n4.2.19  \nio.dropwizard.metrics  \nmetrics-graphite  \n4.2.19  \nio.dropwizard.metrics  \nmetrics-healthchecks  \n4.2.19  \nio.dropwizard.metrics  \nmetrics-jetty9  \n4.2.19  \nio.dropwizard.metrics  \nmetrics-jmx  \n4.2.19  \nio.dropwizard.metrics  \nmetrics-json  \n4.2.19  \nio.dropwizard.metrics  \nmetrics-jvm  \n4.2.19  \nio.dropwizard.metrics  \nmetrics-servlets  \n4.2.19  \nio.netty  \nnetty-all  \n4.1.96.Final  \nio.netty  \nnetty-buffer  \n4.1.96.Final  \nio.netty  \nnetty-codec  \n4.1.96.Final  \nio.netty  \nnetty-codec-http  \n4.1.96.Final  \nio.netty  \nnetty-codec-http2  \n4.1.96.Final  \nio.netty  \nnetty-codec-socks  \n4.1.96.Final  \nio.netty  \nnetty-common  \n4.1.96.Final  \nio.netty  \nnetty-handler  \n4.1.96.Final  \nio.netty  \nnetty-handler-proxy  \n4.1.96.Final  \nio.netty  \nnetty-resolver  \n4.1.96.Final  \nio.netty  \nnetty-tcnative-boringssl-static  \n2.0.61.Final-linux-aarch_64  \nio.netty  \nnetty-tcnative-boringssl-static  \n2.0.61.Final-linux-x86_64  \nio.netty  \nnetty-tcnative-boringssl-static"
    },
    {
        "id": 1047,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "io.netty  \nnetty-tcnative-boringssl-static  \n2.0.61.Final-linux-x86_64  \nio.netty  \nnetty-tcnative-boringssl-static  \n2.0.61.Final-osx-aarch_64  \nio.netty  \nnetty-tcnative-boringssl-static  \n2.0.61.Final-osx-x86_64  \nio.netty  \nnetty-tcnative-boringssl-static  \n2.0.61.Final-windows-x86_64  \nio.netty  \nnetty-tcnative-classes  \n2.0.61.Final  \nio.netty  \nnetty-transport  \n4.1.96.Final  \nio.netty  \nnetty-transport-classes-epoll  \n4.1.96.Final  \nio.netty  \nnetty-transport-classes-kqueue  \n4.1.96.Final  \nio.netty  \nnetty-transport-native-epoll  \n4.1.96.Final  \nio.netty  \nnetty-transport-native-epoll  \n4.1.96.Final-linux-aarch_64  \nio.netty  \nnetty-transport-native-epoll  \n4.1.96.Final-linux-x86_64  \nio.netty  \nnetty-transport-native-kqueue  \n4.1.96.Final-osx-aarch_64  \nio.netty  \nnetty-transport-native-kqueue  \n4.1.96.Final-osx-x86_64  \nio.netty  \nnetty-transport-native-unix-common  \n4.1.96.Final  \nio.prometheus  \nsimpleclient  \n0.7.0  \nio.prometheus  \nsimpleclient_common  \n0.7.0  \nio.prometheus  \nsimpleclient_dropwizard  \n0.7.0  \nio.prometheus  \nsimpleclient_pushgateway  \n0.7.0  \nio.prometheus  \nsimpleclient_servlet  \n0.7.0  \nio.prometheus.jmx  \ncollector  \n0.12.0  \njakarta.annotation  \njakarta.annotation-api  \n1.3.5  \njakarta.servlet  \njakarta.servlet-api  \n4.0.3  \njakarta.validation  \njakarta.validation-api  \n2.0.2"
    },
    {
        "id": 1048,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "1.3.5  \njakarta.servlet  \njakarta.servlet-api  \n4.0.3  \njakarta.validation  \njakarta.validation-api  \n2.0.2  \njakarta.ws.rs  \njakarta.ws.rs-api  \n2.1.6  \njavax.activation  \nactivation  \n1.1.1  \njavax.el  \njavax.el-api  \n2.2.4  \njavax.jdo  \njdo-api  \n3.0.1  \njavax.transaction  \njta  \n1.1  \njavax.transaction  \ntransaction-api  \n1.1  \njavax.xml.bind  \njaxb-api  \n2.2.11  \njavolution  \njavolution  \n5.5.1  \njline  \njline  \n2.14.6  \njoda-time  \njoda-time  \n2.12.1  \nnet.java.dev.jna  \njna  \n5.8.0  \nnet.razorvine  \npickle  \n1.3  \nnet.sf.jpam  \njpam  \n1.1  \nnet.sf.opencsv  \nopencsv  \n2.3  \nnet.sf.supercsv  \nsuper-csv  \n2.2.0  \nnet.snowflake  \nsnowflake-ingest-sdk  \n0.9.6  \nnet.sourceforge.f2j  \narpack_combined_all  \n0.1  \norg.acplt.remotetea  \nremotetea-oncrpc  \n1.1.2  \norg.antlr  \nST4  \n4.0.4  \norg.antlr  \nantlr-runtime  \n3.5.2  \norg.antlr  \nantlr4-runtime  \n4.9.3  \norg.antlr  \nstringtemplate  \n3.2.1  \norg.apache.ant  \nant  \n1.10.11  \norg.apache.ant  \nant-jsch  \n1.10.11  \norg.apache.ant  \nant-launcher  \n1.10.11  \norg.apache.arrow  \narrow-format  \n15.0.0  \norg.apache.arrow  \narrow-memory-core  \n15.0.0  \norg.apache.arrow  \narrow-memory-netty  \n15.0.0  \norg.apache.arrow  \narrow-vector  \n15.0.0  \norg.apache.avro  \navro  \n1.11.3  \norg.apache.avro  \navro-ipc  \n1.11.3  \norg.apache.avro"
    },
    {
        "id": 1049,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "org.apache.arrow  \narrow-vector  \n15.0.0  \norg.apache.avro  \navro  \n1.11.3  \norg.apache.avro  \navro-ipc  \n1.11.3  \norg.apache.avro  \navro-mapred  \n1.11.3  \norg.apache.commons  \ncommons-collections4  \n4.4  \norg.apache.commons  \ncommons-compress  \n1.23.0  \norg.apache.commons  \ncommons-crypto  \n1.1.0  \norg.apache.commons  \ncommons-lang3  \n3.12.0  \norg.apache.commons  \ncommons-math3  \n3.6.1  \norg.apache.commons  \ncommons-text  \n1.10.0  \norg.apache.curator  \ncurator-client  \n2.13.0  \norg.apache.curator  \ncurator-framework  \n2.13.0  \norg.apache.curator  \ncurator-recipes  \n2.13.0  \norg.apache.datasketches  \ndatasketches-java  \n3.1.0  \norg.apache.datasketches  \ndatasketches-memory  \n2.0.0  \norg.apache.derby  \nderby  \n10.14.2.0  \norg.apache.hadoop  \nhadoop-client-runtime  \n3.3.6  \norg.apache.hive  \nhive-beeline  \n2.3.9  \norg.apache.hive  \nhive-cli  \n2.3.9  \norg.apache.hive  \nhive-jdbc  \n2.3.9  \norg.apache.hive  \nhive-llap-client  \n2.3.9  \norg.apache.hive  \nhive-llap-common  \n2.3.9  \norg.apache.hive  \nhive-serde  \n2.3.9  \norg.apache.hive  \nhive-shims  \n2.3.9  \norg.apache.hive  \nhive-storage-api  \n2.8.1  \norg.apache.hive.shims  \nhive-shims-0.23  \n2.3.9  \norg.apache.hive.shims  \nhive-shims-common  \n2.3.9  \norg.apache.hive.shims  \nhive-shims-scheduler  \n2.3.9  \norg.apache.httpcomponents  \nhttpclient  \n4.5.14  \norg.apache.httpcomponents  \nhttpcore  \n4.4.16  \norg.apache.ivy  \nivy  \n2.5.1  \norg.apache.logging.log4j  \nlog4j-1.2-api  \n2.22.1"
    },
    {
        "id": 1050,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "org.apache.httpcomponents  \nhttpcore  \n4.4.16  \norg.apache.ivy  \nivy  \n2.5.1  \norg.apache.logging.log4j  \nlog4j-1.2-api  \n2.22.1  \norg.apache.logging.log4j  \nlog4j-api  \n2.22.1  \norg.apache.logging.log4j  \nlog4j-core  \n2.22.1  \norg.apache.logging.log4j  \nlog4j-layout-template-json  \n2.22.1  \norg.apache.logging.log4j  \nlog4j-slf4j2-impl  \n2.22.1  \norg.apache.orc  \norc-core  \n1.9.2-shaded-protobuf  \norg.apache.orc  \norc-mapreduce  \n1.9.2-shaded-protobuf  \norg.apache.orc  \norc-shims  \n1.9.2  \norg.apache.thrift  \nlibfb303  \n0.9.3  \norg.apache.thrift  \nlibthrift  \n0.12.0  \norg.apache.ws.xmlschema  \nxmlschema-core  \n2.3.0  \norg.apache.xbean  \nxbean-asm9-shaded  \n4.23  \norg.apache.yetus  \naudience-annotations  \n0.13.0  \norg.apache.zookeeper  \nzookeeper  \n3.6.3  \norg.apache.zookeeper  \nzookeeper-jute  \n3.6.3  \norg.checkerframework  \nchecker-qual  \n3.31.0  \norg.codehaus.jackson  \njackson-core-asl  \n1.9.13  \norg.codehaus.jackson  \njackson-mapper-asl  \n1.9.13  \norg.codehaus.janino  \ncommons-compiler  \n3.0.16  \norg.codehaus.janino  \njanino  \n3.0.16  \norg.datanucleus  \ndatanucleus-api-jdo  \n4.2.4  \norg.datanucleus  \ndatanucleus-core  \n4.1.17  \norg.datanucleus  \ndatanucleus-rdbms  \n4.1.19  \norg.datanucleus  \njavax.jdo  \n3.2.0-m3  \norg.eclipse.collections  \neclipse-collections  \n11.1.0  \norg.eclipse.collections  \neclipse-collections-api"
    },
    {
        "id": 1051,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "4.1.19  \norg.datanucleus  \njavax.jdo  \n3.2.0-m3  \norg.eclipse.collections  \neclipse-collections  \n11.1.0  \norg.eclipse.collections  \neclipse-collections-api  \n11.1.0  \norg.eclipse.jetty  \njetty-client  \n9.4.52.v20230823  \norg.eclipse.jetty  \njetty-continuation  \n9.4.52.v20230823  \norg.eclipse.jetty  \njetty-http  \n9.4.52.v20230823  \norg.eclipse.jetty  \njetty-io  \n9.4.52.v20230823  \norg.eclipse.jetty  \njetty-jndi  \n9.4.52.v20230823  \norg.eclipse.jetty  \njetty-plus  \n9.4.52.v20230823  \norg.eclipse.jetty  \njetty-proxy  \n9.4.52.v20230823  \norg.eclipse.jetty  \njetty-security  \n9.4.52.v20230823  \norg.eclipse.jetty  \njetty-server  \n9.4.52.v20230823  \norg.eclipse.jetty  \njetty-servlet  \n9.4.52.v20230823  \norg.eclipse.jetty  \njetty-servlets  \n9.4.52.v20230823  \norg.eclipse.jetty  \njetty-util  \n9.4.52.v20230823  \norg.eclipse.jetty  \njetty-util-ajax  \n9.4.52.v20230823  \norg.eclipse.jetty  \njetty-webapp  \n9.4.52.v20230823  \norg.eclipse.jetty  \njetty-xml  \n9.4.52.v20230823  \norg.eclipse.jetty.websocket  \nwebsocket-api  \n9.4.52.v20230823  \norg.eclipse.jetty.websocket  \nwebsocket-client  \n9.4.52.v20230823  \norg.eclipse.jetty.websocket  \nwebsocket-common  \n9.4.52.v20230823  \norg.eclipse.jetty.websocket  \nwebsocket-server  \n9.4.52.v20230823  \norg.eclipse.jetty.websocket  \nwebsocket-servlet  \n9.4.52.v20230823  \norg.fusesource.leveldbjni"
    },
    {
        "id": 1052,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "9.4.52.v20230823  \norg.eclipse.jetty.websocket  \nwebsocket-servlet  \n9.4.52.v20230823  \norg.fusesource.leveldbjni  \nleveldbjni-all  \n1.8  \norg.glassfish.hk2  \nhk2-api  \n2.6.1  \norg.glassfish.hk2  \nhk2-locator  \n2.6.1  \norg.glassfish.hk2  \nhk2-utils  \n2.6.1  \norg.glassfish.hk2  \nosgi-resource-locator  \n1.0.3  \norg.glassfish.hk2.external  \naopalliance-repackaged  \n2.6.1  \norg.glassfish.hk2.external  \njakarta.inject  \n2.6.1  \norg.glassfish.jersey.containers  \njersey-container-servlet  \n2.40  \norg.glassfish.jersey.containers  \njersey-container-servlet-core  \n2.40  \norg.glassfish.jersey.core  \njersey-client  \n2.40  \norg.glassfish.jersey.core  \njersey-common  \n2.40  \norg.glassfish.jersey.core  \njersey-server  \n2.40  \norg.glassfish.jersey.inject  \njersey-hk2  \n2.40  \norg.hibernate.validator  \nhibernate-validator  \n6.1.7.Final  \norg.ini4j  \nini4j  \n0.5.4  \norg.javassist  \njavassist  \n3.29.2-GA  \norg.jboss.logging  \njboss-logging  \n3.3.2.Final  \norg.jdbi  \njdbi  \n2.63.1  \norg.jetbrains  \nannotations  \n17.0.0  \norg.joda  \njoda-convert  \n1.7  \norg.jodd  \njodd-core  \n3.5.2  \norg.json4s  \njson4s-ast_2.12  \n3.7.0-M11  \norg.json4s  \njson4s-core_2.12  \n3.7.0-M11  \norg.json4s  \njson4s-jackson_2.12  \n3.7.0-M11  \norg.json4s  \njson4s-scalap_2.12  \n3.7.0-M11  \norg.lz4  \nlz4-java  \n1.8.0"
    },
    {
        "id": 1053,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "3.7.0-M11  \norg.json4s  \njson4s-scalap_2.12  \n3.7.0-M11  \norg.lz4  \nlz4-java  \n1.8.0  \norg.mlflow  \nmlflow-spark_2.12  \n2.9.1  \norg.objenesis  \nobjenesis  \n2.5.1  \norg.postgresql  \npostgresql  \n42.6.1  \norg.roaringbitmap  \nRoaringBitmap  \n0.9.45-databricks  \norg.roaringbitmap  \nshims  \n0.9.45-databricks  \norg.rocksdb  \nrocksdbjni  \n8.11.4  \norg.rosuda.REngine  \nREngine  \n2.1.0  \norg.scala-lang  \nscala-compiler_2.12  \n2.12.15  \norg.scala-lang  \nscala-library_2.12  \n2.12.15  \norg.scala-lang  \nscala-reflect_2.12  \n2.12.15  \norg.scala-lang.modules  \nscala-collection-compat_2.12  \n2.11.0  \norg.scala-lang.modules  \nscala-parser-combinators_2.12  \n1.1.2  \norg.scala-lang.modules  \nscala-xml_2.12  \n1.2.0  \norg.scala-sbt  \ntest-interface  \n1.0  \norg.scalacheck  \nscalacheck_2.12  \n1.14.2  \norg.scalactic  \nscalactic_2.12  \n3.2.15  \norg.scalanlp  \nbreeze-macros_2.12  \n2.1.0  \norg.scalanlp  \nbreeze_2.12  \n2.1.0  \norg.scalatest  \nscalatest-compatible  \n3.2.15  \norg.scalatest  \nscalatest-core_2.12  \n3.2.15  \norg.scalatest  \nscalatest-diagrams_2.12  \n3.2.15  \norg.scalatest  \nscalatest-featurespec_2.12  \n3.2.15  \norg.scalatest  \nscalatest-flatspec_2.12  \n3.2.15  \norg.scalatest  \nscalatest-freespec_2.12  \n3.2.15"
    },
    {
        "id": 1054,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "3.2.15  \norg.scalatest  \nscalatest-flatspec_2.12  \n3.2.15  \norg.scalatest  \nscalatest-freespec_2.12  \n3.2.15  \norg.scalatest  \nscalatest-funspec_2.12  \n3.2.15  \norg.scalatest  \nscalatest-funsuite_2.12  \n3.2.15  \norg.scalatest  \nscalatest-matchers-core_2.12  \n3.2.15  \norg.scalatest  \nscalatest-mustmatchers_2.12  \n3.2.15  \norg.scalatest  \nscalatest-propspec_2.12  \n3.2.15  \norg.scalatest  \nscalatest-refspec_2.12  \n3.2.15  \norg.scalatest  \nscalatest-shouldmatchers_2.12  \n3.2.15  \norg.scalatest  \nscalatest-wordspec_2.12  \n3.2.15  \norg.scalatest  \nscalatest_2.12  \n3.2.15  \norg.slf4j  \njcl-over-slf4j  \n2.0.7  \norg.slf4j  \njul-to-slf4j  \n2.0.7  \norg.slf4j  \nslf4j-api  \n2.0.7  \norg.slf4j  \nslf4j-simple  \n1.7.25  \norg.threeten  \nthreeten-extra  \n1.7.1  \norg.tukaani  \nxz  \n1.9  \norg.typelevel  \nalgebra_2.12  \n2.0.1  \norg.typelevel  \ncats-kernel_2.12  \n2.1.1  \norg.typelevel  \nspire-macros_2.12  \n0.17.0  \norg.typelevel  \nspire-platform_2.12  \n0.17.0  \norg.typelevel  \nspire-util_2.12  \n0.17.0  \norg.typelevel  \nspire_2.12  \n0.17.0  \norg.wildfly.openssl  \nwildfly-openssl  \n1.1.3.Final  \norg.xerial  \nsqlite-jdbc  \n3.42.0.0  \norg.xerial.snappy  \nsnappy-java  \n1.1.10.3"
    },
    {
        "id": 1055,
        "url": "https://docs.databricks.com/en/release-notes/runtime/15.3.html",
        "content": "wildfly-openssl  \n1.1.3.Final  \norg.xerial  \nsqlite-jdbc  \n3.42.0.0  \norg.xerial.snappy  \nsnappy-java  \n1.1.10.3  \norg.yaml  \nsnakeyaml  \n2.0  \noro  \noro  \n2.0.8  \npl.edu.icm  \nJLargeArrays  \n1.5  \nsoftware.amazon.cryptools  \nAmazonCorrettoCryptoProvider  \n1.6.1-linux-x86_64  \nsoftware.amazon.ion  \nion-java  \n1.0.2  \nstax  \nstax-api  \n1.0.1"
    },
    {
        "id": 1056,
        "url": "https://docs.databricks.com/en/resources/feature-region-support.html",
        "content": "Features with limited regional availability  \nThis article lists features where there is regional differentiation in feature availability.  \nFeatures that are excluded from at least one region include:  \nSecurity  \nCMK (customer-managed keys)  \nAWS PrivateLink  \nFirewall enablement for serverless compute  \nServerless  \nServerless compute for notebooks and jobs  \nServerless DLT pipelines.  \nServerless SQL warehouses. In the serverless SQL warehouses column, \u201ccompliance security profile supported\u201d means that you can use serverless SQL warehouses with the compliance security profile enabled, which is required to process some types of regulated data.  \nAI and machine learning  \nPredictive optimization  \nVector Search  \nMosaic AI Agent Framework and Agent Evaluation  \nModel serving  \nModel Serving  \nFoundation Model API pay-per-token  \nFoundation Model API provisioned throughput  \nExternal models  \nNote  \nSome features are not included in the tables below. These features, known as Designated Services, use Databricks Geos to manage data residency when processing customer content. A Geo is a group of data center regions that Databricks uses to provide predictability and transparency regarding where your data is processed. See Databricks Designated Services and Databricks Geos: Data residency.  \nThe following tables list the AWS regions supported by Databricks. Some features are available only in a subset of regions. The table indicates whether or not a region supports each of these features. If a feature is supported in all regions, it is not included in the table.  \nSecurity feature availability"
    },
    {
        "id": 1057,
        "url": "https://docs.databricks.com/en/resources/feature-region-support.html",
        "content": "Security feature availability\nRegion  \nLocation  \nCMK  \nPrivateLink  \nFirewall Enablement for Serverless Compute  \nap-northeast-1  \nAsia Pacific (Tokyo)  \n\u2713  \n\u2713  \n\u2713  \nap-northeast-2  \nAsia Pacific (Seoul)  \n\u2713  \n\u2713  \nap-south-1  \nAsia Pacific (Mumbai)  \n\u2713  \n\u2713  \n\u2713  \nap-southeast-1  \nAsia Pacific (Singapore)  \n\u2713  \n\u2713  \n\u2713  \nap-southeast-2  \nAsia Pacific (Sydney)  \n\u2713  \n\u2713  \n\u2713  \nca-central-1  \nCanada (Central)  \n\u2713  \n\u2713  \neu-central-1  \nEU (Frankfurt)  \n\u2713  \n\u2713  \n\u2713  \neu-west-1  \nEU (Ireland)  \n\u2713  \n\u2713  \n\u2713  \neu-west-2  \nEU (London)  \n\u2713  \n\u2713  \neu-west-3  \nEU (Paris)  \n\u2713  \n\u2713  \n\u2713  \nsa-east-1  \nSouth America (Sao Paulo)  \n\u2713  \n\u2713  \n\u2713  \nus-east-1  \nUS East (Northern Virginia)  \n\u2713  \n\u2713  \n\u2713  \nus-east-2  \nUS East (Ohio)  \n\u2713  \n\u2713  \n\u2713  \nus-gov-west-1  \nUS Gov West (Pendleton)  \n\u2713  \n\u2713  \nus-west-1  \nUS West (Northern California)  \nus-west-2  \nUS West (Oregon)  \n\u2713  \n\u2713  \n\u2713\n\nServerless compute feature availability"
    },
    {
        "id": 1058,
        "url": "https://docs.databricks.com/en/resources/feature-region-support.html",
        "content": "Serverless compute feature availability\nRegion  \nLocation  \nServerless Compute for Notebooks and Workflows  \nServerless DLT Pipelines  \nServerless SQL Warehouses  \nap-northeast-1  \nAsia Pacific (Tokyo)  \n\u2713  \n\u2713  \n\u2713  \nap-northeast-2  \nAsia Pacific (Seoul)  \n\u2713  \n\u2713  \n\u2713  \nap-south-1  \nAsia Pacific (Mumbai)  \n\u2713  \n\u2713  \n\u2713  \nap-southeast-1  \nAsia Pacific (Singapore)  \n\u2713  \n\u2713  \n\u2713  \nap-southeast-2  \nAsia Pacific (Sydney)  \n\u2713  \n\u2713 (compliance security)  \n\u2713 (compliance security)  \nca-central-1  \nCanada (Central)  \n\u2713  \n\u2713  \n\u2713  \neu-central-1  \nEU (Frankfurt)  \n\u2713  \n\u2713  \n\u2713  \neu-west-1  \nEU (Ireland)  \n\u2713  \n\u2713  \n\u2713  \neu-west-2  \nEU (London)  \neu-west-3  \nEU (Paris)  \n\u2713  \n\u2713  \n\u2713  \nsa-east-1  \nSouth America (Sao Paulo)  \n\u2713  \n\u2713  \n\u2713  \nus-east-1  \nUS East (Northern Virginia)  \n\u2713  \n\u2713 (compliance security)  \n\u2713 (compliance security)  \nus-east-2  \nUS East (Ohio)  \n\u2713  \n\u2713  \n\u2713  \nus-gov-west-1  \nUS Gov West (Pendleton)  \nus-west-1  \nUS West (Northern California)  \nus-west-2  \nUS West (Oregon)  \n\u2713  \n\u2713  \n\u2713"
    },
    {
        "id": 1059,
        "url": "https://docs.databricks.com/en/resources/feature-region-support.html",
        "content": "AI and machine learning feature availability\nAI and machine learning feature availability\nRegion  \nLocation  \nVector Search  \nPredictive Optimization  \nMosaic AI Agent Framework & Evaluation  \nap-northeast-1  \nAsia Pacific (Tokyo)  \n\u2713  \n\u2713  \nap-northeast-2  \nAsia Pacific (Seoul)  \n\u2713  \nap-south-1  \nAsia Pacific (Mumbai)  \n\u2713  \n\u2713  \nap-southeast-1  \nAsia Pacific (Singapore)  \n\u2713  \nap-southeast-2  \nAsia Pacific (Sydney)  \n\u2713  \n\u2713  \n\u2713  \nca-central-1  \nCanada (Central)  \n\u2713  \n\u2713  \neu-central-1  \nEU (Frankfurt)  \n\u2713  \n\u2713  \n\u2713  \neu-west-1  \nEU (Ireland)  \n\u2713  \n\u2713  \n\u2713  \neu-west-2  \nEU (London)  \neu-west-3  \nEU (Paris)  \n\u2713  \nsa-east-1  \nSouth America (Sao Paulo)  \n\u2713  \nus-east-1  \nUS East (Northern Virginia)  \n\u2713  \n\u2713  \n\u2713  \nus-east-2  \nUS East (Ohio)  \n\u2713  \n\u2713  \n\u2713  \nus-gov-west-1  \nUS Gov West (Pendleton)  \nus-west-1  \nUS West (Northern California)  \nus-west-2  \nUS West (Oregon)  \n\u2713  \n\u2713  \n\u2713\n\nModel serving feature availability"
    },
    {
        "id": 1060,
        "url": "https://docs.databricks.com/en/resources/feature-region-support.html",
        "content": "Model serving feature availability\nRegion  \nLocation  \nCore Model Serving capability *  \nFoundation Model APIs (provisioned throughout) **  \nFoundation Model APIs (pay-per-token)  \nExternal models  \nap-northeast-1  \nAsia Pacific (Tokyo)  \n\u2713  \n\u2713  \n\u2713  \nap-northeast-2  \nAsia Pacific (Seoul)  \nap-south-1  \nAsia Pacific (Mumbai)  \n\u2713  \n\u2713  \n\u2713  \nap-southeast-1  \nAsia Pacific (Singapore)  \n\u2713  \n\u2713  \n\u2713  \nap-southeast-2  \nAsia Pacific (Sydney)  \n\u2713  \n\u2713  \n\u2713  \nca-central-1  \nCanada (Central)  \n\u2713  \n\u2713  \n\u2713  \neu-central-1  \nEU (Frankfurt)  \n\u2713  \n\u2713  \n\u2713  \n\u2713  \neu-west-1  \nEU (Ireland)  \n\u2713  \n\u2713  \n\u2713  \n\u2713  \neu-west-2  \nEU (London)  \neu-west-3  \nEU (Paris)  \nsa-east-1  \nSouth America (Sao Paulo)  \nus-east-1  \nUS East (Northern Virginia)  \n\u2713  \n\u2713  \n\u2713  \n\u2713  \nus-east-2  \nUS East (Ohio)  \n\u2713  \n\u2713  \n\u2713  \n\u2713  \nus-gov-west-1  \nUS Gov West (Pendleton)  \nus-west-1  \nUS West (Northern California)  \n\u2713  \n\u2713  \nus-west-2  \nUS West (Oregon)  \n\u2713  \n\u2713  \n\u2713  \n\u2713  \n* CPU compute only  \n** Includes GPU support"
    },
    {
        "id": 1061,
        "url": "https://docs.databricks.com/en/release-notes/product/2018/november.html",
        "content": "November 2018  \nThese features and Databricks platform improvements were released in November 2018.  \nNote  \nReleases are staged. Your Databricks account may not be updated until up to a week after the initial release date.  \nLibrary UI\nLibrary UI\nImportant  \nThis update was reverted on December 7, 2018.  \nNovember 27-December 4, 2018: Version 2.85  \nIn this release, the library UI has been significantly improved.  \nThe Databricks UI now supports workspace libraries and cluster-attached libraries. A workspace library exists in the Workspace and can be attached to one or more clusters. A cluster-attached library is a library that exists only in the context of the cluster that it is attached to. In addition:  \nYou can now create a library from a file uploaded to object storage.  \nYou can now attach and detach libraries from the library details page and a cluster\u2019s Libraries tab.  \nLibraries installed using the API now display in a cluster\u2019s Libraries tab.\n\nCustom Spark heap memory settings enabled"
    },
    {
        "id": 1062,
        "url": "https://docs.databricks.com/en/release-notes/product/2018/november.html",
        "content": "Custom Spark heap memory settings enabled\nNovember 27-December 4, 2018: Version 2.85  \nThe following Spark memory settings now take effect:  \nspark.executor.memory  \nspark.driver.memory  \nImportant  \nDatabricks has services running on each node so the maximum allowable memory for Spark is less than the memory capacity of the VM reported by the cloud provider. If you want to provide Spark with the maximum amount of heap memory for the executor or driver, don\u2019t specify spark.executor.memory or spark.driver.memory respectively.  \nSome cluster configurations that were previously invalid but ignored may result in cluster failures.\n\nJobs and idle execution context eviction\nJobs and idle execution context eviction\nNovember 27-December 4, 2018: Version 2.85  \nJobs now auto-evict idle execution contexts. See Databricks notebook execution contexts. To minimize auto-eviction, Databricks recommends that you use different clusters for jobs and interactive workloads.\n\nDatabricks Runtime 5.0 for Machine Learning (Beta) release"
    },
    {
        "id": 1063,
        "url": "https://docs.databricks.com/en/release-notes/product/2018/november.html",
        "content": "Databricks Runtime 5.0 for Machine Learning (Beta) release\nNovember 19, 2018  \nDatabricks Runtime 5.0 ML (Beta) provides a ready-to-go environment for machine learning and data science. It contains multiple popular libraries, including TensorFlow, Keras, and XGBoost. It also supports distributed TensorFlow training using Horovod. Databricks Runtime 5.0 ML is built on top of Databricks Runtime 5.0. Databricks Runtime 5.0 ML includes the following new features:  \nHorovodRunner, for running distributed deep learning training jobs using Horovod. See Distributed training.  \nConda support for package management.  \nMLeap integration.  \nGraphFrames integration.  \nSee the complete release notes for Databricks Runtime 5.0 ML (EoS).\n\nDatabricks Runtime 5.0 release"
    },
    {
        "id": 1064,
        "url": "https://docs.databricks.com/en/release-notes/product/2018/november.html",
        "content": "Databricks Runtime 5.0 release\nNovember 8, 2018  \nDatabricks Runtime 5.0 is now available. Databricks Runtime 5.0 includes Apache Spark 2.4.0, new Delta Lake and Structured Streaming features and upgrades, and upgraded Python, R, and Java and Scala libraries. For details, see Databricks Runtime 5.0 (EoS).  \nOn Databricks Runtime 5.0, Databricks now evicts idle execution contexts once a cluster has reached the maximum context limit (145). See Databricks notebook execution contexts.\n\ndisplayHTML support for unrestricted loading of third-party content"
    },
    {
        "id": 1065,
        "url": "https://docs.databricks.com/en/release-notes/product/2018/november.html",
        "content": "displayHTML support for unrestricted loading of third-party content\nNovember 6-13, 2018: Version 2.84  \nPreviously the displayHTML iframe sandbox was missing the allow-same-origin attribute. This meant that the iframe had a null origin, which wasn\u2019t friendly to cross-origin XHR requests, cookies, or accessing embedded iframes. With this release, the displayHTML iframe is served from a new domain, databricksusercontent.com, and the iframe sandbox now includes the allow-same-origin attribute.  \nThere is no need to change your usage of displayHTML if it\u2019s already working for you.  \ndatabricksusercontent.com will need to be accessible from your browser. If it is currently blocked by your corporate network, it will need to be whitelisted by IT."
    },
    {
        "id": 1066,
        "url": "https://docs.databricks.com/en/release-notes/product/2021/april.html",
        "content": "April 2021  \nThese features and Databricks platform improvements were released in April 2021.  \nNote  \nReleases are staged. Your Databricks account may not be updated until a week or more after the initial release date.  \nDatabricks Runtime 8.2 (GA)\nDatabricks Runtime 8.2 (GA)\nApril 22, 2021  \nDatabricks Runtime 8.2 and Databricks Runtime 8.2 ML are now generally available.  \nFor information, see the full release notes at Databricks Runtime 8.2 (EoS) and Databricks Runtime 8.2 for ML (EoS).\n\nAWS PrivateLink for Databricks workspaces (Public Preview)"
    },
    {
        "id": 1067,
        "url": "https://docs.databricks.com/en/release-notes/product/2021/april.html",
        "content": "AWS PrivateLink for Databricks workspaces (Public Preview)\nApril 20, 2021  \nYou can now use AWS PrivateLink to provision secure private workspaces by creating VPC endpoints to both the front-end and back-end interfaces of the Databricks infrastructure. The front-end VPC endpoint ensures that your users connect to the Databricks web application, REST APIs and JDBC/ODBC interface over your private network and the AWS network backbone. The back-end VPC endpoints ensure that clusters in your own managed VPC connect to the secure cluster connectivity relay and REST APIs over AWS network backbone. The workspace must be on the E2 version of the Databricks platform.  \nFor detailed setup instructions, see Enable private connectivity using AWS PrivateLink.\n\nUpdate running workspaces with new credentials or network configurations"
    },
    {
        "id": 1068,
        "url": "https://docs.databricks.com/en/release-notes/product/2021/april.html",
        "content": "Update running workspaces with new credentials or network configurations\nApril 20, 2021  \nYou can now update a running workspace with new configurations for credentials (AWS IAM cross-account roles) and network (Configure a customer-managed VPC) using either the account console or the Account API. The workspace must be on the E2 version of the Databricks platform.\n\nDatabricks can now send in-product messages and product tours directly to your workspace (Public Preview)\nDatabricks can now send in-product messages and product tours directly to your workspace (Public Preview)\nApril 20-26, 2021: Version 3.44  \nDatabricks users can now benefit from suggestions and product tours delivered as in-product messages, improving onboarding and engagement. This feature is enabled by default, but admins can disable it using the admin settings page.\n\nEasier job management with the enhanced jobs user interface"
    },
    {
        "id": 1069,
        "url": "https://docs.databricks.com/en/release-notes/product/2021/april.html",
        "content": "Easier job management with the enhanced jobs user interface\nApril 20-26, 2021: Version 3.44  \nDatabricks has redesigned the Create and run Databricks Jobs user interface to make it easier to manage jobs. You can use the new job details page to perform all job related actions, including running, cloning, and deleting jobs.\n\nCluster policy changes are applied automatically to existing clusters at restart and edit\nCluster policy changes are applied automatically to existing clusters at restart and edit\nApril 20-26, 2021: Version 3.44  \nIt is now much easier to apply cluster policy changes to existing clusters. Any policy definition changes are applied to associated clusters at cluster restart and edit. Nonconforming clusters won\u2019t be able to restart.\n\nTrack retries in your job tasks when task attempts fail\nTrack retries in your job tasks when task attempts fail\nApril 20-26, 2021: Version 3.44  \nYou can now pass the task_retry_count parameter variable to a job task. The value of this variable is a count of the attempts to retry the task when the initial attempt fails. For more information see Pass context about job runs into job tasks."
    },
    {
        "id": 1070,
        "url": "https://docs.databricks.com/en/release-notes/product/2021/april.html",
        "content": "Quickly view cluster details when you create a new cluster\nQuickly view cluster details when you create a new cluster\nApril 20-26, 2021: Version 3.44  \nYou will now see cluster information, including worker node and driver details, to the right of Create Cluster when you create a new all-purpose or job cluster.\n\nMLflow sidebar reflects the most recent experiment\nMLflow sidebar reflects the most recent experiment\nApril 20-26, 2021: Version 3.44  \nThe MLflow Experiment Runs sidebar now displays runs from the experiment that the notebook most recently logged to. Previously the sidebar showed runs only from the notebook experiment.  \nFor details, see Track ML and deep learning training runs.\n\nChange to default channel for conda.yaml files in MLflow"
    },
    {
        "id": 1071,
        "url": "https://docs.databricks.com/en/release-notes/product/2021/april.html",
        "content": "Change to default channel for conda.yaml files in MLflow\nApril 20-26, 2021: Version 3.44  \nWhen you save a model to MLflow, you can specify a conda.yaml file specifying package dependencies. Previously, if you did not specify channels in the conda.yaml file, the model used the defaults channel. This has changed. If you do not specify a channel, the new default channel is conda-forge.\n\nNew free trial and pay-as-you-go customers are now on the E2 version of the platform"
    },
    {
        "id": 1072,
        "url": "https://docs.databricks.com/en/release-notes/product/2021/april.html",
        "content": "New free trial and pay-as-you-go customers are now on the E2 version of the platform\nApril 12, 2021  \nYou can now sign up for the 14-day free trial on Databricks without having to provide credit card information up front, and all free trial customers now use the new, more user-friendly account console to set up their subscriptions, create workspaces, add account admins, manage billing, and view usage. Both free trial and pay-as-you-go customers now enjoy the full benefits of the E2 platform: multiple workspaces per account, enterprise-grade security, account management APIs, and more.  \nSee Get started: Account and workspace setup.\n\nDatabricks Runtime 8.2 (Beta)\nDatabricks Runtime 8.2 (Beta)\nApril 8, 2021  \nDatabricks Runtime 8.2 and Databricks Runtime 8.2 ML are now available as Beta releases.  \nFor information, see the full release notes at Databricks Runtime 8.2 (EoS) and Databricks Runtime 8.2 for ML (EoS).\n\nUser and group limits"
    },
    {
        "id": 1073,
        "url": "https://docs.databricks.com/en/release-notes/product/2021/april.html",
        "content": "User and group limits\nApril 5-12, 2021: Version 3.43  \nEach Databricks workspace is now limited to 10,000 users and 5,000 groups.\n\nEasier monitoring of job run status\nEasier monitoring of job run status\nApril 5-12, 2021: Version 3.43  \nThe job run details page now automatically refreshes every 5 seconds to make it easier to monitor the progress of your jobs.\n\nBetter governance with enhanced audit logging\nBetter governance with enhanced audit logging\nApril 5-12, 2021: Version 3.43  \nThe audit logs now capture canceled job run events, allowing you to better monitor and troubleshoot jobs running in your Databricks clusters. See Job events.\n\nGlobal init scripts no longer run on model serving clusters\nGlobal init scripts no longer run on model serving clusters\nApril 5-12, 2021: Version 3.43  \nGlobal init scripts are run on every cluster in a workspace and can be used to enforce consistent cluster configurations. This configuration is typically not optimal for model serving clusters, so global init scripts are no longer run on model serving clusters.  \nIf you need to run init scripts on model serving clusters, contact your Databricks account team."
    },
    {
        "id": 1074,
        "url": "https://docs.databricks.com/en/release-notes/product/2021/april.html",
        "content": "Databricks Runtime 6.4 series support ends\nDatabricks Runtime 6.4 series support ends\nApril 1, 2021  \nSupport for Databricks Runtime 6.4, Databricks Runtime 6.4 for Machine Learning, and Databricks Runtime 6.4 for Genomics ended on April 1. See Databricks support lifecycles.  \nDatabricks Runtime 6.4 Extended Support will be supported through the end of 2021. For more information, see Databricks Runtime 6.4 Extended Support (EoS)."
    },
    {
        "id": 1075,
        "url": "https://docs.databricks.com/en/release-notes/runtime/databricks-runtime-ver.html",
        "content": "Databricks support lifecycles  \nAs part of Databricks\u2019s commitment to innovation, platform and runtime features might be retired and replaced by new features. Databricks Runtime releases are also retired and replaced on a regular schedule. The following tables list retirement phases and details about corresponding support for platform features and Databricks Runtime releases.  \nFor information about previews and release types, see Databricks preview releases.  \nPlatform feature lifecycle"
    },
    {
        "id": 1076,
        "url": "https://docs.databricks.com/en/release-notes/runtime/databricks-runtime-ver.html",
        "content": "Platform feature lifecycle\nDatabricks platform feature retirement phases are described in the following table:  \nPhase  \nDescription  \nSupport  \nMigration notes  \nLegacy  \nThe feature is still available, but there is a newer, better feature or way to accomplish the tasks that this feature provides. This label is indicative of a future retirement date.  \nFull. Support and documentation are available.  \nMigration to a new replacement feature or a new way of accomplishing the task is encouraged, but not immediately necessary.  \nDeprecated  \nThe feature is no longer in active development. Updates are no longer being released. The feature will soon be retired, so you need to develop a plan to stop using the feature and transition to an alternative.  \nFull. The feature is no longer being updated, but support and documentation are still available.  \nMigration to a new replacement feature or a new way of accomplishing the task is highly encouraged, because important updates are no longer being applied.  \nEnd of Support (EoS)  \nThe feature is no longer in active development and support is officially unavailable.  \nNone. Documentation might still exist, but it has been archived and is no longer being maintained.  \nMigration to a new replacement feature or a new way of accomplishing the task is urgent, because important updates are no longer being applied and support for issues that might arise is no longer available.  \nEnd of Life (EoL)  \nThe feature has been completely removed from the Databricks product.  \nNone  \nMigration to a new replacement feature or a new way of accomplishing the task is required, because the feature is no longer usable. At this point it might be very difficult to migrate."
    },
    {
        "id": 1077,
        "url": "https://docs.databricks.com/en/release-notes/runtime/databricks-runtime-ver.html",
        "content": "Databricks Runtime support lifecycles\nThe following tables describe the stages of support and support policies for Databricks Runtime versions. Databricks releases runtimes as Beta and GA versions. Databricks supports GA versions for six months, unless the runtime version is a long-term support (LTS) version. For information on supported Databricks Runtime versions, see Databricks Runtime release notes versions and compatibility.  \nWorkloads on unsupported Databricks Runtime versions might continue to run, but Databricks does not provide support or fixes.  \nDatabricks Runtime LTS version lifecycle  \nPhase  \nDescription  \nBeta  \nSupport SLAs are not applicable. For more information, see Databricks Runtime releases.  \nGA, full support for LTS version  \nMajor stability and security fixes are backported.  \nDatabricks releases LTS versions every six months and supports them for three full years.  \nSupported LTS releases are published at Supported Databricks Runtime LTS releases.  \nEnd of Support (EoS)  \nIf a version is unsupported:  \nWorkloads running on these versions receive no Databricks support.  \nFixes are not backported.  \nIt is no longer selectable using the UI when you create or update a compute resource.  \nThe end-of-support date is three years after release.  \nUnsupported releases are published at Unsupported Databricks Runtime release notes.  \nEnd of Life (EoL)  \nDatabricks reserves the right to completely remove a release version at any time after support ends, without prior notice.  \nDatabricks Runtime Non-LTS version lifecycle  \nPhase  \nDescription  \nBeta  \nSupport SLAs are not applicable. For more information, see Databricks Runtime releases.  \nGA, full support  \nMajor stability and security fixes are backported.  \nFull support for Databricks Runtime versions lasts for six months, with the exception of long-term support (LTS) versions.  \nSupported releases along with their end-of-support dates are published at All supported Databricks Runtime releases.  \nEnd of Support (EoS)  \nIf a version is unsupported:  \nWorkloads running on these versions receive no Databricks support.  \nFixes are not backported.  \nIt is no longer selectable using the UI when you create or update a compute resource.  \nUnsupported releases are published at Unsupported Databricks Runtime release notes.  \nEnd of Life (EoL)"
    },
    {
        "id": 1078,
        "url": "https://docs.databricks.com/en/release-notes/runtime/databricks-runtime-ver.html",
        "content": "Fixes are not backported.  \nIt is no longer selectable using the UI when you create or update a compute resource.  \nUnsupported releases are published at Unsupported Databricks Runtime release notes.  \nEnd of Life (EoL)  \nDatabricks reserves the right to completely remove a release version at any time after support ends, without prior notice."
    },
    {
        "id": 1079,
        "url": "https://docs.databricks.com/en/security/network/serverless-network-security/index.html",
        "content": "Serverless compute plane networking  \nThis guide introduces tools to secure network access between the compute resources in the Databricks serverless compute plane and customer resources. To learn more about the control plane and the serverless compute plane, see Databricks architecture overview.  \nNote  \nOn or after September 23, 2024, Databricks will charge customers for networking costs incurred from serverless compute resources connecting to external resources. See Databricks pricing.  \nServerless compute plane networking overview\nServerless compute plane networking overview\nServerless compute resources run in the serverless compute plane, which is managed by Databricks. Account admins can configure secure connectivity between the serverless compute plane and their resources. This network connection is labeled as 2 on the diagram below:  \nConnectivity between the control plane and the serverless compute plane is always over the cloud network backbone and not the public internet. For more information on configuring security features on the other network connections in the diagram, see Networking.\n\nData transfer costs"
    },
    {
        "id": 1080,
        "url": "https://docs.databricks.com/en/security/network/serverless-network-security/index.html",
        "content": "Data transfer costs\nServerless compute plane networking products might incur data transfer costs. For detailed information on data transfer pricing and types of data transfer, see the Databricks pricing page. To avoid cross-region charges, Databricks recommends you create a workspace in the same region as your resources.  \nThere is one SKU type for serverless networking products:  \nDatabricks Public Connectivity enables connecting to cloud resources or the internet using public IPs. You are charged for the volume of data transferred through Databricks Public Connectivity. Databricks Public Connectivity is used implicitly depending on which resources users are connecting to. S3 and Dynamo DB gateways are used whenever possible to connect to S3 and Dynamo DB. See Configure a firewall for serverless compute access.\n\nWhat is a network connectivity configuration (NCC)?"
    },
    {
        "id": 1081,
        "url": "https://docs.databricks.com/en/security/network/serverless-network-security/index.html",
        "content": "What is a network connectivity configuration (NCC)?\nServerless network connectivity is managed with network connectivity configurations (NCC). NCCs are account-level regional constructs that are used to manage private endpoints creation and firewall enablement at scale.  \nAccount admins create NCCs in the account console and an NCC can be attached to one or more workspaces to enable firewalls for resources. An NCC contains a list of stable IP addresses. When an NCC is attached to a workspace, serverless compute in that workspace uses one of those IP addresses to connect the cloud resource. You can allow list those networks on your resource firewall. See Configure a firewall for serverless compute access.  \nCreating a resource firewall also affects connectivity from the classic compute plane to your resource. You must also allow list the networks on your resource firewalls to connect to them from classic compute resources.  \nNCC firewall enablement is supported from serverless SQL warehouses, jobs, notebooks, Delta Live Tables pipelines, and model serving CPU endpoints.  \nNCC firewall enablement is not supported for Amazon S3 or Amazon DynamoDB. When reading or writing to Amazon S3 buckets in the same region as your workspace, serverless compute resources use direct access to S3 using AWS gateway endpoints. This applies when serverless compute reads and writes to your workspace storage bucket in your AWS account and to other S3 data sources in the same region."
    },
    {
        "id": 1082,
        "url": "https://docs.databricks.com/en/partners/prep/matillion.html",
        "content": "Connect to Matillion  \nMatillion ETL is an ETL/ELT tool built specifically for cloud database platforms including Databricks. Matillion ETL has a modern, browser-based UI, with powerful, push-down ETL/ELT functionality.  \nYou can integrate your Databricks SQL warehouses (formerly Databricks SQL endpoints) and Databricks clusters with Matillion.  \nConnect to Matillion using Partner Connect"
    },
    {
        "id": 1083,
        "url": "https://docs.databricks.com/en/partners/prep/matillion.html",
        "content": "Connect to Matillion using Partner Connect\nThis section describes how to use Partner Connect to simplify the process of connecting an existing SQL warehouse or cluster in your Databricks workspace to Matillion.  \nRequirements  \nSee the requirements for using Partner Connect.  \nSteps to connect  \nTo connect to Matillion using Partner Connect, follow the steps in this section.  \nTip  \nIf you have an existing Matillion account, Databricks recommends that you connect to Matillion manually. This is because the connection experience in Partner Connect is optimized for new partner accounts.  \nIn the sidebar, click Partner Connect.  \nClick the Matillion tile.  \nThe Email box displays the email address for your Databricks account. Matillion uses this email address to prompt you to either create a new Matillion account or sign in to your existing Matillion account.  \nClick Connect to Matillion ETL or Sign in.  \nA new tab opens in your browser that displays the Matillion Hub.  \nComplete the on-screen instructions in Matillion to create your 14-day trial Matillion account or to sign in to your existing Matillion account.  \nImportant  \nIf an error displays stating that someone from your organization has already created an account with Matillion, contact one of your organization\u2019s administrators and have them add you to your organization\u2019s Matillion account. After they add you, sign in to your existing Matillion account.  \nComplete the on-screen instructions to provide your job details, then click Continue.  \nComplete the on-screen instructions to create an organization, then click Continue.  \nClick the organization you created, then click Add Matillion ETL instance.  \nClick Continue in AWS.  \nThe Amazon EC2 console opens.  \nFollow Launching Matillion ETL using Amazon Machine Image in the Matillion ETL documentation, starting with step 5. Then follow Accessing Matillion ETL on Amazon Web Services (EC2) in the Matillion ETL documentation.  \nFollow the instructions in the Matillion ETL documentation.  \nMatillion ETL opens in your browser, and the Create Project dialog box displays.  \nFollow Create a Delta Lake on Databricks project in the Matillion documentation.  \nFor the settings in the Delta Lake Connection section within these instructions, enter the following information:  \nFor Workspace ID, enter the ID of your Databricks workspace. See Workspace instance names, URLs, and IDs.  \nFor Username, enter the word token.  \nFor Password, enter the value of a Databricks personal access token."
    },
    {
        "id": 1084,
        "url": "https://docs.databricks.com/en/partners/prep/matillion.html",
        "content": "For Username, enter the word token.  \nFor Password, enter the value of a Databricks personal access token.  \nTo get the Workspace ID and generate personal access token, do the following:  \nReturn to the Partner Connect tab in your browser.  \nTake note of the Workspace ID.  \nClick Generate a new token.  \nA new tab opens in your browser that displays the Settings page of the Databricks UI.  \nClick Generate new token.  \nOptionally enter a description (comment) and expiration period.  \nClick Generate.  \nCopy the generated personal access token and store it in a secure location.  \nReturn to the Matillion tab in your browser.  \nFor the settings in the Delta Lake Defaults section within these instructions, for Cluster, choose the name of the SQL warehouse or cluster.  \nContinue with Next steps."
    },
    {
        "id": 1085,
        "url": "https://docs.databricks.com/en/partners/prep/matillion.html",
        "content": "Connect to Matillion manually\nThis section describes how to connect an existing SQL warehouse or cluster in your Databricks workspace to Matillion manually.  \nNote  \nYou can connect to Matillion using Partner Connect to simplify the experience.  \nRequirements  \nBefore you integrate with Matillion manually, you must have the following:  \nA registered Matillion Hub account.  \nA Matillion ETL instance, which you can launch by using AWS CloudFormation, an Amazon Machine Image (AMI), or the AWS Marketplace.  \nA Databricks personal access token.  \nNote  \nAs a security best practice when you authenticate with automated tools, systems, scripts, and apps, Databricks recommends that you use OAuth tokens.  \nIf you use personal access token authentication, Databricks recommends using personal access tokens belonging to service principals instead of workspace users. To create tokens for service principals, see Manage tokens for a service principal.  \nSteps to connect  \nTo connect to Matillion manually, do the following:  \nGet the name of the existing compute resource that you want to use (a SQL warehouse or cluster) within your workspace. Later, you will choose that name to complete the connection between your compute resource and your Matillion ETL instance.  \nTo view SQL warehouses in your workspace, click SQL Warehouses in the sidebar. To create a new SQL warehouse, see Create a SQL warehouse.  \nTo view the clusters in your workspace, click Compute in the sidebar. To create a cluster, see Compute configuration reference.  \nFollow Connect to your Matillion ETL instance and log in to it in the Matillion documentation.  \nFollow Create a Delta Lake on Databricks project in the Matillion documentation.  \nFor the settings in the Delta Lake Connection section within these instructions, enter the following information:  \nFor Workspace ID, enter the ID of your Databricks workspace. See Workspace instance names, URLs, and IDs.  \nFor Username, enter the word token.  \nFor Password, enter the Databricks personal access token.  \nFor the settings in the Delta Lake Defaults section within these instructions, for Cluster, choose the name of the SQL warehouse or cluster.  \nContinue with Next steps."
    },
    {
        "id": 1086,
        "url": "https://docs.databricks.com/en/partners/prep/matillion.html",
        "content": "Next steps\nNext steps\nExplore one or more of the following resources on the Matillion website:  \nMatillion ETL Product Overview  \nUI and Basic Functions  \nDocumentation  \nSupport"
    },
    {
        "id": 1087,
        "url": "https://docs.databricks.com/en/partners/data-governance/alation.html",
        "content": "Connect Databricks to Alation  \nThis article describes how to connect your Databricks workspace to Alation. The Databricks integration with Alation\u2019s data governance platform extends the data discovery, governance, and catalog capabilities of Unity Catalog across data sources.  \nConnect to Alation using Partner Connect\nConnect to Alation using Partner Connect\nTo connect to Alation using Partner Connect, see Connect to data governance partners using Partner Connect.\n\nConnect to Alation manually\nConnect to Alation manually\nTo connect to Databricks from Alation manually, see the following articles in the Alation documentation:  \n(Recommended: Unity Catalog) Databricks Unity Catalog OCF Connector: Install and Configure  \n(Legacy Hive metastore) Databricks on AWS OCF Connector: Install and Configure\n\nAdditional resources\nAdditional resources\nAlation website  \nAlation documentation"
    },
    {
        "id": 1088,
        "url": "https://docs.databricks.com/en/reference/spark.html",
        "content": "Reference for Apache Spark APIs  \nDatabricks is built on top of Apache Spark, a unified analytics engine for big data and machine learning. For more information, see Apache Spark on Databricks.  \nApache Spark has DataFrame APIs for operating on large datasets, which include over 100 operators, in several languages.  \nPySpark APIs for Python developers. See Tutorial: Load and transform data using Apache Spark DataFrames. Key classes include:  \nSparkSession - The entry point to programming Spark with the Dataset and DataFrame API.  \nDataFrame - A distributed collection of data grouped into named columns. See DataFrames and DataFrame-based MLlib.  \nSparkR APIs for R developers. Key classes include:  \nSparkSession - SparkSession is the entry point into SparkR. See Starting Point: SparkSession.  \nSparkDataFrame - A distributed collection of data grouped into named columns. See Datasets and DataFrames, Creating DataFrames, and Creating SparkDataFrames.  \nScala APIs for Scala developers. Key classes include:  \nSparkSession - The entry point to programming Spark with the Dataset and DataFrame API. See Starting Point: SparkSession.  \nDataset - A strongly typed collection of domain-specific objects that can be transformed in parallel using functional or relational operations. Each Dataset also has an untyped view called a DataFrame, which is a Dataset of Row. See Datasets and DataFrames, Creating Datasets, Creating DataFrames, and DataFrame functions.  \nJava APIs for Java developers. Key classes include:  \nSparkSession - The entry point to programming Spark with the Dataset and DataFrame API. See Starting Point: SparkSession.  \nDataset - A strongly typed collection of domain-specific objects that can be transformed in parallel using functional or relational operations. Each Dataset also has an untyped view called a DataFrame, which is a Dataset of Row. See Datasets and DataFrames, Creating Datasets, Creating DataFrames, and DataFrame functions.  \nTo learn how to use the Apache Spark APIs on Databricks, see:  \nPySpark on Databricks  \nDatabricks for R developers"
    },
    {
        "id": 1089,
        "url": "https://docs.databricks.com/en/reference/spark.html",
        "content": "To learn how to use the Apache Spark APIs on Databricks, see:  \nPySpark on Databricks  \nDatabricks for R developers  \nDatabricks for Scala developers  \nFor Java, you can run Java code as a JAR job."
    },
    {
        "id": 1090,
        "url": "https://docs.databricks.com/en/release-notes/product/2018/september.html",
        "content": "September 2018  \nThese features and Databricks platform improvements were released in September 2018.  \nNote  \nReleases are staged. Your Databricks account may not be updated until a week after the initial release date.  \nSCIM provisioning using Okta and Microsoft Entra ID (Preview)\nSCIM provisioning using Okta and Microsoft Entra ID (Preview)\nSeptember 26, 2018  \nYou can now use Okta and Microsoft Entra ID to provision users and groups with Databricks and deprovision them when they leave your organization or no longer need access to Databricks.  \nThis feature is in Preview. See Sync users and groups from your identity provider.\n\nEBS leaked volumes deletion"
    },
    {
        "id": 1091,
        "url": "https://docs.databricks.com/en/release-notes/product/2018/september.html",
        "content": "EBS leaked volumes deletion\nSeptember 25 - October 2, 2018: Version 2.81  \nWhenever a Databricks cluster instance terminates, AWS should automatically delete all EBS volumes associated with the cluster. Unfortunately, if an instance dies during start-up, the EBS volumes never get attached to the instance, and AWS never cleans up the volumes. The AWS support team has confirmed that they are working on a fix for this bug, but they have not provided an ETA. With this release, Databricks now automatically deletes all leaked EBS volumes, including volumes leaked before this release.\n\nSupport for r5 instances\nSupport for r5 instances\nSeptember 25 - October 2, 2018: Version 2.81  \nDatabricks now supports the r5 family of AWS instance types.\n\nSCIM API for provisioning users and groups (Preview)"
    },
    {
        "id": 1092,
        "url": "https://docs.databricks.com/en/release-notes/product/2018/september.html",
        "content": "SCIM API for provisioning users and groups (Preview)\nSeptember 11-18, 2018: Version 2.80  \nDatabricks now supports SCIM, or System for Cross-domain Identity Management, an open standard that allows you to automate user provisioning using a REST API and JSON. SCIM lets you use an identity provider to create users in Databricks and give them the proper level of access, as well as remove access for users (deprovision them) when they leave your organization or no longer need access to Databricks.  \nThis feature is in Preview. See Groups API."
    },
    {
        "id": 1093,
        "url": "https://docs.databricks.com/en/release-notes/delta-live-tables/index.html",
        "content": "Delta Live Tables release notes and the release upgrade process  \nThis article explains the Delta Live Tables release process, how the Delta Live Tables runtime is managed, and provides links to release notes for each Delta Live Tables release.  \nDelta Live Tables runtime channels"
    },
    {
        "id": 1094,
        "url": "https://docs.databricks.com/en/release-notes/delta-live-tables/index.html",
        "content": "Delta Live Tables runtime channels\nDelta Live Tables clusters use runtimes based on Databricks Runtime release notes versions and compatibility. Databricks automatically upgrades the Delta Live Tables runtimes to support enhancements and upgrades to the platform. You can use the channel field in the Delta Live Tables pipeline settings to control the Delta Live Tables runtime version that runs your pipeline. The supported values are:  \ncurrent to use the current runtime version.  \npreview to test your pipeline with upcoming changes to the runtime version.  \nBy default, your pipelines run using the current runtime version. Databricks recommends using the current runtime for production workloads. To learn how to use the preview setting to test your pipelines with the next runtime version, see Automate testing of your pipelines with the next runtime version.  \nTo see the Databricks Runtime versions used with a Delta Live Tables release, see the release notes for that release.  \nFor more information about Delta Live Tables channels, see the channel field in the Delta Live Tables pipeline settings.  \nTo understand how Delta Live Tables manages the upgrade process for each release, see How do Delta Live Tables upgrades work?."
    },
    {
        "id": 1095,
        "url": "https://docs.databricks.com/en/release-notes/delta-live-tables/index.html",
        "content": "How do I find the Databricks Runtime version for a pipeline update?\nHow do I find the Databricks Runtime version for a pipeline update?\nYou can query the Delta Live Tables event log to find the Databricks Runtime version for a pipeline update. See Runtime information.\n\nDelta Live Tables release notes"
    },
    {
        "id": 1096,
        "url": "https://docs.databricks.com/en/release-notes/delta-live-tables/index.html",
        "content": "Delta Live Tables release notes\nDelta Live Tables release notes are organized by year and week-of-year. Because Delta Live Tables is versionless, both workspace and runtime changes take place automatically. The following release notes provide an overview of changes and bug fixes in each release:  \nDelta Live Tables release 2024.29 Delta Live Tables release 2024.22 Delta Live Tables release 2024.20 Delta Live Tables release 2024.13 Delta Live Tables release 2024.11 Delta Live Tables release 2024.09 Delta Live Tables release 2024.05 Delta Live Tables release 2024.02 Delta Live Tables release 2023.50 Delta Live Tables release 2023.48 Delta Live Tables release 2023.45 Delta Live Tables release 2023.43 Delta Live Tables release 2023.41 Delta Live Tables release 2023.37 Delta Live Tables release 2023.35 Delta Live Tables release 2023.30 Delta Live Tables release 2023.27 Delta Live Tables release 2023.23 Delta Live Tables release 2023.21 Delta Live Tables release 2023.19 Delta Live Tables release 2023.17 Delta Live Tables release 2023.16 Delta Live Tables release 2023.13 Delta Live Tables release 2023.11 Delta Live Tables release 2023.06 Delta Live Tables release 2023.03 Delta Live Tables release 2023.01 Delta Live Tables release 2022.49 Delta Live Tables release 2022.46 Delta Live Tables release 2022.44 Delta Live Tables release 2022.42 Delta Live Tables release 2022.40 Delta Live Tables release 2022.37"
    },
    {
        "id": 1097,
        "url": "https://docs.databricks.com/en/release-notes/delta-live-tables/index.html",
        "content": "How do Delta Live Tables upgrades work?\nHow do Delta Live Tables upgrades work?\nDelta Live Tables is considered to be a versionless product, which means that Databricks automatically upgrades the Delta Live Tables runtime to support enhancements and upgrades to the platform. Databricks recommends limiting external dependencies for Delta Live Tables pipelines.  \nDatabricks proactively works to prevent automatic upgrades from introducing errors or issues to production Delta Live Tables pipelines. See Delta Live Tables upgrade process.  \nEspecially for users that deploy Delta Live Tables pipelines with external dependencies, Databricks recommends proactively testing pipelines with preview channels. See Automate testing of your pipelines with the next runtime version.\n\nDelta Live Tables upgrade process"
    },
    {
        "id": 1098,
        "url": "https://docs.databricks.com/en/release-notes/delta-live-tables/index.html",
        "content": "Delta Live Tables upgrade process\nDatabricks manages the Databricks Runtime used by Delta Live Tables compute resources. Delta Live Tables automatically upgrades the runtime in your Databricks workspaces and monitors the health of your pipelines after the upgrade.  \nIf Delta Live Tables detects that a pipeline cannot start because of an upgrade, the runtime version for the pipeline reverts to the previous version that is known to be stable, and the following steps are triggered automatically:  \nThe pipeline\u2019s Delta Live Tables runtime is pinned to the previous known-good version.  \nThe Delta Live Tables UI shows a visual indicator that the pipeline is pinned to a previous version because of an upgrade failure.  \nDatabricks support is notified of the issue.  \nIf the issue is related to a regression in the runtime, Databricks resolves the issue.  \nIf the issue is caused by a custom library or package used by the pipeline, Databricks contacts you to resolve the issue.  \nWhen the issue is resolved, Databricks initiates the upgrade again.  \nImportant  \nDelta Live Tables only reverts pipelines running in production mode with the channel set to current."
    },
    {
        "id": 1099,
        "url": "https://docs.databricks.com/en/release-notes/delta-live-tables/index.html",
        "content": "Automate testing of your pipelines with the next runtime version\nTo ensure changes in the next Delta Live Tables runtime version do not impact your pipelines, use the Delta Live Tables channels feature:  \nCreate a staging pipeline and set the channel to preview.  \nIn the Delta Live Tables UI, create a schedule to run the pipeline weekly and enable alerts to receive an email notification for pipeline failures. Databricks recommends scheduling weekly test runs of pipelines, especially if you use custom pipeline dependencies.  \nIf you receive a notification of a failure and are unable to resolve it, open a support ticket with Databricks.  \nPipeline dependencies  \nDelta Live Tables supports external dependencies in your pipelines; for example, you can install any Python package using the %pip install command. Delta Live Tables also supports using global and cluster-scoped init scripts. However, these external dependencies, particularly init scripts, increase the risk of issues with runtime upgrades. To mitigate these risks, minimize using init scripts in your pipelines. If your processing requires init scripts, automate testing of your pipeline to detect problems early; see Automate testing of your pipelines with the next runtime version. If you use init scripts, Databricks recommends increasing your testing frequency."
    },
    {
        "id": 1100,
        "url": "https://docs.databricks.com/en/query-federation/bigquery.html",
        "content": "Run federated queries on Google BigQuery  \nPreview  \nThis feature is in Public Preview.  \nThis article describes how to set up Lakehouse Federation to run federated queries on BigQuery data that is not managed by Databricks. To learn more about Lakehouse Federation, see What is Lakehouse Federation?.  \nTo connect to your BigQuery database using Lakehouse Federation, you must create the following in your Databricks Unity Catalog metastore:  \nA connection to your BigQuery database.  \nA foreign catalog that mirrors your BigQuery database in Unity Catalog so that you can use Unity Catalog query syntax and data governance tools to manage Databricks user access to the database.  \nBefore you begin"
    },
    {
        "id": 1101,
        "url": "https://docs.databricks.com/en/query-federation/bigquery.html",
        "content": "Before you begin\nWorkspace requirements:  \nWorkspace enabled for Unity Catalog.  \nCompute requirements:  \nNetwork connectivity from your Databricks Runtime cluster or SQL warehouse to the target database systems. See Networking recommendations for Lakehouse Federation.  \nDatabricks clusters must use Databricks Runtime 13.3 LTS or above and shared or single-user access mode.  \nSQL warehouses must be Pro or Serverless.  \nPermissions required:  \nTo create a connection, you must be a metastore admin or a user with the CREATE CONNECTION privilege on the Unity Catalog metastore attached to the workspace.  \nTo create a foreign catalog, you must have the CREATE CATALOG permission on the metastore and be either the owner of the connection or have the CREATE FOREIGN CATALOG privilege on the connection.  \nAdditional permission requirements are specified in each task-based section that follows.\n\nCreate a connection"
    },
    {
        "id": 1102,
        "url": "https://docs.databricks.com/en/query-federation/bigquery.html",
        "content": "Create a connection\nA connection specifies a path and credentials for accessing an external database system. To create a connection, you can use Catalog Explorer or the CREATE CONNECTION SQL command in a Databricks notebook or the Databricks SQL query editor.  \nPermissions required: Metastore admin or user with the CREATE CONNECTION privilege.  \nIn your Databricks workspace, click Catalog.  \nAt the top of the Catalog pane, click the Add icon and select Add a connection from the menu.  \nAlternatively, from the Quick access page, click the External data > button, go to the Connections tab, and click Create connection.  \nEnter a user-friendly Connection name.  \nSelect a Connection type of BigQuery.  \nEnter the following connection property for your BigQuery instance.  \nGoogleServiceAccountKeyJson: A raw JSON object that is used to specify the BigQuery project and provide authentication. You can generate this JSON object and download it from the service account details page in Google Cloud under \u2018KEYS\u2019. The service account must have proper permissions granted in BigQuery, including BigQuery User and BigQuery Data Viewer. The following is an example."
    },
    {
        "id": 1103,
        "url": "https://docs.databricks.com/en/query-federation/bigquery.html",
        "content": "{ \"type\": \"service_account\", \"project_id\": \"PROJECT_ID\", \"private_key_id\": \"KEY_ID\", \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nPRIVATE_KEY\\n-----END PRIVATE KEY-----\\n\", \"client_email\": \"SERVICE_ACCOUNT_EMAIL\", \"client_id\": \"CLIENT_ID\", \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\", \"token_uri\": \"https://accounts.google.com/o/oauth2/token\", \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\", \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\", \"universe_domain\": \"googleapis.com\" }  \n(Optional) Click Test connection to confirm network connectivity. This action does not test authentication.  \n(Optional) Add a comment.  \nClick Create.  \nRun the following command in a notebook or the Databricks SQL query editor. Replace <GoogleServiceAccountKeyJson> with a raw JSON object that specifies the BigQuery project and provides authentication. You can generate this JSON object and download it from the service account details page in Google Cloud under \u2018KEYS\u2019. The service account needs to have proper permissions granted in BigQuery, including BigQuery User and BigQuery Data Viewer. For an example JSON object, view the Catalog Explorer tab on this page.  \nCREATE CONNECTION <connection-name> TYPE bigquery OPTIONS ( GoogleServiceAccountKeyJson '<GoogleServiceAccountKeyJson>' );  \nWe recommend that you use Databricks secrets instead of plaintext strings for sensitive values like credentials. For example:"
    },
    {
        "id": 1104,
        "url": "https://docs.databricks.com/en/query-federation/bigquery.html",
        "content": "We recommend that you use Databricks secrets instead of plaintext strings for sensitive values like credentials. For example:  \nCREATE CONNECTION <connection-name> TYPE bigquery OPTIONS ( GoogleServiceAccountKeyJson secret ('<secret-scope>','<secret-key-user>') )  \nFor information about setting up secrets, see Secret management."
    },
    {
        "id": 1105,
        "url": "https://docs.databricks.com/en/query-federation/bigquery.html",
        "content": "Create a foreign catalog\nA foreign catalog mirrors a database in an external data system so that you can query and manage access to data in that database using Databricks and Unity Catalog. To create a foreign catalog, use a connection to the data source that has already been defined.  \nTo create a foreign catalog, you can use Catalog Explorer or CREATE FOREIGN CATALOG in a Databricks notebook or the Databricks SQL query editor.  \nPermissions required: CREATE CATALOG permission on the metastore and either ownership of the connection or the CREATE FOREIGN CATALOG privilege on the connection.  \nIn your Databricks workspace, click Catalog to open Catalog Explorer.  \nAt the top of the Catalog pane, click the Add icon and select Add a catalog from the menu.  \nAlternatively, from the Quick access page, click the Catalogs button, and then click the Create catalog button.  \nFollow the instructions for creating foreign catalogs in Create catalogs.  \nRun the following SQL command in a notebook or the Databricks SQL editor. Items in brackets are optional. Replace the placeholder values.  \n<catalog-name>: Name for the catalog in Databricks.  \n<connection-name>: The connection object that specifies the data source, path, and access credentials.  \nCREATE FOREIGN CATALOG [IF NOT EXISTS] <catalog-name> USING CONNECTION <connection-name>;"
    },
    {
        "id": 1106,
        "url": "https://docs.databricks.com/en/query-federation/bigquery.html",
        "content": "Supported pushdowns\nSupported pushdowns\nThe following pushdowns are supported:  \nFilters  \nProjections  \nLimit  \nFunctions: partial, only for filter expressions. (String functions, Mathematical functions, Data, Time and Timestamp functions, and other miscellaneous functions, such as Alias, Cast, SortOrder)  \nAggregates  \nSorting, when used with limit  \nThe following pushdowns are not supported:  \nJoins  \nWindows functions\n\nData type mappings\nData type mappings\nThe following table shows the BigQuery to Spark data type mapping.  \nBigQuery type  \nSpark type  \nbignumeric, numeric  \nDecimalType  \nint64  \nLongType  \nfloat64  \nDoubleType  \narray, geography, interval, json, string, struct  \nVarcharType  \nbytes  \nBinaryType  \nbool  \nBooleanType  \ndate  \nDateType  \ndatetime, time, timestamp  \nTimestampType/TimestampNTZType  \nWhen you read from BigQuery, BigQuery Timestamp is mapped to Spark TimestampType if preferTimestampNTZ = false (default). BigQuery Timestamp is mapped to TimestampNTZType if preferTimestampNTZ = true."
    },
    {
        "id": 1107,
        "url": "https://docs.databricks.com/en/partner-connect/prep.html",
        "content": "Connect to data prep partners using Partner Connect  \nTo connect your Databricks workspace to a data preparation and transformation partner solution using Partner Connect, you typically follow the steps in this article.  \nImportant  \nBefore you follow the steps in this article, see the appropriate partner article for important partner-specific information. There might be differences in the connection steps between partner solutions. Some partner solutions also allow you to integrate with Databricks SQL warehouses (formerly Databricks SQL endpoints) or Databricks clusters, but not both.  \nRequirements\nRequirements\nSee the requirements for using Partner Connect.  \nImportant  \nFor partner-specific requirements, see the appropriate partner article.\n\nSteps to connect to a data preparation and transformation partner"
    },
    {
        "id": 1108,
        "url": "https://docs.databricks.com/en/partner-connect/prep.html",
        "content": "Steps to connect to a data preparation and transformation partner\nTo connect your Databricks workspace to a data preparation and transformation partner solution, follow the steps in this section.  \nTip  \nIf you have an existing partner account, Databricks recommends that you follow the steps to connect to the partner solution manually in the appropriate partner article. This is because the connection experience in Partner Connect is optimized for new partner accounts.  \nIn the sidebar, click Partner Connect.  \nClick the partner tile.  \nNote  \nIf the partner tile has a check mark icon inside it, an administrator has already used Partner Connect to connect the partner to your workspace. Skip to step 5. The partner uses the email address for your Databricks account to prompt you to sign in to your existing partner account.  \nSelect a catalog for the partner to write to, then click Next.  \nNote  \nIf a partner doesn\u2019t support Unity Catalog with Partner Connect, the workspace default catalog is used. If your workspace isn\u2019t Unity Catalog-enabled, hive_metastore is used.  \nPartner Connect creates the following resources in your workspace:  \nA Databricks service principal named <PARTNER>_USER.  \nA Databricks personal access token that is associated with the <PARTNER>_USER service principal.  \nClick Next.  \nThe Email box displays the email address for your Databricks account. The partner uses this email address to prompt you to either create a new partner account or sign in to your existing partner account.  \nClick Connect to <Partner> or Sign in.  \nA new tab opens in your web browser, which displays the partner website.  \nComplete the on-screen instructions on the partner website to create your trial partner account or sign in to your existing partner account."
    },
    {
        "id": 1109,
        "url": "https://docs.databricks.com/en/security/privacy/uk-cyber-essentials-plus.html",
        "content": "UK Cyber Essentials Plus compliance controls  \nPreview  \nThe ability for admins to add Enhanced Security and Compliance features is a feature in Public Preview. The compliance security profile and support for compliance standards are generally available (GA).  \nUK Cyber Essentials Plus compliance controls (UKCE+) provide enhancements that help you with cyber essentials compliance for your workspace. UKCE+ is a certification created by the UK government to simplify and standardize IT security practices for commercial organizations who interact with UK government data.  \nUKCE+ require enabling the compliance security profile, which adds monitoring agents, enforces instance types for inter-node encryption, provides a hardened compute image, and other features. For technical details, see Compliance security profile. It is your responsibility to confirm that each workspace has the compliance security profile enabled and confirm that UKCE+ is added as a compliance program.  \nUKCE+ compliance controls is only available in the eu-west-2 region.  \nWhich compute resources get enhanced security"
    },
    {
        "id": 1110,
        "url": "https://docs.databricks.com/en/security/privacy/uk-cyber-essentials-plus.html",
        "content": "Which compute resources get enhanced security\nThe compliance security profile enhancements apply to compute resources in the classic compute plane in all supported regions.  \nSupport for serverless SQL warehouses for the compliance security profile varies by region. See Serverless SQL warehouses support the compliance security profile in some regions.\n\nRequirements"
    },
    {
        "id": 1111,
        "url": "https://docs.databricks.com/en/security/privacy/uk-cyber-essentials-plus.html",
        "content": "Requirements\nYour Databricks account must include the Enhanced Security and Compliance add-on. For details, see the pricing page.  \nYour Databricks workspace must be on the Enterprise pricing tier.  \nYour Databricks workspace must be in the eu-west-2 AWS region.  \nSingle sign-on (SSO) authentication is configured for the workspace.  \nYour workspace must enable the compliance security profile and includes the UKCE+ compliance standard as part of the compliance security profile configuration.  \nYou must use the following VM instance types:  \nGeneral purpose: M-fleet, Md-fleet, M5dn, M5n, M5zn, M6i, M7i, M6id, M6in, M6idn, M6a, M7a  \nCompute optimized: C5a, C5ad, C5n, C6i, C6id, C7i, C6in, C6a, C7a  \nMemory optimized: R-fleet, Rd-fleet, R6i, R7i, R7iz, R6id, R6in, R6idn, R6a, R7a  \nStorage optimized: D3, D3en, P3dn, R5dn, R5n, I4i, I3en  \nAccelerated computing: G4dn, G5, P4d, P4de, P5  \nEnsure that sensitive information is never entered in customer-defined input fields, such as workspace names, cluster names, and job names."
    },
    {
        "id": 1112,
        "url": "https://docs.databricks.com/en/security/privacy/uk-cyber-essentials-plus.html",
        "content": "Enable UK Cyber Essentials Plus compliance controls on a workspace\nEnable UK Cyber Essentials Plus compliance controls on a workspace\nTo configure your workspace to support processing of data regulated by the UKCE+ standard, the workspace must have the compliance security profile enabled. You can enable the compliance security profile and add the UKCE+ compliance standard across all workspaces or only on some workspaces.  \nTo enable the compliance security profile and add the UKCE+ compliance standard for an existing workspace, see Enable enhanced security and compliance features on an existing workspace.  \nTo set an account-level setting to enable the compliance security profile and UKCE+ for new workspaces, see Set account-level defaults for all new workspaces.\n\nPreview features that are supported for processing of data regulated under UKCE+ standard"
    },
    {
        "id": 1113,
        "url": "https://docs.databricks.com/en/security/privacy/uk-cyber-essentials-plus.html",
        "content": "Preview features that are supported for processing of data regulated under UKCE+ standard\nThe following preview features are supported for processing of processing of data regulated under UKCE+ standard:  \nSCIM provisioning  \nIAM passthrough  \nSecret paths in environment variables  \nSystem tables  \nFiltering sensitive table data with row filters and column masks  \nLakehouse Federation to Redshift  \nUnity Catalog-enabled DLT pipelines  \nScala support for shared clusters  \nDelta Live Tables Hive metastore to Unity Catalog clone API  \nDelta Live Tables with serverless compute  \nAI/BI Genie\n\nDoes Databricks permit the processing of data regulated under UKCE+ standard on Databricks?\nDoes Databricks permit the processing of data regulated under UKCE+ standard on Databricks?\nYes, if you comply with the requirements, enable the compliance security profile, and add the UKCE+ compliance standard as part of the compliance security profile configuration."
    },
    {
        "id": 1114,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "Read and write XML files  \nPreview  \nThis feature is in Public Preview.  \nThis article describes how to read and write XML files.  \nExtensible Markup Language (XML) is a markup language for formatting, storing, and sharing data in textual format. It defines a set of rules for serializing data ranging from documents to arbitrary data structures.  \nNative XML file format support enables ingestion, querying, and parsing of XML data for batch processing or streaming. It can automatically infer and evolve schema and data types, supports SQL expressions like from_xml, and can generate XML documents. It doesn\u2019t require external jars and works seamlessly with Auto Loader, read_files and COPY INTO. You can optionally validate each row-level XML record against an XML Schema Definition (XSD).  \nRequirements\nRequirements\nDatabricks Runtime 14.3 and above\n\nParse XML records"
    },
    {
        "id": 1115,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "Parse XML records\nXML specification mandates a well-formed structure. However, this specification doesn\u2019t immediately map to a tabular format. You must specify the rowTag option to indicate the XML element that maps to a DataFrame Row. The rowTag element becomes the top-level struct. The child elements of rowTag become the fields of the top-level struct.  \nYou can specify the schema for this record or let it be inferred automatically. Because the parser only examines the rowTag elements, DTD and external entities are filtered out.  \nThe following examples illustrate schema inference and parsing of an XML file using different rowTag options:  \nxmlString = \"\"\" <books> <book id=\"bk103\"> <author>Corets, Eva</author> <title>Maeve Ascendant</title> </book> <book id=\"bk104\"> <author>Corets, Eva</author> <title>Oberon's Legacy</title> </book> </books>\"\"\" xmlPath = \"dbfs:/tmp/books.xml\" dbutils.fs.put(xmlPath, xmlString, True)  \nval xmlString = \"\"\" <books> <book id=\"bk103\"> <author>Corets, Eva</author> <title>Maeve Ascendant</title> </book> <book id=\"bk104\"> <author>Corets, Eva</author> <title>Oberon's Legacy</title> </book> </books>\"\"\" val xmlPath = \"dbfs:/tmp/books.xml\" dbutils.fs.put(xmlPath, xmlString)  \nRead the XML file with rowTag option as \u201cbooks\u201d:  \ndf = spark.read.option(\"rowTag\", \"books\").format(\"xml\").load(xmlPath) df.printSchema() df.show(truncate=False)"
    },
    {
        "id": 1116,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "val df = spark.read.option(\"rowTag\", \"books\").xml(xmlPath) df.printSchema() df.show(truncate=false)  \nOutput:  \nroot |-- book: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- _id: string (nullable = true) | | |-- author: string (nullable = true) | | |-- title: string (nullable = true) +------------------------------------------------------------------------------+ |book | +------------------------------------------------------------------------------+ |[{bk103, Corets, Eva, Maeve Ascendant}, {bk104, Corets, Eva, Oberon's Legacy}]| +------------------------------------------------------------------------------+  \nRead the XML file with rowTag as \u201cbook\u201d:  \ndf = spark.read.option(\"rowTag\", \"book\").format(\"xml\").load(xmlPath) # Infers three top-level fields and parses `book` in separate rows:"
    },
    {
        "id": 1117,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "val df = spark.read.option(\"rowTag\", \"book\").xml(xmlPath) // Infers three top-level fields and parses `book` in separate rows:  \nOutput:  \nroot |-- _id: string (nullable = true) |-- author: string (nullable = true) |-- title: string (nullable = true) +-----+-----------+---------------+ |_id |author |title | +-----+-----------+---------------+ |bk103|Corets, Eva|Maeve Ascendant| |bk104|Corets, Eva|Oberon's Legacy| +-----+-----------+---------------+"
    },
    {
        "id": 1118,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "Data source options\nData source options\nData source options for XML can be specified the following ways:  \nThe .option/.options methods of the following:  \nDataFrameReader  \nDataFrameWriter  \nDataStreamReader  \nDataStreamWriter  \nThe following built-in functions:  \nfrom_xml  \nto_xml  \nschema_of_xml  \nThe OPTIONS clause of CREATE TABLE USING DATA_SOURCE  \nFor a list of options, see Auto Loader options.\n\nXSD support"
    },
    {
        "id": 1119,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "XSD support\nYou can optionally validate each row-level XML record by an XML Schema Definition (XSD). The XSD file is specified in the rowValidationXSDPath option. The XSD does not otherwise affect the schema provided or inferred. A record that fails the validation is marked as \u201ccorrupted\u201d and handled based on the corrupt record handling mode option described in the option section.  \nYou can use XSDToSchema to extract a Spark DataFrame schema from a XSD file. It supports only simple, complex, and sequence types, and only supports basic XSD functionality."
    },
    {
        "id": 1120,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "You can use XSDToSchema to extract a Spark DataFrame schema from a XSD file. It supports only simple, complex, and sequence types, and only supports basic XSD functionality.  \nimport org.apache.spark.sql.execution.datasources.xml.XSDToSchema import org.apache.hadoop.fs.Path val xsdPath = \"dbfs:/tmp/books.xsd\" val xsdString = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\" ?> <xs:schema xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"> <xs:element name=\"book\"> <xs:complexType> <xs:sequence> <xs:element name=\"author\" type=\"xs:string\" /> <xs:element name=\"title\" type=\"xs:string\" /> <xs:element name=\"genre\" type=\"xs:string\" /> <xs:element name=\"price\" type=\"xs:decimal\" /> <xs:element name=\"publish_date\" type=\"xs:date\" /> <xs:element name=\"description\" type=\"xs:string\" /> </xs:sequence> <xs:attribute name=\"id\" type=\"xs:string\" use=\"required\" /> </xs:complexType> </xs:element> </xs:schema>\"\"\" dbutils.fs.put(xsdPath, xsdString, true) val schema1 = XSDToSchema.read(xsdString) val schema2 = XSDToSchema.read(new Path(xsdPath))  \nThe following table shows the conversion of XSD data types to Spark data types:  \nXSD Data Types  \nSpark Data Types  \nboolean  \nBooleanType  \ndecimal  \nDecimalType  \nunsignedLong  \nDecimalType(38, 0)  \ndouble  \nDoubleType  \nfloat  \nFloatType  \nbyte  \nByteType"
    },
    {
        "id": 1121,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "XSD Data Types  \nSpark Data Types  \nboolean  \nBooleanType  \ndecimal  \nDecimalType  \nunsignedLong  \nDecimalType(38, 0)  \ndouble  \nDoubleType  \nfloat  \nFloatType  \nbyte  \nByteType  \nshort, unsignedByte  \nShortType  \ninteger, negativeInteger, nonNegativeInteger, nonPositiveInteger, positiveInteger, unsignedShort  \nIntegerType  \nlong, unsignedInt  \nLongType  \ndate  \nDateType  \ndateTime  \nTimestampType  \nOthers  \nStringType"
    },
    {
        "id": 1122,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "Parse nested XML\nXML data in a string-valued column in an existing DataFrame can be parsed with schema_of_xml and from_xml that returns the schema and the parsed results as new struct columns. XML data passed as an argument to schema_of_xml and from_xml must be a single well-formed XML record.  \nschema_of_xml  \nSyntax  \nschema_of_xml(xmlStr [, options] )  \nArguments  \nxmlStr: A STRING expression specifying a single well-formed XML record.  \noptions: An optional MAP<STRING,STRING> literal specifying directives.  \nReturns  \nA STRING holding a definition of a struct with n fields of strings where the column names are derived from the XML element and attribute names. The field values hold the derived formatted SQL types.  \nfrom_xml  \nSyntax  \nfrom_xml(xmlStr, schema [, options])  \nArguments  \nxmlStr: A STRING expression specifying a single well-formed XML record.  \nschema: A STRING expression or invocation of the schema_of_xml function.  \noptions: An optional MAP<STRING,STRING> literal specifying directives.  \nReturns  \nA struct with field names and types matching the schema definition. Schema must be defined as comma-separated column name and data type pairs as used in, for example, CREATE TABLE. Most options shown in the data source options are applicable with the following exceptions:  \nrowTag: Because there is only one XML record, the rowTag option is not applicable.  \nmode (default: PERMISSIVE): Allows a mode for dealing with corrupt records during parsing.  \nPERMISSIVE: When it meets a corrupted record, puts the malformed string into a field configured by columnNameOfCorruptRecord, and sets malformed fields to null. To keep corrupt records, you can set a string type field named columnNameOfCorruptRecord in a user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. When inferring a schema, it implicitly adds a columnNameOfCorruptRecord field in an output schema.  \nFAILFAST: Throws an exception when it meets corrupted records."
    },
    {
        "id": 1123,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "Structure conversion\nDue to the structure differences between DataFrame and XML, there are some conversion rules from XML data to DataFrame and from DataFrame to XML data. Note that handling attributes can be disabled with the option excludeAttribute.  \nConversion from XML to DataFrame  \nAttributes: Attributes are converted as fields with the heading prefix attributePrefix.  \n<one myOneAttrib=\"AAAA\"> <two>two</two> <three>three</three> </one>  \nproduces a schema below:  \nroot |-- _myOneAttrib: string (nullable = true) |-- two: string (nullable = true) |-- three: string (nullable = true)  \nCharacter data in an element containing attribute(s) or child element(s): These are parsed into the valueTag field. If there are multiple occurrences of character data, the valueTag field is converted to an array type.  \n<one> <two myTwoAttrib=\"BBBBB\">two</two> some value between elements <three>three</three> some other value between elements </one>  \nproduces a schema below:  \nroot |-- _VALUE: array (nullable = true) | |-- element: string (containsNull = true) |-- two: struct (nullable = true) | |-- _VALUE: string (nullable = true) | |-- _myTwoAttrib: string (nullable = true) |-- three: string (nullable = true)  \nConversion from DataFrame to XML  \nElement as an array in an array: Writing a XML file from DataFrame having a field ArrayType with its element as ArrayType would have an additional nested field for the element. This would not happen in reading and writing XML data but writing a DataFrame read from other sources. Therefore, roundtrip in reading and writing XML files has the same structure but writing a DataFrame read from other sources is possible to have a different structure.  \nDataFrame with a schema below:"
    },
    {
        "id": 1124,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "DataFrame with a schema below:  \n|-- a: array (nullable = true) | |-- element: array (containsNull = true) | | |-- element: string (containsNull = true)  \nand with data below:  \n+------------------------------------+ | a| +------------------------------------+ |[WrappedArray(aa), WrappedArray(bb)]| +------------------------------------+  \nproduces a XML file below:  \n<a> <item>aa</item> </a> <a> <item>bb</item> </a>  \nThe element name of the unnamed array in the DataFrame is specified by the option arrayElementName (Default: item)."
    },
    {
        "id": 1125,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "Rescued data column\nThe rescued data column ensures that you never lose or miss out on data during ETL. You can enable the rescued data column to capture any data that wasn\u2019t parsed because one or more fields in a record have one of the following issues:  \nAbsent from the provided schema  \nDoes not match the data type of the provided schema  \nHas a case mismatch with the field names in the provided schema  \nThe rescued data column is returned as a JSON document containing the columns that were rescued, and the source file path of the record. To remove the source file path from the rescued data column, you can set the following SQL configuration:  \nspark.conf.set(\"spark.databricks.sql.rescuedDataColumn.filePath.enabled\", \"false\")  \nspark.conf.set(\"spark.databricks.sql.rescuedDataColumn.filePath.enabled\", \"false\").  \nYou can enable the rescued data column by setting the option rescuedDataColumn to a column name when reading data, such as _rescued_data with spark.read.option(\"rescuedDataColumn\", \"_rescued_data\").format(\"xml\").load(<path>).  \nThe XML parser supports three modes when parsing records: PERMISSIVE, DROPMALFORMED, and FAILFAST. When used together with rescuedDataColumn, data type mismatches do not cause records to be dropped in DROPMALFORMED mode or throw an error in FAILFAST mode. Only corrupt records (incomplete or malformed XML) are dropped or throw errors."
    },
    {
        "id": 1126,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "Schema inference and evolution in Auto Loader\nFor a detailed discussion of this topic and applicable options, see Configure schema inference and evolution in Auto Loader. You can configure Auto Loader to automatically detect the schema of loaded XML data, allowing you to initialize tables without explicitly declaring the data schema and evolve the table schema as new columns are introduced. This eliminates the need to manually track and apply schema changes over time.  \nBy default, Auto Loader schema inference seeks to avoid schema evolution issues due to type mismatches. For formats that don\u2019t encode data types (JSON, CSV, and XML), Auto Loader infers all columns as strings, including nested fields in XML files. The Apache Spark DataFrameReader uses a different behavior for schema inference, selecting data types for columns in XML sources based on sample data. To enable this behavior with Auto Loader, set the option cloudFiles.inferColumnTypes to true.  \nAuto Loader detects the addition of new columns as it processes your data. When Auto Loader detects a new column, the stream stops with an UnknownFieldException. Before your stream throws this error, Auto Loader performs schema inference on the latest micro-batch of data and updates the schema location with the latest schema by merging new columns to the end of the schema. The data types of existing columns remain unchanged. Auto Loader supports different modes for schema evolution, which you set in the option cloudFiles.schemaEvolutionMode.  \nYou can use schema hints to enforce the schema information that you know and expect on an inferred schema. When you know that a column is of a specific data type, or if you want to choose a more general data type (for example, a double instead of an integer), you can provide an arbitrary number of hints for column data types as a string using SQL schema specification syntax. When the rescued data column is enabled, fields named in a case other than that of the schema are loaded to the _rescued_data column. You can change this behavior by setting the option readerCaseSensitive to false, in which case Auto Loader reads data in a case-insensitive way."
    },
    {
        "id": 1127,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "Examples\nThe examples in this section use an XML file available for download in the Apache Spark GitHub repo.  \nRead and write XML  \ndf = (spark.read .format('xml') .options(rowTag='book') .load(xmlPath)) # books.xml selected_data = df.select(\"author\", \"_id\") (selected_data.write .options(rowTag='book', rootTag='books') .xml('newbooks.xml'))  \nval df = spark.read .option(\"rowTag\", \"book\") .xml(xmlPath) // books.xml val selectedData = df.select(\"author\", \"_id\") selectedData.write .option(\"rootTag\", \"books\") .option(\"rowTag\", \"book\") .xml(\"newbooks.xml\")  \ndf <- loadDF(\"books.xml\", source = \"xml\", rowTag = \"book\") # In this case, `rootTag` is set to \"ROWS\" and `rowTag` is set to \"ROW\". saveDF(df, \"newbooks.xml\", \"xml\", \"overwrite\")  \nYou can manually specify the schema when reading data:"
    },
    {
        "id": 1128,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "You can manually specify the schema when reading data:  \nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType custom_schema = StructType([ StructField(\"_id\", StringType(), True), StructField(\"author\", StringType(), True), StructField(\"description\", StringType(), True), StructField(\"genre\", StringType(), True), StructField(\"price\", DoubleType(), True), StructField(\"publish_date\", StringType(), True), StructField(\"title\", StringType(), True) ]) df = spark.read.options(rowTag='book').xml('books.xml', schema = customSchema) selected_data = df.select(\"author\", \"_id\") selected_data.write.options(rowTag='book', rootTag='books').xml('newbooks.xml')"
    },
    {
        "id": 1129,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "import org.apache.spark.sql.types.{StructType, StructField, StringType, DoubleType} val customSchema = StructType(Array( StructField(\"_id\", StringType, nullable = true), StructField(\"author\", StringType, nullable = true), StructField(\"description\", StringType, nullable = true), StructField(\"genre\", StringType, nullable = true), StructField(\"price\", DoubleType, nullable = true), StructField(\"publish_date\", StringType, nullable = true), StructField(\"title\", StringType, nullable = true))) val df = spark.read.option(\"rowTag\", \"book\").schema(customSchema).xml(xmlPath) // books.xml val selectedData = df.select(\"author\", \"_id\") selectedData.write.option(\"rootTag\", \"books\").option(\"rowTag\", \"book\").xml(\"newbooks.xml\")  \ncustomSchema <- structType( structField(\"_id\", \"string\"), structField(\"author\", \"string\"), structField(\"description\", \"string\"), structField(\"genre\", \"string\"), structField(\"price\", \"double\"), structField(\"publish_date\", \"string\"), structField(\"title\", \"string\")) df <- loadDF(\"books.xml\", source = \"xml\", schema = customSchema, rowTag = \"book\") # In this case, `rootTag` is set to \"ROWS\" and `rowTag` is set to \"ROW\". saveDF(df, \"newbooks.xml\", \"xml\", \"overwrite\")  \nSQL API  \nXML data source can infer data types:"
    },
    {
        "id": 1130,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "SQL API  \nXML data source can infer data types:  \nDROP TABLE IF EXISTS books; CREATE TABLE books USING XML OPTIONS (path \"books.xml\", rowTag \"book\"); SELECT * FROM books;  \nYou can also specify column names and types in DDL. In this case, the schema is not inferred automatically.  \nDROP TABLE IF EXISTS books; CREATE TABLE books (author string, description string, genre string, _id string, price double, publish_date string, title string) USING XML OPTIONS (path \"books.xml\", rowTag \"book\");  \nLoad XML using COPY INTO  \nDROP TABLE IF EXISTS books; CREATE TABLE IF NOT EXISTS books; COPY INTO books FROM \"/FileStore/xmltestDir/input/books.xml\" FILEFORMAT = XML FORMAT_OPTIONS ('mergeSchema' = 'true', 'rowTag' = 'book') COPY_OPTIONS ('mergeSchema' = 'true');  \nRead XML with row validation  \ndf = (spark.read .format(\"xml\") .option(\"rowTag\", \"book\") .option(\"rowValidationXSDPath\", xsdPath) .load(inputPath)) df.printSchema()  \nval df = spark.read .option(\"rowTag\", \"book\") .option(\"rowValidationXSDPath\", xsdPath) .xml(inputPath) df.printSchema  \nParse nested XML (from_xml and schema_of_xml)"
    },
    {
        "id": 1131,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "Parse nested XML (from_xml and schema_of_xml)  \nfrom pyspark.sql.functions import from_xml, schema_of_xml, lit, col xml_data = \"\"\" <book id=\"bk103\"> <author>Corets, Eva</author> <title>Maeve Ascendant</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-11-17</publish_date> </book> \"\"\" df = spark.createDataFrame([(8, xml_data)], [\"number\", \"payload\"]) schema = schema_of_xml(df.select(\"payload\").limit(1).collect()[0][0]) parsed = df.withColumn(\"parsed\", from_xml(col(\"payload\"), schema)) parsed.printSchema() parsed.show()  \nimport org.apache.spark.sql.functions.{from_xml,schema_of_xml,lit} val xmlData = \"\"\" <book id=\"bk103\"> <author>Corets, Eva</author> <title>Maeve Ascendant</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-11-17</publish_date> </book>\"\"\".stripMargin val df = Seq((8, xmlData)).toDF(\"number\", \"payload\") val schema = schema_of_xml(xmlData) val parsed = df.withColumn(\"parsed\", from_xml($\"payload\", schema)) parsed.printSchema() parsed.show()  \nfrom_xml and schema_of_xml with SQL API"
    },
    {
        "id": 1132,
        "url": "https://docs.databricks.com/en/query/formats/xml.html",
        "content": "from_xml and schema_of_xml with SQL API  \nSELECT from_xml(' <book id=\"bk103\"> <author>Corets, Eva</author> <title>Maeve Ascendant</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-11-17</publish_date> </book>', schema_of_xml(' <book id=\"bk103\"> <author>Corets, Eva</author> <title>Maeve Ascendant</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-11-17</publish_date> </book>') );  \nLoad XML with Auto Loader  \nquery = (spark.readStream .format(\"cloudFiles\") .option(\"cloudFiles.format\", \"xml\") .option(\"rowTag\", \"book\") .option(\"cloudFiles.inferColumnTypes\", True) .option(\"cloudFiles.schemaLocation\", schemaPath) .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\") .load(inputPath) .writeStream .option(\"mergeSchema\", \"true\") .option(\"checkpointLocation\", checkPointPath) .trigger(availableNow=True) .toTable(\"table_name\") )  \nval query = spark.readStream .format(\"cloudFiles\") .option(\"cloudFiles.format\", \"xml\") .option(\"rowTag\", \"book\") .option(\"cloudFiles.inferColumnTypes\", true) .option(\"cloudFiles.schemaLocation\", schemaPath) .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\") .load(inputPath) .writeStream .option(\"mergeSchema\", \"true\") .option(\"checkpointLocation\", checkPointPath) .trigger(Trigger.AvailableNow() .toTable(\"table_name\") )"
    },
    {
        "id": 1133,
        "url": "https://docs.databricks.com/en/security/network/front-end/front-end-private-connect.html",
        "content": "Configure private connectivity to Databricks  \nThis article describes private connectivity between users and their Databricks workspaces. For information on how to configure private connectivity from the control plane to the classic compute plane, see Classic compute plane networking.  \nPrivate connectivity to Databricks overview"
    },
    {
        "id": 1134,
        "url": "https://docs.databricks.com/en/security/network/front-end/front-end-private-connect.html",
        "content": "Private connectivity to Databricks overview\nAWS PrivateLink provides private connectivity from AWS VPCs and on-premises networks to AWS services without exposing the traffic to the public network. Databricks supports using PrivateLink to allow users and applications to connect to Databricks over a VPC interface endpoint. This connection is supported when connecting to the web application, REST API, and the Databricks Connect API.  \nYou can optionally mandate private connectivity for the workspace, which means Databricks rejects any connections over the public network. You must configure private connectivity from users to Databricks and from the control plane to the compute plane in order to mandate private connectivity for a worksapce.  \nYou can enable PrivateLink while creating a workspace or on an existing workspace. To enable private connectivity to Databricks, see Enable private connectivity using AWS PrivateLink."
    },
    {
        "id": 1135,
        "url": "https://docs.databricks.com/en/release-notes/index.html",
        "content": "Databricks release notes  \nDatabricks release notes are organized by release vehicle:  \nDatabricks platform release notes cover the features that developed for the Databricks platform.  \nDatabricks Runtime release notes versions and compatibility cover the features that developed for Databricks Runtime. This includes proprietary features and optimizations. A Databricks Runtime version includes the set of core components that run on the clusters managed by Databricks. Each new verion provides updates that substantially improve the usability, performance, and security of big data analytics.  \nDatabricks SQL release notes cover the features that developed for the Databricks SQL user interface and SQL warehouses. You can also find the available Databricks SQL versions, the rollout schedule, and features related to each version.  \nDatabricks developer tools and SDKs release notes cover the features for IDE extensions and plugins, command-line interfaces, SDKs, and SQL connectors and drivers.  \nDatabricks Connect release notes cover the features and fixes for Databricks Connect.  \nDelta Live Tables release notes cover the features, fixes, and runtime upgrade process for Delta Live Tables.  \nServerless compute release notes cover the features released on serverless compute for notebooks and jobs. You can also find the current serverless compute version, upcoming behavioral changes, and limitations related to serverless compute.  \nLearn about the kinds of preview releases and how Databricks supports them."
    },
    {
        "id": 1136,
        "url": "https://docs.databricks.com/en/release-notes/product/2023/december.html",
        "content": "December 2023  \nThese features and Databricks platform improvements were released in December 2023.  \nNote  \nReleases are staged. Your Databricks workspace might not be updated until a week or more after the initial release date.  \nShare dynamic views using Delta Sharing (Public Preview)\nShare dynamic views using Delta Sharing (Public Preview)\nDecember 21, 2023  \nYou can now use Delta Sharing to share dynamic views that restrict access to certain table data based on recipient properties. Dynamic view sharing requires the Databricks-to-Databricks sharing flow. See Add dynamic views to a share to filter rows and columns.\n\nShare volumes using Delta Sharing (Public Preview)"
    },
    {
        "id": 1137,
        "url": "https://docs.databricks.com/en/release-notes/product/2023/december.html",
        "content": "Share volumes using Delta Sharing (Public Preview)\nDecember 20, 2023  \nYou can now use Delta Sharing to share volumes between Databricks workspaces on different Unity Catalog metastores (including workspaces on different Databricks accounts and different clouds). Volumes are Unity Catalog objects that represent a logical volume of storage in a cloud object storage location. They are intended primarily to provide governance over non-tabular data assets. Delta Sharing on Databricks provides a native integration with Unity Catalog that allows you to manage, govern, audit, and track usage of shared volumes data.  \nSee Add volumes to a share.\n\nEntity Relationship Diagram for primary keys and foreign keys\nEntity Relationship Diagram for primary keys and foreign keys\nDecember 18, 2023  \nIn Catalog Explorer, you can now view primary key and foreign key relationships as a graph in the Entity Relationship Diagram (ERD). For more information, see View the Entity Relationship Diagram.\n\nUnity Catalog volume file upload size limit increase\nUnity Catalog volume file upload size limit increase\nDecember 13, 2023  \nThe Unity Catalog volume file upload size limit has increased to 5 GB. See Upload files to a Unity Catalog volume.\n\nNew notebook cell results rendering available (Public Preview)"
    },
    {
        "id": 1138,
        "url": "https://docs.databricks.com/en/release-notes/product/2023/december.html",
        "content": "New notebook cell results rendering available (Public Preview)\nDecember 8, 2023  \nYou can now select a new cell result table rendering that allows searching results, copying subsets of the results to the clipboard, and other features to make it easier to work with tabular outputs. For details, see Results table.\n\nNotebook editor themes available\nNotebook editor themes available\nDecember 8, 2023  \nYou can now select from a variety of themes in the notebook editor. To do so, in the notebook, select View > Editor theme.\n\nExternal models support in Model Serving is in Public Preview\nExternal models support in Model Serving is in Public Preview\nDecember 11, 2023  \nWith external models support in Mosaic AI Model Serving, you can now access, manage, and govern third party hosted models, referred to as external models. This support allows you to add endpoints for accessing models hosted outside of Databricks, such as Azure OpenAI GPT models, Anthropic Claude Models, or Amazon Bedrock Models. Once configured, you can grant teams and applications access to these models, enabling them to query via a standard interface without exposing credentials.  \nSee External models in Mosaic AI Model Serving.\n\nDatabricks Online Tables is Public Preview"
    },
    {
        "id": 1139,
        "url": "https://docs.databricks.com/en/release-notes/product/2023/december.html",
        "content": "Databricks Online Tables is Public Preview\nDecember 8, 2023  \nDatabricks Online Tables are fully serverless tables that auto-scale with the request load and provide low latency and high throughput access to data of any scale. Online Tables are designed to work with Mosaic AI Model Serving, Feature & Function Serving, and retrieval-augmented generation (RAG) applications where they are used for fast data lookups. For details, see Use online tables for real-time feature serving.\n\nRepos & Git Integration Settings UI now correctly notes support for GitHub Enterprise Server\nRepos & Git Integration Settings UI now correctly notes support for GitHub Enterprise Server\nDecember 7, 2023  \nThe Databricks Repos and Git Integration settings provider (Settings > Linked accounts > Git integration) was updated to specify a dropdown option for Git provider \u201cGitHub Enterprise Server\u201d, which was previously \u201cGitHub Enterprise\u201d.\n\nDatabricks JDBC driver 2.6.36"
    },
    {
        "id": 1140,
        "url": "https://docs.databricks.com/en/release-notes/product/2023/december.html",
        "content": "Databricks JDBC driver 2.6.36\nDecember 6, 2023  \nWe have released version 2.6.36 of the Databricks JDBC driver (download). This release adds native OAuth support for interactive flow mode (user-to-machine or U2M) and client credential flow mode (machine-to-machine or M2M). For more information, see Databricks JDBC Driver.  \nThe driver can launch a browser from the desktop to initiate the browser-based authentication flow.  \nThe driver can allow the client to set the OAuth client ID and client secrets to enable OAuth for apps and services.\n\nSupport for referencing workspace files from init scripts\nSupport for referencing workspace files from init scripts\nDecember 6, 2023  \nIn Databricks Runtime 11.3 LTS and above, you can now reference other workspace files such as libraries, configuration files, or shell scripts from init scripts stored with workspace files. See What are init scripts?.\n\nFeature & Function Serving is Public Preview"
    },
    {
        "id": 1141,
        "url": "https://docs.databricks.com/en/release-notes/product/2023/december.html",
        "content": "Feature & Function Serving is Public Preview\nDecember 5, 2023  \nWith Databricks Feature & Function Serving, data in the Databricks platform can be made available to models or applications deployed outside of Databricks. Like Mosaic AI Model Serving endpoints, Feature & Function Serving endpoints automatically scale to adjust to real-time traffic and provide a high-availability, low-latency service at any scale. For details, see What is Databricks Feature Serving?.  \nYou can use Databricks Feature & Function Serving to serve structured data for retrieval augmented generation (RAG) applications. For an example notebook, see online tables with RAG applications.\n\nFoundation Model APIs is Public Preview\nFoundation Model APIs is Public Preview\nDecember 4, 2023  \nYou can access and query state-of-the-art open source models from a dedicated model serving endpoint. With Foundation Model APIs, you can quickly and easily build applications that leverage a high-quality generative AI model without maintaining your own model deployment. The Foundation Model APIs are provided on a pay-per-token basis and the supported models are accessible in your Databricks Workspace. See Databricks Foundation Model APIs.\n\nNew unified admin settings UI"
    },
    {
        "id": 1142,
        "url": "https://docs.databricks.com/en/release-notes/product/2023/december.html",
        "content": "New unified admin settings UI\nDecember 4, 2023  \nThe workspace admin settings UI has been updated to incorporate previously separate SQL settings. The new UI also allows for easy navigation between workspace setting and user settings.\n\nInit scripts on DBFS are end-of-life\nInit scripts on DBFS are end-of-life\nDecember 1, 2023  \nInit scripts on DBFS are now end-of-life. Support has been extended in some workspaces for legacy workloads, but will be removed in the future. All init scripts stored in DBFS should be migrated. See Recommendations for init scripts.\n\nLegacy global and cluster-named init scripts are end-of-life\nLegacy global and cluster-named init scripts are end-of-life\nDecember 1, 2023  \nLegacy global and cluster-named init scripts are now end-of-life. Support has been extended in some workspaces for legacy workloads, but will be removed in the future. All legacy global and cluster-named init scripts should be migrated. See Recommendations for init scripts."
    },
    {
        "id": 1143,
        "url": "https://docs.databricks.com/en/release-notes/product/2019/january.html",
        "content": "January 2019  \nThese features and Databricks platform improvements were released in January 2019.  \nNote  \nReleases are staged. Your Databricks account may not be updated until up to a week after the initial release date.  \nUpcoming change: Python 3 to become the default when you create clusters\nUpcoming change: Python 3 to become the default when you create clusters\nJanuary 29, 2019  \nWhen Databricks platform version 2.91 releases in mid-February, the default Python version for new clusters will switch from Python 2 to Python 3. Existing clusters will not change their Python versions, of course. But if you\u2019ve been in the habit of taking the Python 2 default when you create new clusters, you\u2019ll need to start paying attention to your Python version selection.\n\nDatabricks Runtime 5.2 for Machine Learning (Beta) release"
    },
    {
        "id": 1144,
        "url": "https://docs.databricks.com/en/release-notes/product/2019/january.html",
        "content": "Databricks Runtime 5.2 for Machine Learning (Beta) release\nJanuary 24, 2019  \nDatabricks Runtime 5.2 ML is built on top of Databricks Runtime 5.2 (EoS). It contains many popular machine learning libraries, including TensorFlow, PyTorch, Keras, and XGBoost, and provides distributed TensorFlow training using Horovod. In addition to library updates since Databricks Runtime ML 5.1, Databricks Runtime 5.2 ML includes the following new features:  \nGraphFrames now supports the Pregel API (Python) with Databricks\u2019s performance optimizations.  \nHorovodRunner adds:  \nOn a GPU cluster, training processes are mapped to GPUs instead of worker nodes to simplify the support of multi-GPU instance types. This built-in support allows you to distribute to all of the GPUs on a multi-GPU machine without custom code.  \nHorovodRunner.run() now returns the return value from the first training process.  \nSee the complete release notes for Databricks Runtime 5.2 ML. d ## Databricks Runtime 5.2 release  \nJanuary 24, 2019  \nDatabricks Runtime 5.2 is now available. Databricks Runtime 5.2 includes Apache Spark 2.4.0, new Delta Lake and Structured Streaming features and upgrades, and upgraded Python, R, Java, and Scala libraries. For details, see Databricks Runtime 5.2 (EoS)."
    },
    {
        "id": 1145,
        "url": "https://docs.databricks.com/en/release-notes/product/2019/january.html",
        "content": "Cluster configuration JSON view\nCluster configuration JSON view\nJanuary 15-22, 2019  \nThe cluster configuration page now supports a JSON view:  \nThe JSON view is read-only. However, you can copy the JSON and use it to create and update clusters with the Clusters API.\n\nLibrary UI\nLibrary UI\nJanuary 2-9, 2019: Version 2.88  \nThe library UI improvements that were originally released in November 2018 and reverted shortly thereafter have been re-released. These updates make it easier to upload, install, and manage libraries for your Databricks clusters.  \nThe Databricks UI now supports both workspace libraries and cluster-installed libraries. A workspace library exists in the Workspace and can be installed on one or more clusters. A cluster-installed library is a library that exists only in the context of the cluster that it is installed on. In addition:  \nYou can now create a library from a file uploaded to object storage.  \nYou can now install and uninstall libraries from the library details page and a cluster\u2019s Libraries tab.  \nLibraries installed using the API now display in a cluster\u2019s Libraries tab.  \nFor details, see Libraries.\n\nCluster Events"
    },
    {
        "id": 1146,
        "url": "https://docs.databricks.com/en/release-notes/product/2019/january.html",
        "content": "Cluster Events\nJanuary 2-9, 2019: Version 2.88  \nNew cluster events were added to reflect Spark driver status. For details, see Clusters API.\n\nCluster UI\nCluster UI\nJanuary 2-9, 2019: Version 2.88  \nNote  \nThese updates will be rolled out to some Databricks customers in version 2.88 and the remainder in 2.89, which will be released during the third week of January.  \nThe cluster creation page has been cleaned up and reorganized for ease of use, including a new Advanced Options toggle."
    },
    {
        "id": 1147,
        "url": "https://docs.databricks.com/en/release-notes/product/2022/may.html",
        "content": "May 2022  \nThese features and Databricks platform improvements were released in May 2022.  \nNote  \nReleases are staged. Your Databricks account may not be updated until a week or more after the initial release date.  \nCopy and paste notebook cells between tabs and windows\nCopy and paste notebook cells between tabs and windows\nMay 31 - June 6, 2022: Version 3.73  \nYou can now copy and paste cells between notebooks in different browser tabs or windows. See Cut, copy, and paste cells.\n\nAdditional data type support for Databricks Feature Store automatic feature lookup\nAdditional data type support for Databricks Feature Store automatic feature lookup\nMay 31 - June 6, 2022: Version 3.73  \nDatabricks Feature Store now supports additional data types for automatic feature lookup: DecimalType, ArrayType, MapType. See Automatic feature lookup with Databricks Model Serving.\n\nDatabricks Runtime 11.0 (Beta)"
    },
    {
        "id": 1148,
        "url": "https://docs.databricks.com/en/release-notes/product/2022/may.html",
        "content": "Databricks Runtime 11.0 (Beta)\nMay 24, 2022  \nDatabricks Runtime 11.0, 11.0 Photon, and 11.0 ML are now available as Beta releases.  \nSee the full release notes at Databricks Runtime 11.0 (EoS) and Databricks Runtime 11.0 for Machine Learning (EoS).\n\nImproved workspace search (Public Preview)\nImproved workspace search (Public Preview)\nMay 16-23, 2022: Version 3.72  \nYou can now search for notebooks, libraries, folders, files, and repos by name. You can also search for content within a notebook and see a preview of the matching content. Search results can be filtered by type. See Search for workspace objects.\n\nExplore SQL cell results in Python notebooks natively using Python"
    },
    {
        "id": 1149,
        "url": "https://docs.databricks.com/en/release-notes/product/2022/may.html",
        "content": "Explore SQL cell results in Python notebooks natively using Python\nMay 16-23, 2022: Version 3.72  \nIn Python notebooks, the results of SQL cells are now available as a PySpark DataFrame _sqldf for easy exploration in Python. See Explore SQL cell results in Python notebooks using Python.  \nNote  \nThis feature was delayed and will be rolled out over Databricks platform releases 3.74 through 3.76.\n\nDatabricks Repos: Support for more files in a repo\nDatabricks Repos: Support for more files in a repo\nMay 16-23, 2022: Version 3.72  \nDatabricks increased the number of files supported in a repo:  \nThe notebook limit: 5,000.  \nThe limit for the total of all files including notebooks: 10,000.  \nPreviously, you were limited to 5,000 total files in a repo.  \nFor details, see File and repo size limits.\n\nDatabricks Repos: Fix to issue with MLflow experiment data loss"
    },
    {
        "id": 1150,
        "url": "https://docs.databricks.com/en/release-notes/product/2022/may.html",
        "content": "Databricks Repos: Fix to issue with MLflow experiment data loss\nMay 16-23, 2022: Version 3.72  \nPreviously, in some situations, switching branches in Databricks Repos could cause the MLflow experiment associated with a notebook to be deleted. This issue has been fixed for most scenarios.\n\nUpgrade wizard makes it easier to copy databases and multiple tables to Unity Catalog (Public Preview)\nUpgrade wizard makes it easier to copy databases and multiple tables to Unity Catalog (Public Preview)\nMay 13, 2022  \nIf you use Unity Catalog for data governance, you can now use an upgrade wizard to copy complete schemas (databases) and multiple tables from your default Hive metastore to the Unity Catalog metastore. The wizard also streamlines permission assignment for the upgraded schemas and tables. Only upgrade of external tables is supported with this release. See Upgrade a schema or multiple tables from the Hive metastore to Unity Catalog external tables using the upgrade wizard.\n\nPower BI Desktop system-wide HTTP proxy support\nPower BI Desktop system-wide HTTP proxy support\nMay 11, 2022  \nThe Power BI connector now supports automatic detection of the system HTTP proxy configuration in Power BI Desktop. See Automated HTTP proxy detection."
    },
    {
        "id": 1151,
        "url": "https://docs.databricks.com/en/release-notes/product/2022/may.html",
        "content": "Streamline billing and account management by signing up for Databricks using AWS Marketplace\nStreamline billing and account management by signing up for Databricks using AWS Marketplace\nMay 10, 2022  \nYou can now use AWS Marketplace to sign up for Databricks. AWS users with the Purchaser role can manage your subscription. Charges appear on the AWS Billing & Cloud Management dashboard alongside your other AWS charges. After the free trial period, you are billed only for the resources you use. For more information, see Sign up through AWS Marketplace.\n\nDatabricks Runtime 10.5 and 10.5 ML are GA; 10.5 Photon is Public Preview\nDatabricks Runtime 10.5 and 10.5 ML are GA; 10.5 Photon is Public Preview\nMay 4, 2022  \nDatabricks Runtime 10.5 and Databricks Runtime 10.5 ML are now generally available. Databricks Runtime 10.5 Photon is in Public Preview.  \nSee Databricks Runtime 10.5 (EoS) and Databricks Runtime 10.5 for Machine Learning (EoS)."
    },
    {
        "id": 1152,
        "url": "https://docs.databricks.com/en/release-notes/product/2022/may.html",
        "content": "Authenticate to the account console using SAML 2.0 (Public Preview)\nAuthenticate to the account console using SAML 2.0 (Public Preview)\nMay 3, 2022  \nDatabricks now supports SSO authentication to the account console using SAML 2.0 as an alternative to OIDC. See Configure SSO in Databricks. This feature is available as a Public Preview.\n\nDatabricks JDBC driver 2.6.25\nDatabricks JDBC driver 2.6.25\nMay 3, 2022  \nWe have released the Databricks JDBC driver 2.6.25 (download). The driver is also available as an Apache Maven artifact. This release optimizes the default configuration used to connect to Databricks. See also Databricks ODBC Driver Downloads.\n\nSee the user a pipeline runs as in the Delta Live Tables UI"
    },
    {
        "id": 1153,
        "url": "https://docs.databricks.com/en/release-notes/product/2022/may.html",
        "content": "See the user a pipeline runs as in the Delta Live Tables UI\nMay 2-9, 2022: Version 3.71  \nThe Run as user value is added to the Pipeline details panel in the Delta Live Tables user interface. The Run as user is the pipeline owner, and pipeline updates run with this user\u2019s permissions."
    },
    {
        "id": 1154,
        "url": "https://docs.databricks.com/en/schemas/create-schema.html",
        "content": "Create schemas  \nThis article shows how to create schemas in Unity Catalog and the legacy Hive metastore.  \nTo learn about schemas in Databricks, including a comparison of schema behavior in Unity Catalog and Hive metastore, see What are schemas in Databricks?.  \nBefore you begin"
    },
    {
        "id": 1155,
        "url": "https://docs.databricks.com/en/schemas/create-schema.html",
        "content": "Before you begin\nTo create a schema in Unity Catalog:  \nYou must have a Unity Catalog metastore linked to the workspace where you perform the schema creation.  \nYou must have the USE CATALOG and CREATE SCHEMA data permissions on the schema\u2019s parent catalog. Either a metastore admin or the owner of the catalog can grant you these privileges. If you are a metastore admin, you can grant these privileges to yourself.  \nTo specify an optional managed storage location for the tables and volumes in the schema, an external location must be defined in Unity Catalog, and you must have the CREATE MANAGED STORAGE privilege on the external location. See Specify a managed storage location in Unity Catalog.  \nThe cluster that you use to run a notebook to create a schema must use a Unity Catalog-compliant access mode. See Access modes. SQL warehouses always support Unity Catalog.  \nTo create a schema in Hive metastore:  \nPermissions required depend on whether you are using table access control. See Hive metastore privileges and securable objects (legacy).\n\nCreate a schema"
    },
    {
        "id": 1156,
        "url": "https://docs.databricks.com/en/schemas/create-schema.html",
        "content": "Create a schema\nTo create a schema in Unity Catalog, you can use Catalog Explorer or SQL commands. To create a schema in Hive metastore, you must use SQL commands.  \nLog in to a workspace that is linked to the Unity Catalog metastore.  \nClick Catalog.  \nIn the Catalog pane on the left, click the catalog you want to create the schema in.  \nIn the detail pane, click Create schema.  \nGive the schema a name and add any comment that would help users understand the purpose of the schema.  \n(Optional) Specify a managed storage location. Requires the CREATE MANAGED STORAGE privilege on the target external location. See Specify a managed storage location in Unity Catalog and Managed locations for schemas.  \nClick Create.  \nGrant privileges on the schema. See Manage privileges in Unity Catalog.  \nClick Save.  \nRun the following SQL commands in a notebook or the SQL query editor. Items in brackets are optional. You can use either SCHEMA or DATABASE. Replace the placeholder values:  \n<catalog-name>: The name of the parent catalog for the schema. If you are creating a schema in Hive metastore and you are in a Unity Catalog-enabled workspace, use hive_metastore as the catalog name. If your workspace is not enabled for Unity Catalog, don\u2019t specify a catalog at all.  \n<schema-name>: A name for the schema.  \n<location-path>: Optional path to a managed storage location. Use with MANAGED LOCATION for Unity Catalog and with LOCATION for Hive metastore. In Unity Catalog, you must have the CREATE MANAGED STORAGE privilege on the external location for the path that you specify. See Specify a managed storage location in Unity Catalog and Managed locations for schemas.  \n<comment>: Optional description or other comment.  \n<property-key> = <property-value> [ , ... ]: Optional. Spark SQL properties and values to set for the schema.  \nFor more detailed parameter descriptions, see CREATE SCHEMA.  \nCREATE { DATABASE | SCHEMA } [ IF NOT EXISTS ] <catalog-name>.<schema-name> [ MANAGED LOCATION '<location-path>' | LOCATION '<location-path>'] [ COMMENT <comment> ] [ WITH DBPROPERTIES ( <property-key = property_value [ , ... ]> ) ];"
    },
    {
        "id": 1157,
        "url": "https://docs.databricks.com/en/schemas/create-schema.html",
        "content": "Grant privileges on the schema. For Unity Catalog privileges, see Manage privileges in Unity Catalog.  \nYou can also create a schema by using the Databricks Terraform provider and databricks_schema. You can retrieve a list of schema IDs by using databricks_schemas"
    },
    {
        "id": 1158,
        "url": "https://docs.databricks.com/en/schemas/create-schema.html",
        "content": "Next steps\nNext steps\nTo learn how to add tables, views, and volumes to your schema, see What is a table?, What is a view?, and What are Unity Catalog volumes?.  \nTo learn how to add AI models to your schema, see Manage model lifecycle in Unity Catalog.  \nTo learn how to view, update, and drop existing schemas, see Manage schemas."
    },
    {
        "id": 1159,
        "url": "https://docs.databricks.com/en/security/privacy/hipaa.html",
        "content": "HIPAA compliance features  \nPreview  \nThe ability for admins to add Enhanced Security and Compliance features is a feature in Public Preview. The compliance security profile and support for compliance standards are generally available (GA).  \nHIPAA compliance features requires enabling the compliance security profile, which adds monitoring agents, enforces instance types for inter-node encryption, provides a hardened compute image, and other features. For technical details, see Compliance security profile. It is your responsibility to confirm that each workspace has the compliance security profile enabled.  \nTo use the compliance security profile, your Databricks account must include the Enhanced Security and Compliance add-on. For details, see the pricing page.  \nThis feature requires your workspace to be on the Enterprise pricing tier.  \nEnsure that sensitive information is never entered in customer-defined input fields, such as workspace names, cluster names, and job names.  \nWhich compute resources get enhanced security\nWhich compute resources get enhanced security\nThe compliance security profile enhancements apply to compute resources in the classic compute plane in all regions.  \nServerless SQL warehouse support for the compliance security profile varies by region. See Serverless SQL warehouses support the compliance security profile in some regions."
    },
    {
        "id": 1160,
        "url": "https://docs.databricks.com/en/security/privacy/hipaa.html",
        "content": "HIPAA overview\nHIPAA overview\nThe Health Insurance Portability and Accountability Act of 1996 (HIPAA), the Health Information Technology for Economic and Clinical Health (HITECH), and the regulations issued under HIPAA are a set of US healthcare laws. Among other provisions, these laws establish requirements for the use, disclosure, and safeguarding of protected health information (PHI).  \nHIPAA applies to covered entities and business associates that create, receive, maintain, transmit, or access PHI. When a covered entity or business associate engages the services of a cloud service provider (CSP), such as Databricks, the CSP becomes a business associate under HIPAA.  \nHIPAA regulations require that covered entities and their business associates enter into a contract called a Business Associate Agreement (BAA) to ensure the business associates will protect PHI adequately. Among other things, a BAA establishes the permitted and required uses and disclosures of PHI by the business associate, based on the relationship between the parties and the activities and services being performed by the business associate.\n\nDoes Databricks permit the processing of PHI data on Databricks?"
    },
    {
        "id": 1161,
        "url": "https://docs.databricks.com/en/security/privacy/hipaa.html",
        "content": "Does Databricks permit the processing of PHI data on Databricks?\nYes, if you enable the compliance security profile and add the HIPAA compliance standard as part of the compliance security profile configuration. Contact your Databricks account team for more information. It is your responsibility before you process PHI data to have a BAA agreement with Databricks.\n\nEnable HIPAA on a workspace"
    },
    {
        "id": 1162,
        "url": "https://docs.databricks.com/en/security/privacy/hipaa.html",
        "content": "Enable HIPAA on a workspace\nThis section assumes you are on the E2 version of the Databricks platform.  \nIf you are an existing HIPAA customer and your account is not yet on the E2 version of the Databricks platform,  \nNote that the E2 platform is a multi-tenant platform and your choice to deploy HIPAA on E2 will be treated as a waiver of any provision in your contract that would be in conflict with our ability to provide you HIPAA on the E2 platform.  \nTo configure your workspace to support processing of data regulated by the HIPAA compliance standard, the workspace must have the compliance security profile enabled. You can enable it and add the HIPAA compliance standard across all workspaces or only on some workspaces.  \nTo enable the compliance security profile and add the HIPAA compliance standard for an existing workspace, see Enable enhanced security and compliance features on an existing workspace.  \nTo set an account-level setting to enable the compliance security profile and HIPAA for new workspaces, see Set account-level defaults for all new workspaces.  \nImportant  \nYou are wholly responsible for ensuring your own compliance with all applicable laws and regulations. Information provided in Databricks online documentation does not constitute legal advice, and you should consult your legal advisor for any questions regarding regulatory compliance.  \nDatabricks does not support the use of preview features for the processing of PHI on the HIPAA on E2 platform, with the exception of the features listed in Preview features that are supported for processing of PHI data."
    },
    {
        "id": 1163,
        "url": "https://docs.databricks.com/en/security/privacy/hipaa.html",
        "content": "Preview features that are supported for processing of PHI data\nPreview features that are supported for processing of PHI data\nThe following preview features are supported for processing of PHI:  \nSCIM provisioning  \nIAM passthrough  \nSecret paths in environment variables  \nSystem tables  \nFiltering sensitive table data with row filters and column masks  \nLakehouse Federation to Redshift  \nUnity Catalog-enabled DLT pipelines  \nScala support for shared clusters  \nDelta Live Tables Hive metastore to Unity Catalog clone API  \nDelta Live Tables with serverless compute  \nAI/BI Genie\n\nShared responsibility of HIPAA compliance"
    },
    {
        "id": 1164,
        "url": "https://docs.databricks.com/en/security/privacy/hipaa.html",
        "content": "Shared responsibility of HIPAA compliance\nComplying with HIPAA has three major areas, with different responsibilities. While each party has numerous responsibilities, below we enumerate key responsibilities of ours, along with your responsibilities.  \nThis article use the Databricks terminology control plane and a compute plane, which are two main parts of how Databricks works:  \nThe Databricks control plane includes the backend services that Databricks manages in its own AWS account.  \nThe compute plane is where your data lake is processed. The classic compute plane includes a VPC in your AWS account, and clusters of compute resources to process your notebooks, jobs, and pro or classic SQL warehouses.  \nKey responsibilities of AWS include:  \nPerform its obligations as a business associate under your BAA with AWS.  \nProvide you the EC2 machines under your contract with AWS that support HIPAA compliance.  \nProvide hardware-accelerated encryption at rest and in-transit encryption within the AWS Nitro Instances that is adequate under HIPAA.  \nDelete encryption keys and data when Databricks releases the EC2 instances.  \nKey responsibilities of Databricks include:  \nEncrypt in-transit PHI data that is transmitted to or from the control plane.  \nEncrypt PHI data at rest in the control plane  \nLimit the set of instance types to the AWS Nitro instance types that enforce in-transit encryption and encryption at rest. For the list of supported instance types, see AWS Nitro System and HIPAA compliance features. Databricks limits the instance types both in the account console and through the API.  \nDeprovision EC2 instances when you indicate in Databricks that they are to be deprovisioned, for example auto-termination or manual termination, so that AWS can wipe them.  \nKey responsibilities of yours:  \nConfigure your workspace to use either customer-managed keys for managed services or the Store interactive notebook results in customer account feature.  \nDo not use preview features within Databricks to process PHI other than features listed in Preview features that are supported for processing of PHI data  \nFollow security best practices, such as disable unnecessary egress from the compute plane and use the Databricks secrets feature (or other similar functionality) to store access keys that provide access to PHI.  \nEnter into a business associate agreement with AWS to cover all data processed within the VPC where the EC2 instances are deployed."
    },
    {
        "id": 1165,
        "url": "https://docs.databricks.com/en/security/privacy/hipaa.html",
        "content": "Enter into a business associate agreement with AWS to cover all data processed within the VPC where the EC2 instances are deployed.  \nDo not do something within a virtual machine that would be a violation of HIPAA. For example, direct Databricks to send unencrypted PHI to an endpoint.  \nEnsure that all data that may contain PHI is encrypted at rest when you store it in locations that the Databricks platform may interact with. This includes setting the encryption settings on each workspace\u2019s root S3 bucket that is part of workspace creation. You are responsible for ensuring the encryption (as well as performing backups) for this storage and all other data sources.  \nEnsure that all data that may contain PHI is encrypted in transit between Databricks and any of your data storage locations or external locations you access from a compute plane machine. For example, any APIs that you use in a notebook that might connect to external data source must use appropriate encryption on any outgoing connections.  \nEnsure that all data that may contain PHI is encrypted at rest when you store it in locations that the Databricks platform may interact with. This includes setting the encryption settings on each workspace\u2019s root storage that is part of workspace creation.  \nEnsure the encryption (as well as performing backups) for your root S3 bucket and all other data sources.  \nEnsure that all data that may contain PHI is encrypted in transit between Databricks and any of your data storage locations or external locations you access from a compute plane machine. For example, any APIs that you use in a notebook that might connect to external data source must use appropriate encryption on any outgoing connections.  \nNote the following about customer-managed keys:  \nYou can add customer-managed keys for your workspace\u2019s root S3 bucket using the customer-managed keys for workspace storage feature, but Databricks does not require you to do so.  \nAs an optional part of the customer-managed keys for workspace storage feature, you can add customer-managed keys for EBS volumes, but this is not necessary for HIPAA compliance.  \nNote  \nIf you are an existing HIPAA customer and your workspace is not on the E2 version of the Databricks platform, to create a cluster, see the legacy article Create and verify a cluster for legacy HIPAA support."
    },
    {
        "id": 1166,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Databricks platform release notes  \nAugust 2024 July 2024 June 2024 May 2024 April 2024 March 2024 February 2024 January 2024 December 2023 November 2023 October 2023 September 2023 August 2023 July 2023 June 2023 May 2023 April 2023 March 2023 February 2023 January 2023 December 2022 November 2022 October 2022 September 2022 August 2022 July 2022 June 2022 May 2022 April 2022 March 2022 February 2022 January 2022 December 2021 November 2021 October 2021 September 2021 August 2021 July 2021 June 2021 May 2021 April 2021 March 2021 February 2021 January 2021 December 2020 November 2020 October 2020 September 2020 August 2020 July 2020 June 2020 May 2020 April 2020 March 2020 February 2020 January 2020 December 2019 November 2019 October 2019 September 2019 August 2019 July 2019 June 2019 May 2019 April 2019 March 2019 February 2019 January 2019 December 2018 November 2018 October 2018 September 2018 August 2018 July 2018 June 2018 May 2018 April 2018 March 2018 February 2018 January 2018"
    },
    {
        "id": 1167,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Databricks on AWS GovCloud is now GA Delta Sharing: More Delta Lake features now supported by the Python and Power BI connectors Delta Sharing adds support for TimestampNTZ The Databricks Jobs For each task is GA Databricks Runtime 15.4 LTS is GA Configure your workspace\u2019s default access mode for jobs compute New slash commands for Databricks Assistant Workspace search now supports volumes Databricks JDBC driver 2.6.40 Databricks personal access tokens revoked if unused after 90 days Serverless SQL warehouses with the compliance security profile is now GA Clusters API now supports partial configuration updates Wrap lines in notebook cells Specify columns to sync for Mosaic AI Vector Search Files can no longer have identical names in workspace folders Compute policy enforcement now available Collaborate on data projects securely and privately using Databricks Clean Rooms (Public Preview) Mosaic AI Vector Search is now HIPAA-compliant Row filters and column masks are now GA, with improvements Canadian Centre for Cybersecurity (CCCS) Medium (Protected B) compliance controls Lakehouse Federation is generally available (GA) Meta Llama 3.1 405B models supported in Mosaic AI Model Training Create a new workspace with enhanced security and compliance features"
    },
    {
        "id": 1168,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Embed and drag & drop images in notebooks Command palette available in notebooks Workflow system schema renamed to lakeflow LakeFlow Connect (gated Public Preview) Support for Cloudflare R2 storage is GA Monitor Databricks Assistant activities with system tables (Public Preview) Sharing schemas using Delta Sharing is now GA Mosaic AI Agent Framework is available in eu-central-1 Databricks Assistant can diagnose issues with jobs (Public Preview) Updates to Databricks Git folders authentication and sharing behaviors Cluster library installation timeout Compute plane outbound IP addresses must be added to a workspace IP allow list Databricks Runtime 9.1 series support extended Single sign-on (SSO) is supported in Lakehouse Federation for SQL Server Enable cross-Geo processing Model sharing using Delta Sharing is now generally available Share comments and primary key constraints using Delta Sharing New Databricks JDBC Driver (OSS) Databricks Runtime 15.4 LTS (Beta) Scala is GA on Unity Catalog shared compute Single-user compute supports fine-grained access control, materialized views, and streaming tables Node timeline system table is now available (Public Preview) Meta Llama 3.1 is now supported in Model Serving Unity Catalog will soon drop support for storage credentials that use non-self-assuming IAM roles Notebooks: toggle more visible cell titles / in workspace asset names is deprecated Delta Sharing lets you share tables that use liquid clustering Query history system table is now available (Public Preview) Vulnerability scan reports are now emailed to admins Partition metadata logging for Unity Catalog external tables Serverless compute for workflows is GA Serverless compute for notebooks is GA Databricks Connect for Python now supports serverless compute Filter data outputs using natural language prompts Plaintext secrets support for external models Forecast time series data using ai_forecast() Lakehouse Federation supports Salesforce Data Cloud (Public Preview) Databricks Assistant system table now available (Public Preview) Account SCIM API v2.1 (Public Preview) Mosaic AI Model Training available to all us-west-2 customers (Public Preview) UK Cyber Essentials Plus compliance controls End of life for Databricks-managed passwords Sign-in with one-time passcodes and external accounts Resource quota increase for tables per Unity Catalog metastore Databricks Assistant can diagnose notebook errors automatically OAuth"
    },
    {
        "id": 1169,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Essentials Plus compliance controls End of life for Databricks-managed passwords Sign-in with one-time passcodes and external accounts Resource quota increase for tables per Unity Catalog metastore Databricks Assistant can diagnose notebook errors automatically OAuth in Databricks on AWS GovCloud"
    },
    {
        "id": 1170,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Notebooks - out of order syntax highlighting Notebooks Assistant automatically diagnoses errors Catalog Explorer improvements Serve multiple external models from a single model serving endpoint Databricks Assistant is now GA Databricks Assistant now gives titles to threads Serverless notebooks environment manager Notebooks debugging console Quickly add a periodic schedule to a Databricks workflow Emergency access for single sign-on GA Databricks Runtime 15.3 is GA Databricks Geos is GA New improved catalog creation UI The billing system tables are enabled by default Lakehouse Monitoring is GA Volume sharing using Delta Sharing is now generally available Create budgets to monitor account spending (Public Preview) Importable cost management dashboard added to account console (Public Preview) Unified login is now generally available Customer managed keys (CMK) for Databricks Vector Search are now GA Mosaic AI Agent Framework (Public Preview) Predictive optimization is now GA Route optimization is available for serving endpoints Column mapping is now GA Workflow system tables are now available (Public Preview) Function calling is Public Preview Query a vector search index using vector_search() Mosaic AI Vector Search now supports hybrid search GTE is now supported in Model Serving Route optimized serving endpoints enforce network restrictions Databricks Assistant: Threads & queries experience enabled by default New entry points for in-product help Improved notebook Markdown editor Quickly access Catalog Explorer tables from notebooks Databricks ODBC driver 2.8.2 The new Databricks Notebook UI is now generally available"
    },
    {
        "id": 1171,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Serverless firewall configuration now supports more compute types Databricks Runtime 15.0 series support ends Databricks Runtime 15.3 (Beta) The compute metrics UI is now available on all Databricks Runtime versions Improved search and filtering in notebook and SQL editor results tables New dashboard helps Databricks Marketplace providers monitor listing usage View system-generated federated queries in Query Profile Compute plane outbound IP addresses must be added to a workspace IP allow list OAuth is supported in Lakehouse Federation for Snowflake Bulk move and delete workspace objects from the workspace browser Unity Catalog objects are available in recents and favorites New dbt-databricks connector 1.8.0 adopts decoupled dbt architecture New compliance and security settings APIs (Public Preview) Databricks Runtime 15.2 is GA New Tableau connector for Delta Sharing New deep learning recommendation model examples Bind storage credentials and external locations to specific workspaces (Public Preview) Git folders are GA Pre-trained models in Unity Catalog (Public Preview) Mosaic AI Vector Search is GA The compliance security profile now supports AWS Graviton instance types Databricks Assistant autocomplete (Public Preview) Meta Llama 3 support in Foundation Model Training New changes to Git folder UI Compute now uses EBS GP3 volumes for autoscaling local storage Unified Login now supported with AWS PrivateLink (Private Preview) Foundation Model Training (Public Preview) Allow users to copy data to the clipboard from results table Attribute tag values for Unity Catalog objects can now be 1000 characters long (Public Preview) New Previews page New capabilities for Mosaic AI Vector Search Credential passthrough and Hive metastore table access controls are deprecated Databricks JDBC driver 2.6.38 Unified Login now supported with AWS PrivateLink (Public Preview) Databricks Runtime 15.2 (Beta) Notebooks now detect and auto-complete column names for Spark Connect DataFrames"
    },
    {
        "id": 1172,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Databricks Runtime 15.1 is GA Databricks Assistant: Threads & history Cancel pending serving endpoint updates in Model Serving Data lineage now captures reads on tables with column masks and row-level security Meta Llama 3 is supported in Model Serving for AWS Notebooks now automatically detect SQL New columns added to the billable usage system table (Public Preview) Delta Sharing supports tables that use column mapping (Public Preview) Get serving endpoint schemas (Public Preview) Creation and installation of workspace libraries is no longer available Jobs created through the UI are now queued by default Configuring access to resources from serving endpoints is GA Serverless compute for workflows is in public preview Lakehouse Federation supports foreign tables with case-sensitive identifiers Compute cloning now clones any libraries installed on the original compute Route optimization is available for serving endpoints Delta Live Tables notebook developer experience improvements (Public Preview) Databricks on AWS GovCloud (Public Preview)  \nDBRX Base and DBRX Instruct are now available in Model Serving Model Serving is HIPAA compliant in all regions Provisioned throughput in Foundation Model APIs is GA and HIPAA compliant MLflow now enforces quota limits for experiments and runs The Jobs UI is updated to better manage jobs deployed by Databricks Asset Bundles Google Cloud Vertex AI supported as model provider for external models Accessing resources from serving endpoints using instance profiles is GA Interactive notebook debugging Self-service sign-up for private exchange providers in Marketplace Databricks Runtime 15.0 is GA Databricks Repos changed to Git folders Databricks Runtime 14.1 and 14.2 series support extended Databricks ODBC driver 2.8.0 SQL warehouses for notebooks is GA Delegate the ability to view an object\u2019s metadata in Unity Catalog (Public Preview) Databricks Runtime 15.0 (Beta) Databricks Runtime 14.0 series support ends New computation for sys.path and CWD in Repos Feature Serving is GA Predictive optimization available in more regions"
    },
    {
        "id": 1173,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Use Delta Live Tables in Feature Engineering (Public Preview) Restrict creating a personal access token for a service principal Restrict changing a job owner and the run as setting Automatic cluster update is enabled if the compliance security profile is enabled (GA) Account admins can enable enhanced security and compliance features (Public Preview) Support for Cloudflare R2 storage to avoid cross-region egress fees (Public Preview) Notebooks for monitoring and managing Delta Sharing egress costs are now available Add data UI supports XML file format Support for cloud storage firewall from serverless compute (Public Preview) Use AI Functions to invoke a generative AI model from Foundation Model APIs Unity Catalog volumes are GA Full-page AI-powered search Run SQL notebook jobs on a SQL warehouse File arrival triggers in Databricks Jobs is GA Search for machine learning models in Unity Catalog using global workspace search Databricks Git server proxy is GA Databricks Git server proxy no longer requires CAN_ATTACH_TO permissions Workspace file support for the dbt and SQL file tasks is GA Databricks Connect is GA for Scala Create tables from files in volumes Databricks Runtime 14.3 LTS is GA Delta Sharing supports tables that use deletion vectors (Public Preview)  \nNative XML file format support (Public Preview) Share AI models using Databricks Marketplace (Public Preview) Workspace path update Streamlined creation of Databricks jobs Monitor GPU model serving workloads using inference tables URI path-based access to Unity Catalog external volumes Access controls lists can be enabled on upgraded workspaces Marketplace listing events system table now available (Public Preview) Updated UI for notebook cells (Public Preview) Quick Fix help with syntax errors in the notebook Databricks Runtime 14.3 LTS (Beta) Share AI models using Delta Sharing (Public Preview) Databricks Marketplace supports volume sharing Create widgets from the Databricks UI Warehouse events system table is now available (Public Preview) UI experience for OAuth app registration Reuse subnets across workspaces for customer-managed VPCs Workspace file size limit is now 500MB Feature removal notice for legacy Git integration in Databricks Databricks ODBC driver 2.7.7 Databricks Runtime 13.2 series support ends"
    },
    {
        "id": 1174,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Share dynamic views using Delta Sharing (Public Preview) Share volumes using Delta Sharing (Public Preview) Entity Relationship Diagram for primary keys and foreign keys Unity Catalog volume file upload size limit increase New notebook cell results rendering available (Public Preview) Notebook editor themes available External models support in Model Serving is in Public Preview Databricks Online Tables is Public Preview Repos & Git Integration Settings UI now correctly notes support for GitHub Enterprise Server Databricks JDBC driver 2.6.36 Support for referencing workspace files from init scripts Feature & Function Serving is Public Preview Foundation Model APIs is Public Preview New unified admin settings UI Init scripts on DBFS are end-of-life Legacy global and cluster-named init scripts are end-of-life  \nMosaic AI Vector Search is Public Preview IAM policies for storage credentials now require an external ID Access controls lists can no longer be disabled AI assistive features are enabled by default New behaviors and actions in Catalog Explorer for volumes Databricks Runtime 14.2 is GA Databricks SQL Connector for Python version 3.0.0 Libraries in workspace files supported on no-isolation shared clusters Deprecation of workspace libraries Delegate the ability to create a storage credential in Unity Catalog Search for Databricks Marketplace listings using global workspace search Consume data products in Databricks Marketplace using external platforms Automatic enablement of Unity Catalog for new workspaces Authentication using OAuth is GA Databricks Runtime 14.2 (beta) Databricks Marketplace includes Databricks Solution Accelerators Lakehouse Federation adds support for Google BigQuery"
    },
    {
        "id": 1175,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Published partner OAuth applications are easier to use by default View the YAML source for a Databricks job Add conditional logic to your Databricks workflows Configure parameters on a Databricks job that can be referenced by all job tasks Support for new GPU instance types Auto-enable deletion vectors Unity Catalog support for UNDROP TABLE is GA Partner Connect supports Dataiku Databricks AutoML generated notebooks are now saved as MLflow artifacts Predictive optimization (Public Preview) Compute system tables are now available (Public Preview) On-demand feature computation is GA Feature Engineering in Unity Catalog is GA AI-generated table comments (Public Preview) Compliance security profile works with serverless SQL warehouses in ap-southeast-2 region (Public Preview) Models in Unity Catalog is GA Libraries now supported in compute policies (Public Preview) Partner Connect supports Monte Carlo Semantic search (Public Preview) Enable Databricks Assistant at the workspace level IP access lists for the account console is GA New Photon defaults Databricks Runtime 14.1 is GA Developer tools release notes have moved Databricks extension for Visual Studio Code updated to version 1.1.5 Predictive I/O for updates is GA Deletion vectors are GA Automatic enablement of Unity Catalog for new workspaces Infosec Registered Assessors Program (IRAP) compliance controls Partner Connect supports RudderStack Databricks CLI updated to version 0.207.0 (Public Preview) Run selected cells in a notebook Use workspace-catalog binding to give read-only access to a catalog New in-product Help experience (Public Preview) Databricks extension for Visual Studio Code updated to version 1.1.4 Databricks SDK for Python updated to version 0.10.0 (Beta) Databricks SDK for Go updated to version 0.22.0 (Beta)"
    },
    {
        "id": 1176,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Lakehouse Federation available on single-user clusters Databricks Runtime 14.1 (Beta) GPU model serving optimized for LLMs in Public Preview Prevent runs of a job from being skipped when you reach concurrency limits Databricks Terraform provider updated to version 1.27.0 Databricks CLI updated to version 0.206.0 (Public Preview) Databricks SDK for Go updated to version 0.21.0 (Beta) Inference tables for model serving endpoints is Public Preview Databricks ODBC driver 2.7.5 Databricks extension for Visual Studio Code updated to version 1.1.3 Running jobs as a service principal is GA Databricks CLI updated to version 0.205.2 (Public Preview) Databricks CLI updated to version 0.205.1 (Public Preview) Databricks SDK for Go updated to version 0.20.0 (Beta) Databricks SDK for Python updated to version 0.9.0 (Beta) Databricks SDK for Python updated to version 0.7.1 (Beta) Databricks Terraform provider updated to version 1.26.0 Databricks Connect for Databricks Runtime 14.0 Configure Tableau and Power BI OAuth with SAML SSO Lakeview dashboards in Public Preview Fleet clusters now support Graviton instances Databricks Connect V2 is Public Preview for Scala GPU model serving in Public Preview On-demand feature computation now available in Unity Catalog (Public Preview) Unified login for accounts created before June 21, 2023 (Public Preview) Databricks Runtime 14.0 is GA Account email notifications upgrade Databricks Terraform provider updated to version 1.25.1 Databricks SDK for Go updated to version 0.19.2 (Beta) Databricks CLI updated to version 0.205.0 (Public Preview) Reserved location for /Workspace Databricks Asset Bundles (Public Preview) Databricks Terraform provider updated to version 1.25.0 Databricks extension for Visual Studio Code updated to version 1.1.2 Partner Connect supports Snowplow Databricks CLI updated to version 0.204.1 (Public Preview) Filter sensitive data with row filters and column masks (Public Preview) System tables now include Marketplace schema (Public Preview) Pricing system table is"
    },
    {
        "id": 1177,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Connect supports Snowplow Databricks CLI updated to version 0.204.1 (Public Preview) Filter sensitive data with row filters and column masks (Public Preview) System tables now include Marketplace schema (Public Preview) Pricing system table is now available (Public Preview) Databricks SDK for Go updated to version 0.19.1 (Beta) Databricks ODBC driver 2.7.3 Databricks CLI updated to version 0.204.0 (Public Preview) Data Explorer is now Catalog Explorer Databricks SDK for Go updated to version 0.19.0 (Beta) Databricks SDK for Python updated to version 0.8.0 (Beta) Delegate allowlist privileges in Unity Catalog GitHub apps integration in Repos is GA"
    },
    {
        "id": 1178,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Tables now appear in navigational search Databricks CLI updated to version 0.203.3 (Public Preview) Databricks JDBC driver 2.6.34 Databricks SDK for Go updated to version 0.18.0 (Beta) Databricks SDK for Python updated to version 0.7.0 (Beta) Databricks Terraform provider updated to version 1.24.1 Init scripts on DBFS end of life extended to Dec 1, 2023 Databricks Runtime 14.0 (Beta) Custom workspace tags Unified navigation experience is GA Allow additional ports from your classic compute plane to control plane by January 31, 2024 Databricks Terraform provider updated to version 1.24.0 Shared access mode clusters can now connect to public networks outside of Databricks VPC Databricks Runtime for Genomics setting removed from the workspace admin settings page Container Services setting removed from the workspace admin settings page Databricks CLI updated to version 0.203.2 (Public Preview) Go to definition for functions and variables in Python notebooks Databricks Runtime 13.3 LTS is GA Introducing tags with Unity Catalog Allowlist for init scripts, JARs, and Maven coordinates on Unity Catalog shared clusters is in Public Preview Volumes support for init scripts and JARs is in Public Preview Easier Databricks Repos .ipynb file output commits IPYNB notebook support in Databricks Repos is GA Databricks SDK for Go updated to version 0.17.0 (Beta) Databricks SDK for Python updated to version 0.6.0 (Beta) Databricks CLI updated to version 0.203.1 (Public Preview) Unified schema browser is now GA Databricks SDK for Go updated to version 0.16.0 (Beta) Partner Connect supports Census Databricks SDK for Python updated to version 0.5.0 (Beta) Programmatic write support for workspace files Access resources from serving endpoints with instance profiles (Public Preview) Databricks CLI updated to version 0.203.0 (Public Preview) Databricks Terraform provider updated to version 1.23.0 Groups can now be renamed Databricks SDK for Go updated to version 0.15.0 (Beta) Databricks SDK for"
    },
    {
        "id": 1179,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "(Public Preview) Databricks Terraform provider updated to version 1.23.0 Groups can now be renamed Databricks SDK for Go updated to version 0.15.0 (Beta) Databricks SDK for Python updated to version 0.4.0 (Beta) Third-party iFraming prevention configuration setting was removed Databricks extension for Visual Studio Code updated to version 1.1.1 LangChain available in 13.1 and above Feature Engineering in Unity Catalog is Public Preview Improved error handling for repeated continuous job failures Share schemas using Delta Sharing (Public Preview) Run tasks conditionally in your Databricks jobs Compliance security profile works with serverless SQL warehouses in some regions (Public Preview) Databricks Terraform provider updated to version 1.22.0 Lakehouse Monitoring is Public Preview Databricks Runtime 13.3 LTS (Beta) New Git operations are generally available: Merge branches, rebase and pull with conflict resolution"
    },
    {
        "id": 1180,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Email addresses in Databricks are now case insensitive Workspace admins can now create account groups Group manager role is in Public Preview Databricks CLI updated to version 0.202.0 (Public Preview) Databricks SDK for Python updated to version 0.3.0 (Beta) Databricks SDK for Go updated to version 0.14.1 (Beta) Databricks SDK for Go updated to version 0.14.0 (Beta) Run another job as a task in a Databricks job All users can access data products in Databricks Marketplace by default Classic keyboard shortcuts mode Lakehouse Federation lets you run queries against external database providers (Public Preview) Move to trash enabled for Repos Create alerts for slow-running or stuck jobs Databricks SDK for Go updated to version 0.13.0 (Beta) Databricks SDK for Python updated to version 0.2.0 (Beta) Databricks CLI updated to version 0.201.0 (Public Preview) Databricks SDK for Python updated to version 0.2.1 (Beta) Databricks Assistant is in Public Preview Deactivate users and service principals from your account Account-level SCIM provisioning now deactivates users when they are deactivated in the identity provider Trash directory admin access Prevention of MIME type sniffing and XSS attack page rendering are now always enabled Unity Catalog volumes are in Public Preview Simplified experience for submitting product feedback from the workspace Databricks extension for Visual Studio Code updated to version 1.1.0 Functions now displayed in Catalog Explorer (Public Preview) Databricks Terraform provider updated to version 1.21.0 The maximum offset for the List all jobs and List job runs API requests is now limited Databricks Runtime 13.2 is GA Delta Sharing and Databricks Marketplace support view sharing (Public Preview) Init scripts on DBFS reach end of life on Sept 1, 2023"
    },
    {
        "id": 1181,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Databricks Terraform provider updated to version 1.20.0 Databricks CLI updated to version 0.200.1 (Public Preview) Databricks SDK for Go updated to version 0.12.0 (Beta) Databricks SDK for Go updated to version 0.11.0 (Beta) Databricks SDK for Python (Beta) Databricks SDK for Go (Beta) Access audit log, billable usage, and lineage system tables (Public Preview) Models in Unity Catalog (Public Preview) Databricks Marketplace is now GA Databricks Runtime 13.2 (Beta) Databricks extension for Visual Studio Code (General Availability) See a visual overview of completed job runs in the Databricks Jobs UI Databricks CLI (Public Preview) Unified Login GA for new accounts Test single sign-on Improved pagination of results from List all jobs and List job runs API requests Full-page workspace browser includes Repos Databricks Terraform provider updated to version 1.19.0 Run jobs as a service principal (Public Preview) New service principal UI provides better management experience Home folders restored when users are re-added to workspaces Databricks Marketplace: Private exchanges are now available Databricks Marketplace: Consumers can uninstall data products using the UI Databricks Marketplace: providers can create their own profiles Databricks Connect V2 is GA for Python Databricks Terraform provider updated to version 1.18.0 New Databricks Marketplace providers Databricks Runtime 13.1 is GA Partner Connect supports Hunters View data from notebooks, SQL editor, and Catalog Explorer with the unified schema browser (Public Preview)"
    },
    {
        "id": 1182,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Create or modify table UI supports Avro, Parquet, and text file uploads Use the add data UI to load data using Unity Catalog external locations (Public Preview) Run Databricks notebooks on SQL warehouses (Public Preview) Prevent Enter key from accepting autocomplete suggestion Databricks Terraform provider updated to version 1.17.0 Upload data UI supports new column data types Configure your workspace to use IMDS v2 (GA) M7g and R7g Graviton3 instances are now supported on Databricks Bind Unity Catalog catalogs to specific workspaces All users can connect to Fivetran using Partner Connect (Public Preview) Authentication using OAuth tokens for service principals (Public Preview) Schedule automatic cluster update (Public Preview) Databricks Terraform provider updated to version 1.16.1 Databricks JDBC driver 2.6.33 Partner Connect supports Alation New default theme for editor Databricks Terraform provider updated to version 1.16.0 New region: Europe (Paris) Compliance security profile now supports more EC2 instance types Databricks Runtime 13.1 (Beta) Run file-based SQL queries in a Databricks workflow Databricks Terraform provider updated to version 1.15.0 Account nicknames now available in the account console Share notebooks using Delta Sharing Deprecation of cluster-scoped init scripts on DBFS New region: South America (S\u00e3o Paulo) Unified navigation (Public Preview) AWS fleet instance types now available"
    },
    {
        "id": 1183,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Databricks Marketplace (Public Preview): an open marketplace for data, analytics, and AI Configure the Python formatter Workspace files are GA Cluster-scoped init scripts can now be stored in workspace files New Delta Sharing privileges enable delegation of share, recipient, and provider management tasks Databricks Connect V2 (Public Preview) New cluster metrics UI Databricks Runtime 13.0 is GA Control access to the account console by IP address ranges (Public Preview) Databricks Terraform provider updated to version 1.14.3 Audit log entries for changed admin settings for workspaces and accounts Workspaces with security profile or ESM include audit logs rows for system and monitor logs Databricks Terraform provider updated to versions 1.14.1 and 1.14.2 Combined SQL user settings and general user settings Legacy notebook visualizations deprecated Create or modify table from file upload page supports JSON file uploads  \nDatabricks Terraform provider updated to version 1.14.0 Databricks Runtime 7.3 LTS ML support ends C7g Graviton 3 instances are now supported on Databricks Distributed training with TorchDistributor Databricks Runtime 13.0 (Beta) Improved file editor Databricks no longer creates a serverless starter SQL warehouse In SQL Warehouses API, enabling serverless compute now must be explicit Changes for workspace settings for serverless SQL warehouses Changes for serverless compute settings for accounts and workspaces Databricks SQL Serverless is GA .ipynb (Jupyter) notebook support in Repos (preview) Support for reload4j Execute SQL cells in the notebook in parallel Create job tasks using Python code stored in a Git repo Databricks Terraform provider updated to version 1.13.0 Databricks Terraform provider updated to version 1.12.0 SQL admin console and workspace admin console combined Model Serving is GA Automatic feature lookup is GA New Catalog Explorer availability View frequent queries and users of a table using the Insights tab Exact match search is available in global search View lineage information for your Databricks jobs Databricks Runtime 12.2 LTS and Databricks Runtime 12.2 LTS ML are GA Workspace files are now in Public Preview"
    },
    {
        "id": 1184,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "SAML single sign-on (SSO) in the account console is generally available Ray on Databricks (Public Preview) Notebook cell output results limit increased Databricks Jobs now supports running continuous jobs Trigger your Databricks job when new files arrive Databricks Terraform provider updated to version 1.10.0 Legacy global init scripts and cluster-named init scripts disabled Improvements to the MLflow experiment UI Databricks Runtime 12.2 (Beta) Databricks extension for Visual Studio Code (Public Preview) Serverless Real-Time Inference Public Preview now available to all customers Databricks Terraform provider updated to version 1.9.2 Variable explorer in Databricks notebooks  \nAuthenticate to Power BI and Tableau using OAuth Audit logs include entries for OAuth SSO authentication to the account console (Public Preview) Account SCIM is now GA Easier creation and editing of Databricks jobs in the UI Improvements to the Databricks Jobs UI when viewing job runs REST API Reference is now available for browsing API documentation New account console home screen provides better account management experience Databricks Terraform provider updated to version 1.9.1 Account users can update email preferences in the account console Region support consolidated onto one page Databricks Runtime 12.1 and Databricks Runtime 12.1 ML are GA Cluster policies now support limiting the max number of clusters per user Databricks Terraform provider updated to version 1.9.0 Partner Connect supports connecting to Privacera Databricks Terraform provider updated to version 1.8.0 Databricks Runtime 12.1 (Beta) Partner Connect supports Sigma New left and right sidebars in Databricks notebooks  \nDatabricks SQL Driver for Go is Generally Available Prevent concurrent workspace updates Databricks Terraform provider updated to version 1.7.0 Databricks Runtime 12.0 and 12.0 ML are GA Jobs are now available in global search Billable usage graphs can now aggregate by individual tags Use SQL to specify schema- and catalog-level storage locations for Unity Catalog managed tables Capturing lineage data with Unity Catalog is now generally available Databricks ODBC driver 2.6.29 Databricks JDBC driver 2.6.32 Partner Connect supports connecting to AtScale Improved serverless SQL warehouse support for customer-managed keys"
    },
    {
        "id": 1185,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Enhanced notifications for your Databricks jobs (Public Preview) Databricks Runtime 12.0 (Beta) Upload data UI can now be disabled via admin settings Partner Connect support for Unity Catalog is GA Work with large repositories with Sparse Checkout Databricks Terraform provider updated to version 1.6.5 Databricks Terraform provider updated to versions 1.6.3 and 1.6.4 Specify a cloud storage location for Unity Catalog managed tables at the catalog and schema levels Access recent objects from the search field in the top bar of your workspace Create or modify table from file upload page now supports multiple files Create or modify table from file upload page now supports overwrite Search for jobs by name with the Jobs API 2.1 Databricks Terraform provider updated to version 1.6.2 Search for tables in Unity Catalog is GA  \nGA: Repos support for non-notebook files Deploy models for streaming inference with Delta Live Tables notebooks Connect to Fivetran from the add data UI Databricks SQL Driver for Node.js is Generally Available Partner Connect supports connecting to erwin Data Modeler by Quest Enforce user isolation cluster types on a workspace Databricks Runtime 11.3 LTS and 11.3 LTS ML are GA Format Python code in notebooks (Public Preview) IP access lists no longer block PrivateLink traffic AWS PrivateLink support is now generally available Improvements to AWS PrivateLink support for updating workspaces Update a failed workspace with Databricks-managed VPC to use a customer-managed VPC Personal Compute cluster policy is available by default to all users Add data UI provides a central UI for loading data to Databricks Create or modify table from file upload page unifies experience for small file upload to Delta Lake Partner Connect supports connecting to Hevo Data Enable admin protection for No Isolation Shared clusters SQL persona integrated with new search experience Databricks is a FedRAMP\u00ae Authorized Cloud Service Offering (CSO) at the moderate impact Level Serverless SQL warehouses are available in regions eu-central-1 and us-east-2 Privilege inheritance in now supported in Unity Catalog Top navigation bar in the UI Databricks Runtime 11.3 (Beta) The account console is available in multiple languages"
    },
    {
        "id": 1186,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "New reference solution for natural language processing More regions for Unity Catalog Protect and control access to some types of encrypted data with customer-managed keys (GA) Compliance security profile workspaces support i4i instance types Audit logs now include events for web terminal Audit logs now include events for managing credentials for Git repos Select cluster policies directly in the Delta Live Tables UI New data trasformation card on workspace landing pages Orchestrate Databricks SQL tasks in your Databricks workflows (Public Preview) Delta cache is now disk cache Capture and view lineage data with Unity Catalog (Public Preview) Search for tables using Catalog Explorer (Public Preview) View and organize assets in the workspace browser across personas Databricks Runtime 11.2 and 11.2 ML are GA Support for AWS Graviton instances is GA"
    },
    {
        "id": 1187,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Account users can access the account console Databricks ODBC driver 2.6.26 Databricks JDBC driver 2.6.29 Databricks Feature Store client now available on PyPI Unity Catalog is GA Delta Sharing is GA Databricks Runtime 11.2 (Beta) Reduced message volume in the Delta Live Tables UI for continuous pipelines Easier cluster configuration for your Delta Live Tables pipelines Orchestrate dbt tasks in your Databricks workflows (Public Preview) Users can be members of multiple Databricks accounts Identity federation is GA Partner Connect supports connecting to Stardog Databricks Feature Store integration with Serverless Real-Time Inference Additional data type support for Databricks Feature Store automatic feature lookup Bring your own key: Encrypt Git credentials Cluster UI preview and access mode replaces security mode Unity Catalog limitations (Public Preview) Serverless Real-Time Inference in Public Preview Serverless SQL warehouses improvements Share VPC endpoints among Databricks accounts AWS PrivateLink private access level ANY is deprecated Improvements to AWS PrivateLink connectivity Improved workspace search is now GA Use generated columns when you create Delta Live Tables datasets Improved editing for notebooks with Monaco-based editor (Experimental) Compliance controls FedRAMP Moderate, PCI-DSS, and HIPAA (GA) Add security controls with the compliance security profile (GA) Add image hardening and monitoring agents with enhanced security monitoring (GA) Databricks Runtime 10.3 series support ends Delta Live Tables now supports refreshing only selected tables in pipeline updates Job execution now waits for cluster libraries to finish installing  \nDatabricks Runtime 11.1 and 11.1 ML are GA Photon is GA Notification upon notebook completion Increased limit for the number of jobs in your Databricks workspaces Verbose audit logs now record when Databricks SQL queries are run Databricks SQL Serverless supports instance profiles whose name does not match its associated role Configure your workspace to use IMDS v2 (Public Preview) Databricks JDBC driver 2.6.27 Databricks ODBC driver 2.6.25 Databricks Runtime 11.1 (Beta) Improved notebook visualizations"
    },
    {
        "id": 1188,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "ALTER TABLE permission changes for Unity Catalog Databricks Runtime 6.4 Extended Support reaches end of support Databricks Runtime 10.2 series support ends Databricks ODBC driver 2.6.24 Databricks Terraform provider is now GA Serverless SQL warehouses available for E2 workspaces (Public Preview) Enable enhanced security controls with a security profile (Public Preview) PCI-DSS compliance controls (Public Preview) HIPAA compliance controls for E2 (Public Preview) Enhanced security monitoring (Public Preview) Databricks Runtime 11.0 and 11.0 ML are GA; 11.0 Photon is Public Preview Change to Repos default working directory in Databricks Runtime 11.0 Databricks Runtime 10.1 series support ends Audit logs can now record when a notebook command is run Delta Live Tables now supports SCD type 2 Create Delta Live Tables pipelines directly in the Databricks UI Select the Delta Live Tables channel when you create or edit a pipeline Communicate between tasks in your Databricks jobs with task values Enable account switching in the Databricks UI Updating the AWS Region for a failed workspace is no longer supported  \nCopy and paste notebook cells between tabs and windows Additional data type support for Databricks Feature Store automatic feature lookup Databricks Runtime 11.0 (Beta) Improved workspace search (Public Preview) Explore SQL cell results in Python notebooks natively using Python Databricks Repos: Support for more files in a repo Databricks Repos: Fix to issue with MLflow experiment data loss Upgrade wizard makes it easier to copy databases and multiple tables to Unity Catalog (Public Preview) Power BI Desktop system-wide HTTP proxy support Streamline billing and account management by signing up for Databricks using AWS Marketplace Databricks Runtime 10.5 and 10.5 ML are GA; 10.5 Photon is Public Preview Authenticate to the account console using SAML 2.0 (Public Preview) Databricks JDBC driver 2.6.25 See the user a pipeline runs as in the Delta Live Tables UI"
    },
    {
        "id": 1189,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Use tags to better manage your Databricks jobs Databricks Runtime 10.0 series support ends Get a visual overview of your job runs with the new jobs matrix view Save time and resources when your Databricks job runs are unsuccessful View the run history for job tasks Assign a new cluster in the jobs UI when the Single User access no longer exists Databricks Runtime 10.5 (Beta) Feature Store now supports publishing features to AWS DynamoDB The Delta Live Tables UI is enhanced to disable unauthorized actions Databricks AutoML is generally available Use datasets from Unity Catalog with AutoML Delta Live Tables is GA on AWS and Azure, and in Public Preview on GCP Delta Live Tables SQL interface: non-breaking change to table names  \nBetter performance and cost for your Delta Live Tables pipelines with Databricks Enhanced Autoscaling Files in Repos enabled by default in new workspaces Databricks Feature Store is generally available Share an experiment from the experiment page RStudio Workbench bug fix New workspace language options Databricks Runtime 10.4 LTS and 10.4 LTS ML are GA; 10.4 Photon is Public Preview Unity Catalog is available in Public Preview Delta Sharing is available in Public Preview Enhanced access control for Delta Live Tables pipelines Test Delta Live Tables preview functionality with the new channel setting (Public Preview) Improved error handling for Delta Live Tables Python functions (Public Preview) Improvements to Databricks Repos Audit logging for cluster policy changes Databricks Runtime 10.4 (Beta)"
    },
    {
        "id": 1190,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Easier scheduling for your Delta Live Tables pipelines (Public Preview) Easily browse the history of your Delta Live Tables pipeline updates (Public Preview) Ensure job idempotency for the Jobs API Run now request Jobs service stability and scalability improvements Compare MLflow runs from different experiments Improvements to MLflow compare runs display Improved visibility into job run owners in the clusters UI Drop dataset columns in AutoML Experiments page is GA Support for temporary tables in the Delta Live Tables Python interface User interface improvements for Delta Live Tables (Public Preview) Databricks Runtime 9.0 series support ends Data Science & Engineering landing page updates Databricks Repos now supports AWS CodeCommit for Git integration Improved visualization for your Delta Live Tables pipelines (Public Preview) Updated Markdown parser Delta Live Tables now supports change data capture processing (Public Preview) Select algorithm frameworks to use with AutoML Customer-managed VPCs are now available in ap-northeast-2 Databricks hosted MLflow models can now look up features from online stores Databricks Runtime 10.3 and 10.3 ML are GA; 10.3 Photon is Public Preview  \nMLflow Model Registry Webhooks on Databricks (Public Preview) Breaking change: cluster idempotency token cleared on cluster termination Databricks Runtime 10.3 (Beta) View information on recent job runs Use Markdown in Databricks Repos file editor Improved cluster management for jobs that orchestrate multiple tasks Add or rotate the customer-managed key for managed services on a running workspace Delta Sharing Private Preview adds functionality and new terms Address AWS GuardDuty alerts related to Databricks access to your S3 bucket Databricks Runtime 8.3 and Databricks Runtime 8.4 series support ends Databricks JDBC driver 2.6.22 Support for G5 family of GPU-accelerated EC2 instances (Public Preview) New Share button replaces Permissions icon in notebooks New workspace language options"
    },
    {
        "id": 1191,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Databricks Runtime 6.4 Extended Support series end-of-support date extended Databricks Runtime 5.5 Extended Support series reaches end of support Databricks JDBC driver 2.6.21 Databricks Connector for Tableau 2021.4 Databricks Runtime 10.2 and 10.2 ML are GA; 10.2 Photon is Public Preview Workspaces in the ap-southeast-1 region now support AWS PrivateLink Updated Markdown parser User interface improvements for Delta Live Tables Databricks Runtime 8.3 series support extended Databricks Runtime 10.2 (Beta) Revert of recent breaking change that removed escaping and quotes from $ in environment variable values for cluster creation serverless SQL warehouses are available in region eu-west-1  \nCreate tags for feature tables (Public Preview) Syntax highlighting and autocomplete for SQL commands in Python cells Rename, delete, and change permissions for MLflow experiments from experiment page (Public Preview) New data profiles in notebooks: tabular and graphic summaries of your data (Public Preview) Improved logging when schemas evolve while running a Delta Live Tables pipeline Databricks Partner Connect GA Breaking change: remove escaping and quotes from $ in environment variable values for cluster creation Ease of use improvements for Files in Repos Support for legacy SQL widgets ends on January 15, 2022 User interface improvements for Databricks jobs Delta Sharing Connector for Power BI Databricks ODBC driver 2.6.19 Databricks Runtime 10.1 and 10.1 ML are GA; 10.1 Photon is Public Preview Databricks Runtime 10.1 (Beta) Rename and delete MLflow experiments (Public Preview) Photon support for additional cluster instance families You can now create a cluster policy by cloning an existing policy Single sign-on (SSO) in the account console is Generally Available Change the default language of notebooks and notebook cells more easily Use Files in Repos from the web terminal"
    },
    {
        "id": 1192,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Databricks Runtime 8.2 series support ends Databricks Runtime 10.0 and 10.0 ML are GA; 10.0 Photon is Public Preview Limit the set of VPC endpoints your workspace can use for AWS PrivateLink connections (Public Preview) User interface improvements for Delta Live Tables (Public Preview) Specify a fixed-size cluster when you create a new pipeline in Delta Live Tables (Public Preview) View data quality metrics for tables in Delta Live Tables triggered pipelines (Public Preview) Jobs orchestration is now GA Databricks Connector for Power BI Repos now supports arbitrary file types More detailed job run output with the Jobs API Improved readability of notebook paths in the Jobs UI Open your Delta Live Tables pipeline in a new tab or window New escape sequence for $ in legacy input widgets in SQL Faster model deployment with automatically generated batch inference notebook  \nDatabricks Runtime 10.0 (Beta) Databricks ODBC driver 2.6.18 Customer control of workspace login by Databricks staff Databricks Runtime 9.1 LTS and 9.1 LTS ML are GA; 9.1 LTS Photon is Public Preview Databricks Runtime 8.1 series support ends Databricks JDBC driver 2.6.19 Share feature tables across workspaces Security and usability improvements when resetting passwords Repos now supports .gitignore Enhanced jobs UI is now standard for all workspaces Databricks now available in region ap-southeast-1 PrivateLink supported in all availability zones within the supported regions Databricks Runtime 9.1 (Beta) Streamlined management of settings for Databricks jobs Databricks SQL Public Preview available in all workspaces Delete feature tables from Feature Store Grant view pipeline permissions in the Delta Live Tables UI (Public Preview) Reduce cluster resource usage with Delta Live Tables (Public Preview) Use MLflow models in your Delta Live Tables pipelines (Public Preview) Find Delta Live Tables pipelines by name (Public Preview) PyTorch TorchScript and other third-party libraries are now supported in Databricks jobs Databricks Runtime 8.0 series support ends"
    },
    {
        "id": 1193,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Databricks Repos GA Serverless SQL provides instant compute, minimal management, and cost optimization for SQL queries (Public Preview) User interface improvements for Delta Live Tables (Public Preview) More control over how tables are materialized in Delta Live Tables pipelines (Public Preview) Increased timeout for long-running notebook jobs Jobs service stability and scalability improvements User entitlements granted by group membership are displayed in the admin console Manage MLflow experiment permissions (Public Preview) Improved job creation from notebooks Improved support for collapsing notebook headings Databricks Runtime 9.0 and 9.0 ML are GA; 9.0 Photon is Public Preview Low-latency delivery of audit logs is generally available Databricks Runtime 9.0 (Beta) Manage repos programmatically with the Databricks CLI (Public Preview) Manage repos programmatically with the Databricks REST API (Public Preview) Databricks Runtime 7.6 series support ends Log delivery APIs now report delivery status Use the AWS EBS SSD gp3 volume type for all clusters in a workspace Audit events are logged when you interact with Databricks Repos Improved job creation and management workflow Simplified instructions for setting Git credentials (Public Preview) Import multiple notebooks in .html format Usability improvements for Delta Live Tables Configure Databricks for SSO with Microsoft Entra ID in your Azure tenant"
    },
    {
        "id": 1194,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Manage MLflow experiment permissions with the Databricks REST API Databricks web interface is localized in Portuguese and French (Public Preview) Databricks Runtime 5.5 LTS for Machine Learning support ends, replaced by Extended Support version Databricks Light 2.4 support ends September 5, replaced by Extended Support version Reduced permissions for cross-account IAM roles Feature freshness information available in Databricks Feature Store UI (Public Preview) Display up to 10,000 result rows Bulk import and export notebooks in a folder as source files Autocomplete in SQL notebooks now uses all-caps for SQL keywords Reorderable and resizable widgets in notebooks Databricks UI usability fixes Quickly define pipeline settings when you create a new Delta Live Tables pipeline Databricks Runtime 8.4 and 8.4 ML are GA; 8.4 Photon is Public Preview Use Spark SQL with the Delta Live Tables Python API Enhanced data processing and analysis with Databricks jobs (Public Preview) Reduced cost for Delta Live Tables default clusters (Public Preview) Sort pipelines by name in the Delta Live Tables UI (Public Preview) Changes to Compute page Databricks Runtime 5.5 LTS support ends, replaced by Databricks Runtime 5.5 Extended Support through the end of 2021 Repos API (Public Preview) Databricks Runtime 8.4 (Beta)"
    },
    {
        "id": 1195,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Correction: Repos for Git is enabled by default in new and existing workspaces in some regions Change to Feature Store permissions Improved access to results in the MLflow runs table Better cost visibility for Delta Live Tables Enhanced data quality constraints for Delta Live Tables API changes for updating and replacing IP address lists Databricks ODBC driver 2.6.17 Use an API to download usage data directly Databricks Runtime 7.5 series support ends Optimize performance and control costs by using different pools for the driver node and worker nodes Photon runtimes now support i3.xlarge instances (Public Preview) Registry-wide permissions for Model Registry A user\u2019s home directory is no longer protected when you delete a user using the SCIM API Accelerate SQL workloads with Photon (Public Preview) Databricks Runtime 8.3 and 8.3 ML are GA; 8.3 Photon is Public Preview Python and SQL table access control (GA) Jobs UI and API now show the owner of a job run Protect sensitive Spark configuration properties and environment variables using secrets (Public Preview) Repos for Git is enabled by default in new and existing workspaces in some regions Redesigned Workspace Settings UI Updates to ListTokens and ListAllTokens database queries expired tokens Confirmation now required when granting or revoking Admin permissions Changes to keyboard shortcuts in the web UI"
    },
    {
        "id": 1196,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Databricks Machine Learning: a data-native and collaborative solution for the full ML lifecycle SQL Analytics is renamed to Databricks SQL Create and manage ETL pipelines using Delta Live Tables (Public Preview) Reduced scope of required egress rules for customer-managed VPCs Workspaces in the eu-west-2 region now support AWS PrivateLink Encrypt Databricks SQL queries and query history using your own key (Public Preview) Increased limit for the number of terminated all-purpose clusters Increased limit for the number of pinned clusters Manage where notebook results are stored (Public Preview) The new improved account console is GA Customer-managed keys for workspace storage (Public Preview) Changes to the Account API for customer-managed keys Google Cloud Storage connector (GA) Databricks Runtime 7.4 series support ends Better governance with enhanced audit logging Use SSO to authenticate to the account console (Public Preview) Repos users can now integrate with Azure DevOps using personal access tokens Jobs service stability and scalability improvements (Public Preview) Service principals provide API-only access to Databricks resources (Public Preview)  \nDatabricks Runtime 8.2 (GA) AWS PrivateLink for Databricks workspaces (Public Preview) Update running workspaces with new credentials or network configurations Databricks can now send in-product messages and product tours directly to your workspace (Public Preview) Easier job management with the enhanced jobs user interface Cluster policy changes are applied automatically to existing clusters at restart and edit Track retries in your job tasks when task attempts fail Quickly view cluster details when you create a new cluster MLflow sidebar reflects the most recent experiment Change to default channel for conda.yaml files in MLflow New free trial and pay-as-you-go customers are now on the E2 version of the platform Databricks Runtime 8.2 (Beta) User and group limits Easier monitoring of job run status Better governance with enhanced audit logging Global init scripts no longer run on model serving clusters Databricks Runtime 6.4 series support ends"
    },
    {
        "id": 1197,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Databricks now supports dark mode for viewing notebooks Databricks Runtime 8.1 (GA) Easier job creation and management with the enhanced jobs user interface (Public Preview) Track job retry attempts with a new sequential value returned for each job run attempt Increased limit for the number of saved jobs in Premium and Enterprise workspaces Easier way to connect to Databricks from your favorite BI tools and SQL clients Databricks Repos let you use Git repositories to integrate Databricks with CI/CD systems Automatic retries for failed job clusters reverted Databricks Runtime 8.1 (Beta) Limit username and password authentication with password ACLs (GA) Receive email notification about activity in Model Registry Model Serving now supports additional model types New options for searching Model Registry Increased limit for the number of terminated all-purpose clusters Increased limit for the number of pinned clusters in a workspace Databricks Runtime 8.0 (GA)  \nNew Databricks Power BI connector (GA) Add and manage account admins using the SCIM API (E2 accounts, Public Preview) Added modification_time to the DBFS REST API get-status and list responses Easily copy long experiment names in MLflow Adjust memory size and number of cores for serving clusters Web terminal is now GA Auto-AZ: automatic selection of availability zone (AZ) when you launch clusters, available on all deployment types Delegate account management beyond the account owner (E2 accounts, Public Preview) Separate account-level and workspace-level audit logging configurations help you monitor account and workspace activity more effectively Databricks Runtime 7.2 series support ends Databricks Runtime 7.6 GA Databricks Runtime 8.0 (Beta) Databricks Runtime for Genomics now deprecated View more readable JSON in the MLflow run artifact display Provide comments in the Model Registry using REST API Easily specify default cluster values in API calls Tune cluster worker configuration according to current worker allocation Pass context specific information to a job\u2019s task with task parameter variables Error messages from job failures no longer contain possibly sensitive information Download usage data from the new account console for E2 accounts"
    },
    {
        "id": 1198,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Databricks Runtime 7.1 series support ends Start clusters faster with Docker images preloaded into instance pools Notebook find and replace now supports changing all occurrences of a match Single Node clusters (GA) Free form cluster policy type renamed to Unrestricted Cluster policy field not shown if a user only has access to one policy G4 family of GPU-accelerated EC2 instances GA Databricks Runtime 7.0 series support ends Billable usage and audit log S3 bucket policy and object ACL changes E2 platform comes to the Asia Pacific region  \nDatabricks Runtime 7.5 GA Existing Databricks accounts migrate to E2 platform today Jobs API now supports updating existing jobs New global init script framework is GA New account console enables customers on the E2 platform to create and manage multiple workspaces (Public Preview) Databricks Runtime 7.5 (Beta) Auto-AZ: automatic selection of availability zone (AZ) when you launch clusters Jobs API end_time field now uses epoch time Find DBFS files using new visual browser Visibility controls for jobs, clusters, notebooks, and other workspace objects are now enabled by default on new workspaces Improved display of nested runs in MLflow Admins can now lock user accounts (Public Preview) Updated NVIDIA driver Use your own keys to secure notebooks (Public Preview)  \nDatabricks Runtime 6.6 series support ends MLflow Model Registry GA Filter experiment runs based on whether a registered model is associated Partner integrations gallery now available through the Data tab Cluster policies now use allowlist and blocklist as policy type names Automatic retries when the creation of a job cluster fails Navigate notebooks using the table of contents Databricks SQL (Public Preview) Web terminal available on Databricks Community Edition Single Node clusters now support Databricks Container Services Databricks Runtime 7.4 GA Databricks JDBC driver update Databricks Connect 7.3 (Beta)"
    },
    {
        "id": 1199,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "New Databricks Power BI connector available in the online Power BI service (Public Preview) Databricks Runtime 7.4 (Beta) Expanded experiment access control (ACLs) High fidelity import and export of Jupyter notebook (ipynb) files SCIM API improvement: both indirect and direct groups returned in user record response Databricks Runtime 6.5 series support ends Self-service, low-latency audit log configuration (Public Preview) SCIM API improvement: $ref field response Databricks Runtime 7.3, 7.3 ML, and 7.3 Genomics declared Long Term Support (LTS) Render images at higher resolution using matplotlib  \nDatabricks Runtime 7.3, 7.3 ML, and 7.3 Genomics are now GA Debugging hints for SAML credential passthrough misconfigurations Single Node clusters (Public Preview) DBFS REST API rate limiting New sidebar icons Running jobs limit increase Artifact access control lists (ACLs) in MLflow MLflow usability improvements New Databricks Power BI connector (Public Preview) New JDBC and ODBC drivers bring faster and lower latency BI MLflow Model Serving (Public Preview) Clusters UI improvements Visibility controls for jobs, clusters, notebooks, and other workspace objects Ability to create tokens no longer permitted by default Support for c5.24xlarge instances MLflow Model Registry supports sharing of models across workspaces Databricks Runtime 7.3 (Beta) E2 architecture\u2014now GA\u2014provides better security, scalability, and management tools Account API is generally available on the E2 version of the platform Secure cluster connectivity (no public IPs) is now the default on the E2 version of the platform"
    },
    {
        "id": 1200,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Token Management API is GA and admins can use the Admin Console to grant and revoke user access to tokens Message size limits for Shiny apps increased Improved instructions for setting up a cluster in local mode View version of notebook associated with a run Databricks Runtime 7.2 GA Databricks Runtime 7.2 ML GA Databricks Runtime 7.2 Genomics GA Permissions API (Public Preview) Databricks Connect 7.1 (GA) Repeatable installation order for cluster libraries Customer-managed VPC is GA Secure cluster connectivity (no public IPs) is GA Multi-workspace API (Account API) adds pricing tier Create model from MLflow registered models page (Public Preview) Databricks Container Services supports GPU images  \nWeb terminal (Public Preview) New, more secure global init script framework (Public Preview) IP access lists now GA New file upload dialog SCIM API filter and sort improvements Databricks Runtime 7.1 GA Databricks Runtime 7.1 ML GA Databricks Runtime 7.1 Genomics GA Databricks Connect 7.1 (Public Preview) IP Access List API updates Python notebooks now support multiple outputs per cell View notebook code and results cells side by side Pause job schedules Jobs API endpoints validate run ID Format SQL in notebooks automatically Support for r5.8xlarge and r5.16xlarge instances Use password access control to configure which users are required to log in using SSO or authenticate using tokens (Public Preview) Reproducible order of installation for Maven and CRAN libraries Take control of your users\u2019 personal access tokens with the Token Management API (Public Preview) Customer-managed VPC deployments (Public Preview) can now use regional VPC endpoints Encrypt traffic between cluster worker nodes (Public Preview) Table access control supported on all accounts with the Premium plan (Public Preview) IAM credential passthrough supported on all accounts with the Premium plan (Public Preview) Restore cut notebook cells Assign jobs CAN MANAGE permission to non-admin users Non-admin Databricks users can view and filter by username using the SCIM API Link to view cluster specification when you view job run details"
    },
    {
        "id": 1201,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Billable usage logs delivered to your own S3 bucket (Public Preview) Databricks Connect now supports Databricks Runtime 6.6 Databricks Runtime 7.0 ML GA Databricks Runtime 7.0 GA, powered by Apache Spark 3.0 Databricks Runtime 7.0 for Genomics GA Stage-dependent access controls for MLflow models Notebooks now support disabling auto-scroll Skipping instance profile validation now available in the UI Account ID is displayed in account console Internet Explorer 11 support ends on August 15 Databricks Runtime 6.2 series support ends Simplify and control cluster creation using cluster policies (Public Preview) SCIM Me endpoint now returns SCIM compliant response G4 family of GPU-accelerated EC2 instances now available for machine learning application deployments (Beta) Deploy multiple workspaces in your Databricks account (Public Preview) Deploy Databricks workspaces in your own VPC (Public Preview) Secure cluster connectivity with no open ports on your VPCs and no public IP addresses on Databricks workers (Public Preview) Restrict access to Databricks using IP access lists (Public Preview) Encrypt locally attached disks (Public Preview)  \nDatabricks Runtime 6.6 for Genomics GA Databricks Runtime 6.6 ML GA Databricks Runtime 6.6 GA Easily view large numbers of MLflow registered models Libraries configured to be installed on all clusters are not installed on clusters running Databricks Runtime 7.0 and above Databricks Runtime 7.0 for Genomics (Beta) Databricks Runtime 7.0 ML (Beta) Databricks Runtime 6.6 for Genomics (Beta) Databricks Runtime 6.6 ML (Beta) Databricks Runtime 6.6 (Beta) Job clusters now tagged with job name and ID DBFS REST API delete endpoint size limit Restore deleted notebook cells Jobs pending queue limit"
    },
    {
        "id": 1202,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "MLflow tracking UI enhancement Notebook usability improvements Databricks Connect now supports Databricks Runtime 6.5 Databricks Runtime 6.1 and 6.1 ML support ends Databricks Runtime 6.5 GA Databricks Runtime 6.5 for Machine Learning GA Databricks Runtime 6.5 for Genomics GA Authenticate to S3 buckets automatically using your IAM credentials (Public Preview) IAM role renamed to instance profile Easier notebook title changes Cluster termination reporting enhancement DBFS REST API delete endpoint size limit Databricks Runtime 6.0 and 6.0 ML support ends  \nManaged MLflow Model Registry collaborative hub available (Public Preview) Load data from hundreds of data sources into Delta Lake using Stitch Databricks Runtime 7.0 (Beta) previews Apache Spark 3.0 Databricks Runtime 6.5 ML (Beta) Databricks Runtime 6.5 (Beta) Optimized autoscaling on all-purpose clusters running Databricks Runtime 6.4 and above Single-sign-on (SSO) now available on all pricing plans Develop and test Shiny applications inside RStudio Server Change the default language of a notebook Databricks to add anonymized usage analytics Databricks Connect now supports Databricks Runtime 6.4 Databricks Connect now supports Databricks Runtime 6.3  \nDatabricks Runtime 6.4 for Genomics GA Databricks Runtime 6.4 ML GA Databricks Runtime 6.4 GA The Clusters and Jobs UIs now reflect new cluster terminology and cluster pricing New interactive charts offer rich client-side interactions New data ingestion network adds partner integrations with Delta Lake (Public Preview) Flags to manage workspace security and notebook features now available  \nAll cluster and pool tags now propagate to usage reports Cluster and pool tag propagation to EC2 instances is more accurate Databricks Runtime 6.3 for Genomics GA Databricks Runtime 6.3 ML GA Databricks Runtime 6.3 GA Cluster worker machine images now use chrony for NTP Cluster standard autoscaling step is now configurable SCIM API supports pagination for Get Users and Get Groups (Public Preview) File browser swimlane widths increased to 240px Databricks Runtime 3.5 LTS support ends"
    },
    {
        "id": 1203,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Databricks Connect now supports Databricks Runtime 6.2 Databricks Runtime 6.2 for Genomics GA Azure Databricks SCIM provisioning connector available in the app gallery Databricks Runtime 5.3 and 5.4 support ends Databricks Runtime 6.2 ML GA Databricks Runtime 6.2 GA Databricks Connect now supports Databricks Runtime 6.1  \nDatabricks Runtime 6.2 ML Beta Databricks Runtime 6.2 Beta Configure clusters with your own container image using Databricks Container Services Cluster detail now shows only cluster ID in the HTTP path Secrets referenced by Spark configuration properties and environment variables (Public Preview)  \nDatabricks Runtime 6.1 for Genomics GA Databricks Runtime 6.1 for Machine Learning GA MLflow API calls are now rate limited Pools of instances for quick cluster launch generally available New instance types (Beta) Databricks Runtime 6.1 GA Databricks Runtime 6.0 for Genomics GA Non-admin Databricks users can read user and group names and IDs using SCIM API Workspace API returns notebook and folder object IDs Databricks Runtime 6.0 ML GA Databricks Runtime 6.0 GA Account usage reports now show usage by user name  \nDatabricks Runtime 5.2 support ends Launch pool-backed automated clusters that use Databricks Light (Public Preview) Beta support for m5a and r5a instances pandas DataFrames now render in notebooks without scaling Python version selector display now dynamic Databricks Runtime 6.0 Beta  \nWorkspace library installation enhancement Clusters UI now reflects more consistent interactive and automated cluster terminology Databricks Runtime 5.5 and Databricks Runtime 5.5 ML are LTS Instance allocation notifications for pools New cluster events MLflow updates"
    },
    {
        "id": 1204,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "Coming soon: Databricks 6.0 will not support Python 2 Ideas Portal Preload the Databricks Runtime version on pool idle instances Custom cluster tags and pool tags play better together MLflow 1.1 brings several UI and API improvements pandas DataFrame display renders like it does in Jupyter New regions Databricks Runtime 5.5 with Conda (Beta) Set permissions on pools (Public Preview) Databricks Runtime 5.5 for Machine Learning Databricks Runtime 5.5 Keep a pool of instances on standby for quick cluster launch (Public Preview) Global series color  \nAccount usage chart updated to display usage grouped by workload type RStudio integration no longer limited to high concurrency clusters MLflow 1.0 Databricks Runtime 5.4 with Conda (Beta) Databricks Runtime 5.4 for Machine Learning Databricks Runtime 5.4  \nCluster event log filtering JDBC/ODBC connectivity available without Premium plan or above  \nMLflow on Databricks (GA) Delta Lake on Databricks MLflow runs sidebar C5d series Amazon EC2 instance types (Beta) Databricks Runtime 5.3 (GA) Databricks Runtime 5.3 ML (GA)  \nPurge deleted MLflow experiments and runs Databricks Light generally available Searchable cluster selector Upcoming usage display changes Manage groups from the Admin Console Notebooks automatically have associated MLflow experiment Z1d series Amazon EC2 instance types (Beta) Two private IP addresses per node Databricks Delta public community  \nManaged MLflow on Databricks Public Preview Azure Data Lake Storage Gen2 connector is generally available Python 3 now the default when you create clusters Additional cluster instance types Delta Lake generally available  \nUpcoming change: Python 3 to become the default when you create clusters Databricks Runtime 5.2 for Machine Learning (Beta) release Cluster configuration JSON view Library UI Cluster Events Cluster UI  \nDatabricks Runtime 5.1 for Machine Learning (Beta) release Databricks Runtime 5.1 release Library UI  \nLibrary UI Custom Spark heap memory settings enabled Jobs and idle execution context eviction Databricks Runtime 5.0 for Machine Learning (Beta) release Databricks Runtime 5.0 release displayHTML support for unrestricted loading of third-party content"
    },
    {
        "id": 1205,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "SCIM provisioning using OneLogin Copy notebook file path without opening notebook  \nSCIM provisioning using Okta and Microsoft Entra ID (Preview) EBS leaked volumes deletion Support for r5 instances SCIM API for provisioning users and groups (Preview)  \nWorkspace sidebar redesign New environment variables in init scripts EBS leaked volumes logging and deletion AWS r3 and c3 instance types now deprecated Audit logging for ACL changes Cluster-scoped init scripts Collapsible headings  \nLibraries API supports Python wheel files IPython notebook export New instance types (beta) Cluster mode and High Concurrency clusters Table access control RStudio integration R Markdown support Home page redesign, with ability to drop files to import data Widget default behavior Table creation UI Multi-line JSON data import  \nCluster log purge Trash folder Reduced log retention period Gzipped API responses Table import UI  \nGeneral Data Protection Regulation (GDPR) HorovodEstimator MLeap ML Model Export Notebook cells: hide and show Doc site search Databricks Runtime 4.1 for Machine Learning (Beta) New GPU cluster types Secret management Cluster pinning Cluster autostart Workspace purging Databricks CLI 0.7.1 Display() support for image data types Databricks Delta update S3 Select connector  \nAWS account updates Spark error tips Databricks CLI 0.7.0 Increase init script output truncation limit Clusters API: added UPSIZE_COMPLETED event type Command autocomplete Serverless pools upgraded to Databricks Runtime 4.0  \nCommand execution details Databricks CLI 0.6.1 supports --profile ACLs enabled by default for new Operational Security customers New doc site theme Cluster event log Databricks CLI: 0.6.0 release Job run management Edit cluster permissions now requires edit mode Databricks ML Model Export"
    },
    {
        "id": 1206,
        "url": "https://docs.databricks.com/en/release-notes/product/index.html",
        "content": "New line chart supports time-series data More visualization improvements Delete job runs using Job API Bring your own S3 bucket KaTeX math rendering library updated Databricks CLI: 0.5.0 release DBUtils API library Filter for your jobs only Spark-submit from the Create Job page Select Python 3 from the Create Cluster page Workspace UI improvements Autocomplete for SQL commands and database names Serverless pools now support R Distributed TensorFlow and Keras Libraries Support XGBoost available as a Spark Package Table access control for SQL and Python (Beta)  \nMount points for Azure Blob storage containers and Data Lake Stores Table Access Control for SQL and Python (Private Preview) Exporting notebook job run results via API Apache Airflow 1.9.0 includes Databricks integration"
    },
    {
        "id": 1207,
        "url": "https://docs.databricks.com/en/release-notes/product/2019/september.html",
        "content": "September 2019  \nThese features and Databricks platform improvements were released in September 2019.  \nNote  \nReleases are staged. Your Databricks account may not be updated until up to a week after the initial release date.  \nDatabricks Runtime 5.2 support ends\nDatabricks Runtime 5.2 support ends\nSeptember 30, 2019  \nSupport for Databricks Runtime 5.2 ended on September 30. See Databricks support lifecycles.\n\nLaunch pool-backed automated clusters that use Databricks Light (Public Preview)\nLaunch pool-backed automated clusters that use Databricks Light (Public Preview)\nSeptember 26 - October 1, 2019: Version 3.3  \nWhen we introduced Pool configuration reference in July, you couldn\u2019t select Databricks Light as your runtime version when you configured a pool-backed cluster for an automated job. Now you can have both quick cluster start times and cost-efficient clusters!\n\nBeta support for m5a and r5a instances"
    },
    {
        "id": 1208,
        "url": "https://docs.databricks.com/en/release-notes/product/2019/september.html",
        "content": "Beta support for m5a and r5a instances\nSeptember 26 - October 1, 2019: Version 3.3  \nDatabricks now provides beta support for the m5a and r5a series of EC2 instances, for use cases when you need to be economical.\n\npandas DataFrames now render in notebooks without scaling\npandas DataFrames now render in notebooks without scaling\nSeptember 12-17, 2019: Version 3.2  \nIn Databricks notebooks, displayHTML was scaling some framed HTML content to fit the available width of the rendered notebook. While this behavior is desirable for images, it rendered wide pandas DataFrames poorly. But not anymore!\n\nPython version selector display now dynamic\nPython version selector display now dynamic\nSeptember 12-17, 2019: Version 3.2  \nWhen you select a Databricks runtime that doesn\u2019t support Python 2 (like Databricks 6.0), the cluster creation page hides the Python version selector.\n\nDatabricks Runtime 6.0 Beta"
    },
    {
        "id": 1209,
        "url": "https://docs.databricks.com/en/release-notes/product/2019/september.html",
        "content": "Databricks Runtime 6.0 Beta\nSeptember 12, 2019  \nDatabricks Runtime 6.0 Beta brings many library upgrades and new features, including:  \nNew Scala and Java APIs for Delta Lake DML commands, as well as the vacuum and history utility commands.  \nEnhanced DBFS FUSE v2 client for faster and more reliable reads and writes during model training.  \nSupport for multiple matplotlib plots per notebook cell.  \nUpdate to Python 3.7, as well as updated numpy, pandas, matplotlib, and other libraries.  \nSunset of Python 2 support.  \nFor more information, see the complete Databricks Runtime 6.0 (EoS) release notes."
    },
    {
        "id": 1210,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/ai_similarity.html",
        "content": "ai_similarity function  \nApplies to: Databricks SQL Databricks Runtime  \nPreview  \nThis feature is in Public Preview.  \nIn the preview:  \nThe underlying language model can handle several languages, however these functions are tuned for English.  \nThere is rate limiting for the underlying Foundation Model APIs. See Foundation Model APIs limits to update these limits.  \nThe ai_similarity() function invokes a state-of-the-art generative AI model from Databricks Foundation Model APIs to compare two strings and computes the semantic similarity score using SQL.  \nRequirements"
    },
    {
        "id": 1211,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/ai_similarity.html",
        "content": "Requirements\nImportant  \nThe underlying models that might be used at this time are licensed under the MIT License or Llama 2 community license. Databricks recommends reviewing these licenses to ensure compliance with any applicable terms. If models emerge in the future that perform better according to Databricks\u2019s internal benchmarks, Databricks may change the model (and the list of applicable licenses provided on this page).  \nCurrently, bge-large-en-v1.5 is the underlying model that powers this AI function.  \nThis function is only available on workspaces in Foundation Model APIs pay-per-token supported regions.  \nThis function is not available on Databricks SQL Classic.  \nCheck the Databricks SQL pricing page.  \nNote  \nIn Databricks Runtime 15.1 and above, this function is supported in Databricks notebooks, including notebooks that are run as a task in a Databricks workflow.\n\nSyntax\nSyntax\nai_similarity(expr1, expr2)\n\nArguments\nArguments\nexpr1: A STRING expression.  \nexpr2: A STRING expression.\n\nReturns"
    },
    {
        "id": 1212,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/ai_similarity.html",
        "content": "Returns\nA FLOAT value, representing the semantic similarity between the two input strings. The output score is relative and should only be used for ranking. Score of 1 means the two text are equal.\n\nExamples\nExamples\n> SELECT ai_similarity('Apache Spark', 'Apache Spark'); 1.0 > SELECT company_name FROM customers ORDER BY ai_similarity(company_name, 'Databricks') DESC LIMIT 1 Databricks Inc."
    },
    {
        "id": 1213,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/current_timestamp.html",
        "content": "current_timestamp function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the current timestamp at the start of query evaluation.  \nSyntax\nSyntax\ncurrent_timestamp()\n\nArguments\nArguments\nThis function takes no arguments.\n\nReturns\nReturns\nA TIMESTAMP.  \nThe braces are optional.\n\nExamples\nExamples\n> SELECT current_timestamp(); 2020-04-25 15:49:11.914 > SELECT current_timestamp; 2020-04-25 15:49:11.914\n\nRelated functions\nRelated functions\ncurrent_date function  \ncurrent_timezone function  \nnow function"
    },
    {
        "id": 1214,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/decimal.html",
        "content": "decimal function  \nApplies to: Databricks SQL Databricks Runtime  \nCasts the value expr to DECIMAL. This function is a synonym for CAST(expr AS decimal(10, 0)). See cast function for details.  \nSyntax\nSyntax\ndecimal(expr)\n\nArguments\nArguments\nexpr: An expression that can be cast to DECIMAL.\n\nReturns\nReturns\nThe result is DECIMAL(10, 0).\n\nExamples\nExamples\n> SELECT decimal('5.2'); 5\n\nRelated functions\nRelated functions\ncast function"
    },
    {
        "id": 1215,
        "url": "https://docs.databricks.com/en/sql/language-manual/data-types/special-floating-point-values.html",
        "content": "Special floating point values  \nApplies to: Databricks SQL Databricks Runtime  \nSeveral special floating point values are treated in a case-insensitive manner:  \nInf, +Inf, Infinity, +Infinity: positive infinity  \n-Inf, -Infinity: negative infinity  \nNaN: not a number  \nPositive and negative infinity semantics\nPositive and negative infinity semantics\nPositive and negative infinity have the following semantics:  \nPositive infinity multiplied by any positive value returns positive infinity.  \nNegative infinity multiplied by any positive value returns negative infinity.  \nPositive infinity multiplied by any negative value returns negative infinity.  \nNegative infinity multiplied by any negative value returns positive infinity.  \nPositive or negative infinity multiplied by 0 returns NaN.  \nPositive or negative infinity is equal to itself.  \nIn aggregations, all positive infinity values are grouped together. Similarly, all negative infinity values are grouped together.  \nPositive infinity and negative infinity are treated as normal values in join keys.  \nPositive infinity sorts lower than NaN and higher than any other values.  \nNegative infinity sorts lower than any other values.\n\nNaN semantics"
    },
    {
        "id": 1216,
        "url": "https://docs.databricks.com/en/sql/language-manual/data-types/special-floating-point-values.html",
        "content": "NaN semantics\nWhen dealing with float or double types that do not exactly match standard floating point semantics, NaN has the following semantics:  \nNaN = NaN returns true.  \nIn aggregations, all NaN values are grouped together.  \nNaN is treated as a normal value in join keys.  \nNaN values go last when in ascending order, larger than any other numeric value.\n\nExamples"
    },
    {
        "id": 1217,
        "url": "https://docs.databricks.com/en/sql/language-manual/data-types/special-floating-point-values.html",
        "content": "Examples\n> SELECT double('infinity'); Infinity > SELECT float('-inf'); -Infinity > SELECT float('NaN'); NaN > SELECT double('infinity') * 0; NaN > SELECT double('-infinity') * (-1234567); Infinity > SELECT double('infinity') < double('NaN'); true > SELECT double('NaN') = double('NaN'); true > SELECT double('inf') = double('infinity'); true > SELECT COUNT(*), c2 FROM VALUES (1, double('infinity')), (2, double('infinity')), (3, double('inf')), (4, double('-inf')), (5, double('NaN')), (6, double('NaN')), (7, double('-infinity')) AS test(c1, c2) GROUP BY c2; 2 NaN 2 -Infinity 3 Infinity\n\nRelated\nRelated\nFLOAT type  \nDOUBLE type"
    },
    {
        "id": 1218,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/coloncolonsign.html",
        "content": ":: (colon colon sign) operator  \nApplies to: Databricks SQL Databricks Runtime  \nCasts the value expr to the target data type type. This operator is a synonym for cast function.  \nSyntax\nSyntax\nexpr :: type\n\nArguments\nArguments\nexpr: Any castable expression.\n\nReturns\nReturns\nThe result is type type.\n\nExamples\nExamples\n> SELECT '20'::INTEGER; 20 > SELECT typeof(NULL::STRING); string\n\nRelated functions\nRelated functions\ncast function  \ntry_cast function"
    },
    {
        "id": 1219,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/array_except.html",
        "content": "array_except function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns an array of the elements in array1 but not in array2.  \nSyntax\nSyntax\narray_except(array1, array2)\n\nArguments\nArguments\narray1: An ARRAY of any type with comparable elements.  \narray2: An ARRAY of elements sharing a least common type with the elements of array1.\n\nReturns\nReturns\nAn ARRAY of matching type to array1 with no duplicates.\n\nExamples\nExamples\n> SELECT array_except(array(1, 2, 2, 3), array(1, 1, 3, 5)); [2]\n\nRelated\nRelated\narray_append function  \narray_compact function  \narray_distinct function  \narray_intersect function  \narray_sort function  \narray_remove function  \narray_union function  \nSQL data type rules"
    },
    {
        "id": 1220,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/grouping.html",
        "content": "grouping function  \nApplies to: Databricks SQL Databricks Runtime  \nIndicates whether a specified column in a GROUPING SET, ROLLUP, or CUBE represents a subtotal.  \nSyntax\nSyntax\ngrouping(col)\n\nArguments\nArguments\ncol: A column reference identified in a GROUPING SET, ROLLUP, or CUBE.\n\nReturns\nReturns\nAn INTEGER.  \nThe result is 1 for a specified row if the row represents a subtotal over the grouping of col, or 0 if it is not.\n\nExamples\nExamples\n> SELECT name, grouping(name), sum(age) FROM VALUES (2, 'Alice'), (5, 'Bob') people(age, name) GROUP BY cube(name); Alice 0 2 Bob 0 5 NULL 1 7\n\nRelated functions\nRelated functions\ngrouping_id function"
    },
    {
        "id": 1221,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/ai_classify.html",
        "content": "ai_classify function  \nApplies to: Databricks SQL Databricks Runtime  \nPreview  \nThis feature is in Public Preview.  \nIn the preview:  \nThe underlying language model can handle several languages, however these functions are tuned for English.  \nThere is rate limiting for the underlying Foundation Model APIs. See Foundation Model APIs limits to update these limits.  \nThe ai_classify() function allows you to invoke a state-of-the-art generative AI model to classify input text according to labels you provide using SQL. This function uses a chat model serving endpoint made available by Databricks Foundation Model APIs.  \nRequirements"
    },
    {
        "id": 1222,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/ai_classify.html",
        "content": "Requirements\nImportant  \nThe underlying models that might be used at this time are licensed under the Apache 2.0 license or Llama 2 community license. Databricks recommends reviewing these licenses to ensure compliance with any applicable terms. If models emerge in the future that perform better according to Databricks\u2019s internal benchmarks, Databricks may change the model (and the list of applicable licenses provided on this page).  \nCurrently, Mixtral-8x7B Instruct is the underlying model that powers these AI functions.  \nThis function is only available on workspaces in Foundation Model APIs pay-per-token supported regions.  \nThis function is not available on Databricks SQL Classic.  \nCheck the Databricks SQL pricing page.  \nNote  \nIn Databricks Runtime 15.1 and above, this function is supported in Databricks notebooks, including notebooks that are run as a task in a Databricks workflow.\n\nSyntax\nSyntax\nai_classify(content, labels)\n\nArguments\nArguments\ncontent: A STRING expression, the text to be classified.  \nlabels: An ARRAY<STRING> literal, the expected output classification labels. Must contain at least 2 elements, and no more than 20 elements.\n\nReturns"
    },
    {
        "id": 1223,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/ai_classify.html",
        "content": "Returns\nA STRING. The value matches one of the strings provided in the labels argument. Returns null if the content cannot be classified.\n\nExamples\nExamples\n> SELECT ai_classify(\"My password is leaked.\", ARRAY(\"urgent\", \"not urgent\")); urgent > SELECT description, ai_classify(description, ARRAY('clothing', 'shoes', 'accessories', 'furniture')) AS category FROM products  \nai_analyze_sentiment function"
    },
    {
        "id": 1224,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/binary.html",
        "content": "binary function  \nApplies to: Databricks SQL Databricks Runtime  \nCasts the value of expr to BINARY. This function is a synonym for CAST(expr AS BINARY). See cast function for details.  \nSyntax\nSyntax\nbinary(expr)\n\nArguments\nArguments\nexpr: Any expression that that can be cast to BINARY.\n\nReturns\nReturns\nA BINARY.\n\nExamples\nExamples\n> SELECT binary('Spark SQL'); [53 70 61 72 6B 20 53 51 4C] > SELECT binary(1984); [00 00 07 C0]\n\nRelated functions\nRelated functions\ncast function"
    },
    {
        "id": 1225,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/chr.html",
        "content": "chr function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the character at the supplied UTF-16 code point. This function is a synonym for char function.  \nSyntax\nSyntax\nchr(expr)\n\nArguments\nArguments\nexpr: An expression that evaluates to an integral numeric.\n\nReturns\nReturns\nThe result type is STRING.  \nIf the argument is less than 0, an empty string is returned. If the argument is larger than 255, it is treated as modulo 256. This implies char covers the ASCII and Latin-1 Supplement range of UTF-16.\n\nExamples\nExamples\n> SELECT chr(65); A\n\nRelated functions\nRelated functions\nchar function  \nascii function"
    },
    {
        "id": 1226,
        "url": "https://docs.databricks.com/en/sql/language-manual/delta-fsck.html",
        "content": "FSCK REPAIR TABLE  \nApplies to: Databricks SQL Databricks Runtime  \nRemoves the file entries from the transaction log of a Delta table that can no longer be found in the underlying file system. This can happen when these files have been manually deleted.  \nSyntax\nSyntax\nFSCK REPAIR TABLE table_name [DRY RUN]\n\nParameters\nParameters\ntable_name  \nIdentifies an existing Delta table. The name must not include a temporal specification.  \nDRY RUN  \nShows information about the file entries that would be removed from the transaction log of a Delta table by FSCK REPAIR TABLE, because they can no longer be found in the underlying file system. This can happen when these files have been manually deleted. File entries are either a data file path or a combination of a data file path and deletion vector file path. File entries are included in the output when the data file is missing, when the deletion vector file is missing, or when both are missing.  \nBy default, DRY RUN only returns the first 1000 files. You can increase this threshold by setting the SparkSession variable spark.databricks.delta.fsck.maxNumEntriesInResult to a higher value before running the command in a notebook.\n\nReturns"
    },
    {
        "id": 1227,
        "url": "https://docs.databricks.com/en/sql/language-manual/delta-fsck.html",
        "content": "Returns\nFor DRY RUN A report of the form:  \ndataFilePath STRING NOT NULL  \ndataFileMissing BOOLEAN NOT NULL  \ndeletionVectorPath STRING  \ndeletionVectorFileMissing BOOLEAN NOT NULL\n\nExamples"
    },
    {
        "id": 1228,
        "url": "https://docs.databricks.com/en/sql/language-manual/delta-fsck.html",
        "content": "Examples\n\u2014 Assume file1.parquet is missing and no DV is expected. > FSCK REPAIR TABLE t DRY RUN; dataFilePath dataFileMissing deletionVectorPath deletionVectorFileMissing ------------- --------------- ------------------ ------------------------- file1.parquet true null false \u2014 Assume dv1.bin is missing. > FSCK REPAIR TABLE t DRY RUN; dataFilePath dataFileMissing deletionVectorPath deletionVectorFileMissing ------------- --------------- ------------------ ------------------------- file1.parquet false dv1.bin true"
    },
    {
        "id": 1229,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/ai_summarize.html",
        "content": "ai_summarize function  \nApplies to: Databricks SQL Databricks Runtime  \nPreview  \nThis feature is in Public Preview.  \nIn the preview:  \nThe underlying language model can handle several languages, however these functions are tuned for English.  \nThere is rate limiting for the underlying Foundation Model APIs. See Foundation Model APIs limits to update these limits.  \nThe ai_summarize() function allows you to invoke a state-of-the-art generative AI model to generate a summary of a given text using SQL. This function uses a chat model serving endpoint made available by Databricks Foundation Model APIs.  \nRequirements"
    },
    {
        "id": 1230,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/ai_summarize.html",
        "content": "Requirements\nImportant  \nThe underlying models that might be used at this time are licensed under the Apache 2.0 license or Llama 2 community license. Databricks recommends reviewing these licenses to ensure compliance with any applicable terms. If models emerge in the future that perform better according to Databricks\u2019s internal benchmarks, Databricks may change the model (and the list of applicable licenses provided on this page).  \nCurrently, Mixtral-8x7B Instruct is the underlying model that powers these AI functions.  \nThis function is only available on workspaces in Foundation Model APIs pay-per-token supported regions.  \nThis function is not available on Databricks SQL Classic.  \nCheck the Databricks SQL pricing page.  \nNote  \nIn Databricks Runtime 15.1 and above, this function is supported in Databricks notebooks, including notebooks that are run as a task in a Databricks workflow.\n\nSyntax\nSyntax\nai_summarize(content[, max_words])\n\nArguments"
    },
    {
        "id": 1231,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/ai_summarize.html",
        "content": "Arguments\ncontent: A STRING expression, the text to be summarized.  \nmax_words: An optional non-negative integral numeric expression representing the best-effort target number of words in the returned summary text. The default value is 50. If set to 0, there is no word limit.\n\nReturns\nReturns\nA STRING.  \nIf content is NULL, the result is NULL.\n\nExamples"
    },
    {
        "id": 1232,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/ai_summarize.html",
        "content": "Examples\n> SELECT ai_summarize( 'Apache Spark is a unified analytics engine for large-scale data processing. ' || 'It provides high-level APIs in Java, Scala, Python and R, and an optimized ' || 'engine that supports general execution graphs. It also supports a rich set ' || 'of higher-level tools including Spark SQL for SQL and structured data ' || 'processing, pandas API on Spark for pandas workloads, MLlib for machine ' || 'learning, GraphX for graph processing, and Structured Streaming for incremental ' || 'computation and stream processing.', 20 ) \"Apache Spark is a unified, multi-language analytics engine for large-scale data processing with additional tools for SQL, machine learning, graph processing, and stream computing.\"\n\nRelated functions\nRelated functions\nai_translate function  \nai_fix_grammar function  \nai_query function"
    },
    {
        "id": 1233,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/asterisksign.html",
        "content": "* (asterisk sign) operator  \nApplies to: Databricks SQL Databricks Runtime  \nReturns multiplier multiplied by multiplicand.  \nSyntax\nSyntax\nmultiplier * multiplicand\n\nArguments\nArguments\nmultiplier: A numeric or INTERVAL expression.  \nmultiplicand: A numeric expression or INTERVAL expression.  \nYou may not specify an INTERVAL for both arguments.\n\nReturns"
    },
    {
        "id": 1234,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/asterisksign.html",
        "content": "Returns\nIf both multiplier and multiplicand are DECIMAL, the result is DECIMAL.  \nIf multiplier or multiplicand is an INTERVAL, the result is of the same type.  \nIf both multiplier and multiplicand are integral numeric types, the result is the larger of the two types.  \nIn all other cases the result is a DOUBLE.  \nIf either the multiplier or the multiplicand is 0, the operator returns 0.  \nIf the result of the multiplication is outside the bound for the result type an ARITHMETIC_OVERFLOW error is raised.  \nUse try_multiply to return NULL on overflow.  \nWarning  \nIn Databricks Runtime, if spark.sql.ansi.enabled is false, the result \u201cwraps\u201d if it is out of bounds for integral types, and the result is NULL for fractional types.\n\nExamples\nExamples\n> SELECT 3 * 2; 6 > SELECT 2L * 2L; 4L > SELECT INTERVAL '3' YEAR * 3; 9-0 > SELECT 100Y * 100Y; Error: ARITHMETIC_OVERFLOW\n\nRelated functions"
    },
    {
        "id": 1235,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/asterisksign.html",
        "content": "Related functions\ndiv operator  \n- (minus sign) operator  \n+ (plus sign) operator  \n/ (slash sign) operator  \nsum aggregate function  \ntry_multiply function"
    },
    {
        "id": 1236,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/input_file_block_start.html",
        "content": "input_file_block_start function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the start offset in bytes of the block being read.  \nThis function is not available on Unity Catalog.  \nIn Databricks SQL AND Databricks Runtime 13.3 LTS and above this function is deprecated. Please use _metadata.file_block_start.  \nSyntax\nSyntax\ninput_file_block_start()\n\nArguments\nArguments\nThis function takes no arguments.\n\nReturns\nReturns\nA BIGINT.  \nIf the information is not available -1 is returned.  \nThe function is non-deterministic.\n\nExamples\nExamples\n> SELECT input_file_block_start(); -1\n\nRelated functions\nRelated functions\n_metadata"
    },
    {
        "id": 1237,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/greatest.html",
        "content": "greatest function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the greatest value of all arguments, skipping null values.  \nSyntax\nSyntax\ngreatest(expr1, expr2 [, ...])\n\nArguments\nArguments\nexprN: Any expression of a comparable type with a shared least common type across all exprN.\n\nReturns\nReturns\nThe result type is the least common type of the arguments.\n\nExamples\nExamples\n> SELECT greatest(10, 9, 2, 4, 3); 10 > DESCRIBE SELECT greatest(10, 9e2, 2.2) AS greatest; greatest double\n\nRelated\nRelated\nleast function  \nSQL data type rules"
    },
    {
        "id": 1238,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/approx_top_k.html",
        "content": "approx_top_k aggregate function  \nApplies to: Databricks SQL Databricks Runtime 10.4 LTS and above  \nReturns the top k most frequently occurring item values in an expr along with their approximate counts.  \nSyntax\nSyntax\napprox_top_k(expr[, k[, maxItemsTracked]]) [FILTER ( WHERE cond ) ]  \nThis function can also be invoked as a window function using the OVER clause.\n\nArguments\nArguments\nexpr: An expression of STRING, BOOLEAN, DATE, TIMESTAMP, or numeric type.  \nk: An optional INTEGER literal greater than 0. If k is not specified, it defaults to 5.  \nmaxItemsTracked: An optional INTEGER literal greater than or equal to k. If maxItemsTracked is not specified, it defaults to 10000.  \ncond: An optional boolean expression filtering the rows used for aggregation.\n\nReturns"
    },
    {
        "id": 1239,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/approx_top_k.html",
        "content": "Returns\nResults are returned as an ARRAY of type STRUCT, where each STRUCT contains an item field for the value (with its original input type) and a count field (of type LONG) with the approximate number of occurrences. The array is sorted by count descending.  \nThe aggregate function returns the top k most frequently occurring item values in an expression expr along with their approximate counts. The error in each count may be up to 2.0 * numRows / maxItemsTracked where numRows is the total number of rows. Higher values of maxItemsTracked provide better accuracy at the cost of increased memory usage. Expressions that have fewer than maxItemsTracked distinct items will yield exact item counts. Results include NULL values as their own item in the results.\n\nExamples"
    },
    {
        "id": 1240,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/approx_top_k.html",
        "content": "Examples\n> SELECT approx_top_k(expr) FROM VALUES (0), (0), (1), (1), (2), (3), (4), (4) AS tab(expr); [{'item':4,'count':2},{'item':1,'count':2},{'item':0,'count':2},{'item':3,'count':1},{'item':2,'count':1}] > SELECT approx_top_k(expr, 2) FROM VALUES 'a', 'b', 'c', 'c', 'c', 'c', 'd', 'd' AS tab(expr); [{'item':'c','count',4},{'item':'d','count':2}] > SELECT approx_top_k(expr, 10, 100) FROM VALUES (0), (1), (1), (2), (2), (2) AS tab(expr); [{'item':2,'count':3},{'item':1,'count':2},{'item':0,'count':1}]"
    },
    {
        "id": 1241,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/approx_top_k.html",
        "content": "Related functions\nRelated functions\napprox_count_distinct aggregate function  \napprox_percentile aggregate function"
    },
    {
        "id": 1242,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/array_distinct.html",
        "content": "array_distinct function  \nApplies to: Databricks SQL Databricks Runtime  \nRemoves duplicate values from array.  \nSyntax\nSyntax\narray_distinct(array)\n\nArguments\nArguments\narray: An ARRAY expression.\n\nReturns\nReturns\nThe function returns an array of the same type as the input argument where all duplicate values have been removed.\n\nExamples\nExamples\n> SELECT array_distinct(array(1, 2, 3, NULL, 3)); [1,2,3,NULL]\n\nRelated functions\nRelated functions\narray_except function  \narray_intersect function  \narray_sort function  \narray_remove function  \narray_union function"
    },
    {
        "id": 1243,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/approx_percentile.html",
        "content": "approx_percentile aggregate function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the approximate percentile of the expr within the group.  \nSyntax\nSyntax\napprox_percentile ( [ALL | DISTINCT] expr, percentile [, accuracy] ) [ FILTER ( WHERE cond ) ]  \nThis function can also be invoked as a window function using the OVER clause.\n\nArguments\nArguments\nexpr: A numeric expression.  \npercentile: A numeric literal between 0 and 1 or a literal array of numeric values, each between 0 and 1.  \naccuracy: An INTEGER literal greater than 0. If accuracy is omitted it is set to 10000.  \ncond: An optional boolean expression filtering the rows used for aggregation.\n\nReturns"
    },
    {
        "id": 1244,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/approx_percentile.html",
        "content": "Returns\nThe aggregate function returns the expression that is the smallest value in the ordered group (sorted from least to greatest) such that no more than percentile of expr values is less than the value or equal to that value.  \nIf percentile is an array, approx_percentile returns the approximate percentile array of expr at percentile. The accuracy parameter controls approximation accuracy at the cost of memory. A higher value of accuracy yields better accuracy, 1.0/accuracy is the relative error of the approximation. This function is a synonym for percentile_approx aggregate function.  \nIf DISTINCT is specified the function operates only on a unique set of expr values.\n\nExamples"
    },
    {
        "id": 1245,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/approx_percentile.html",
        "content": "Examples\n> SELECT approx_percentile(col, array(0.5, 0.4, 0.1), 100) FROM VALUES (0), (1), (2), (10) AS tab(col); [1,1,0] > SELECT approx_percentile(col, 0.5, 100) FROM VALUES (0), (6), (6), (7), (9), (10) AS tab(col); 6 > SELECT approx_percentile(DISTINCT col, 0.5, 100) FROM VALUES (0), (6), (6), (7), (9), (10) AS tab(col); 7\n\nRelated\nRelated\napprox_count_distinct aggregate function  \napprox_top_k aggregate function  \nhistogram_numeric aggregate function  \npercentile aggregate function  \npercentile_approx aggregate function  \nWindow functions"
    },
    {
        "id": 1246,
        "url": "https://docs.databricks.com/en/sql/language-manual/data-types/tinyint-type.html",
        "content": "TINYINT type  \nApplies to: Databricks SQL Databricks Runtime  \nRepresents 1-byte signed integer numbers.  \nSyntax\nSyntax\n{ TINYINT | BYTE }\n\nLimits\nLimits\nThe range of numbers is from -128 to 127.\n\nLiterals\nLiterals\n[ + | - ] digit [ ... ] Y  \ndigit: Any numeral from 0 to 9.  \nThe Y postfix is case insensitive.\n\nExamples\nExamples\n> SELECT +1Y; 1 > SELECT CAST('5' AS TINYINT); 5\n\nRelated\nRelated\nSMALLINT type  \nINT type  \nBIGINT type  \nDECIMAL type  \nFLOAT type  \nDOUBLE type  \ncast function"
    },
    {
        "id": 1247,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/dense_rank.html",
        "content": "dense_rank ranking window function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the rank of a value compared to all values in the partition.  \nSyntax\nSyntax\ndense_rank()\n\nArguments\nArguments\nThis function takes no arguments.\n\nReturns\nReturns\nAn INTEGER.  \nThe OVER clause of the window function must include an ORDER BY clause. Unlike the function rank ranking window function, dense_rank will not produce gaps in the ranking sequence. Unlike row_number ranking window function, dense_rank does not break ties. If the order is not unique the duplicates share the same relative later position.\n\nExamples\nExamples\n> SELECT a, b, dense_rank() OVER(PARTITION BY a ORDER BY b), rank() OVER(PARTITION BY a ORDER BY b), row_number() OVER(PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b); A1 1 1 1 1 A1 1 1 1 2 A1 2 2 3 3 A2 3 1 1 1\n\nRelated functions"
    },
    {
        "id": 1248,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/dense_rank.html",
        "content": "Related functions\nrank ranking window function  \nrow_number ranking window function  \ncume_dist analytic window function  \nWindow functions"
    },
    {
        "id": 1249,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/charindex.html",
        "content": "charindex function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the position of the first occurrence of substr in str after position pos. This function is a synonym for locate function.  \nSyntax\nSyntax\ncharindex(substr, str [, pos])\n\nArguments\nArguments\nsubstr: A STRING expression.  \nstr: A STRING expression.  \npos: An INTEGER expression.\n\nReturns\nReturns\nAn INTEGER.  \nThe specified pos and return value are 1-based. If pos is omitted, substr is searched from the beginning of str. If pos is less than 1, the result is 0.\n\nExamples\nExamples\n> SELECT charindex('bar', 'abcbarbar'); 4 > SELECT charindex('bar', 'abcbarbar', 5); 7\n\nRelated functions\nRelated functions\nposition function  \ninstr function  \nlocate function"
    },
    {
        "id": 1250,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/ampersandsign.html",
        "content": "& (ampersand sign) operator  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the bitwise AND of expr1 and expr2.  \nSyntax\nSyntax\nexpr1 & expr2\n\nArguments\nArguments\nexpr1: An integral numeric type expression.  \nexpr2: An integral numeric type expression.\n\nReturns\nReturns\nThe result type matches the widest type of expr1 and expr2.\n\nExamples\nExamples\n> SELECT 3 & 5; 1\n\nRelated functions\nRelated functions\n| (pipe sign) operator  \n~ (tilde sign) operator  \n^ (caret sign) operator  \nbit_count function"
    },
    {
        "id": 1251,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/aggregate.html",
        "content": "aggregate function  \nApplies to: Databricks SQL Databricks Runtime  \nAggregates elements in an array using a custom aggregator. This function is a synonym for reduce function.  \nSyntax\nSyntax\naggregate(expr, start, merge [, finish])\n\nArguments\nArguments\nexpr: An ARRAY expression.  \nstart: An initial value of any type.  \nmerge: A lambda function used to aggregate the current element.  \nfinish: An optional lambda function used to finalize the aggregation.\n\nReturns\nReturns\nThe result type matches the result type of the finish lambda function if exists or start.  \nApplies an expression to an initial state and all elements in the array, and reduces this to a single state. The final state is converted into the final result by applying a finish function.  \nThe merge function takes two parameters. The first being the accumulator, the second the element to be aggregated. The accumulator and the result must be of the type of start. The optional finish function takes one parameter and returns the final result.\n\nExamples"
    },
    {
        "id": 1252,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/aggregate.html",
        "content": "Examples\n> SELECT aggregate(array(1, 2, 3), 0, (acc, x) -> acc + x); 6 > SELECT aggregate(array(1, 2, 3), 0, (acc, x) -> acc + x, acc -> acc * 10); 60 > SELECT aggregate(array(1, 2, 3, 4), named_struct('sum', 0, 'cnt', 0), (acc, x) -> named_struct('sum', acc.sum + x, 'cnt', acc.cnt + 1), acc -> acc.sum / acc.cnt) AS avg 2.5\n\nRelated functions\nRelated functions\narray function  \nreduce function"
    },
    {
        "id": 1253,
        "url": "https://docs.databricks.com/en/sparkr/renv.html",
        "content": "renv on Databricks  \nrenv is an R package that lets users manage R dependencies specific to the notebook.  \nUsing renv, you can create and manage the R library environment for your project, save the state of these libraries to a lockfile, and later restore libraries as required. Together, these tools can help make projects more isolated, portable, and reproducible.  \nBasic renv workflow"
    },
    {
        "id": 1254,
        "url": "https://docs.databricks.com/en/sparkr/renv.html",
        "content": "Basic renv workflow\nIn this section:  \nInstall renv  \nInitialize renv session with pre-installed R libraries  \nUse renv to install additional packages  \nUse renv to save your R notebook environment to DBFS  \nReinstall a renv environment given a lockfile from DBFS  \nInstall renv  \nYou can install renv as a cluster-scoped library or as a notebook-scoped library. To install renv as a notebook-scoped library, use:  \nrequire(devtools) install_version( package = \"renv\", repos = \"http://cran.us.r-project.org\" )  \nDatabricks recommends using a CRAN snapshot as the repository to fix the package version.  \nInitialize renv session with pre-installed R libraries  \nThe first step when using renv is to initialize a session using renv::init(). Set libPaths to change the default download location to be your R notebook-scoped library path.  \nrenv::init(settings = list(external.libraries=.libPaths())) .libPaths(c(.libPaths()[2], .libPaths())  \nUse renv to install additional packages  \nYou can now use renv\u2019s API to install and remove R packages. For example, to install the latest version of digest, run the following inside of a notebook cell.  \nrenv::install(\"digest\")  \nTo install an old version of digest, run the following inside of a notebook cell.  \nrenv::install(\"digest@0.6.18\")  \nTo install digest from GitHub, run the following inside of a notebook cell.  \nrenv::install(\"eddelbuettel/digest\")  \nTo install a package from Bioconductor, run the following inside of a notebook cell.  \n# (note: requires the BiocManager package) renv::install(\"bioc::Biobase\")  \nNote that the renv::install API uses the renv Cache.  \nUse renv to save your R notebook environment to DBFS  \nRun the following command once before saving the environment.  \nrenv::settings$snapshot.type(\"all\")"
    },
    {
        "id": 1255,
        "url": "https://docs.databricks.com/en/sparkr/renv.html",
        "content": "Use renv to save your R notebook environment to DBFS  \nRun the following command once before saving the environment.  \nrenv::settings$snapshot.type(\"all\")  \nThis sets renv to snapshot all packages that are installed into libPaths, not just the ones that are currently used in the notebook. See renv documentation for more information.  \nNow you can run the following inside of a notebook cell to save the current state of your environment.  \nrenv::snapshot(lockfile=\"/dbfs/PATH/TO/WHERE/YOU/WANT/TO/SAVE/renv.lock\", force=TRUE)  \nThis updates the lockfile by capturing all packages installed on libPaths. It also moves your lockfile from the local filesystem to DBFS, where it persists even if your cluster terminates or restarts.  \nReinstall a renv environment given a lockfile from DBFS  \nFirst, make sure that your new cluster is running an identical Databricks Runtime version as the one you first created the renv environment on. This ensures that the pre-installed R packages are identical. You can find a list of these in each runtime\u2019s release notes. After you Install renv, run the following inside of a notebook cell.  \nrenv::init(settings = list(external.libraries=.libPaths())) .libPaths(c(.libPaths()[2], .libPaths())) renv::restore(lockfile=\"/dbfs/PATH/TO/WHERE/YOU/SAVED/renv.lock\", exclude=c(\"Rserve\", \"SparkR\"))  \nThis copies your lockfile from DBFS into the local file system and then restores any packages specified in the lockfile.  \nNote  \nTo avoid missing repository errors, exclude the Rserve and SparkR packages from package restoration. Both of these packages are pre-installed in all runtimes."
    },
    {
        "id": 1256,
        "url": "https://docs.databricks.com/en/sparkr/renv.html",
        "content": "renv Cache\nrenv Cache\nA very useful feature of renv is its global package cache, which is shared across all renv projects on the cluster. It speeds up installation times and saves disk space. The renv cache does not cache packages downloaded via the devtools API or install.packages() with any additional arguments other than pkgs."
    },
    {
        "id": 1257,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/ceil.html",
        "content": "ceil function  \nApplies to: Databricks SQL Databricks Runtime 11.3 LTS and above  \nReturns the smallest number not smaller than expr rounded up to targetScale digits relative to the decimal point. This function is a synonym of ceiling function.  \nSyntax\nSyntax\nceil(expr [, targetScale])\n\nArguments\nArguments\nexpr: An expression that evaluates to a numeric.  \ntargetScale: An optional INTEGER literal greater than -38 specifying by how many digits after the decimal points to round up.\n\nReturns"
    },
    {
        "id": 1258,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/ceil.html",
        "content": "Returns\nIf no targetScale is given:  \nIf expr is DECIMAL(p, s), returns DECIMAL(p - s + 1, 0).  \nFor all other cases, returns a BIGINT.  \nIf targetScale is specified and expr is a:  \nTINYINT  \nReturns a DECIMAL(p, 0) with p = max(3, -targetScale + 1).  \nSMALLINT  \nReturns a DECIMAL(p, 0) with p = max(5, -targetScale + 1).  \nINTEGER  \nReturns a DECIMAL(p, 0) with p = max(10, -targetScale + 1)).  \nBIGINT  \nReturns a DECIMAL(p, 0) with p = max(20, -targetScale + 1)).  \nFLOAT  \nReturns a DECIMAL(p, s) with p = max(14, -targetScale + 1)) and s = min(7, max(0, targetScale))  \nDOUBLE  \nReturns a DECIMAL(p, s) with p = max(30, -targetScale + 1)) and s = min(15, max(0, targetScale))  \nDECIMAL(p_in, s_in)  \nReturns a DECIMAL(p, s) with p = max(p_in - s_in + 1, -targetScale + 1)) and s = min(s_in, max(0, targetScale))  \nIf targetScale is negative the rounding occurs to -targetScale digits to the left of the decimal point.  \nThe default targetScale is 0, which rounds up to the next bigger integral number."
    },
    {
        "id": 1259,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/ceil.html",
        "content": "Examples\nExamples\n> SELECT ceil(-0.1); 0 > SELECT ceil(5); 5 > SELECT ceil(5.4); 6 > SELECT ceil(3345.1, -2); 3400 > SELECT ceil(-12.345, 1); -12.3\n\nRelated functions\nRelated functions\nfloor function  \nceiling function  \nbround function  \nround function"
    },
    {
        "id": 1260,
        "url": "https://docs.databricks.com/en/semi-structured/complex-types.html",
        "content": "Transform complex data types  \nWhile working with nested data types, Databricks optimizes certain transformations out-of-the-box. The following code examples demonstrate patterns for working with complex and nested data types in Databricks.  \nDot notation for accessing nested data\nDot notation for accessing nested data\nYou can use dot notation (.) to access a nested field.  \ndf.select(\"column_name.nested_field\")  \nSELECT column_name.nested_field FROM table_name\n\nSelect all nested fields\nSelect all nested fields\nUse the star operator (*) to select all fields within a given field.  \nNote  \nThis only unpacks nested fields at the specified depth.  \ndf.select(\"column_name.*\")  \nSELECT column_name.* FROM table_name\n\nCreate a new nested field"
    },
    {
        "id": 1261,
        "url": "https://docs.databricks.com/en/semi-structured/complex-types.html",
        "content": "Create a new nested field\nUse the struct() function to create a new nested field.  \nfrom pyspark.sql.functions import struct, col df.select(struct(col(\"field_to_nest\").alias(\"nested_field\")).alias(\"column_name\"))  \nSELECT struct(field_to_nest AS nested_field) AS column_name FROM table_name\n\nNest all fields into a column\nNest all fields into a column\nUse the star operator (*) to nest all fields from a data source as a single column.  \nfrom pyspark.sql.functions import struct df.select(struct(\"*\").alias(\"column_name\"))  \nSELECT struct(*) AS column_name FROM table_name\n\nSelect a named field from a nested column\nSelect a named field from a nested column\nUse square brackets [] to select nested fields from a column.  \nfrom pyspark.sql.functions import col df.select(col(\"column_name\")[\"field_name\"])  \nSELECT column_name[\"field_name\"] FROM table_name"
    },
    {
        "id": 1262,
        "url": "https://docs.databricks.com/en/semi-structured/complex-types.html",
        "content": "Explode nested elements from a map or array\nExplode nested elements from a map or array\nUse the explode() function to unpack values from ARRAY and MAP type columns.  \nARRAY columns store values as a list. When unpacked with explode(), each value becomes a row in the output.  \nfrom pyspark.sql.functions import explode df.select(explode(\"array_name\").alias(\"column_name\"))  \nSELECT explode(array_name) AS column_name FROM table_name  \nMAP columns store values as ordered key-value pairs. When unpacked with explode(), each key becomes a column and values become rows.  \nfrom pyspark.sql.functions import explode df.select(explode(\"map_name\").alias(\"column1_name\", \"column2_name\"))  \nSELECT explode(map_name) AS (column1_name, column2_name) FROM table_name\n\nCreate an array from a list or set"
    },
    {
        "id": 1263,
        "url": "https://docs.databricks.com/en/semi-structured/complex-types.html",
        "content": "Create an array from a list or set\nUse the functions collect_list() or collect_set() to transform the values of a column into an array. collect_list() collects all values in the column, while collect_set() collects only unique values.  \nNote  \nSpark does not guarantee the order of items in the array resulting from either operation.  \nfrom pyspark.sql.functions import collect_list, collect_set df.select(collect_list(\"column_name\").alias(\"array_name\")) df.select(collect_set(\"column_name\").alias(\"set_name\"))  \nSELECT collect_list(column_name) AS array_name FROM table_name; SELECT collect_set(column_name) AS set_name FROM table_name;\n\nSelect a column from a map in an array"
    },
    {
        "id": 1264,
        "url": "https://docs.databricks.com/en/semi-structured/complex-types.html",
        "content": "Select a column from a map in an array\nYou can also use dot notation (.) to access fields in maps that are contained within an array. This returns an array of all values for the specified field.  \nConsider the following data structure:  \n{ \"column_name\": [ {\"field1\": 1, \"field2\":\"a\"}, {\"field1\": 2, \"field2\":\"b\"} ] }  \nYou can return the values from field1 as an array with the following query:  \ndf.select(\"column_name.field1\")  \nSELECT column_name.field1 FROM table_name\n\nTransform nested data to JSON"
    },
    {
        "id": 1265,
        "url": "https://docs.databricks.com/en/semi-structured/complex-types.html",
        "content": "Transform nested data to JSON\nUse the to_json function to convert a complex data type to JSON.  \nfrom pyspark.sql.functions import to_json df.select(to_json(\"column_name\").alias(\"json_name\"))  \nSELECT to_json(column_name) AS json_name FROM table_name  \nTo encode all contents of a query or DataFrame, combine this with struct(*).  \nfrom pyspark.sql.functions import to_json, struct df.select(to_json(struct(\"*\")).alias(\"json_name\"))  \nSELECT to_json(struct(*)) AS json_name FROM table_name  \nNote  \nDatabricks also supports to_avro and to_protobuf for transforming complex data types for interoperability with integrated systems.\n\nTransform JSON data to complex data"
    },
    {
        "id": 1266,
        "url": "https://docs.databricks.com/en/semi-structured/complex-types.html",
        "content": "Transform JSON data to complex data\nUse the from_json function to convert JSON data to native complex data types.  \nNote  \nYou must specify the schema for the JSON data.  \nfrom pyspark.sql.functions import from_json schema = \"column1 STRING, column2 DOUBLE\" df.select(from_json(\"json_name\", schema).alias(\"column_name\"))  \nSELECT from_json(json_name, \"column1 STRING, column2 DOUBLE\") AS column_name FROM table_name\n\nNotebook: transform complex data types\nNotebook: transform complex data types\nThe following notebooks provide examples for working with complex data types for Python, Scala, and SQL.  \nTransforming complex data types Python notebook  \nOpen notebook in new tab Copy link for import  \nTransforming complex data types Scala notebook  \nOpen notebook in new tab Copy link for import  \nTransforming complex data types SQL notebook  \nOpen notebook in new tab Copy link for import"
    },
    {
        "id": 1267,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/bool_or.html",
        "content": "bool_or aggregate function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns true if at least one value in expr is true within the group.  \nThe bool_or aggregate function is synonymous with any aggregate function.  \nSyntax\nSyntax\nbool_or(expr) [FILTER ( WHERE cond ) ]  \nThis function can also be invoked as a window function using the OVER clause.\n\nArguments\nArguments\nexpr: A BOOLEAN expression.  \ncond: An optional boolean expression filtering the rows used for aggregation.\n\nReturns\nReturns\nA BOOLEAN.\n\nExamples\nExamples\n> SELECT bool_or(col) FROM VALUES (true), (false), (false) AS tab(col); true > SELECT bool_or(col) FROM VALUES (NULL), (true), (false) AS tab(col); true > SELECT bool_or(col) FROM VALUES (false), (false), (NULL) AS tab(col); false\n\nRelated\nRelated\nbool_and aggregate function  \nevery aggregate function  \nsome aggregate function  \nany aggregate function  \nWindow functions"
    },
    {
        "id": 1268,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/add_months.html",
        "content": "add_months function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the date that is numMonths after startDate.  \nSyntax\nSyntax\nadd_months(startDate, numMonths)\n\nArguments\nArguments\nstartDate: A DATE expression.  \nnumMonths: An integral number.\n\nReturns\nReturns\nA DATE. If the result exceeds the number of days of the month the result is rounded down to the end of the month. If the result exceeds the supported range for a date an overflow error is reported.\n\nExamples\nExamples\n> SELECT add_months('2016-08-31', 1); 2016-09-30 > SELECT add_months('2016-08-31', -6); 2016-02-29\n\nRelated functions\nRelated functions\ndateadd function  \ndatediff (timestamp) function  \nmonths_between function"
    },
    {
        "id": 1269,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/from_unixtime.html",
        "content": "from_unixtime function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns unixTime in fmt.  \nSyntax\nSyntax\nfrom_unixtime(unixTime [, fmt])\n\nArguments\nArguments\nunixTime: A BIGINT expression representing seconds elapsed since 1969-12-31 at 16:00:00.  \nfmt: An optional STRING expression with a valid format.\n\nReturns\nReturns\nA STRING.  \nSee Datetime patterns for valid formats. The \u2018yyyy-MM-dd HH:mm:ss\u2019 pattern is used if omitted.\n\nExamples\nExamples\n> SELECT from_unixtime(0, 'yyyy-MM-dd HH:mm:ss'); 1969-12-31 16:00:00 > SELECT from_unixtime(0); 1969-12-31 16:00:00\n\nRelated functions\nRelated functions\nto_unix_timestamp function  \nDatetime patterns"
    },
    {
        "id": 1270,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/any.html",
        "content": "any aggregate function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns true if at least one value of expr in the group is true. The any aggregate function is synonymous with max aggregate function, but limited to a boolean argument. The function is also a synonym for bool_or aggregate function.  \nSyntax\nSyntax\nany(expr) [FILTER ( WHERE cond ) ]  \nThis function can also be invoked as a window function using the OVER clause.\n\nArguments\nArguments\nexpr: A BOOLEAN expression.  \ncond: An optional BOOLEAN expression filtering the rows used for aggregation.\n\nReturns\nReturns\nA BOOLEAN.\n\nExamples"
    },
    {
        "id": 1271,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/any.html",
        "content": "Examples\n> SELECT any(col) FROM VALUES (true), (false), (false) AS tab(col); true > SELECT any(col) FROM VALUES (NULL), (true), (false) AS tab(col); true > SELECT any(col) FROM VALUES (false), (false), (NULL) AS tab(col); false > SELECT any(col1) FILTER (WHERE col2 = 1) FROM VALUES (false, 1), (false, 2), (true, 2), (NULL, 1) AS tab(col1, col2); false\n\nRelated\nRelated\nbool_or aggregate function  \nmax aggregate function  \nmin aggregate function  \nsome aggregate function  \nWindow functions"
    },
    {
        "id": 1272,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/hypot.html",
        "content": "hypot function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns sqrt(expr1 * expr1 + expr2 * expr2).  \nSyntax\nSyntax\nhypot(expr1, expr2)\n\nArguments\nArguments\nexpr1: An expression that evaluates to a numeric.  \nexpr2: An expression that evaluates to a numeric.\n\nReturns\nReturns\nA DOUBLE.\n\nExamples\nExamples\n> SELECT hypot(3, 4); 5.0\n\nRelated functions\nRelated functions\ncbrt function  \nsqrt function"
    },
    {
        "id": 1273,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/h3_tessellateaswkb.html",
        "content": "h3_tessellateaswkb function  \nApplies to: Databricks SQL Databricks Runtime 14.2 and above  \nReturns a tessellation of the input geography using H3 cells at the specified resolution. The tessellation is represented by an ARRAY of structs, each representing an element of the tessellation. Each element of the tessellation consists of a H3 cell ID (represented as a long integer), a Boolean value indicating whether the input geography fully covers the cell, and a BINARY value corresponding to the WKB description of the intersection of the input geography with the H3 cell.  \nThe returned H3 cells collectively form a minimal covering set of hexagons or pentagons, at the specified resolution, that fully cover the input geography. The geographies returned (via their WKB representations) form a tessellation of the input geography.  \nSyntax\nSyntax\nh3_tessellateaswkb ( geographyExpr, resolutionExpr )\n\nArguments"
    },
    {
        "id": 1274,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/h3_tessellateaswkb.html",
        "content": "Arguments\ngeographyExpr: A BINARY or STRING expression representing a geography in WKB, WKT, or GeoJSON. The geography is expected to have longitude and latitude coordinates in degrees that refer to the WGS84 coordinate reference system.  \nresolutionExpr: An INT expression, with a value between 0 and 15 inclusive, specifying the resolution for the H3 cell IDs.\n\nReturns"
    },
    {
        "id": 1275,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/h3_tessellateaswkb.html",
        "content": "Returns\nAn ARRAY of named structs with three fields of type BIGINT, BOOLEAN, and BINARY, named cellid, core, and chip, respectively, representing the tessellation of the input geography with H3 cells at the specified resolution.  \nThe first field in the struct is an H3 cell ID (represented as a long integer). The second field in the struct is a Boolean value indicating whether the H3 cell is a core cell, in which case the field\u2019s value is true or not, in which case the field\u2019s value is false. A core cell is an H3 cell that is fully covered by the input geography (that is, its intersection with the input geography is the cell itself). The third field is a BINARY value representing the WKB description of the geography that is the intersection of the input geography and the H3 cell. The returned H3 cells collectively form a minimal covering set of the input geography. The geographies, corresponding to the returned WKB descriptions, collectively form a tessellation (decomposition) of the input geography.  \nThe function returns NULL if any of the input expressions is NULL. If the first input argument is of type BINARY, the input value is expected to be the WKB description of a point, linestring, polygon, multipoint, multilinestring, or multipolygon. If the first input argument is of type STRING, the input value is expected to be either the WKT or the GeoJSON description of a point, linestring, polygon, multipoint, multilinestring, or multipolygon. The dimension of the input geography can be 2D, 3DZ, 3DM, or 4D."
    },
    {
        "id": 1276,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/h3_tessellateaswkb.html",
        "content": "Error conditions\nError conditions\nIf geographyExpr is of type BINARY and the value is either an invalid WKB or represents a geometry collection, the function returns WKB_PARSE_ERROR.  \nIf geographyExpr is of type STRING and the value is either an invalid WKT or represents a geometry collection, the function returns WKT_PARSE_ERROR.  \nIf geographyExpr is of type STRING and the value is either an invalid GeoJSON or represents a geometry collection, the function returns GEOJSON_PARSE_ERROR.  \nIf resolutionExpr is smaller than 0 or larger than 15, the function returns H3_INVALID_RESOLUTION_VALUE.\n\nExamples"
    },
    {
        "id": 1277,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/h3_tessellateaswkb.html",
        "content": "> SELECT h3_h3tostring(cellid), core, hex(chip) FROM (SELECT inline(h3_tessellateaswkb('MULTIPOINT(20 0,20 10,40 30)', 0))) ORDER BY 1 802dfffffffffff false 010100000000000000000044400000000000003E40 806bfffffffffff false 010400000002000000010100000000000000000034400000000000000000010100000000000000000034400000000000002440 -- Feeding an empty geometry collection in GeoJSON format (geometry collections are not supported). > SELECT h3_tessellateaswkb('{\"type\":\"GeometryCollection\",\"geometries\":[]}', 2) [GEOJSON_PARSE_ERROR] Error parsing GeoJSON: Invalid or unsupported type '\"GeometryCollection\"' at position 9 SQLSTATE: 22023 -- Feeding an invalid WKB (invalid endianness value) > SELECT h3_tessellateaswkb(unhex('020700000000'), 2) [WKB_PARSE_ERROR] Error parsing WKB: Invalid byte order 2 at position 1 SQLSTATE: 22023 -- Feeding an invalid polygon in WKT (polygon is not closed) > SELECT h3_tessellateaswkb('POLYGON((-122.4194 37.7749,-118.2437 34.0522,-74.0060 40.7128,-74.0060 40.7128))', 2) [WKT_PARSE_ERROR] Error parsing WKT: Found non-closed ring at position 80 SQLSTATE: 22023 -- Resolution is out of range. > SELECT h3_tessellateaswkb('POLYGON((-122.4194"
    },
    {
        "id": 1278,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/h3_tessellateaswkb.html",
        "content": "Found non-closed ring at position 80 SQLSTATE: 22023 -- Resolution is out of range. > SELECT h3_tessellateaswkb('POLYGON((-122.4194 37.7749,-118.2437 34.0522,-74.0060 40.7128,-122.4194 37.7749))', 16) [H3_INVALID_RESOLUTION_VALUE] H3 resolution 16 must be between 0 and 15, inclusive SQLSTATE: 22023"
    },
    {
        "id": 1279,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/h3_tessellateaswkb.html",
        "content": "Related functions\nRelated functions\nh3_coverash3 function  \nh3_coverash3string function  \nh3_polyfillash3 function  \nh3_polyfillash3string function  \nh3_try_polyfillash3 function  \nh3_try_polyfillash3string function"
    },
    {
        "id": 1280,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/stack.html",
        "content": "stack table-valued generator function  \nApplies to: Databricks SQL Databricks Runtime  \nSeparates expr1, \u2026, exprN into numRows rows.  \nSyntax\nSyntax\nstack(numRows, expr1 [, ...] )\n\nArguments\nArguments\nnumRows: An INTEGER literal greater than 0 specifying the number of rows produced.  \nexprN: An expression of any type. The type of any exprN must match the type of expr(N+numRows).\n\nReturns"
    },
    {
        "id": 1281,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/stack.html",
        "content": "Returns\nA set of numRows rows which includes max(1, (N/numRows)) columns produced by this function. An incomplete row is padded with NULLs.  \nBy default, the produced columns are named col0, \u2026 col(n-1).  \nstack is equivalent to the `VALUES` clause.  \nApplies to: Databricks Runtime 12.1 and earlier:  \nstack can only be placed in the SELECT list as the root of an expression or following a LATERAL VIEW. When placing the function in the SELECT list there must be no other generator function in the same SELECT list or UNSUPPORTED_GENERATOR.MULTI_GENERATOR is raised.  \nApplies to: Databricks SQL Databricks Runtime 12.2 LTS and above:  \nInvocation from the LATERAL VIEW clause or the SELECT list is deprecated. Instead, invoke stack as a table_reference.\n\nExamples"
    },
    {
        "id": 1282,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/stack.html",
        "content": "Examples\nApplies to: Databricks Runtime 12.1 and earlier:  \n> SELECT 'hello', stack(2, 1, 2, 3) AS (first, second), 'world'; hello 1 2 world hello 3 NULL world > SELECT 'hello', stack(2, 1, 2, 3) AS (first, second), stack(2, 'a', 'b') AS (third) 'world'; Error: UNSUPPORTED_GENERATOR.MULTI_GENERATOR -- Equivalent usage of VALUES > SELECT 'hello', s1.*, s2.*, 'world' FROM VALUES(1, 2), (3, NULL) AS s1(first, second), VALUES('a'), ('b') AS s2(third); hello 1 2 a world hello 3 NULL a world hello 1 2 b world hello 3 NULL b world  \nApplies to: Databricks SQL Databricks Runtime 12.2 LTS and above:  \n> SELECT 'hello', s.*, 'world' FROM stack(2, 1, 2, 3) AS s(first, second); hello 1 2 world hello 3 NULL world > SELECT 'hello', s1.*, s2.*, 'world' FROM stack(2, 1, 2, 3) AS s1(first, second), stack(2, 'a', 'b') AS s2(third); hello 1 2 a world hello 3 NULL a world hello 1 2 b world hello 3 NULL b world"
    },
    {
        "id": 1283,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/stack.html",
        "content": "Related functions\nRelated functions\nexplode table-valued generator function  \nexplode_outer table-valued generator function  \ninline table-valued generator function  \ninline_outer table-valued generator function  \nposexplode_outer table-valued generator function  \nposexplode table-valued generator function"
    },
    {
        "id": 1284,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/unix_timestamp.html",
        "content": "unix_timestamp function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the UNIX timestamp of current or specified time.  \nSyntax\nSyntax\nunix_timestamp([expr [, fmt] ] )\n\nArguments\nArguments\nexpr: An optional DATE, TIMESTAMP, or a STRING expression in a valid datetime format.  \nfmt: An optional STRING expression specifying the format if expr is a STRING.\n\nReturns\nReturns\nA BIGINT.  \nIf no argument is provided the default is the current timestamp. fmt is ignored if expr is a DATE or TIMESTAMP. If expr is a STRING fmt is used to translate the string to a TIMESTAMP before computing the unix timestamp.  \nThe default fmt value is 'yyyy-MM-dd HH:mm:ss'.  \nSee Datetime patterns for valid date and time format patterns.  \nIf fmt or expr are invalid the function raises an error.  \nNote  \nIn Databricks Runtime, if spark.sql.ansi.enabled is false, the function returns NULL instead of an error for malformed timestamps.\n\nExamples"
    },
    {
        "id": 1285,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/unix_timestamp.html",
        "content": "Examples\n> SELECT unix_timestamp(); 1476884637 > SELECT unix_timestamp('2016-04-08', 'yyyy-MM-dd'); 1460041200\n\nRelated functions\nRelated functions\ntimestamp function"
    },
    {
        "id": 1286,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/percentile_cont.html",
        "content": "percentile_cont aggregate function  \nApplies to: Databricks SQL Databricks Runtime 10.4 LTS and above  \nReturns the value that corresponds to the percentile of the provided sortKeys using a continuous distribution model.  \nSyntax\nSyntax\npercentile_cont ( percentile ) WITHIN GROUP (ORDER BY sortKey [ASC | DESC] )  \nThis function can also be invoked as a window function using the OVER clause.\n\nArguments\nArguments\npercentile: A numeric literal between 0 and 1 or a literal array of numeric literals, each between 0 and 1.  \nsortKey: A numeric expression over which the percentile will be computed.  \nASC or DESC: Optionally specify whether the percentile is computed using ascending or descending order. The default is ASC.\n\nReturns\nReturns\nDOUBLE if percentile is numeric, or an ARRAY of DOUBLE if percentile is an ARRAY.  \nThe aggregate function returns the interpolated percentile within the group of sortKeys.\n\nExamples"
    },
    {
        "id": 1287,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/percentile_cont.html",
        "content": "Examples\n-- Return the median, 40%-ile and 10%-ile. > SELECT percentile_cont(array(0.5, 0.4, 0.1)) WITHIN GROUP (ORDER BY col) FROM VALUES (0), (1), (2), (10) AS tab(col); [1.5, 1.2000000000000002, 0.30000000000000004] -- Return the interpolated median. > SELECT percentile_cont(0.50) WITHIN GROUP (ORDER BY col) FROM VALUES (0), (6), (6), (7), (9), (10) AS tab(col); 6.5\n\nRelated\nRelated\npercentile_approx aggregate function  \napprox_count_distinct aggregate function  \npercentile aggregate function  \nWindow functions"
    },
    {
        "id": 1288,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/try_url_decode.html",
        "content": "try_url_decode function  \nApplies to: Databricks Runtime 15.4 and above  \nTranslates a string back from application/x-www-form-urlencoded format, or NULL if the format is incorrect.  \nSyntax\nSyntax\ntry_url_decode(str)\n\nArguments\nArguments\nstr: A STRING expression to decode.\n\nReturns\nReturns\nA STRING.  \nIf the string does not comply with the application/x-www-form-urlencoded format, the function returns NULL. To raise CANNOT_DECODE_URL instead, use the url_decode() function.\n\nExamples"
    },
    {
        "id": 1289,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/try_url_decode.html",
        "content": "Examples\n> SELECT try_url_encode('http://spark.apache.org/path?query=1'); http%3A%2F%2Fspark.apache.org%2Fpath%3Fquery%3D1 > SELECT try_url_decode('http%3A%2F%2Fspark.apache.org%2Fpath%3Fquery%3D1'); http://spark.apache.org/path?query=1 > SELECT try_url_decode('http%3A%2F%2spark.apache.org'); NULL > SELECT url_decode('http%3A%2F%2spark.apache.org'); Error: CANNOT_DECODE_URL > SELECT parse_url('http://spark.apache.org/path?query=1', 'QUERY', 'query'); 1 > SELECT parse_url('http%3A%2F%2Fspark.apache.org%2Fpath%3Fquery%3D1', 'QUERY', 'query'); 1"
    },
    {
        "id": 1290,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/try_url_decode.html",
        "content": "Related functions\nRelated functions\nparse_url function  \nurl_decode function  \nurl_encode function"
    },
    {
        "id": 1291,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/mean.html",
        "content": "mean aggregate function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the mean calculated from values of a group. This function is a synonym for avg aggregate function.  \nSyntax\nSyntax\nmean ( [ALL | DISTINCT] expr ) [FILTER ( WHERE cond ) ]  \nThis function can also be invoked as a window function using the OVER clause.\n\nArguments\nArguments\nexpr: An expression that evaluates to a numeric.  \ncond: An optional boolean expression filtering the rows used for aggregation.\n\nReturns"
    },
    {
        "id": 1292,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/mean.html",
        "content": "Returns\nThe result type is computed as for the arguments:  \nDECIMAL(p, s): The result type is a` DECIMAL(p + 4, s + 4)`. If the maximum precision for DECIMAL is reached, the increase in scale is limited to avoid loss of significant digits.  \nyear-month interval: The result is an INTERVAL YEAR TO MONTH.  \nday-time interval: The result is an INTERVAL DAY TO SECOND.  \nIn all other cases the result is a DOUBLE.  \nNulls within the group are ignored. If a group is empty or consists only of nulls the result is NULL.  \nIf DISTINCT is specified the mean is computed after duplicates have been removed.  \nWarning  \nIn Databricks Runtime, if spark.sql.ansi.enabled is false, an overflow returns NULL instead of an error.\n\nExamples"
    },
    {
        "id": 1293,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/mean.html",
        "content": "Examples\n> SELECT mean(col) FROM VALUES (1), (2), (3) AS tab(col); 2.0 > SELECT mean(DISTINCT col) FROM VALUES (1), (1), (2), (NULL) AS tab(col); 1.5 > SELECT mean(col) FROM VALUES (1), (2), (NULL) AS tab(col); 1.5\n\nRelated functions\nRelated functions\nsum aggregate function  \nmin aggregate function  \nmax aggregate function  \naggregate function  \navg aggregate function"
    },
    {
        "id": 1294,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/percentile.html",
        "content": "percentile aggregate function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the exact percentile value of expr at the specified percentage in a group.  \nSyntax\nSyntax\npercentile ( [ALL | DISTINCT] expr, percentage [, frequency] ) [FILTER ( WHERE cond ) ]  \nThis function can also be invoked as a window function using the OVER clause.\n\nArguments\nArguments\nexpr: An expression that evaluates to a numeric.  \npercentage: A numeric expression between 0 and 1 or an ARRAY of numeric expressions, each between 0 and 1.  \nfrequency: An optional integral number literal greater than 0.  \ncond: An optional boolean expression filtering the rows used for aggregation.\n\nReturns\nReturns\nDOUBLE if percentage is numeric, or an ARRAY of DOUBLE if percentage is an ARRAY.  \nFrequency describes the number of times expr must be counted. A frequency of 10 for a specific value is equivalent to that value appearing 10 times in the window at a frequency of 1. The default frequency is 1.  \nIf DISTINCT is specified the function operates only on a unique set of expr values.\n\nExamples"
    },
    {
        "id": 1295,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/percentile.html",
        "content": "Examples\n> SELECT percentile(col, 0.3) FROM VALUES (0), (10), (10) AS tab(col); 6.0 > SELECT percentile(DISTINCT col, 0.3) FROM VALUES (0), (10), (10) AS tab(col); 3.0 > SELECT percentile(col, 0.3, freq) FROM VALUES (0, 1), (10, 2) AS tab(col, freq); 6.0 > SELECT percentile(col, array(0.25, 0.75)) FROM VALUES (0), (10) AS tab(col); [2.5,7.5]\n\nRelated functions\nRelated functions\napprox_percentile aggregate function  \nhistogram_numeric aggregate function  \npercentile_approx aggregate function"
    },
    {
        "id": 1296,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/monotonically_increasing_id.html",
        "content": "monotonically_increasing_id function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns monotonically increasing 64-bit integers.  \nSyntax\nSyntax\nmonotonically_increasing_id()\n\nArguments\nArguments\nThis function takes no arguments.\n\nReturns\nReturns\nA BIGINT.  \nThe generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\n\nExamples\nExamples\n> SELECT monotonically_increasing_id(); 0\n\nRelated functions\nRelated functions\nuuid function"
    },
    {
        "id": 1297,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/stddev_pop.html",
        "content": "stddev_pop aggregate function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the population standard deviation calculated from the values of a group.  \nSyntax\nSyntax\nstddev_pop ( [ALL | DISTINCT] expr ) [FILTER ( WHERE cond ) ]  \nThis function can also be invoked as a window function using the OVER clause.\n\nArguments\nArguments\nexpr: An expression that evaluates to a numeric.  \ncond: An optional Boolean expression filtering the rows used for aggregation.\n\nReturns\nReturns\nA DOUBLE.  \nIf DISTINCT is specified the function operates only on a unique set of expr values.\n\nExamples\nExamples\n> SELECT stddev_pop(col) FROM VALUES (1), (2), (3), (3) AS tab(col); 0.82915619758885 > SELECT stddev_pop(DISTINCT col) FROM VALUES (1), (2), (3), (3) AS tab(col); 0.816496580927726\n\nRelated"
    },
    {
        "id": 1298,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/stddev_pop.html",
        "content": "Related\nstd aggregate function  \nstddev aggregate function  \nstddev_samp aggregate function  \nWindow functions"
    },
    {
        "id": 1299,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/repeat.html",
        "content": "repeat function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the string that repeats expr n times.  \nSyntax\nSyntax\nrepeat(expr, n)\n\nArguments\nArguments\nexpr: A STRING expression.  \nn: An INTEGER expression.\n\nReturns\nReturns\nA STRING.  \nIf n is less than 1, an empty string.\n\nExamples\nExamples\n> SELECT repeat('123', 2); 123123\n\nRelated functions\nRelated functions\nspace function"
    },
    {
        "id": 1300,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/mask.html",
        "content": "mask function  \nApplies to: Databricks SQL Databricks Runtime 12.2 LTS and above  \nReturns a masked version of the input str.  \nIn Databricks SQL and Databricks Runtime 13.3 LTS and above this function supports named parameter invocation.  \nSyntax\nSyntax\nmask(str [, upperChar [, lowerChar [, digitChar [, otherChar ] ] ] ] )\n\nArguments\nArguments\nstr: A STRING expression.  \nupperChar: A single character STRING literal used to substitute upper case characters. The default is 'X'. If upperChar is NULL, upper case characters remain unmasked.  \nlowerChar: A single character STRING literal used to substitute lower case characters. The default is 'x'. If lowerChar is NULL, lower case characters remain unmasked.  \ndigitChar: A single character STRING literal used to substitute digits. The default is 'n'. If digitChar is NULL, digits remain unmasked.  \notherChar: A single character STRING literal used to substitute any other character. The default is NULL, which leaves these characters unmasked.\n\nReturns\nReturns\nA STRING.\n\nExamples"
    },
    {
        "id": 1301,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/mask.html",
        "content": "Examples\n> SELECT mask('AaBb123-&^ % \uc11c\uc6b8 \u00c4'); XxXxnnn-&^ % \uc11c\uc6b8 X > SELECT mask('AaBb123-&^ % \uc11c\uc6b8 \u00c4', 'Z', 'z', '9', 'X'); ZzZz999XXXXXXXXXZ > SELECT mask('AaBb123-&^ % \uc11c\uc6b8 \u00c4', lowerchar => 'z', otherchar => 'X'); AzBz123XXXXXXXXX\u00c4 > SELECT mask('AaBb123-&^ % \uc11c\uc6b8 \u00c4', otherchar => '?'); AaBb123?????????\u00c4 > SELECT mask('AaBb123-&^ % \uc11c\uc6b8 \u00c4', NULL, NULL, NULL, NULL); AaBb123-&^ % \uc11c\uc6b8 \u00c4\n\nRelated functions\nRelated functions\ncrc32 function  \nhash function  \nsha function  \nsha1 function  \nsha2 function"
    },
    {
        "id": 1302,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/sentences.html",
        "content": "sentences function  \nApplies to: Databricks SQL Databricks Runtime  \nSplits str into an array of array of words.  \nSyntax\nSyntax\nsentences(str [, lang, country] )\n\nArguments\nArguments\nstr: A STRING expression to be parsed.  \nlang: An optional STRING expression with a language code from ISO 639 Alpha-2 (e.g. \u2018DE\u2019), Alpha-3, or a language subtag of up to 8 characters.  \ncountry: An optional STRING expression with a country code from ISO 3166 alpha-2 country code or a UN M.49 numeric-3 area code.\n\nReturns\nReturns\nAn ARRAY of ARRAY of STRING.  \nThe default for lang is en and country US.\n\nExamples\nExamples\n> SELECT sentences('Hi there! Good morning.'); [[Hi, there],[Good, morning]] > SELECT sentences('Hi there! Good morning.', 'en', 'US'); [[Hi, there],[Good, morning]]\n\nRelated functions\nRelated functions\nsplit function"
    },
    {
        "id": 1303,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/lag.html",
        "content": "lag analytic window function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the value of expr from a preceding row within the partition.  \nSyntax\nSyntax\nlag( expr [, offset [, default] ] ) [ IGNORE NULLS | RESPECT NULLS ] OVER clause\n\nArguments\nArguments\nexpr: An expression of any type.  \noffset: An optional INTEGER literal specifying the offset.  \ndefault: An expression of the same type as expr.  \nIGNORE NULLS or RESPECT NULLS: When IGNORE NULLS is specified, any expr value that is NULL is ignored. The default is RESPECT NULLS.  \nOVER clause: The clause describing the windowing. See: Window functions.\n\nReturns"
    },
    {
        "id": 1304,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/lag.html",
        "content": "Returns\nThe result type matches expr.  \nIf offset is positive the value originates from the row preceding the current row by offset specified the ORDER BY in the OVER clause. An offset of 0 uses the current row\u2019s value. A negative offset uses the value from a row following the current row. If you do not specify offset it defaults to 1, the immediately following row.  \nIf there is no row at the specified offset within the partition, the specified default is used. The default default is NULL. You must provide an ORDER BY clause.  \nThis function is a synonym to lead(expr, -offset, default).\n\nExamples\nExamples\n> SELECT a, b, lag(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b); A1 1 NULL A1 1 1 A1 2 1 A2 3 NULL\n\nRelated functions\nRelated functions\nlead analytic window function  \nlast aggregate function  \nlast_value aggregate function  \nfirst_value aggregate function  \nWindow functions"
    },
    {
        "id": 1305,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/overlay.html",
        "content": "overlay function  \nApplies to: Databricks SQL Databricks Runtime  \nReplaces input with replace that starts at pos and is of length len.  \nSyntax\nSyntax\noverlay(input, replace, pos[, len])  \noverlay(input PLACING replace FROM pos [FOR len])\n\nArguments\nArguments\ninput: A STRING or BINARY expression.  \nreplace: An expression of the same type as input.  \npos: An INTEGER expression.  \nlen: An optional INTEGER expression.\n\nReturns\nReturns\nThe result type matches the type of input.  \nIf pos is negative the position is counted starting from the back. len must be greater or equal to 0. len specifies the length of the snippet within input to be replaced. The default for len is the length of replace.\n\nExamples"
    },
    {
        "id": 1306,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/overlay.html",
        "content": "Examples\n> SELECT overlay('Spark SQL', 'ANSI ', 7, 0); Spark ANSI SQL > SELECT overlay('Spark SQL' PLACING '_' FROM 6); Spark_SQL > SELECT overlay('Spark SQL' PLACING 'CORE' FROM 7); Spark CORE > SELECT overlay('Spark SQL' PLACING 'ANSI ' FROM 7 FOR 0); Spark ANSI SQL > SELECT overlay('Spark SQL' PLACING 'tructured' FROM 2 FOR 4); Structured SQL > SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('_', 'utf-8') FROM 6); [53 70 61 72 6B 5F 53 51 4C]\n\nRelated functions\nRelated functions\nreplace function  \nregexp_replace function"
    },
    {
        "id": 1307,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/sqrt.html",
        "content": "sqrt function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the square root of expr.  \nSyntax\nSyntax\nsqrt(expr)\n\nArguments\nArguments\nexpr: An expression that evaluates to a numeric.\n\nReturns\nReturns\nA DOUBLE.  \nIf expr is negative the result is NaN.\n\nExamples\nExamples\n> SELECT sqrt(4); 2.0\n\nRelated functions\nRelated functions\ncbrt function"
    },
    {
        "id": 1308,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/pi.html",
        "content": "pi function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns pi.  \nSyntax\nSyntax\npi()\n\nArguments\nArguments\nThe function takes no argument.\n\nReturns\nReturns\nA DOUBLE.\n\nExamples\nExamples\n> SELECT pi(); 3.141592653589793\n\nRelated functions\nRelated functions\ne function"
    },
    {
        "id": 1309,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/try_subtract.html",
        "content": "try_subtract function  \nApplies to: Databricks SQL Databricks Runtime 10.4 LTS and above  \nReturns the subtraction of expr2 from expr1, or NULL on overflow.  \nSyntax\nSyntax\ntry_subtract ( expr1 , expr2 )\n\nArguments\nArguments\nexpr1: A numeric, DATE, TIMESTAMP, or INTERVAL expression.  \nexpr2: If expr1 is a numeric expr2 must be numeric expression, or an INTERVAL otherwise.\n\nReturns"
    },
    {
        "id": 1310,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/try_subtract.html",
        "content": "Returns\nIf expr1 is a numeric, the common maximum type of the arguments.  \nIf expr1 is a DATE and expr2 is a day-time interval the result is a TIMESTAMP.  \nIf expr1 and expr2 are year-month intervals the result is a year-month interval of sufficiently wide units to represent the result.  \nIf expr1 and expr2 are day-time intervals the result is a day-time interval of sufficiently wide units to represent the result.  \nOtherwise, the result type matches expr1.  \nIf both expressions are interval they must be of the same class.  \nIf the result overflows the result type Databricks SQL returns NULL.  \nWhen you subtract a year-month interval from a DATE Databricks SQL will assure that the resulting date is well formed.\n\nExamples"
    },
    {
        "id": 1311,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/try_subtract.html",
        "content": "Examples\n> SELECT try_subtract(1, 2); -1 > SELECT try_subtract(DATE'2021-03-20', INTERVAL '2' MONTH); 2021-1-20 > SELECT try_subtract(TIMESTAMP'2021-03-20 12:15:29', INTERVAL '3' SECOND); 2021-03-20 12:15:26 > SELECT typeof(try_subtract(INTERVAL '3' DAY, INTERVAL '2' HOUR)); interval day to hour > SELECT try_subtract(DATE'2021-03-31', INTERVAL '1' MONTH); 2021-02-28 > SELECT try_subtract(-128Y, 1Y); NULL\n\nRelated functions\nRelated functions\n- (minus sign) operator  \n/ (slash sign) operator  \n* (asterisk sign) operator  \nsum aggregate function  \ntry_add function  \ntry_divide function  \ntry_multiply function"
    },
    {
        "id": 1312,
        "url": "https://docs.databricks.com/en/sql/language-manual/index.html",
        "content": "SQL language reference  \nThis is a SQL command reference for Databricks SQL and Databricks Runtime.  \nFor information about using SQL with Delta Live Tables, see Delta Live Tables SQL language reference.  \nGeneral reference\nGeneral reference\nThis general reference describes data types, functions, identifiers, literals, and semantics:  \n\"Applies to\" label How to read a syntax diagram How to add comments to SQL statements Configuration parameters Data types and literals Functions SQL data type rules Datetime patterns H3 geospatial functions Lambda functions Window functions Identifiers Names IDENTIFIER clause NULL semantics Expressions Parameter markers Variables Name resolution JSON path expressions Partitions ANSI compliance in Databricks Runtime Apache Hive compatibility Principals Privileges and securable objects in Unity Catalog Privileges and securable objects in the Hive metastore Refresh Unity Catalog metadata External locations External tables Storage credentials Volumes Delta Sharing Federated queries (Lakehouse Federation) Information schema Reserved words\n\nDDL statements"
    },
    {
        "id": 1313,
        "url": "https://docs.databricks.com/en/sql/language-manual/index.html",
        "content": "DDL statements\nYou use data definition statements to create or modify the structure of database objects in a database:  \nALTER CATALOG ALTER CONNECTION ALTER CREDENTIAL ALTER DATABASE ALTER LOCATION ALTER MATERIALIZED VIEW ALTER PROVIDER ALTER RECIPIENT ALTER STREAMING TABLE ALTER TABLE ALTER SCHEMA ALTER SHARE ALTER VIEW ALTER VOLUME COMMENT ON CREATE BLOOMFILTER INDEX CREATE CATALOG CREATE CONNECTION CREATE DATABASE CREATE FUNCTION (SQL) CREATE FUNCTION (External) CREATE LOCATION CREATE MATERIALIZED VIEW CREATE RECIPIENT CREATE SCHEMA CREATE SERVER CREATE SHARE CREATE STREAMING TABLE CREATE TABLE CREATE VIEW CREATE VOLUME DECLARE VARIABLE DROP BLOOMFILTER INDEX DROP CATALOG DROP CONNECTION DROP DATABASE DROP CREDENTIAL DROP FUNCTION DROP LOCATION DROP PROVIDER DROP RECIPIENT DROP SCHEMA DROP SHARE DROP TABLE DROP VARIABLE DROP VIEW DROP VOLUME MSCK REPAIR TABLE REFRESH FOREIGN (CATALOG, SCHEMA, or TABLE) REFRESH (MATERIALIZED VIEW or STREAMING TABLE) SYNC TRUNCATE TABLE UNDROP TABLE\n\nDML statements\nDML statements\nYou use data manipulation statements to add, change, or delete data from a Delta Lake table:  \nCOPY INTO DELETE FROM INSERT INTO INSERT OVERWRITE DIRECTORY INSERT OVERWRITE DIRECTORY with Hive format LOAD DATA MERGE INTO UPDATE\n\nData retrieval statements"
    },
    {
        "id": 1314,
        "url": "https://docs.databricks.com/en/sql/language-manual/index.html",
        "content": "Data retrieval statements\nYou use a query to retrieve rows from one or more tables according to the specified clauses. The full syntax and brief description of supported clauses are explained in the Query article. The related SQL statements SELECT and VALUES are also included in this section.  \nQuery SELECT VALUES  \nDatabricks SQL also provides the ability to generate the logical and physical plan for a query using the EXPLAIN statement.  \nEXPLAIN\n\nDelta Lake statements\nDelta Lake statements\nYou use Delta Lake SQL statements to manage tables stored in Delta Lake format:  \nCACHE SELECT CONVERT TO DELTA DESCRIBE HISTORY FSCK REPAIR TABLE GENERATE OPTIMIZE REORG TABLE RESTORE VACUUM  \nFor details on using Delta Lake statements, see What is Delta Lake?.\n\nAuxiliary statements"
    },
    {
        "id": 1315,
        "url": "https://docs.databricks.com/en/sql/language-manual/index.html",
        "content": "Auxiliary statements\nYou use auxiliary statements to collect statistics, manage caching, explore metadata, set configurations, and manage resources:  \nAnalyze statement  \nApache Spark Cache statements  \nDescribe statements  \nShow statements  \nConfiguration, variable management, and misc statements  \nResource management  \nAnalyze statement  \nANALYZE TABLE  \nApache Spark Cache statements  \nApplies to: Databricks Runtime  \nCACHE TABLE CLEAR CACHE REFRESH CACHE REFRESH FUNCTION REFRESH TABLE UNCACHE TABLE  \nDescribe statements  \nDESCRIBE CATALOG DESCRIBE CONNECTION DESCRIBE CREDENTIAL DESCRIBE DATABASE DESCRIBE FUNCTION DESCRIBE LOCATION DESCRIBE PROVIDER DESCRIBE QUERY DESCRIBE RECIPIENT DESCRIBE SCHEMA DESCRIBE SHARE DESCRIBE TABLE DESCRIBE VOLUME  \nShow statements  \nLIST SHOW ALL IN SHARE SHOW CATALOGS SHOW COLUMNS SHOW CONNECTIONS SHOW CREATE TABLE SHOW CREDENTIALS SHOW DATABASES SHOW FUNCTIONS SHOW GROUPS SHOW LOCATIONS SHOW PARTITIONS SHOW PROVIDERS SHOW RECIPIENTS SHOW SCHEMAS SHOW SHARES SHOW SHARES IN PROVIDER SHOW TABLE SHOW TABLES SHOW TABLES DROPPED SHOW TBLPROPERTIES SHOW USERS SHOW VIEWS SHOW VOLUMES  \nConfiguration, variable management, and misc statements  \nEXECUTE IMMEDIATE RESET SET SET TIMEZONE SET VARIABLE USE CATALOG USE DATABASE USE SCHEMA  \nResource management  \nApplies to: Databricks Runtime  \nADD ARCHIVE ADD FILE ADD JAR LIST ARCHIVE LIST FILE LIST JAR  \nApplies to: Databricks SQL Connector  \nGET PUT INTO REMOVE\n\nSecurity statements"
    },
    {
        "id": 1316,
        "url": "https://docs.databricks.com/en/sql/language-manual/index.html",
        "content": "Security statements\nYou use security SQL statements to manage access to data:  \nALTER GROUP CREATE GROUP DENY DROP GROUP GRANT GRANT SHARE REPAIR PRIVILEGES REVOKE REVOKE SHARE SHOW GRANTS SHOW GRANTS ON SHARE SHOW GRANTS TO RECIPIENT  \nFor details about using these statements, see Hive metastore privileges and securable objects (legacy)."
    },
    {
        "id": 1317,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/try_to_binary.html",
        "content": "try_to_binary function  \nApplies to: Databricks SQL preview Databricks Runtime 11.3 LTS and above  \nReturns expr cast to BINARY based on fmt, or NULL if the input is not valid.  \nSyntax\nSyntax\ntry_to_binary(expr [, fmt] )\n\nArguments\nArguments\nexpr: A STRING expression to cast.  \nfmt: A STRING literal describing how to interpret expr.\n\nReturns\nReturns\nA BINARY.\n\nNotes"
    },
    {
        "id": 1318,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/try_to_binary.html",
        "content": "Notes\nIf fmt is supplied, it must be one of (case-insensitive):  \n'HEX'  \nexpr must be a hexadecimal string. Each character must be a hexadecimal digit and there must be an even number of digits. The result is the binary representation of the hexadecimal string.  \nIf expr is not a well-formed hexadecimal value the function returns NULL. Use to_binary to return an error instead.  \n'BASE64'  \nexpr must be a RFC 4648 \u00a74: base64 (standard) encoded string. The result is the decoded binary data.  \n'UTF-8' or 'UTF8'  \nexpr is interpreted as a UTF-8 string. The result is the binary representation of the string.  \nThe default fmt is 'HEX'.\n\nExamples"
    },
    {
        "id": 1319,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/try_to_binary.html",
        "content": "Examples\n> SELECT cast(to_binary('537061726B') AS STRING); Spark > SELECT cast(to_binary('hello', 'hex') AS STRING); Error: CONVERSION_INVALID_INPUT > SELECT cast(try_to_binary('hello', 'hex') AS STRING); NULL > SELECT cast(to_binary('537061726B', 'hex') AS STRING); Spark > SELECT cast(to_binary('U3Bhcms=', 'base64') AS STRING); Spark > SELECT cast(to_binary('U3Bhxcms=', 'base64') AS STRING); Error: CONVERSION_INVALID_INPUT > SELECT cast(try_to_binary('U3Bhxcms=', 'base64') AS STRING); NULL > SELECT hex(to_binary('\uc11c\uc6b8\uc2dc(Seoul)', 'UTF-8')); EC849CEC9AB8EC8B9C2853656F756C29"
    },
    {
        "id": 1320,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/try_to_binary.html",
        "content": "Related functions\nRelated functions\nbase64 function  \ncast function  \ndecode (character set) function  \nencode function  \nhex function  \nto_binary function  \nunbase64 function  \nunhex function"
    },
    {
        "id": 1321,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/length.html",
        "content": "length function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the character length of string data or number of bytes of binary data. This function is a synonym for character_length function and char_length function.  \nSyntax\nSyntax\nlength(expr)\n\nArguments\nArguments\nexpr: A STRING or BINARY expression.\n\nReturns\nReturns\nAn INTEGER.  \nThe length of string data includes the trailing spaces. The length of binary data includes trailing binary zeros.\n\nExamples\nExamples\n> SELECT length('Spark SQL '); 10 > select length('\u5e8a\u524d\u660e\u6708\u5149') 5\n\nRelated functions\nRelated functions\ncharacter_length function  \nchar_length function"
    },
    {
        "id": 1322,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/skewness.html",
        "content": "skewness aggregate function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the skewness value calculated from values of a group.  \nSyntax\nSyntax\nskewness ( [ALL | DISTINCT ] expr ) [FILTER ( WHERE cond ) ]  \nThis function can also be invoked as a window function using the OVER clause.\n\nArguments\nArguments\nexpr: An expression that evaluates to a numeric.  \ncond: An optional Boolean expression filtering the rows used for aggregation.\n\nReturns\nReturns\nA DOUBLE.  \nIf DISTINCT is specified the function operates only on a unique set of expr values.\n\nExamples"
    },
    {
        "id": 1323,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/skewness.html",
        "content": "Examples\n> SELECT skewness(col) FROM VALUES (-10), (-20), (100), (1000), (1000) AS tab(col); 0.3853941073355022 > SELECT skewness(DISTINCT col) FROM VALUES (-10), (-20), (100), (1000), (1000) AS tab(col); 1.1135657469022011 > SELECT skewness(col) FROM VALUES (-1000), (-100), (10), (20) AS tab(col); -1.1135657469022011\n\nRelated\nRelated\nkurtosis aggregate function  \nWindow functions"
    },
    {
        "id": 1324,
        "url": "https://docs.databricks.com/en/sql/language-manual/information-schema/recipient_allowed_ip_ranges.html",
        "content": "RECIPIENT_ALLOWED_IP_RANGES  \nApplies to: Databricks SQL Databricks Runtime 10.4 LTS and above  \nINFORMATION_SCHEMA.RECIPIENT_ALLOWED_IP_RANGES lists allowed IP ranges for open recipients.  \nInformation is displayed only for recipients the user has permission to interact with.  \nThis is an extension to the SQL Standard Information Schema.  \nDefinition\nDefinition\nThe RECIPIENT_ALLOWED_IP_RANGES relation contains the following columns:  \nName  \nData type  \nNullable  \nDescription  \nRECIPIENT_NAME  \nSTRING  \nNo  \nName of the recipient.  \nALLOWED_IP_RANGE  \nSTRING  \nNo  \nAllowed IP range in IP address or CIDR format.\n\nConstraints\nConstraints\nThe following constraints apply to the RECIPIENT_ALLOWED_IP_RANGES relation:  \nClass  \nName  \nColumn List  \nDescription  \nForeign key  \nRANGES_RECIPIENTS_FK  \nRECIPIENT_NAME  \nReferences RECIPIENTS\n\nExamples\nExamples\n> SELECT recipient_name, allowed_ip_range FROM information_schema.recipient_allowed_ip_ranges\n\nRelated\nRelated\nDESCRIBE RECIPIENT  \nInformation schema  \nINFORMATION_SCHEMA.RECIPIENTS  \nSHOW RECIPIENTS"
    },
    {
        "id": 1325,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/timestampdiff.html",
        "content": "timestampdiff function  \nApplies to: Databricks SQL Databricks Runtime 10.4 LTS and above  \nReturns the difference between two timestamps measured in units.  \nSyntax\nSyntax\ntimestampdiff(unit, start, end) unit { MICROSECOND | MILLISECOND | SECOND | MINUTE | HOUR | DAY | WEEK | MONTH | QUARTER | YEAR }\n\nArguments\nArguments\nunit: A unit of measure.  \nstart: A starting TIMESTAMP expression.  \nend: An ending TIMESTAMP expression.\n\nReturns\nReturns\nA BIGINT.  \nIf start is greater than end the result is negative.  \nThe function counts whole elapsed units based on UTC with a DAY being 86400 seconds.  \nOne month is considered elapsed when the calendar month has increased and the calendar day and time is equal or greater to the start. Weeks, quarters, and years follow from that.\n\nExamples"
    },
    {
        "id": 1326,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/timestampdiff.html",
        "content": "Examples\n-- One second shy of a month elapsed > SELECT timestampdiff(MONTH, TIMESTAMP'2021-02-28 12:00:00', TIMESTAMP'2021-03-28 11:59:59'); 0 -- One month has passed even though its' not end of the month yet because day and time line up. > SELECT timestampdiff(MONTH, TIMESTAMP'2021-02-28 12:00:00', TIMESTAMP'2021-03-28 12:00:00'); 1 -- Start is greater than the end > SELECT timestampdiff(YEAR, DATE'2021-01-01', DATE'1900-03-28'); -120\n\nRelated functions\nRelated functions\nadd_months function  \ndate_add (days) function  \ndate_sub function  \ndatediff function  \ndatediff (timestamp) function  \ntimestamp function  \ntimestampadd function"
    },
    {
        "id": 1327,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/shiftrightunsigned.html",
        "content": "shiftrightunsigned function  \nApplies to: Databricks SQL Databricks Runtime  \nReturns a bitwise unsigned right shifted by n bits.  \nSyntax\nSyntax\nshiftrightunsigned(expr, n)\n\nArguments\nArguments\nexpr: An INTEGER or BIGINT expression.  \nn: An INTEGER expression specifying the number of bits to shift.\n\nReturns\nReturns\nThe result type matches expr.  \nWhen n is negative the result is 0.\n\nExamples\nExamples\n> SELECT shiftrightunsigned(4, 1); 2 > SELECT shiftrightunsigned(-4, 1); 2147483646\n\nRelated functions\nRelated functions\nshiftleft function  \nshiftright function"
    },
    {
        "id": 1328,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/regr_count.html",
        "content": "regr_count aggregate function  \nApplies to: Databricks SQL Databricks Runtime 11.3 LTS and above  \nReturns the number of non-null value pairs yExpr, xExpr in the group.  \nSyntax\nSyntax\nregr_count ( [ALL | DISTINCT] yExpr, xExpr ) [FILTER ( WHERE cond ) ]  \nThis function can also be invoked as a window function using the OVER clause.\n\nArguments\nArguments\nyExpr: A numeric expression, the dependent variable.  \nxExpr: A numeric expression, the independent variable.  \ncond: An optional Boolean expression filtering the rows used for the function.\n\nReturns\nReturns\nA BIGINT.  \nregr_count(yExpr, xExpr) is equivalent to count_if(yExpr IS NOT NULL AND xExpr IS NOT NULL).  \nIf DISTINCT is specified, only unique rows are counted.\n\nExamples"
    },
    {
        "id": 1329,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/regr_count.html",
        "content": "Examples\n> SELECT regr_count(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS t(y, x); 4 > SELECT regr_count(y, x) FROM VALUES (1, 2), (2, NULL), (2, 3), (2, 4) AS t(y, x); 3 > SELECT regr_count(y, x) FROM VALUES (1, 2), (2, NULL), (NULL, 3), (2, 4) AS t(y, x); 2\n\nRelated\nRelated\navg aggregate function  \ncount aggregate function  \ncount_if aggregate function  \nmin aggregate function  \nmax aggregate function  \nregr_avgx aggregate function  \nregr_avgy aggregate function  \nregr_sxx aggregate function  \nregr_sxy aggregate function  \nregr_syy aggregate function  \nsum aggregate function  \nWindow functions"
    },
    {
        "id": 1330,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/split_part.html",
        "content": "split_part function  \nApplies to: Databricks SQL Databricks Runtime 11.3 LTS and above  \nSplits str around occurrences of delim and returns the partNum part.  \nSyntax\nSyntax\nsplit_part(str, delim, partNum)\n\nArguments\nArguments\nstr: A STRING expression to be split.  \ndelimiter: A STRING expression serving as delimiter for the parts.  \npartNum: An INTEGER expression electing the part to be returned.\n\nReturns\nReturns\nA STRING.  \nIf partNum >= 1: The partNums part counting from the beginning of str will be returned.  \nIf partNum <= -1: The abs(partNum)s part counting from the end of str will be returned.  \nIf partNum is beyond the number of parts in str: The function returns an empty string.  \nIf partNum is 0: split_part raises an INVALID_INDEX_OF_ZERO.\n\nExamples"
    },
    {
        "id": 1331,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/split_part.html",
        "content": "Examples\n> SELECT '->' || split_part('Hello,world,!', ',', 1) || '<-'; ->Hello<- > SELECT '->' || split_part('Hello,world,!', ',', 2) || '<-'; ->world<- > SELECT '->' || split_part('Hello,world,!', ',', 100) || '<-'; -><- > SELECT '->' || split_part('Hello,world,!', ',', -2) || '<-'; ->world<- > SELECT '->' || split_part('Hello,world,!', ',', -100) || '<-'; -><- > SELECT '->' || split_part('', ',', 1) || '<-'; -><- > SELECT '->' || split_part('Hello', '', 3) || '<-'; -><- > SELECT '->' || split_part('Hello,World,!', ',', 0) || '<-'; ERROR: INVALID_INDEX_OF_ZERO"
    },
    {
        "id": 1332,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/transform_values.html",
        "content": "transform_values function  \nApplies to: Databricks SQL Databricks Runtime  \nTransforms values in a map in expr using the function func.  \nSyntax\nSyntax\ntransform_values(expr, func)\n\nArguments\nArguments\nexpr: A MAP expression.  \nfunc: A lambda function.\n\nReturns\nReturns\nA MAP where the values have the type of the result of the lambda functions and the keys have the type of the expr MAP keys.  \nThe lambda function must have 2 parameters. The first parameter represents the key. The second parameter represents the value.  \nThe lambda function produces a new value for each entry in the map.\n\nExamples"
    },
    {
        "id": 1333,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/transform_values.html",
        "content": "Examples\n> SELECT transform_values(map_from_arrays(array(1, 2, 3), array(1, 2, 3)), (k, v) -> v + 1); {1 -> 2, 2 -> 3, 3 -> 4} > SELECT transform_values(map_from_arrays(array(1, 2, 3), array(1, 2, 3)), (k, v) -> k + v); {1 -> 2, 2 -> 4, 3 -> 6}\n\nRelated functions\nRelated functions\ntransform function  \ntransform_keys function"
    },
    {
        "id": 1334,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/user.html",
        "content": "user function  \nApplies to: Databricks SQL Databricks Runtime 13.3 LTS and above  \nReturns the user executing the statement. This function is an alias for current_user.  \nNote  \nThe SQL standard differentiates between CURRENT_USER and `SESSION_USER`. In Databricks SQL and Databricks Runtime 14.1 and above you should use SESSION_USER instead of CURRENT_USER or USER.  \nSyntax\nSyntax\nuser()\n\nArguments\nArguments\nThis function takes no arguments.\n\nReturns\nReturns\nA STRING.  \nThe braces are optional.\n\nExamples\nExamples\n> SELECT user(); user1 > SELECT user; user1\n\nRelated functions\nRelated functions\nis_member function  \ncurrent_user function"
    },
    {
        "id": 1335,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/ltgtsign.html",
        "content": "<> (lt gt sign) operator  \nApplies to: Databricks SQL Databricks Runtime  \nReturns true if expr1 does not equal expr2, or false otherwise. This function is a synonym for != (bangeq sign) operator.  \nSyntax\nSyntax\nexpr1 <> expr2\n\nArguments\nArguments\nexpr1: An expression of any comparable type.  \nexpr2: An expression that shares a least common type with expr1.\n\nReturns\nReturns\nA BOOLEAN.\n\nExamples\nExamples\n> SELECT 2 <> 2; false > SELECT 3 <> 2; true > SELECT 1 <> '1'; false > SELECT true <> NULL; NULL > SELECT NULL <> NULL; NULL\n\nRelated\nRelated\n< (lt sign) operator  \n<= (lt eq sign) operator  \n> (gt sign) operator  \n>= (gt eq sign) operator  \n<=> (lt eq gt sign) operator  \n= (eq sign) operator  \n!= (bangeq sign) operator  \nSQL data type rules"
    },
    {
        "id": 1336,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/tinyint.html",
        "content": "tinyint function  \nApplies to: Databricks SQL Databricks Runtime  \nCasts expr to TINYINT. This function is a synonym for CAST(expr AS TINYINT). See cast function for details.  \nSyntax\nSyntax\ntinyint(expr)\n\nArguments\nArguments\nexpr: Any expression which is castable to TINYINT.\n\nReturns\nReturns\nThe result is TINYINT.\n\nExamples\nExamples\n> SELECT tinyint('12'); 12 > SELECT tinyint(5.4); 5\n\nRelated functions\nRelated functions\ncast function"
    },
    {
        "id": 1337,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/read_kafka.html",
        "content": "read_kafka table-valued function  \nApplies to: Databricks SQL Databricks Runtime 13.3 LTS and above  \nReads data from an Apache Kafka cluster and returns the data in tabular form.  \nCan read data from one or more Kafka topics. It supports both batch queries and streaming ingestion.  \nSyntax\nSyntax\nread_kafka([option_key => option_value ] [, ...])\n\nArguments\nArguments\nThis function requires named parameter invocation.  \noption_key: The name of the option to configure. You must use backticks (`) for options that contain dots (.).  \noption_value: A constant expression to set the option. Accepts literals and scalar functions.\n\nReturns"
    },
    {
        "id": 1338,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/read_kafka.html",
        "content": "Returns\nRecords read from an Apache Kafka cluster with the following schema:  \nkey BINARY: The key of the Kafka record.  \nvalue BINARY NOT NULL: The value of the Kafka record.  \ntopic STRING NOT NULL: The name of the Kafka topic the record is read from.  \npartition INT NOT NULL: The ID of the Kafka partition the record is read from.  \noffset BIGINT NOT NULL: The offset number of the record in the Kafka TopicPartition.  \ntimestamp TIMESTAMP NOT NULL: A timestamp value for the record. The timestampType column defines what this timestamp corresponds to.  \ntimestampType INTEGER NOT NULL: The type of the timestamp specified in the timestamp column.  \nheaders ARRAY<STRUCT<key: STRING, VALUE: BINARY>>: Header values provided as part of the record (if enabled).\n\nExamples"
    },
    {
        "id": 1339,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/read_kafka.html",
        "content": "Examples\n-- A batch query to read from a topic. > SELECT value::string as value FROM read_kafka( bootstrapServers => 'kafka_server:9092', subscribe => 'events' ) LIMIT 10; -- A more advanced query with security credentials for Kafka. > SELECT * FROM read_kafka( bootstrapServers => 'kafka_server:9092', subscribe => 'events', startingOffsets => 'earliest', `kafka.security.protocol` => 'SASL_SSL', `kafka.sasl.mechanism` => 'PLAIN', `kafka.sasl.jaas.config` => 'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{USER_NAME}\" password=\"{PASSWORD}\";', ); -- Streaming ingestion from Kafka with JSON parsing. > CREATE OR REFRESH STREAMING TABLE catalog.schema.raw_events AS SELECT value::string:events, -- extract the field `events` to_timestamp(value::string:ts) as ts -- extract the field `ts` and cast to timestamp FROM STREAM read_kafka( bootstrapServers => 'kafka_server:9092', subscribe => 'events' );"
    },
    {
        "id": 1340,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/read_kafka.html",
        "content": "Options\nYou can find a detailed list of options in the Apache Spark documentation.  \nRequired options  \nProvide the option below for connecting to your Kafka cluster.  \nOption  \nbootstrapServers  \nType: String  \nA comma-separated list of host/port pairs pointing to Kafka cluster.  \nDefault value: None  \nProvide only one of the options below to configure which Kafka topics to pull data from.  \nOption  \nassign  \nType: String  \nA JSON string that contains the specific topic-partitions to consume from. For example, for '{\"topicA\":[0,1],\"topicB\":[2,4]}', topicA\u2019s 0\u2019th and 1st partitions will be consumed from.  \nDefault value: None  \nsubscribe  \nType: String  \nA comma-separated list of Kafka topics to read from.  \nDefault value: None  \nsubscribePattern  \nType: String  \nA regular expression matching topics to subscribe to.  \nDefault value: None  \nMiscellaneous options  \nread_kafka can be used in batch queries and in streaming queries. The options below specify which type of query they apply to.  \nOption  \nendingOffsets  \nType: String Query Type: batch only  \nThe offsets to read until for a batch query, either \"latest\" to specify the latest records, or a JSON string specifying an ending offset for each TopicPartition. In the JSON, -1 as an offset can be used to refer to latest. -2 (earliest) as an offset is not allowed.  \nDefault value: \"latest\"  \nendingOffsetsByTimestamp  \nType: String Query Type: batch only  \nA JSON string specifying an ending timestamp to read until for each TopicPartition. The timestamps need to be provided as a long value of the timestamp in milliseconds since 1970-01-01 00:00:00 UTC, for example 1686444353000. See note below on details of behavior with timestamps. endingOffsetsByTimestamp takes precedence over endingOffsets.  \nDefault value: None  \nendingTimestamp  \nType: String Query Type: batch only"
    },
    {
        "id": 1341,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/read_kafka.html",
        "content": "Default value: None  \nendingTimestamp  \nType: String Query Type: batch only  \nA string value of the timestamp in milliseconds since 1970-01-01 00:00:00 UTC, for example \"1686444353000\". If Kafka doesn\u2019t return the matched offset, the offset will be set to latest. See note below on details of behavior with timestamps. Note: endingTimestamp takes precedence over endingOffsetsByTimestamp and endingOffsets.  \nDefault value: None  \nincludeHeaders  \nType: Boolean Query Type: streaming and batch  \nWhether to include the Kafka headers in the row.  \nDefault value: false  \nkafka.<consumer_option>  \nType: String Query Type: streaming and batch  \nAny Kafka consumer specific options can be passed in with the kafka. prefix. These options need to be surrounded by backticks when provided, otherwise you will get a parser error. You can find the options in the Kafka documentation.  \nNote: You should not set the following options with this function: key.deserializer, value.deserializer, bootstrap.servers, group.id  \nDefault value: None  \nmaxOffsetsPerTrigger  \nType: Long Query Type: streaming only  \nRate limit on the maximum number of offsets or rows processed per trigger interval. The specified total number of offsets will be proportionally split across TopicPartitions.  \nDefault value: None  \nstartingOffsets  \nType: String Query Type: streaming and batch  \nThe start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a JSON string specifying a starting offset for each TopicPartition. In the JSON, -2 as an offset can be used to refer to earliest, -1 to latest.  \nNote: For batch queries, latest (either implicitly or by using -1 in JSON) is not allowed. For streaming queries, this only applies when a new query is started. Restarted streaming queries will continue from the offsets defined in the query checkpoint. Newly discovered partitions during a query will start at earliest.  \nDefault value: \"latest\" for streaming, \"earliest\" for batch  \nstartingOffsetsByTimestamp  \nType: String Query Type: streaming and batch"
    },
    {
        "id": 1342,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/read_kafka.html",
        "content": "Default value: \"latest\" for streaming, \"earliest\" for batch  \nstartingOffsetsByTimestamp  \nType: String Query Type: streaming and batch  \nA JSON string specifying a starting timestamp for each TopicPartition. The timestamps need to be provided as a long value of the timestamp in milliseconds since 1970-01-01 00:00:00 UTC, for example 1686444353000. See note below on details of behavior with timestamps. If Kafka doesn\u2019t return the matched offset, the behavior will follow to the value of the option startingOffsetsByTimestampStrategy. startingOffsetsByTimestamp takes precedence over startingOffsets.  \nNote: For streaming queries, this only applies when a new query is started. Restarted streaming queries will continue from the offsets defined in the query checkpoint. Newly discovered partitions during a query will start at earliest.  \nDefault value: None  \nstartingOffsetsByTimestampStrategy  \nType: String Query Type: streaming and batch  \nThis strategy is used when the specified starting offset by timestamp (either global or per partition) doesn\u2019t match with the offset Kafka returned. The available strategies are:  \n\"error\": fail the query  \n\"latest\": assigns the latest offset for these partitions so that Spark can read newer records from these partitions in later micro-batches.  \nDefault value: \"error\"  \nstartingTimestamp  \nType: String Query Type: streaming and batch  \nA string value of the timestamp in milliseconds since 1970-01-01 00:00:00 UTC, for example \"1686444353000\". See note below on details of behavior with timestamps. If Kafka doesn\u2019t return the matched offset, the behavior will follow to the value of the option startingOffsetsByTimestampStrategy. startingTimestamp takes precedence over startingOffsetsByTimestamp and startingOffsets.  \nNote: For streaming queries, this only applies when a new query is started. Restarted streaming queries will continue from the offsets defined in the query checkpoint. Newly discovered partitions during a query will start earliest.  \nDefault value: None  \nNote"
    },
    {
        "id": 1343,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/read_kafka.html",
        "content": "Note: For streaming queries, this only applies when a new query is started. Restarted streaming queries will continue from the offsets defined in the query checkpoint. Newly discovered partitions during a query will start earliest.  \nDefault value: None  \nNote  \nThe returned offset for each partition is the earliest offset whose timestamp is greater than or equal to the given timestamp in the corresponding partition. The behavior varies across options if Kafka doesn\u2019t return the matched offset - check the description of each option.  \nSpark simply passes the timestamp information to KafkaConsumer.offsetsForTimes, and doesn\u2019t interpret or reason about the value. For more details on KafkaConsumer.offsetsForTimes, please refer to the documentation). Also, the meaning of timestamp here can vary according to the Kafka configuration (log.message.timestamp.type). For details, see Apache Kafka documentation."
    },
    {
        "id": 1344,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/read_kafka.html",
        "content": "Related articles\nRelated articles\nCREATE STREAMING TABLE  \nread_files table-valued function  \nread_pubsub streaming table-valued function"
    },
    {
        "id": 1345,
        "url": "https://docs.databricks.com/en/sql/language-manual/functions/str_to_map.html",
        "content": "str_to_map function  \nApplies to: Databricks SQL Databricks Runtime  \nCreates a map after splitting the input into key-value pairs using delimiters.  \nSyntax\nSyntax\nstr_to_map(expr [, pairDelim [, keyValueDelim] ] )\n\nArguments\nArguments\nexpr: An STRING expression.  \npairDelim: An optional STRING literal defaulting to ',' that specifies how to split entries.  \nkeyValueDelim: An optional STRING literal defaulting to ':' that specifies how to split each key-value pair.\n\nReturns\nReturns\nA MAP of STRING for both keys and values.  \nBoth pairDelim and keyValueDelim are treated as regular expressions.\n\nExamples\nExamples\n> SELECT str_to_map('a:1,b:2,c:3', ',', ':'); {a -> 1, b -> 2, c -> 3} > SELECT str_to_map('a'); {a->NULL}\n\nRelated functions\nRelated functions\nmap function"
    },
    {
        "id": 1346,
        "url": "https://docs.databricks.com/en/sql/language-manual/information-schema/catalog_privileges.html",
        "content": "CATALOG_PRIVILEGES  \nApplies to: Databricks SQL Databricks Runtime 10.4 LTS and above Unity Catalog only  \nINFORMATION_SCHEMA.CATALOG_PRIVILEGES lists principals that have privileges on a catalog.  \nDefinition\nDefinition\nThe CATALOG_PRIVILEGES relation contains the following columns:  \nName  \nData type  \nNullable  \nStandard  \nDescription  \nGRANTOR  \nSTRING  \nNo  \nYes  \nPrincipal that granted the privilege.  \nGRANTEE  \nSTRING  \nNo  \nYes  \nPrincipal to which the privilege is granted.  \nCATALOG_NAME  \nSTRING  \nNo  \nYes  \nCatalog on which the privilege is granted.  \nPRIVILEGE_TYPE  \nSTRING  \nNo  \nYes  \nPrivilege being granted.  \nIS_GRANTABLE  \nSTRING  \nNo  \nYes  \nAlways NO. Reserved for future use.  \nINHERITED_FROM  \nSTRING  \nYes  \nNo  \nThe ancestor relation that the privilege is inherited from.\n\nConstraints\nConstraints\nThe following constraints apply to the CATALOG_PRIVILEGES relation:  \nClass  \nName  \nColumn List  \nDescription  \nPrimary key  \nCATPRIVS_PK  \nGRANTOR, GRANTEE, CATALOG_NAME, PRIVILEGE_TYPE  \nUnique identifier for the granted privilege.  \nForeign key  \nCATPRIVS_CATS_FK  \nCATALOG_NAME  \nReferences CATALOGS\n\nExamples\nExamples\n> SELECT catalog_name, grantee FROM information_schema.catalog_privileges;\n\nRelated\nRelated\nInformation schema  \nINFORMATION_SCHEMA.CATALOGS  \nSHOW GRANTS"
    },
    {
        "id": 1347,
        "url": "https://docs.databricks.com/en/structured-streaming/production.html",
        "content": "Production considerations for Structured Streaming  \nThis article contains recommendations to configure production incremental processing workloads with Structured Streaming on Databricks to fulfill latency and cost requirements for real-time or batch applications. Understanding key concepts of Structured Streaming on Databricks can help you avoid common pitfalls as you scale up the volume and velocity of data and move from development to production.  \nDatabricks has introduced Delta Live Tables to reduce the complexities of managing production infrastructure for Structured Streaming workloads. Databricks recommends using Delta Live Tables for new Structured Streaming pipelines; see What is Delta Live Tables?.  \nNote  \nCompute auto-scaling has limitations scaling down cluster size for Structured Streaming workloads. Databricks recommends using Delta Live Tables with Enhanced Autoscaling for streaming workloads. See Optimize the cluster utilization of Delta Live Tables pipelines with Enhanced Autoscaling.  \nUsing notebooks for Structured Streaming workloads"
    },
    {
        "id": 1348,
        "url": "https://docs.databricks.com/en/structured-streaming/production.html",
        "content": "Using notebooks for Structured Streaming workloads\nInteractive development with Databricks notebooks requires you attach your notebooks to a cluster in order to execute queries manually. You can schedule Databricks notebooks for automated deployment and automatic recovery from query failure using Jobs.  \nRecover from Structured Streaming query failures with Jobs  \nMonitoring Structured Streaming queries on Databricks  \nUse scheduler pools for multiple streaming workloads  \nYou can visualize Structured Streaming queries in notebooks during interactive development, or for interactive monitoring of production workloads. You should only visualize a Structured Streaming query in production if a human will regularly monitor the output of the notebook. While the trigger and checkpointLocation parameters are optional, as a best practice Databricks recommends that you always specify them in production.\n\nControlling batch size, frequency, and output mode for Structured Streaming on Databricks\nControlling batch size, frequency, and output mode for Structured Streaming on Databricks\nStructured Streaming on Databricks has enhanced options for helping to control costs and latency while streaming with Auto Loader and Delta Lake. Output modes allow you to control how Databricks writes to your sinks.  \nConfigure Structured Streaming batch size on Databricks  \nConfigure Structured Streaming trigger intervals  \nSelect an output mode for Structured Streaming"
    },
    {
        "id": 1349,
        "url": "https://docs.databricks.com/en/structured-streaming/production.html",
        "content": "What is stateful streaming?\nWhat is stateful streaming?\nA stateful Structured Streaming query requires incremental updates to intermediate state information, whereas a stateless Structured Streaming query only tracks information about which rows have been processed from the source to the sink.  \nStateful operations include streaming aggregation, streaming dropDuplicates, stream-stream joins, mapGroupsWithState, and flatMapGroupsWithState.  \nThe intermediate state information required for stateful Structured Streaming queries can lead to unexpected latency and production problems if not configured properly.  \nIn Databricks Runtime 13.3 LTS and above, you can enable changelog checkpointing with RocksDB to lower checkpoint duration and end-to-end latency for Structured Streaming workloads. Databricks recommends enabling changelog checkpointing for all Structured Streaming stateful queries. See Enable changelog checkpointing.  \nOptimize stateful Structured Streaming queries  \nConfigure RocksDB state store on Databricks  \nApply watermarks to control data processing thresholds"
    },
    {
        "id": 1350,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select.html",
        "content": "SELECT  \nApplies to: Databricks SQL Databricks Runtime  \nComposes a result set from one or more table references. The SELECT clause can be part of a query which also includes common table expressions (CTE), set operations, and various other clauses.  \nSyntax\nSyntax\nSELECT [ hints ] [ ALL | DISTINCT ] { named_expression | star_clause } [, ...] FROM table_reference [, ...] [ LATERAL VIEW clause ] [ WHERE clause ] [ GROUP BY clause ] [ HAVING clause] [ QUALIFY clause ] named_expression expression [ column_alias ] star_clause [ { table_name | view_name } . ] * [ except_clause ] except_clause EXCEPT ( { column_name | field_name } [, ...] )\n\nParameters"
    },
    {
        "id": 1351,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select.html",
        "content": "Parameters\nhints  \nHints help the Databricks optimizer make better planning decisions. Databricks supports hints that influence selection of join strategies and repartitioning of the data.  \nALL  \nSelect all matching rows from the table references. Enabled by default.  \nDISTINCT  \nSelect all matching rows from the table references after removing duplicates in results.  \nnamed_expression  \nAn expression with an optional assigned name.  \nexpression  \nA combination of one or more values, operators, and SQL functions that evaluates to a value.  \ncolumn_alias  \nAn optional column identifier naming the expression result. If no column_alias is provided Databricks SQL derives one.  \nstar_clause  \nA shorthand to name all the referenceable columns in the FROM clause or a specific table reference\u2019s columns or fields in the FROM clause.  \ntable_reference  \nA source of input for the SELECT. This input reference can be turned into a streaming reference by using the STREAM keyword prior to the reference.  \nLATERAL VIEW  \nUsed in conjunction with generator functions such as EXPLODE, which generates a virtual table containing one or more rows. LATERAL VIEW applies the rows to each original output row.  \nIn Databricks SQL, and starting with Databricks Runtime 12.2 this clause is deprecated. You should invoke a table valued generator function as a table_reference.  \nWHERE  \nFilters the result of the FROM clause based on the supplied predicates.  \nGROUP BY  \nThe expressions that are used to group the rows. This is used in conjunction with aggregate functions (MIN, MAX, COUNT, SUM, AVG) to group rows based on the grouping expressions and aggregate values in each group. When a FILTER clause is attached to an aggregate function, only the matching rows are passed to that function.  \nHAVING  \nThe predicates by which the rows produced by GROUP BY are filtered. The HAVING clause is used to filter rows after the grouping is performed. If you specify HAVING without GROUP BY, it indicates a GROUP BY without grouping expressions (global aggregate).  \nQUALIFY  \nThe predicates that are used to filter the results of window functions. To use QUALIFY, at least one window function is required to be present in the SELECT list or the QUALIFY clause."
    },
    {
        "id": 1352,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select.html",
        "content": "Select on Delta table\nIn addition to the standard SELECT options, Delta tables support the time travel options described in this section. For details, see Work with Delta Lake table history.  \nAS OF syntax  \ntable_identifier TIMESTAMP AS OF timestamp_expression table_identifier VERSION AS OF version  \ntimestamp_expression can be any one of:  \n'2018-10-18T22:15:12.013Z', that is, a string that can be cast to a timestamp  \ncast('2018-10-18 13:36:32 CEST' as timestamp)  \n'2018-10-18', that is, a date string  \ncurrent_timestamp() - interval 12 hours  \ndate_sub(current_date(), 1)  \nAny other expression that is or can be cast to a timestamp  \nversion is a long value that can be obtained from the output of DESCRIBE HISTORY table_spec.  \nNeither timestamp_expression nor version can be subqueries.  \nExample  \n> SELECT * FROM events TIMESTAMP AS OF '2018-10-18T22:15:12.013Z' > SELECT * FROM events VERSION AS OF 123  \n@ syntax  \nUse the @ syntax to specify the timestamp or version. The timestamp must be in yyyyMMddHHmmssSSS format. You can specify a version after @ by prepending a v to the version. For example, to query version 123 for the table events, specify events@v123.  \nExample  \n> SELECT * FROM events@20190101000000000 > SELECT * FROM events@v123"
    },
    {
        "id": 1353,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select.html",
        "content": "Examples\n-- select all referencable columns from all tables > SELECT * FROM VALUES(1, 2) AS t1(c1, c2), VALUES(3, 4) AS t2(c3, c4); 1 2 3 4 -- select all referencable columns from one table > SELECT t2.* FROM VALUES(1, 2) AS t1(c1, c2), VALUES(3, 4) AS t2(c3, c4); 3 4 -- select all referencable columns from all tables except t2.c4 > SELECT * EXCEPT(c4) FROM VALUES(1, 2) AS t1(c1, c2), VALUES(3, 4) AS t2(c3, c4); 1 2 3 -- select all referencable columns from a table, except a nested field. > SELECT * EXCEPT(c2.b) FROM VALUES(1, named_struct('a', 2, 'b', 3)) AS t(c1, c2); 1 { \"a\" : 2 } -- Removing all fields results in an empty struct > SELECT * EXCEPT(c2.b, c2.a) FROM VALUES(1, named_struct('a', 2, 'b', 3)) AS t(c1, c2); 1 { } -- Overlapping names result in an error > SELECT * EXCEPT(c2, c2.a) FROM VALUES(1, named_struct('a', 2, 'b', 3)) AS t(c1, c2); Error: EXCEPT_OVERLAPPING_COLUMNS"
    },
    {
        "id": 1354,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select.html",
        "content": "Related articles\nRelated articles\nCLUSTER BY clause Common table expression (CTE) DISTRIBUTE BY clause GROUP BY clause HAVING clause QUALIFY clause Hints VALUES clause JOIN LATERAL VIEW clause LIMIT clause OFFSET clause ORDER BY clause PIVOT clause Query TABLESAMPLE clause Set operators SORT BY clause Star clause Table-valued function (TVF) table reference UNPIVOT clause WHERE clause WINDOW clause Window functions"
    },
    {
        "id": 1355,
        "url": "https://docs.databricks.com/en/structured-streaming/foreach.html",
        "content": "Use foreachBatch to write to arbitrary data sinks  \nThis article discusses using foreachBatch with Structured Streaming to write the output of a streaming query to data sources that do not have an existing streaming sink.  \nThe code pattern streamingDF.writeStream.foreachBatch(...) allows you to apply batch functions to the output data of every micro-batch of the streaming query. Functions used with foreachBatch take two parameters:  \nA DataFrame that has the output data of a micro-batch.  \nThe unique ID of the micro-batch.  \nYou must use foreachBatch for Delta Lake merge operations in Structured Streaming. See Upsert from streaming queries using foreachBatch.  \nApply additional DataFrame operations"
    },
    {
        "id": 1356,
        "url": "https://docs.databricks.com/en/structured-streaming/foreach.html",
        "content": "Apply additional DataFrame operations\nMany DataFrame and Dataset operations are not supported in streaming DataFrames because Spark does not support generating incremental plans in those cases. Using foreachBatch() you can apply some of these operations on each micro-batch output. For example, you can use foreachBath() and the SQL MERGE INTO operation to write the output of streaming aggregations into a Delta table in update mode. See more details in MERGE INTO.  \nImportant  \nforeachBatch() provides only at-least-once write guarantees. However, you can use the batchId provided to the function as way to deduplicate the output and get an exactly-once guarantee. In either case, you will have to reason about the end-to-end semantics yourself.  \nforeachBatch() does not work with the continuous processing mode as it fundamentally relies on the micro-batch execution of a streaming query. If you write data in continuous mode, use foreach() instead.  \nAn empty dataframe can be invoked with foreachBatch() and user code needs to be resilient to allow for proper operation. An example is shown here:  \n.foreachBatch( (outputDf: DataFrame, bid: Long) => { // Process valid data frames only if (!outputDf.isEmpty) { // business logic } } ).start()"
    },
    {
        "id": 1357,
        "url": "https://docs.databricks.com/en/structured-streaming/foreach.html",
        "content": "Behavior changes for foreachBatch in Databricks Runtime 14.0\nBehavior changes for foreachBatch in Databricks Runtime 14.0\nIn Databricks Runtime 14.0 and above on compute configured with shared access mode, the following behavior changes apply:  \nprint() commands write output to the driver logs.  \nYou cannot access the dbutils.widgets submodule inside the function.  \nAny files, modules, or objects referenced in the function must be serializable and available on Spark.\n\nReuse existing batch data sources\nReuse existing batch data sources\nUsing foreachBatch(), you can use existing batch data writers for data sinks that might not have Structured Streaming support. Here are a few examples:  \nCassandra Scala example  \nAzure Synapse Analytics Python example  \nMany other batch data sources can be used from foreachBatch(). See Connect to data sources.\n\nWrite to multiple locations"
    },
    {
        "id": 1358,
        "url": "https://docs.databricks.com/en/structured-streaming/foreach.html",
        "content": "Write to multiple locations\nIf you need to write the output of a streaming query to multiple locations, Databricks recommends using multiple Structured Streaming writers for best parallelization and throughput.  \nUsing foreachBatch to write to multiple sinks serializes the execution of streaming writes, which can increase latency for each micro-batch.  \nIf you do use foreachBatch to write to multiple Delta tables, see Idempotent table writes in foreachBatch."
    },
    {
        "id": 1359,
        "url": "https://docs.databricks.com/en/structured-streaming/index.html",
        "content": "Streaming on Databricks  \nYou can use Databricks for near real-time data ingestion, processing, machine learning, and AI for streaming data.  \nDatabricks offers numerous optimzations for streaming and incremental processing. For most streaming or incremental data processing or ETL tasks, Databricks recommends Delta Live Tables. See What is Delta Live Tables?.  \nMost incremental and streaming workloads on Databricks are powered by Structured Streaming, including Delta Live Tables and Auto Loader. See What is Auto Loader?.  \nDelta Lake and Structured Streaming have tight integration to power incremental processing in the Databricks lakehouse. See Delta table streaming reads and writes.  \nFor real-time model serving, see Model serving with Databricks.  \nTo learn more about building streaming solutions on the Databricks platform, see the data streaming product page.  \nDatabricks has specific features for working with semi-structured data fields contained in Avro, protocol buffers, and JSON data payloads. To learn more, see:  \nTransform Avro payload Protocol buffers  \nWhat is Structured Streaming?"
    },
    {
        "id": 1360,
        "url": "https://docs.databricks.com/en/structured-streaming/index.html",
        "content": "What is Structured Streaming?\nApache Spark Structured Streaming is a near-real time processing engine that offers end-to-end fault tolerance with exactly-once processing guarantees using familiar Spark APIs. Structured Streaming lets you express computation on streaming data in the same way you express a batch computation on static data. The Structured Streaming engine performs the computation incrementally and continuously updates the result as streaming data arrives.  \nIf you\u2019re new to Structured Streaming, see Run your first Structured Streaming workload.  \nFor information about using Structured Streaming with Unity Catalog, see Using Unity Catalog with Structured Streaming.\n\nWhat streaming sources and sinks does Databricks support?\nWhat streaming sources and sinks does Databricks support?\nDatabricks recommends using Auto Loader to ingest supported file types from cloud object storage into Delta Lake. For ETL pipelines, Databricks recommends using Delta Live Tables (which uses Delta tables and Structured Streaming). You can also configure incremental ETL workloads by streaming to and from Delta Lake tables.  \nIn addition to Delta Lake and Auto Loader, Structured Streaming can connect to messaging services such as Apache Kafka.  \nYou can also Use foreachBatch to write to arbitrary data sinks.\n\nAdditional resources"
    },
    {
        "id": 1361,
        "url": "https://docs.databricks.com/en/structured-streaming/index.html",
        "content": "Additional resources\nApache Spark provides a Structured Streaming Programming Guide that has more information about Structured Streaming.  \nFor reference information about Structured Streaming, Databricks recommends the following Apache Spark API references:  \nPython  \nScala  \nJava"
    },
    {
        "id": 1362,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-materialized-view.html",
        "content": "CREATE MATERIALIZED VIEW  \nApplies to: Databricks SQL  \nA materialized view is a view where precomputed results are available for query and can be updated to reflect changes in the input. Each time a materialized view is refreshed, query results are recalculated to reflect changes in upstream datasets. All materialized views are backed by a DLT pipeline. You can refresh materialized views manually or on a schedule.  \nTo learn more about how to perform a manual refresh, see REFRESH (MATERIALIZED VIEW or STREAMING TABLE).  \nTo learn more about how to schedule a refresh, see Examples or ALTER MATERIALIZED VIEW.  \nSyntax"
    },
    {
        "id": 1363,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-materialized-view.html",
        "content": "Syntax\n{ CREATE OR REPLACE MATERIALIZED VIEW | CREATE MATERIALIZED VIEW [ IF NOT EXISTS ] } view_name [ column_list ] [ view_clauses ] AS query column_list ( { column_name column_type column_properties } [, ...] [ , table_constraint ] [...]) column_properties { NOT NULL | COMMENT column_comment | column_constraint | MASK clause } [ ... ] view_clauses { PARTITIONED BY (col [, ...]) | COMMENT view_comment | TBLPROPERTIES clause | SCHEDULE [ REFRESH ] CRON cron_string [ AT TIME ZONE timezone_id ] | WITH { ROW FILTER clause } } [...]\n\nParameters"
    },
    {
        "id": 1364,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-materialized-view.html",
        "content": "Parameters\nREPLACE  \nIf specified, replaces the view and its content if it already exists.  \nIF NOT EXISTS  \nCreates the view if it does not exist. If a view by this name already exists, the CREATE MATERIALIZED VIEW statement is ignored.  \nYou may specify at most one of IF NOT EXISTS or OR REPLACE.  \nview_name  \nThe name of the newly created view. The fully qualified view name must be unique.  \ncolumn_list  \nOptionally labels the columns in the query result of the view. If you provide a column list the number of column aliases must match the number of expressions in the query. If no column list is specified, aliases are derived from the body of the view.  \ncolumn_name  \nThe column names must be unique and map to the output columns of the query.  \ncolumn_type  \nSpecifies the column\u2019s data type. Not all data types supported by Databricks are supported by materialized views.  \ncolumn_comment  \nAn optional STRING literal describing the column name. This option must be specified along with column_type. If the column type is not specified, the column comment is skipped.  \ncolumn_constraint  \nAdds an informational primary key or informational foreign key constraint to the column in a materialized view. If the column type is not specified, the column constraint is skipped.  \nMASK clause  \nPreview  \nThis feature is in Public Preview.  \nAdds a column mask function to anonymize sensitive data. All future queries from that column will receive the result of evaluating that function over the column in place of the column\u2019s original value. This can be useful for fine-grained access control purposes wherein the function can inspect the identity and/or group memberships of the invoking user in order to decide whether to redact the value. If the column type is not specified, the column mask is skipped.  \ntable_constraint  \nAdds an informational primary key or informational foreign key constraint to the table in a materialized view. If the column type is not specified, the table constraint is skipped.  \nview_clauses  \nOptionally specify partitioning, comments, user defined properties, and a refresh schedule for the new materialized view. Each sub clause may only be specified once.  \nPARTITIONED BY  \nAn optional list of columns of the table to partition the table by.  \nCOMMENT view_comment  \nA STRING literal to describe the table.  \nTBLPROPERTIES  \nOptionally sets one or more user defined properties."
    },
    {
        "id": 1365,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-materialized-view.html",
        "content": "PARTITIONED BY  \nAn optional list of columns of the table to partition the table by.  \nCOMMENT view_comment  \nA STRING literal to describe the table.  \nTBLPROPERTIES  \nOptionally sets one or more user defined properties.  \nSCHEDULE [ REFRESH ] CRON cron_string [ AT TIME ZONE timezone_id ]  \nIf provided, schedules the streaming table or the materialized view to refresh its data with the given quartz cron schedule. Only time_zone_values are accepted. AT TIME ZONE LOCAL is not supported. If AT TIME ZONE is absent, the session time zone is used. If AT TIME ZONE is absent and the session time zone is not set, an error is thrown. SCHEDULE is semantically equivalent to SCHEDULE REFRESH.  \nWITH ROW FILTER clause  \nPreview  \nThis feature is in Public Preview.  \nAdds a row filter function to the table. All future queries from that table will receive subset of its rows for which the function evaluates to boolean TRUE. This can be useful for fine-grained access control purposes wherein the function can inspect the identity and/or group memberships of the invoking user in order to decide whether to filter certain rows.  \nAS query  \nA query that constructs the view from base tables or other views."
    },
    {
        "id": 1366,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-materialized-view.html",
        "content": "Required permissions\nRequired permissions\nThe user who creates a materialized view (MV) is the MV owner and needs to have the following permissions:  \nSELECT privilege over the base tables referenced by the MV.  \nUSE CATALOG privilege on the parent catalog and the USE SCHEMA privilege on the parent schema.  \nCREATE privilege on the schema for the MV.  \nFor a user to be able to refresh the MV, they require:  \nUSE CATALOG privilege on the parent catalog and the USE SCHEMA privilege on the parent schema.  \nOwnership of the MV or REFRESH privilege on the MV.  \nThe owner of the MV must have the SELECT privilege over the base tables referenced by the MV.  \nFor a user to be able to query the MV, they require:  \nUSE CATALOG privilege on the parent catalog and the USE SCHEMA privilege on the parent schema.  \nSELECT privilege over the materialized view.\n\nRow filters and column masks"
    },
    {
        "id": 1367,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-materialized-view.html",
        "content": "Row filters and column masks\nPreview  \nThis feature is in Public Preview.  \nRow filters let you specify a function that applies as a filter whenever a table scan fetches rows. These filters ensure that subsequent queries only return rows for which the filter predicate evaluates to true.  \nColumn masks let you mask a column\u2019s values whenever a table scan fetches rows. All future queries involving that column will receive the result of evaluating the function over the column, replacing the column\u2019s original value.  \nFor more information on how to use row filters and column masks, see Filter sensitive table data using row filters and column masks.  \nManaging Row Filters and Column Masks  \nRow filters and column masks on materialized views should be added through the CREATE statement.  \nBehavior  \nRefresh as Definer: When the REFRESH MATERIALIZED VIEW statement refreshes a materialized view, row filter functions run with the definer\u2019s rights (as the table owner). This means the table refresh uses the security context of the user who created the materialized view.  \nQuery: While most filters run with the definer\u2019s rights, functions that check user context (such as CURRENT_USER and IS_MEMBER) are exceptions. These functions run as the invoker. This approach enforces user-specific data security and access controls based on the current user\u2019s context.  \nWhen creating materialized views over source tables that contain row filters and column masks, the refresh of the materialized view is always a full refresh. A full refresh reprocesses all data available in the source with the latest definitions. This ensures that security policies on the source tables are evaluated and applied with the most up-to-date data and definitions.  \nObservability  \nUse DESCRIBE EXTENDED, INFORMATION_SCHEMA, or the Catalog Explorer to examine the existing row filters and column masks that apply to a given materialized view. This functionality allows users to audit and review data access and protection measures on materialized views."
    },
    {
        "id": 1368,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-materialized-view.html",
        "content": "Limitations\nLimitations\nWhen a materialized view with a sum aggregate over a NULL-able column has the last non-NULL value removed from that column - and thus only NULL values remain in that column - the materialized view\u2019s resultant aggregate value returns zero instead of NULL.  \nColumn-reference does not require an alias. Non-column reference expressions require an alias, as in the following example:  \nAllowed: SELECT col1, SUM(col2) AS sum_col2 FROM t GROUP BY col1  \nNot Allowed: SELECT col1, SUM(col2) FROM t GROUP BY col1  \nNOT NULL must be manually specified along with PRIMARY KEY in order to be a valid statement.  \nMaterialized views do not support identity columns or surrogate keys.  \nMaterialized views do not support OPTIMIZE and VACUUM commands. Maintenance happens automatically.  \nMaterialized views do not support expectations to define data quality constraints.\n\nExamples"
    },
    {
        "id": 1369,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-materialized-view.html",
        "content": "Examples\n-- Create a materialized view if it doesn't exist > CREATE MATERIALIZED VIEW IF NOT EXISTS subscribed_movies AS SELECT mo.member_id, mb.full_name, mo.movie_title FROM movies AS mo INNER JOIN members AS mb ON mo.member_id = mb.id; -- Create and schedule a materialized view to be refreshed daily at midnight. -- Note: All columns in a GROUP BY need to be explicitly aliased > CREATE MATERIALIZED VIEW daily_sales COMMENT 'Daily sales numbers' SCHEDULE CRON '0 0 0 * * ? *' AS SELECT date AS date, sum(sales) AS sumOfSales FROM table1 GROUP BY date; -- Create a materialized view with a table constraint > CREATE MATERIALIZED VIEW IF NOT EXISTS subscribed_movies( member_id int NOT NULL, full_name string, movie_title string, CONSTRAINT movie_pk PRIMARY KEY(member_id) ) AS SELECT mo.member_id, mb.full_name, mo.movie_title FROM movies AS mo INNER JOIN members AS mb ON mo.member_id = mb.id; -- Create or replace the materialized view to remove the table constraint and add a partition > CREATE OR REPLACE MATERIALIZED VIEW subscribed_movies PARTITIONED BY (member_id) AS SELECT mo.member_id, mb.full_name, mo.movie_title FROM movies AS mo INNER JOIN members AS mb ON mo.member_id = mb.id; -- Create a materialized view with a row filter and a column mask > CREATE MATERIALIZED VIEW masked_view ( id int, name string, region string, ssn string MASK catalog.schema.ssn_mask_fn ) WITH ROW FILTER catalog.schema.us_filter_fn ON (region) AS SELECT id, name, region, ssn FROM employees;"
    },
    {
        "id": 1370,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-materialized-view.html",
        "content": "Related articles\nRelated articles\nALTER MATERIALIZED VIEW  \nconstraint clause  \nDROP VIEW  \nSHOW CREATE TABLE  \nFiltering sensitive table data with row filters and column masks  \nDESCRIBE TABLE  \nREFRESH"
    },
    {
        "id": 1371,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-aux-resource-mgmt-list-archive.html",
        "content": "LIST ARCHIVE  \nApplies to: Databricks Runtime 10.4 LTS and above  \nLists the ARCHIVEs added by ADD ARCHIVE.  \nSyntax\nSyntax\nLIST [ARCHIVE | ARCHIVES] [file_name [...]]\n\nParameters\nParameters\nfile_name  \nOptional a name of an archive to list.\n\nExamples\nExamples\n> ADD ARCHIVES /tmp/test.zip /tmp/test_2.tar.gz; > LIST ARCHIVE; file:/tmp/test.zip file:/tmp/test_2.tar.gz > LIST ARCHIVE /tmp/test.zip /some/random.tgz /another/random.tar; file:/tmp/test.zip\n\nRelated statements\nRelated statements\nADD ARCHIVE  \nADD JAR  \nADD FILE  \nLIST FILE  \nLIST JAR"
    },
    {
        "id": 1372,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html",
        "content": "CREATE TABLE [USING]  \nApplies to: Databricks SQL Databricks Runtime  \nDefines a managed or external table, optionally using a data source.  \nSyntax"
    },
    {
        "id": 1373,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html",
        "content": "Syntax\n{ { [CREATE OR] REPLACE TABLE | CREATE [EXTERNAL] TABLE [ IF NOT EXISTS ] } table_name [ table_specification ] [ USING data_source ] [ table_clauses ] [ AS query ] } table_specification ( { column_identifier column_type [ column_properties ] } [, ...] [ , table_constraint ] [...] ) column_properties { NOT NULL | GENERATED ALWAYS AS ( expr ) | GENERATED { ALWAYS | BY DEFAULT } AS IDENTITY [ ( [ START WITH start ] [ INCREMENT BY step ] ) ] | DEFAULT default_expression | COMMENT column_comment | column_constraint | MASK clause } [ ... ] table_clauses { OPTIONS clause | PARTITIONED BY clause | CLUSTER BY clause | clustered_by_clause | LOCATION path [ WITH ( CREDENTIAL credential_name ) ] | COMMENT table_comment | TBLPROPERTIES clause | WITH { ROW FILTER clause } } [...] clustered_by_clause { CLUSTERED BY ( cluster_column [, ...] ) [ SORTED BY ( { sort_column [ ASC | DESC ] } [, ...] ) ] INTO num_buckets BUCKETS }"
    },
    {
        "id": 1374,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html",
        "content": "Parameters\nREPLACE  \nIf specified, replaces the table and its content if it already exists. This clause is only supported for Delta Lake tables.  \nREPLACE preserves the table history.  \nNote  \nDatabricks strongly recommends using REPLACE instead of dropping and re-creating Delta Lake tables.  \nEXTERNAL  \nIf specified, creates an external table. When creating an external table you must also provide a LOCATION clause. When an external table is dropped the files at the LOCATION will not be dropped.  \nIF NOT EXISTS  \nIf specified and a table with the same name already exists, the statement is ignored.  \nIF NOT EXISTS cannot coexist with REPLACE, which means CREATE OR REPLACE TABLE IF NOT EXISTS is not allowed.  \ntable_name  \nThe name of the table to be created. The name must not include a temporal specification. If the name is not qualified the table is created in the current schema.  \nTables created in hive_metastore can only contain alphanumeric ASCII characters and underscores (INVALID_SCHEMA_OR_RELATION_NAME).  \ntable_specification  \nThis optional clause defines the list of columns, their types, properties, descriptions, and column constraints.  \nIf you do not define columns the table schema you must specify either AS query or LOCATION.  \ncolumn_identifier  \nA unique name for the column.  \nColumn identifiers of Delta Lake tables without column mapping property ('delta.columnMapping.mode' = 'name') must not contain the characters \u2018 \u2018 (space), \u2018,\u2019, \u2018;\u2019, \u2018{\u2018, \u2018}\u2019, \u2018(\u2018, \u2018)\u2019. \u2018n\u2019, \u2018t\u2019, and \u2018=\u2019.  \nColumn identifiers of AVRO table must start with \u2018\u2019 or a Unicode letter (including non-ASCII letters) and be followed by a combination of \u2018\u2019, Unicode letters and digits.  \ncolumn_type  \nSpecifies the data type of the column. Not all data types supported by Databricks are supported by all data sources.  \nNOT NULL  \nIf specified the column will not accept NULL values. This clause is only supported for Delta Lake tables.  \nGENERATED ALWAYS AS ( expr )  \nWhen you specify this clause the value of this column is determined by the specified expr.  \nexpr may be composed of literals, column identifiers within the table, and deterministic, built-in SQL functions or operators except:  \nAggregate functions  \nAnalytic window functions  \nRanking window functions"
    },
    {
        "id": 1375,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html",
        "content": "expr may be composed of literals, column identifiers within the table, and deterministic, built-in SQL functions or operators except:  \nAggregate functions  \nAnalytic window functions  \nRanking window functions  \nTable valued generator functions  \nAlso expr must not contain any subquery.  \nGENERATED { ALWAYS | BY DEFAULT } AS IDENTITY [ ( [ START WITH start ] [ INCREMENT BY step ] ) ]  \nApplies to: Databricks SQL Databricks Runtime 10.4 LTS and above  \nDefines an identity column. When you write to the table, and do not provide values for the identity column, it will be automatically assigned a unique and statistically increasing (or decreasing if step is negative) value. This clause is only supported for Delta Lake tables. This clause can only be used for columns with BIGINT data type.  \nThe automatically assigned values start with start and increment by step. Assigned values are unique but are not guaranteed to be contiguous. Both parameters are optional, and the default value is 1. step cannot be 0.  \nIf the automatically assigned values are beyond the range of the identity column type, the query will fail.  \nWhen ALWAYS is used, you cannot provide your own values for the identity column.  \nThe following operations are not supported:  \nPARTITIONED BY an identity column  \nUPDATE an identity column  \nNote  \nDeclaring an identity column on a Delta table disables concurrent transactions. Only use identity columns in use cases where concurrent writes to the target table are not required.  \nDEFAULT default_expression  \nApplies to: Databricks SQL Databricks Runtime 11.3 LTS and above  \nDefines a DEFAULT value for the column which is used on INSERT, UPDATE, and MERGE ... INSERT when the column is not specified.  \nIf no default is specified DEFAULT NULL is applied for nullable columns.  \ndefault_expression may be composed of literals, and built-in SQL functions or operators except:  \nAggregate functions  \nAnalytic window functions  \nRanking window functions  \nTable valued generator functions  \nAlso default_expression must not contain any subquery.  \nDEFAULT is supported for CSV, JSON, PARQUET, and ORC sources.  \nCOMMENT column_comment  \nA string literal to describe the column.  \ncolumn_constraint  \nAdds a primary key or foreign key constraint to the column in a Delta Lake table.  \nConstraints are not supported for tables in the hive_metastore catalog.  \nTo add a check constraint to a Delta Lake table use ALTER TABLE."
    },
    {
        "id": 1376,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html",
        "content": "Adds a primary key or foreign key constraint to the column in a Delta Lake table.  \nConstraints are not supported for tables in the hive_metastore catalog.  \nTo add a check constraint to a Delta Lake table use ALTER TABLE.  \nMASK clause  \nApplies to: Databricks SQL Databricks Runtime 12.2 LTS and above Unity Catalog only  \nPreview  \nThis feature is in Public Preview.  \nAdds a column mask function to anonymize sensitive data. All future queries from that column will receive the result of evaluating that function over the column in place of the column\u2019s original value. This can be useful for fine-grained access control purposes wherein the function can inspect the identity and/or group memberships of the invoking user in order to decide whether to redact the value.  \ntable_constraint  \nAdds an informational primary key or informational foreign key constraints to the Delta Lake table.  \nKey constraints are not supported for tables in the hive_metastore catalog.  \nTo add a check constraint to a Delta Lake table use ALTER TABLE.  \nUSING data_source  \nThe file format to use for the table. data_source must be one of:  \nAVRO  \nBINARYFILE  \nCSV  \nDELTA  \nJSON  \nORC  \nPARQUET  \nTEXT  \nThe following additional file formats to use for the table are supported in Databricks Runtime:  \nJDBC  \nLIBSVM  \na fully-qualified class name of a custom implementation of org.apache.spark.sql.sources.DataSourceRegister.  \nIf USING is omitted, the default is DELTA.  \nFor any data_source other than DELTA you must also specify a LOCATION unless the table catalog is hive_metastore.  \nThe following applies to: Databricks Runtime  \nHIVE is supported to create a Hive SerDe table in Databricks Runtime. You can specify the Hive-specific file_format and row_format using the OPTIONS clause, which is a case-insensitive string map. The option_keys are:  \nFILEFORMAT  \nINPUTFORMAT  \nOUTPUTFORMAT  \nSERDE  \nFIELDDELIM  \nESCAPEDELIM  \nMAPKEYDELIM  \nLINEDELIM  \ntable_clauses  \nOptionally specify location, partitioning, clustering, options, comments, and user defined properties for the new table. Each sub clause may only be specified once.  \nPARTITIONED BY  \nAn optional clause to partition the table by a subset of columns.  \nNote"
    },
    {
        "id": 1377,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html",
        "content": "PARTITIONED BY  \nAn optional clause to partition the table by a subset of columns.  \nNote  \nIf you don\u2019t define a Delta table, partitioning columns are placed at the end of the table, even if they are defined earlier in the column specification. Consider using CLUSTER BY instead of PARTITIONED BY for Delta tables.  \nCLUSTER BY  \nApplies to: Databricks SQL Databricks Runtime 13.3 and later  \nAn optional clause to cluster a Delta table by a subset of columns. To cluster other tables use clustered_by_clause.  \nDelta Lake liquid clustering cannot be combined with PARTITIONED BY.  \nclustered_by_clause  \nOptionally cluster the table or each partition into a fixed number of hash buckets using a subset of the columns.  \nClustering is not supported for Delta Lake tables.  \nCLUSTERED BY  \nSpecifies the set of columns by which to cluster each partition, or the table if no partitioning is specified.  \ncluster_column  \nAn identifier referencing a column_identifier in the table. If you specify more than one column there must be no duplicates. Since a clustering operates on the partition level you must not name a partition column also as a cluster column.  \nSORTED BY  \nOptionally maintains a sort order for rows in a bucket.  \nsort_column  \nA column to sort the bucket by. The column must not be partition column. Sort columns must be unique.  \nASC or DESC  \nOptionally specifies whether sort_column is sorted in ascending (ASC) or descending (DESC) order. The default values is ASC.  \nINTO num_buckets BUCKETS  \nAn INTEGER literal specifying the number of buckets into which each partition (or the table if no partitioning is specified) is divided.  \nLOCATION path [ WITH ( CREDENTIAL credential_name ) ]  \nAn optional path to the directory where table data is stored, which could be a path on distributed storage. path must be a STRING literal. If you specify no location the table is considered a managed table and Databricks creates a default table location.  \nSpecifying a location makes the table an external table.  \nFor tables that do not reside in the hive_metastore catalog, the table path must be protected by an external location unless a valid storage credential is specified.  \nYou cannot create external tables in locations that overlap with the location of managed tables."
    },
    {
        "id": 1378,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html",
        "content": "You cannot create external tables in locations that overlap with the location of managed tables.  \nFor a Delta Lake table the table configuration is inherited from the LOCATION if data is present. Therefore, if any TBLPROPERTIES, table_specification, or PARTITIONED BY clauses are specified for Delta Lake tables they must exactly match the Delta Lake location data.  \nOPTIONS  \nSets or resets one or more user defined table options.  \nCOMMENT table_comment  \nA string literal to describe the table.  \nTBLPROPERTIES  \nOptionally sets one or more user defined properties.  \nWITH ROW FILTER clause  \nApplies to: Databricks SQL Databricks Runtime 12.2 LTS and above Unity Catalog only  \nAdds a row filter function to the table. All future queries from that table will receive subset of its rows for which the function evaluates to boolean TRUE. This can be useful for fine-grained access control purposes wherein the function can inspect the identity and/or group memberships of the invoking user in order to decide whether to filter certain rows.  \nAS query  \nThis optional clause populates the table using the data from query. When you specify a query you must not also specify a table_specification. The table schema is derived from the query.  \nNote that Databricks overwrites the underlying data source with the data of the input query, to make sure the table gets created contains exactly the same data as the input query."
    },
    {
        "id": 1379,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html",
        "content": "Examples\nExamples\n-- Creates a Delta table > CREATE TABLE student (id INT, name STRING, age INT); -- Use data from another table > CREATE TABLE student_copy AS SELECT * FROM student; -- Creates a CSV table from an external directory > CREATE TABLE student USING CSV LOCATION '/path/to/csv_files'; -- Specify table comment and properties > CREATE TABLE student (id INT, name STRING, age INT) COMMENT 'this is a comment' TBLPROPERTIES ('foo'='bar'); -- Specify table comment and properties with different clauses order > CREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES ('foo'='bar') COMMENT 'this is a comment'; -- Create partitioned table > CREATE TABLE student (id INT, name STRING, age INT) PARTITIONED BY (age); -- Create a table with a generated column > CREATE TABLE rectangles(a INT, b INT, area INT GENERATED ALWAYS AS (a * b));\n\nRelated articles"
    },
    {
        "id": 1380,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html",
        "content": "Related articles\nALTER TABLE  \nCONSTRAINT  \nCLUSTER BY  \nCREATE TABLE LIKE  \nCREATE TABLE CLONE  \nDROP TABLE  \nPARTITIONED BY  \nTable properties and table options"
    },
    {
        "id": 1381,
        "url": "https://docs.databricks.com/en/views/create-views.html",
        "content": "Create and manage views  \nThis article shows how to create views in Unity Catalog. See What is a view?.  \nRequired permissions"
    },
    {
        "id": 1382,
        "url": "https://docs.databricks.com/en/views/create-views.html",
        "content": "Required permissions\nTo create a view:  \nYou must have the USE CATALOG permission on the parent catalog and the USE SCHEMA and CREATE TABLE permissions on the parent schema. A metastore admin or the catalog owner can grant you all of these privileges. A schema owner can grant you USE SCHEMA and CREATE TABLE privileges on the schema.  \nYou must be able to read the tables and views referenced in the view (SELECT on the table or view, as well as USE CATALOG on the catalog and USE SCHEMA on the schema).  \nIf a view references tables in the workspace-local Hive metastore, the view can be accessed only from the workspace that contains the workspace-local tables. For this reason, Databricks recommends creating views only from tables or views that are in the Unity Catalog metastore.  \nYou cannot create a view that references a view that has been shared with you using Delta Sharing. See What is Delta Sharing?.  \nTo read a view, the permissions required depend on the compute type, Databricks Runtime version, and access mode:  \nFor single-user compute resources on Databricks Runtime 15.4 and above, shared compute resources, and SQL warehouses, you need SELECT on the view itself, USE CATALOG on its parent catalog, and USE SCHEMA on its parent schema.  \nNote  \nIf you are using a single-user compute resource on Databricks Runtime 15.4 LTS and above, you must also verify that your workspace is enabled for serverless compute, which runs the data filtering functionality that supports view access without requiring access to the view\u2019s underlying tables and views. You might therefore be charged for serverless compute resources when you use single-user compute to query views. See Fine-grained access control on single-user compute. Data filtering using Databricks Runtime 15.4 and serverless compute is in Public Preview.  \nFor single-user compute resources on Databricks Runtime 15.3 and below, you must also have SELECT on all tables and views that the view references, in addition to USE CATALOG on their parent catalogs and USE SCHEMA on their parent schemas."
    },
    {
        "id": 1383,
        "url": "https://docs.databricks.com/en/views/create-views.html",
        "content": "Create a view\nCreate a view\nTo create a view, run the following SQL command. Items in brackets are optional. Replace the placeholder values:  \n<catalog-name>: The name of the catalog.  \n<schema-name>: The name of the schema.  \n<view-name>: A name for the view.  \n<query>: The query, columns, and tables and views used to compose the view.  \nCREATE VIEW <catalog-name>.<schema-name>.<view-name> AS SELECT <query>;  \nFor example, to create a view named sales_redacted from columns in the sales_raw table:  \nCREATE VIEW sales_metastore.sales.sales_redacted AS SELECT user_id, email, country, product, total FROM sales_metastore.sales.sales_raw;  \nYou can also create a view by using the Databricks Terraform provider and databricks_table. You can retrieve a list of view full names by using databricks_views.\n\nDrop a view"
    },
    {
        "id": 1384,
        "url": "https://docs.databricks.com/en/views/create-views.html",
        "content": "Drop a view\nYou must be the view\u2019s owner to drop a view. To drop a view, run the following SQL command:  \nDROP VIEW IF EXISTS catalog_name.schema_name.view_name;"
    },
    {
        "id": 1385,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-drop-catalog.html",
        "content": "DROP CATALOG  \nApplies to: Databricks SQL Databricks Runtime 10.4 LTS and above Unity Catalog only  \nDrops a catalog. An exception is thrown if the catalog does not exist in the metastore. To drop a catalog you must be its owner.  \nSyntax\nSyntax\nDROP CATALOG [ IF EXISTS ] catalog_name [ RESTRICT | CASCADE ]\n\nParameters\nParameters\nIF EXISTS  \nIf specified, no exception is thrown when the catalog does not exist.  \ncatalog_name:  \nThe name of an existing catalog in the metastore. If the name does not exist, an exception is thrown.  \nRESTRICT  \nIf specified, restricts dropping a non-empty catalog. Enabled by default.  \nCASCADE  \nIf specified, drops all of the associated databases (schemas) and the objects within them, recursively. In Unity Catalog, dropping a catalog using CASCADE soft-deletes tables: managed table files will be cleaned up after 30 days, but external files are not deleted.\n\nExamples"
    },
    {
        "id": 1386,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-drop-catalog.html",
        "content": "Examples\n-- Create a `vaccine` catalog > CREATE CATALOG vaccine COMMENT 'This catalog is used to maintain information about vaccines'; -- Drop the catalog and its schemas > DROP CATALOG vaccine CASCADE; -- Drop the catalog using IF EXISTS and only if it is empty. > DROP CATALOG IF EXISTS vaccine RESTRICT;\n\nRelated articles\nRelated articles\nCREATE CATALOG  \nDESCRIBE CATALOG  \nSHOW CATALOGS"
    },
    {
        "id": 1387,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select-qualify.html",
        "content": "QUALIFY clause  \nApplies to: Databricks SQL Databricks Runtime 10.4 LTS and above.  \nFilters the results of window functions. To use QUALIFY, at least one window function is required to be present in the SELECT list or the QUALIFY clause.  \nSyntax\nSyntax\nQUALIFY boolean_expression\n\nParameters\nParameters\nboolean_expression  \nAny expression that evaluates to a result type boolean. Two or more expressions may be combined together using the logical operators ( AND, OR).  \nThe expressions specified in the QUALIFY clause cannot contain aggregate functions.\n\nExamples"
    },
    {
        "id": 1388,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select-qualify.html",
        "content": "Examples\nCREATE TABLE dealer (id INT, city STRING, car_model STRING, quantity INT); INSERT INTO dealer VALUES (100, 'Fremont', 'Honda Civic', 10), (100, 'Fremont', 'Honda Accord', 15), (100, 'Fremont', 'Honda CRV', 7), (200, 'Dublin', 'Honda Civic', 20), (200, 'Dublin', 'Honda Accord', 10), (200, 'Dublin', 'Honda CRV', 3), (300, 'San Jose', 'Honda Civic', 5), (300, 'San Jose', 'Honda Accord', 8); -- QUALIFY with window functions in the SELECT list. > SELECT city, car_model, RANK() OVER (PARTITION BY car_model ORDER BY quantity) AS rank FROM dealer QUALIFY rank = 1; city car_model rank -------- ------------ ---- San Jose Honda Accord 1 Dublin Honda CRV 1 San Jose Honda Civic 1 -- QUALIFY with window functions in the QUALIFY clause. SELECT city, car_model FROM dealer QUALIFY RANK() OVER (PARTITION BY car_model ORDER BY quantity) = 1; city car_model -------- ------------ San Jose Honda Accord Dublin Honda CRV San Jose Honda Civic"
    },
    {
        "id": 1389,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select-qualify.html",
        "content": "Related statements\nRelated statements\nSELECT  \nWHERE clause  \nGROUP BY clause  \nORDER BY clause  \nSORT BY clause  \nCLUSTER BY clause (SELECT)  \nDISTRIBUTE BY clause  \nLIMIT clause  \nPIVOT clause  \nLATERAL VIEW clause"
    },
    {
        "id": 1390,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-aux-show-connections.html",
        "content": "SHOW CONNECTIONS  \nApplies to: Databricks SQL Databricks Runtime 13.3 LTS and above  \nLists the connections that match an optionally supplied regular expression pattern. If no pattern is supplied then the command lists all the connections in the system.  \nSyntax\nSyntax\nSHOW CONNECTIONS [ [ LIKE ] regex_pattern ]  \nYou can also use SERVERS instead of CONNECTIONS.\n\nParameters\nParameters\nregex_pattern  \nA regular expression pattern that is used to filter the results of the statement.  \nExcept for * and | character, the pattern works like a regular expression.  \n* alone matches 0 or more characters and | is used to separate multiple different regular expressions, any of which can match.  \nThe leading and trailing blanks are trimmed in the input pattern before processing. The pattern match is case-insensitive.\n\nReturns"
    },
    {
        "id": 1391,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-aux-show-connections.html",
        "content": "Returns\nA result set of cvonnections with the following columns:  \nname STRING NOT NULL: names of connections in the metastore.  \nconnection_type STRING NOT NULL: The type of the connection. For example: postgresql.  \ncreated_at STRING NOT NULL: The timestamp when the connection was created, in ISO 8601 format.  \ncreated_by STRING NOT NULL: The principal who created the connection.  \ncomment STRING: An optional user specified comment on a connection.\n\nExamples"
    },
    {
        "id": 1392,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-aux-show-connections.html",
        "content": "Examples\n> SHOW CONNECTIONS; name connection_type created_at created_by comment ------------------- --------------- ---------------------------- ------------- --------------------- mysql_connection mysql 2022-01-01T00:00:00.000+0000 alf@melmak.et mysql connection postgres_connection postgresql 2022-06-12T13:30:00.000+0000 alf@melmak.et postgresql connection > SHOW CONNECTIONS LIKE 'mysql*'; name connection_type created_at created_by comment ---------------- --------------- ---------------------------- ------------- --------------------- mysql_connection mysql 2022-01-01T00:00:00.000+0000 alf@melmak.et mysql connection"
    },
    {
        "id": 1393,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-aux-show-connections.html",
        "content": "Related articles\nRelated articles\nALTER CONNECTION  \nCOMMENT ON CONNECTION  \nCREATE CONNECTION  \nDESCRIBE CONNECTION  \nDROP CONNECTION  \nINFORMATION_SCHEMA.CONNECTIONS"
    },
    {
        "id": 1394,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-parameter-marker.html",
        "content": "Parameter markers  \nParameter markers are named or unnamed typed placeholder variables used to supply values from the API invoking the SQL statement.  \nUsing parameter markers protects your code from SQL injection attacks since it clearly separates provided values from the SQL statements.  \nYou cannot mix named and unnamed parameter markers in the same SQL statement.  \nYou must not reference a parameter marker in a DDL statement, such as a generated column or DEFAULT definition, a view, or a SQL function.  \nExceptions are references to parameter markers in the IDENTIFIER clause, which can be used to parameterize table or column names in certain DDL statements. See IDENTIFIER clause.  \nParameter markers can be provided by:  \nPython using its pyspark.sql.SparkSession.sql() API.  \nScala using its org.apache.spark.sql.SparkSession.sql() API.  \nJava using its org.apache.spark.sql.SparkSession.sql() API.  \nNamed parameter markers"
    },
    {
        "id": 1395,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-parameter-marker.html",
        "content": "Named parameter markers\nApplies to: Databricks Runtime 12.1 and above  \nNamed parameter markers are typed placeholder variables. The API invoking the SQL statement must supply name-value pairs to associate each parameter marker with a value.  \nSyntax  \n:parameter_name  \nParameters  \nnamed_parameter_name  \nA reference to a supplied parameter marker in form of an unqualified identifier.  \nNotes  \nYou can reference the same parameter marker multiple times within the same SQL Statement. If no value has been bound to the parameter marker an UNBOUND_SQL_PARAMETER error is raised. You are not required to reference all supplied parameter markers.  \nThe mandatory preceding : (colon) differentiates the namespace of named parameter markers from that of column names and SQL parameters.  \nExamples  \nThe following example defines two parameter markers:  \nlater: An INTERVAL HOUR with value 3.  \nx: A DOUBLE with value 15.0  \nx is referenced multiple times, while later is referenced once.  \n> DECLARE stmtStr = 'SELECT current_timestamp() + :later, :x * :x AS square'; > EXECUTE IMMEDIATE stmtStr USING INTERVAL '3' HOURS AS later, 15.0 AS x; 2024-01-19 16:17:16.692303 225.00"
    },
    {
        "id": 1396,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-parameter-marker.html",
        "content": "import org.apache.spark.sql.SparkSession val spark = SparkSession .builder() .appName(\"Spark named parameter marker example\") .getOrCreate() val argMap = Map(\"later\" -> java.time.Duration.ofHours(3), \"x\" -> 15.0) spark.sql( sqlText = \"SELECT current_timestamp() + :later, :x * :x AS square\", args = argMap).show() // +----------------------------------------+------+ // |current_timestamp() + INTERVAL '03' HOUR|square| // +----------------------------------------+------+ // | 2023-02-27 17:48:...|225.00| // +----------------------------------------+------+"
    },
    {
        "id": 1397,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-parameter-marker.html",
        "content": "import org.apache.spark.sql.*; import static java.util.Map.entry; SparkSession spark = SparkSession .builder() .appName(\"Java Spark named parameter marker example\") .getOrCreate(); Map<String, String> argMap = Map.ofEntries( entry(\"later\", java.time.Duration.ofHours(3)), entry(\"x\", 15.0) ); spark.sql( sqlText = \"SELECT current_timestamp() + :later, :x * :x AS square\", args = argMap).show(); // +----------------------------------------+------+ // |current_timestamp() + INTERVAL '03' HOUR|square| // +----------------------------------------+------+ // | 2023-02-27 17:48:...|225.00| // +----------------------------------------+------+  \nspark.sql(\"SELECT :x * :y * :z AS volume\", args = { \"x\" : 3, \"y\" : 4, \"z\" : 5 }).show() // +------+ // |volume| // +------+ // | 60| // +------+"
    },
    {
        "id": 1398,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-parameter-marker.html",
        "content": "Unnamed parameter markers\nApplies to: Databricks Runtime 13.3 and above  \nUnnamed parameter markers are typed placeholder variables. The API invoking the SQL statement must supply an array of arguments to associate each parameter marker with a value in the order in which they appear.  \nSyntax  \n?  \nParameters  \n?: A reference to a supplied parameter marker in form of a question mark.  \nNotes  \nEach occurrence of an unnamed parameter marker consumes a value provided by the API invoking the SQL statement in order. If no value has been bound to the parameter marker, an UNBOUND_SQL_PARAMETER error is raised. You are not required to consume all provided values.  \nExamples  \nThe following example defines three parameter markers:  \nAn INTERVAL HOUR with value 3.  \nTwo DOUBLE with value 15.0 each.  \nSince the parameters are unnamed each provided value is consumed by at most one parameter.  \n> DECLARE stmtStr = 'SELECT current_timestamp() + ?, ? * ? AS square'; > EXECUTE IMMEDIATE stmtStr USING INTERVAL '3' HOURS, 15.0, 15.0; 2024-01-19 16:17:16.692303 225.00"
    },
    {
        "id": 1399,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-parameter-marker.html",
        "content": "import org.apache.spark.sql.SparkSession val spark = SparkSession .builder() .appName(\"Spark unnamed parameter marker example\") .getOrCreate() val argArr = Array(java.time.Duration.ofHours(3), 15.0, 15.0) spark.sql( sqlText = \"SELECT current_timestamp() + ?, ? * ? AS square\", args = argArr).show() // +----------------------------------------+------+ // |current_timestamp() + INTERVAL '03' HOUR|square| // +----------------------------------------+------+ // | 2023-02-27 17:48:...|225.00| // +----------------------------------------+------+"
    },
    {
        "id": 1400,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-parameter-marker.html",
        "content": "import org.apache.spark.sql.*; SparkSession spark = SparkSession .builder() .appName(\"Java Spark unnamed parameter marker example\") .getOrCreate(); Object[] argArr = new Object[] { java.time.Duration.ofHours(3), 15.0, 15.0 } spark.sql( sqlText = \"SELECT current_timestamp() + ?, ? * ? AS square\", args = argArr).show(); // +----------------------------------------+------+ // |current_timestamp() + INTERVAL '03' HOUR|square| // +----------------------------------------+------+ // | 2023-02-27 17:48:...|225.00| // +----------------------------------------+------+  \nspark.sql(\"SELECT ? * ? * ? AS volume\", args = { 3, 4, 5 }).show() // +------+ // |volume| // +------+ // | 60| // +------+"
    },
    {
        "id": 1401,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-parameter-marker.html",
        "content": "Related articles\nRelated articles\npyspark.sql.SparkSession.sql()  \norg.apache.spark.sql.SparkSession.sql()  \nEXECUTE IMMEDIATE  \nSET VARIABLE"
    },
    {
        "id": 1402,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-comment.html",
        "content": "COMMENT ON  \nApplies to: Databricks SQL Databricks Runtime  \nSets a comment on a catalog, schema, table, share, recipient, provider, or volume.  \nNote  \nIf you want to add an AI-generated comment for a table or table column managed by Unity Catalog, see Add AI-generated comments to a table.  \nCatalogs, shares, recipients, and providers are supported in Unity Catalog only.  \nTo set the comment of a table column, use ALTER TABLE.  \nSyntax\nSyntax\nCOMMENT ON { CATALOG catalog_name | CONNECTION connection_name | PROVIDER provider_name | RECIPIENT recipient_name | { SCHEMA | DATABASE } schema_name | SHARE share_name | TABLE table_name | VOLUME volume_name } IS comment\n\nParameters"
    },
    {
        "id": 1403,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-comment.html",
        "content": "Parameters\ncatalog_name  \nThe name of the catalog where your comment appears. To run this statement, you must be the owner of the catalog.  \nconnection_name  \nApplies to: Databricks SQL Databricks Runtime 13.3 LTS and above Unity Catalog only  \nThe name of the connection where your comment appears. To run this statement, you must be the owner of the connection.  \nschema_name  \nThe name of the schema where your comment appears.  \nIf you use Unity Catalog, to run this statement, you must be the owner of the schema.  \ntable_name  \nThe name of the table you comment on. The name must not include a temporal specification.  \nIf you use Unity Catalog, to run this statement, you must have MODIFY privilege on the table.  \nshare_name  \nApplies to: Databricks SQL Databricks Runtime 11.3 LTS and above  \nThe name of the share where your comment appears. To run this statement, you must be the owner of the share.  \nrecipient_name  \nApplies to: Databricks SQL Databricks Runtime 11.3 LTS and above  \nThe name of the recipient where your comment appears. To run this statement, you must be the owner of the recipient.  \nprovider_name  \nApplies to: Databricks SQL Databricks Runtime 11.3 LTS and above  \nThe name of the provider where your comment appears. To run this statement, you must be the owner of the provider.  \nvolume_name  \nApplies to: Databricks SQL Databricks Runtime 13.3 LTS and above  \nThe name of the volume where your comment appears. To run this statement, you must be the owner of the volume.  \ncomment  \nA STRING literal or NULL. If you specify NULL any existing comment is removed."
    },
    {
        "id": 1404,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-comment.html",
        "content": "Examples\nExamples\n> COMMENT ON CATALOG my_catalog IS 'This is my catalog'; > COMMENT ON CONNECTION mysql_connection IS 'this is a mysql connection'; > COMMENT ON SCHEMA my_schema IS 'This is my schema'; > COMMENT ON TABLE my_table IS 'This is my table'; > COMMENT ON TABLE my_table IS NULL; > COMMENT ON SHARE my_share IS 'A good share'; > COMMENT ON RECIPIENT my_recipient IS 'A good recipient'; > COMMENT ON PROVIDER my_provider IS 'A good provider'; > COMMENT ON PROVIDER my_volume IS 'Huge volume';\n\nRelated articles\nRelated articles\nCREATE CATALOG  \nDESCRIBE CATALOG  \nDROP CATALOG  \nALTER SCHEMA  \nCREATE SCHEMA  \nDESCRIBE SCHEMA  \nDROP SCHEMA  \nALTER TABLE  \nCREATE TABLE  \nDESCRIBE TABLE  \nDROP TABLE  \nALTER SHARE  \nCREATE SHARE  \nDESCRIBE SHARE  \nDROP SHARE  \nALTER RECIPIENT  \nCREATE RECIPIENT  \nDESCRIBE RECIPIENT  \nDROP RECIPIENT  \nALTER PROVIDER  \nDESCRIBE PROVIDER  \nDROP PROVIDER  \nALTER VOLUME  \nDESCRIBE VOLUME  \nDROP VOLUME"
    },
    {
        "id": 1405,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-aux-list.html",
        "content": "LIST  \nApplies to: Databricks SQL Databricks Runtime 10.4 LTS and above Unity Catalog only  \nLists the objects immediately contained at the URL.  \nSyntax\nSyntax\nLIST url [ WITH ( CREDENTIAL credential_name ) ] [ LIMIT limit ]\n\nParameters\nParameters\nurl  \nA STRING literal with the location of the cloud storage described as an absolute URL.  \ncredential_name  \nAn optional named credential used to access this URL. If you supply a credential it must be sufficient to access the URL. If you do not supply a credential the URL must be contained in an external location to to which you have access.  \nlimit  \nAn optional INTEGER constant used to limit the number of objects returned.  \nIn Databricks Runtime 10.4 LTS the default limit is 1001 and only values between 1 and 1001 are supported.\n\nExamples"
    },
    {
        "id": 1406,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-aux-list.html",
        "content": "Examples\n> LIST 's3://us-east-1-dev/some_dir' WITH (CREDENTIAL aws_some_dir) LIMIT 2 path name size modification_time ---------------------------------- ------ ---- ----------------- s3://us-east-1-dev/some_dir/table1 table1 0 ... s3://us-east-1-dev/some_dir/table1 table1 0 ...\n\nRelated articles\nRelated articles\nALTER EXTERNAL LOCATION  \nCREATE EXTERNAL LOCATION  \nDESCRIBE EXTERNAL LOCATIONS  \nDROP EXTERNAL LOCATION"
    },
    {
        "id": 1407,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-function.html",
        "content": "CREATE FUNCTION (External)  \nApplies to: Databricks Runtime  \nCreates a temporary or permanent external function. Temporary functions are scoped at a session level where as permanent functions are created in the persistent catalog and are made available to all sessions. The resources specified in the USING clause are made available to all executors when they are executed for the first time.  \nIn addition to the SQL interface, Spark allows you to create custom user defined scalar and aggregate functions using Scala, Python, and Java APIs. See External user-defined scalar functions (UDFs) and User-defined aggregate functions (UDAFs) for more information.  \nSyntax\nSyntax\nCREATE [ OR REPLACE ] [ TEMPORARY ] FUNCTION [ IF NOT EXISTS ] function_name AS class_name [ resource_locations ]\n\nParameters"
    },
    {
        "id": 1408,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-function.html",
        "content": "Parameters\nOR REPLACE  \nIf specified, the resources for the function are reloaded. This is mainly useful to pick up any changes made to the implementation of the function. This parameter is mutually exclusive to IF NOT EXISTS and cannot be specified together.  \nTEMPORARY  \nIndicates the scope of function being created. When TEMPORARY is specified, the created function is valid and visible in the current session. No persistent entry is made in the catalog for these kind of functions.  \nIF NOT EXISTS  \nIf specified, creates the function only when it does not exist. The creation of function succeeds (no error is thrown) if the specified function already exists in the system. This parameter is mutually exclusive to OR REPLACE and cannot be specified together.  \nfunction_name  \nA name for the function. The function name may be optionally qualified with a schema name.  \nFunctions created in hive_metastore can only contain alphanumeric ASCII characters and underscores.  \nclass_name  \nThe name of the class that provides the implementation for function to be created. The implementing class should extend one of the base classes as follows:  \nShould extend UDF or UDAF in org.apache.hadoop.hive.ql.exec package.  \nShould extend AbstractGenericUDAFResolver, GenericUDF, or GenericUDTF in org.apache.hadoop.hive.ql.udf.generic package.  \nShould extend UserDefinedAggregateFunction in org.apache.spark.sql.expressions package.  \nresource_locations  \nThe list of resources that contain the implementation of the function along with its dependencies.  \nSyntax: USING { { (JAR | FILE | ARCHIVE) resource_uri } , ... }"
    },
    {
        "id": 1409,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-function.html",
        "content": "-- 1. Create a simple UDF `SimpleUdf` that increments the supplied integral value by 10. -- import org.apache.hadoop.hive.ql.exec.UDF; -- public class SimpleUdf extends UDF { -- public int evaluate(int value) { -- return value + 10; -- } -- } -- 2. Compile and place it in a JAR file called `SimpleUdf.jar` in /tmp. -- Create a table called `test` and insert two rows. > CREATE TABLE test(c1 INT); > INSERT INTO test VALUES (1), (2); -- Create a permanent function called `simple_udf`. > CREATE FUNCTION simple_udf AS 'SimpleUdf' USING JAR '/tmp/SimpleUdf.jar'; -- Verify that the function is in the registry. > SHOW USER FUNCTIONS; function ------------------ default.simple_udf -- Invoke the function. Every selected value should be incremented by 10. > SELECT simple_udf(c1) AS function_return_value FROM t1; function_return_value --------------------- 11 12 -- Created a temporary function. > CREATE TEMPORARY FUNCTION simple_temp_udf AS 'SimpleUdf' USING JAR '/tmp/SimpleUdf.jar'; -- Verify that the newly created temporary function is in the registry. -- The temporary function does not have a qualified -- schema associated with it. > SHOW USER FUNCTIONS; function ------------------ default.simple_udf simple_temp_udf -- 1. Modify `SimpleUdf`'s implementation to add supplied integral value by 20. -- import org.apache.hadoop.hive.ql.exec.UDF; -- public class SimpleUdfR extends UDF { -- public int evaluate(int value) { -- return value + 20; -- } -- } -- 2. Compile"
    },
    {
        "id": 1410,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-function.html",
        "content": "-- public class SimpleUdfR extends UDF { -- public int evaluate(int value) { -- return value + 20; -- } -- } -- 2. Compile and place it in a jar file called `SimpleUdfR.jar` in /tmp. -- Replace the implementation of `simple_udf` > CREATE OR REPLACE FUNCTION simple_udf AS 'SimpleUdfR' USING JAR '/tmp/SimpleUdfR.jar'; -- Invoke the function. Every selected value should be incremented by 20. > SELECT simple_udf(c1) AS function_return_value FROM t1; function_return_value --------------------- 21 22"
    },
    {
        "id": 1411,
        "url": "https://docs.databricks.com/en/structured-streaming/scheduler-pools.html",
        "content": "Use scheduler pools for multiple streaming workloads  \nTo enable multiple streaming queries to execute jobs concurrently on a shared cluster, you can configure queries to execute in separate scheduler pools.  \nHow do scheduler pools work?"
    },
    {
        "id": 1412,
        "url": "https://docs.databricks.com/en/structured-streaming/scheduler-pools.html",
        "content": "How do scheduler pools work?\nBy default, all queries started in a notebook run in the same fair scheduling pool. Jobs generated by triggers from all of the streaming queries in a notebook run one after another in first in, first out (FIFO) order. This can cause unnecessary delays in the queries, because they are not efficiently sharing the cluster resources.  \nScheduler pools allow you to declare which Structured Streaming queries share compute resources.  \nThe following example assigns query1 to a dedicated pool, while query2 and query3 share a scheduler pool.  \n# Run streaming query1 in scheduler pool1 spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"pool1\") df.writeStream.queryName(\"query1\").toTable(\"table1\") # Run streaming query2 in scheduler pool2 spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"pool2\") df.writeStream.queryName(\"query2\").toTable(\"table2\") # Run streaming query3 in scheduler pool2 spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"pool2\") df.writeStream.queryName(\"query3\").toTable(\"table3\")  \nNote  \nThe local property configuration must be in the same notebook cell where you start your streaming query.  \nSee Apache fair scheduler documentation for more details."
    },
    {
        "id": 1413,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-aux-cache-refresh.html",
        "content": "REFRESH CACHE  \nApplies to: Databricks Runtime  \nInvalidates and refreshes all the cached data (and the associated metadata) in Apache Spark cache for all Datasets that contains the given data source path. Path matching is by prefix, that is, / would invalidate everything that is cached.  \nSee REFRESH (MATERIALIZED VIEW or STREAMING TABLE) for refreshing the data in streaming tables and materialized views.  \nSyntax\nSyntax\nREFRESH resource_path  \nSee Disk cache vs. Spark cache for the differences between disk caching and the Apache Spark cache.\n\nParameters\nParameters\nresource_path  \nThe path of the resource that is to be refreshed.\n\nExamples\nExamples\n-- The Path is resolved using the datasource's File Index. > CREATE TABLE test(ID INT) using parquet; > INSERT INTO test SELECT 1000; > CACHE TABLE test; > INSERT INTO test SELECT 100; > REFRESH \"hdfs://path/to/table\";\n\nRelated statements\nRelated statements\nCACHE TABLE  \nCLEAR CACHE  \nUNCACHE TABLE  \nREFRESH TABLE  \nREFRESH FUNCTION"
    },
    {
        "id": 1414,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select-tvf.html",
        "content": "Table-valued function (TVF) invocation  \nApplies to: Databricks SQL Databricks Runtime  \nInvokes a function which returns a relation or a set of rows as a [table-reference](sql-ref  \nA TVF can be a:  \nSQL user-defined table function.  \nThe range table-valued function.  \nAny table-valued generator function, such as explode.  \nApplies to: Databricks SQL Databricks Runtime 12.2 LTS and above.  \nSyntax\nSyntax\nfunction_name ( [ expression [, ...] ] ) [ table_alias ]\n\nParameters\nParameters\nfunction_name  \nA table-valued function.  \nexpression  \nA combination of one or more values, operators, and SQL functions that results in a value.  \ntable_alias  \nAn optional label to reference the function result and its columns.\n\nExamples"
    },
    {
        "id": 1415,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select-tvf.html",
        "content": "Examples\n-- range call with end > SELECT * FROM range(6 + cos(3)); 0 1 2 3 4 -- range call with start and end > SELECT * FROM range(5, 10); 5 6 7 8 9 -- range call with numPartitions > SELECT * FROM range(0, 10, 2, 200); 0 2 4 6 8 -- range call with a table alias > SELECT * FROM range(5, 8) AS test; 5 6 7 -- Create a SQL UDTF and invoke it > CREATE OR REPLACE FUNCTION table_func(a INT) RETURNS TABLE RETURN SELECT a * c1 AS res FROM VALUES(1), (2), (3), (4) AS T(c1) > SELECT * FROM table_func(5); 5 10 15 20 -- Using lateral correlation > SELECT table_func.res FROM VALUES(10), (20) AS S(c1), LATERAL table_func(c1); 10 20 20 40 30 60 40 80 -- Scalar fucntion are not allowed in the FROM clause > SELECT * FROM trim('hello '); Error  \nOn Databricks SQL amd Databricks Runtime 12.2 LTS and above:"
    },
    {
        "id": 1416,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select-tvf.html",
        "content": "On Databricks SQL amd Databricks Runtime 12.2 LTS and above:  \n> SELECT * FROM explode(array(10, 20)); 10 20 > SELECT * FROM inline(array(struct(1, 'a'), struct(2, 'b'))); col1 col2 ---- ---- 1 a 2 b > SELECT * FROM posexplode(array(10,20)); pos col --- --- 0 10 1 20 > SELECT * FROM stack(2, 1, 2, 3); col0 col1 ---- ---- 1 2 3 null > SELECT * FROM json_tuple('{\"a\":1, \"b\":2}', 'a', 'b'); c0 c1 --- --- 1 2 > SELECT * FROM parse_url('http://spark.apache.org/path?query=1', 'HOST'); spark.apache.org > SELECT * FROM VALUES(1), (2) AS t1(c1), LATERAL explode (ARRAY(3,4)) AS t2(c2); c1 c2 -- -- 1 3 1 4 2 3 2 4"
    },
    {
        "id": 1417,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-drop-variable.html",
        "content": "DROP VARIABLE  \nApplies to: Databricks SQL Databricks Runtime 14.1 and above  \nDrops a temporary variable.  \nSyntax\nSyntax\nDROP TEMPORARY VARIABLE [ IF EXISTS ] variable_name\n\nParameters\nParameters\nvariable_name  \nThe name of an existing variable. The variable name may be optionally qualified with session or system.session.  \nIF EXISTS  \nIf specified, no exception is thrown when the function does not exist.\n\nExamples\nExamples\n-- Declare and drop a temporary variable > DECLARE VARIABLE myvar; > DROP TEMPORARY VARIABLE myvar; -- A temporary variable can be qualified > DROP TEMPORARY VARIABLE IF EXISTS session.myvar;\n\nRelated statements\nRelated statements\nDECLARE VARIABLE  \nSET VARIABLE"
    },
    {
        "id": 1418,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-window-functions.html",
        "content": "Window functions  \nApplies to: Databricks SQL Databricks Runtime  \nFunctions that operate on a group of rows, referred to as a window, and calculate a return value for each row based on the group of rows. Window functions are useful for processing tasks such as calculating a moving average, computing a cumulative statistic, or accessing the value of rows given the relative position of the current row.  \nSyntax\nSyntax\nfunction OVER { window_name | ( window_name ) | window_spec } function { ranking_function | analytic_function | aggregate_function } over_clause OVER { window_name | ( window_name ) | window_spec } window_spec ( [ PARTITION BY partition [ , ... ] ] [ order_by ] [ window_frame ] )\n\nParameters"
    },
    {
        "id": 1419,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-window-functions.html",
        "content": "Parameters\nfunction  \nThe function operating on the window. Different classes of functions support different configurations of window specifications.  \nranking_function  \nAny of the Ranking window functions.  \nIf specified the window_spec must include an ORDER BY clause, but not a window_frame clause.  \nanalytic_function  \nAny of the Analytic window functions.  \naggregate_function  \nAny of the Aggregate functions.  \nIf specified the function must not include a FILTER clause.  \nwindow_name  \nIdentifies a named window specification defined by the query.  \nwindow_spec  \nThis clause defines how the rows will be grouped, sorted within the group, and which rows within a partition a function operates on.  \npartition  \nOne or more expression used to specify a group of rows defining the scope on which the function operates. If no PARTITION clause is specified the partition is comprised of all rows.  \norder_by  \nThe ORDER BY clause specifies the order of rows within a partition.  \nwindow_frame  \nThe window frame clause specifies a sliding subset of rows within the partition on which the aggregate or analytics function operates.  \nYou can specify SORT BY as an alias for ORDER BY.  \nYou can also specify DISTRIBUTE BY as an alias for PARTITION BY. You can use CLUSTER BY as an alias for PARTITION BY in the absence of ORDER BY."
    },
    {
        "id": 1420,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-window-functions.html",
        "content": "> CREATE TABLE employees (name STRING, dept STRING, salary INT, age INT); > INSERT INTO employees VALUES ('Lisa', 'Sales', 10000, 35), ('Evan', 'Sales', 32000, 38), ('Fred', 'Engineering', 21000, 28), ('Alex', 'Sales', 30000, 33), ('Tom', 'Engineering', 23000, 33), ('Jane', 'Marketing', 29000, 28), ('Jeff', 'Marketing', 35000, 38), ('Paul', 'Engineering', 29000, 23), ('Chloe', 'Engineering', 23000, 25); > SELECT name, dept, salary, age FROM employees; Chloe Engineering 23000 25 Fred Engineering 21000 28 Paul Engineering 29000 23 Helen Marketing 29000 40 Tom Engineering 23000 33 Jane Marketing 29000 28 Jeff Marketing 35000 38 Evan Sales 32000 38 Lisa Sales 10000 35 Alex Sales 30000 33 > SELECT name, dept, RANK() OVER (PARTITION BY dept ORDER BY salary) AS rank FROM employees; Lisa Sales 10000 1 Alex Sales 30000 2 Evan Sales 32000 3 Fred Engineering 21000 1 Tom Engineering 23000 2 Chloe Engineering 23000 2 Paul Engineering 29000 4 Helen Marketing 29000 1 Jane Marketing 29000 1 Jeff Marketing 35000 3 > SELECT name, dept, DENSE_RANK() OVER (PARTITION BY dept ORDER BY salary ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank FROM employees; Lisa Sales 10000 1 Alex Sales 30000 2 Evan Sales 32000 3 Fred Engineering 21000 1 Tom Engineering 23000 2 Chloe Engineering 23000 2 Paul Engineering 29000 3 Helen Marketing 29000 1 Jane Marketing 29000 1 Jeff Marketing 35000 2 > SELECT name, dept, age, CUME_DIST() OVER (PARTITION BY dept ORDER BY age RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cume_dist FROM employees; Alex Sales 33 0.3333333333333333 Lisa Sales 35 0.6666666666666666 Evan Sales 38 1.0 Paul Engineering 23 0.25 Chloe Engineering 25 0.50 Fred Engineering"
    },
    {
        "id": 1421,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-window-functions.html",
        "content": "Lisa Sales 35 0.6666666666666666 Evan Sales 38 1.0 Paul Engineering 23 0.25 Chloe Engineering 25 0.50 Fred Engineering 28 0.75 Tom Engineering 33 1.0 Jane Marketing 28 0.3333333333333333 Jeff Marketing 38 0.6666666666666666 Helen Marketing 40 1.0 > SELECT name, dept, salary, MIN(salary) OVER (PARTITION BY dept ORDER BY salary) AS min FROM employees; Lisa Sales 10000 10000 Alex Sales 30000 10000 Evan Sales 32000 10000 Helen Marketing 29000 29000 Jane Marketing 29000 29000 Jeff Marketing 35000 29000 Fred Engineering 21000 21000 Tom Engineering 23000 21000 Chloe Engineering 23000 21000 Paul Engineering 29000 21000 > SELECT name, salary, LAG(salary) OVER (PARTITION BY dept ORDER BY salary) AS lag, LEAD(salary, 1, 0) OVER (PARTITION BY dept ORDER BY salary) AS lead FROM employees; Lisa Sales 10000 NULL 30000 Alex Sales 30000 10000 32000 Evan Sales 32000 30000 0 Fred Engineering 21000 NULL 23000 Chloe Engineering 23000 21000 23000 Tom Engineering 23000 23000 29000 Paul Engineering 29000 23000 0 Helen Marketing 29000 NULL 29000 Jane Marketing 29000 29000 35000 Jeff Marketing 35000 29000 0"
    },
    {
        "id": 1422,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-window-functions.html",
        "content": "Related articles\nRelated articles\nSELECT  \nORDER BY  \nwindow frame clause  \nnamed window  \nquery  \nAggregate functions  \nAnalytic window functions  \nRanking window functions"
    },
    {
        "id": 1423,
        "url": "https://docs.databricks.com/en/structured-streaming/tutorial.html",
        "content": "Run your first Structured Streaming workload  \nThis article provides code examples and explanation of basic concepts necessary to run your first Structured Streaming queries on Databricks. You can use Structured Streaming for near real-time and incremental processing workloads.  \nStructured Streaming is one of several technologies that power streaming tables in Delta Live Tables. Databricks recommends using Delta Live Tables for all new ETL, ingestion, and Structured Streaming workloads. See What is Delta Live Tables?.  \nNote  \nWhile Delta Live Tables provides a slightly modified syntax for declaring streaming tables, the general syntax for configuring streaming reads and transformations applies to all streaming use cases on Databricks. Delta Live Tables also simplifies streaming by managing state information, metadata, and numerous configurations.  \nRead from a data stream"
    },
    {
        "id": 1424,
        "url": "https://docs.databricks.com/en/structured-streaming/tutorial.html",
        "content": "Read from a data stream\nYou can use Structured Streaming to incrementally ingest data from supported data sources. Some of the most common data sources used in Databricks Structured Streaming workloads include the following:  \nData files in cloud object storage  \nMessage buses and queues  \nDelta Lake  \nDatabricks recommends using Auto Loader for streaming ingestion from cloud object storage. Auto Loader supports most file formats supported by Structured Streaming. See What is Auto Loader?.  \nEach data source provides a number of options to specify how to load batches of data. During reader configuration, the main options you might need to set fall into the following categories:  \nOptions that specify the data source or format (for example, file type, delimiters, and schema).  \nOptions that configure access to source systems (for example, port settings and credentials).  \nOptions that specify where to start in a stream (for example, Kafka offsets or reading all existing files).  \nOptions that control how much data is processed in each batch (for example, max offsets, files, or bytes per batch).\n\nUse Auto Loader to read streaming data from object storage"
    },
    {
        "id": 1425,
        "url": "https://docs.databricks.com/en/structured-streaming/tutorial.html",
        "content": "Use Auto Loader to read streaming data from object storage\nThe following example demonstrates loading JSON data with Auto Loader, which uses cloudFiles to denote format and options. The schemaLocation option enables schema inference and evolution. Paste the following code in a Databricks notebook cell and run the cell to create a streaming DataFrame named raw_df:  \nfile_path = \"/databricks-datasets/structured-streaming/events\" checkpoint_path = \"/tmp/ss-tutorial/_checkpoint\" raw_df = (spark.readStream .format(\"cloudFiles\") .option(\"cloudFiles.format\", \"json\") .option(\"cloudFiles.schemaLocation\", checkpoint_path) .load(file_path) )  \nLike other read operations on Databricks, configuring a streaming read does not actually load data. You must trigger an action on the data before the stream begins.  \nNote  \nCalling display() on a streaming DataFrame starts a streaming job. For most Structured Streaming use cases, the action that triggers a stream should be writing data to a sink. See Preparing your Structured Streaming code for production."
    },
    {
        "id": 1426,
        "url": "https://docs.databricks.com/en/structured-streaming/tutorial.html",
        "content": "Perform a streaming transformation\nPerform a streaming transformation\nStructured Streaming supports most transformations that are available in Databricks and Spark SQL. You can even load MLflow models as UDFs and make streaming predictions as a transformation.  \nThe following code example completes a simple transformation to enrich the ingested JSON data with additional information using Spark SQL functions:  \nfrom pyspark.sql.functions import col, current_timestamp transformed_df = (raw_df.select( \"*\", col(\"_metadata.file_path\").alias(\"source_file\"), current_timestamp().alias(\"processing_time\") ) )  \nThe resulting transformed_df contains query instructions to load and transform each record as it arrives in the data source.  \nNote  \nStructured Streaming treats data sources as unbounded or infinite datasets. As such, some transformations are not supported in Structured Streaming workloads because they would require sorting an infinite number of items.  \nMost aggregations and many joins require managing state information with watermarks, windows, and output mode. See Apply watermarks to control data processing thresholds.\n\nWrite to a data sink"
    },
    {
        "id": 1427,
        "url": "https://docs.databricks.com/en/structured-streaming/tutorial.html",
        "content": "Write to a data sink\nA data sink is the target of a streaming write operation. Common sinks used in Databricks streaming workloads include the following:  \nDelta Lake  \nMessage buses and queues  \nKey-value databases  \nAs with data sources, most data sinks provide a number of options to control how data is written to the target system. During writer configuration, the main options you might need to set fall into the following categories:  \nOutput mode (append by default).  \nA checkpoint location (required for each writer).  \nTrigger intervals; see Configure Structured Streaming trigger intervals.  \nOptions that specify the data sink or format (for example, file type, delimiters, and schema).  \nOptions that configure access to target systems (for example, port settings and credentials).\n\nPerform an incremental batch write to Delta Lake"
    },
    {
        "id": 1428,
        "url": "https://docs.databricks.com/en/structured-streaming/tutorial.html",
        "content": "Perform an incremental batch write to Delta Lake\nThe following example writes to Delta Lake using a specified file path and checkpoint.  \nImportant  \nAlways make sure you specify a unique checkpoint location for each streaming writer you configure. The checkpoint provides the unique identity for your stream, tracking all records processed and state information associated with your streaming query.  \nThe availableNow setting for the trigger instructs Structured Streaming to process all previously unprocessed records from the source dataset and then shut down, so you can safely execute the following code without worrying about leaving a stream running:  \ntarget_path = \"/tmp/ss-tutorial/\" checkpoint_path = \"/tmp/ss-tutorial/_checkpoint\" transformed_df.writeStream .trigger(availableNow=True) .option(\"checkpointLocation\", checkpoint_path) .option(\"path\", target_path) .start()  \nIn this example, no new records arrive in our data source, so repeat execution of this code does not ingest new records.  \nWarning  \nStructured Streaming execution can prevent auto termination from shutting down compute resources. To avoid unexpected costs, be sure to terminate streaming queries."
    },
    {
        "id": 1429,
        "url": "https://docs.databricks.com/en/structured-streaming/tutorial.html",
        "content": "Preparing your Structured Streaming code for production\nPreparing your Structured Streaming code for production\nDatabricks recommends using Delta Live Tables for most Structured Streaming workloads. The following recommendations provide a starting point for preparing Structured Streaming workloads for production:  \nRemove unnecessary code from notebooks that would return results, such as display and count.  \nDo not run Structured Streaming workloads on interactive clusters; always schedule streams as jobs.  \nTo help streaming jobs recover automatically, configure jobs with infinite retries.  \nDo not use auto-scaling for workloads with Structured Streaming.  \nFor more recommendations, see Production considerations for Structured Streaming.\n\nRead data from Delta Lake, transform, and write to Delta Lake"
    },
    {
        "id": 1430,
        "url": "https://docs.databricks.com/en/structured-streaming/tutorial.html",
        "content": "Read data from Delta Lake, transform, and write to Delta Lake\nDelta Lake has extensive support for working with Structured Streaming as both a source and a sink. See Delta table streaming reads and writes.  \nThe following example shows example syntax to incrementally load all new records from a Delta table, join them with a snapshot of another Delta table, and write them to a Delta table:  \n(spark.readStream .table(\"<table-name1>\") .join(spark.read.table(\"<table-name2>\"), on=\"<id>\", how=\"left\") .writeStream .trigger(availableNow=True) .option(\"checkpointLocation\", \"<checkpoint-path>\") .toTable(\"<table-name3>\") )  \nYou must have proper permissions configured to read source tables and write to target tables and the specified checkpoint location. Fill in all parameters denoted with angle brackets (<>) using the relevant values for your data sources and sinks.  \nNote  \nDelta Live Tables provides a fully declarative syntax for creating Delta Lake pipelines and manages properties like triggers and checkpoints automatically. See What is Delta Live Tables?."
    },
    {
        "id": 1431,
        "url": "https://docs.databricks.com/en/structured-streaming/tutorial.html",
        "content": "Read data from Kafka, transform, and write to Kafka\nApache Kafka and other messaging buses provide some of the lowest latency available for large datasets. You can use Databricks to apply transformations to data ingested from Kafka and then write data back to Kafka.  \nNote  \nWriting data to cloud object storage adds additional latency overhead. If you wish to store data from a messaging bus in Delta Lake but require the lowest latency possible for streaming workloads, Databricks recommends configuring separate streaming jobs to ingest data to the lakehouse and apply near real-time transformations for downstream messaging bus sinks.  \nThe following code example demonstrates a simple pattern to enrich data from Kafka by joining it with data in a Delta table and then writing back to Kafka:  \n(spark.readStream .format(\"kafka\") .option(\"kafka.bootstrap.servers\", \"<server:ip>\") .option(\"subscribe\", \"<topic>\") .option(\"startingOffsets\", \"latest\") .load() .join(spark.read.table(\"<table-name>\"), on=\"<id>\", how=\"left\") .writeStream .format(\"kafka\") .option(\"kafka.bootstrap.servers\", \"<server:ip>\") .option(\"topic\", \"<topic>\") .option(\"checkpointLocation\", \"<checkpoint-path>\") .start() )  \nYou must have proper permissions configured for access to your Kafka service. Fill in all parameters denoted with angle brackets (<>) using the relevant values for your data sources and sinks. See Stream processing with Apache Kafka and Databricks."
    },
    {
        "id": 1432,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-catalog.html",
        "content": "CREATE CATALOG  \nApplies to: Databricks SQL Databricks Runtime 10.4 LTS and above Unity Catalog only  \nCreates a catalog with the specified name. If a catalog with the same name already exists, an exception is thrown.  \nWhen you create a FOREIGN catalog it will be populated with all the schemas and their tables visible to the authenticating user.  \nSyntax\nSyntax\nCREATE CATALOG [ IF NOT EXISTS ] catalog_name [ USING SHARE provider_name . share_name ] [ MANAGED LOCATION 'location_path' ] [ COMMENT comment ] CREATE FOREIGN CATALOG [ IF NOT EXISTS ] catalog_name USING CONNECTION connection_name [ COMMENT comment ] OPTIONS ( { option_name = option_value } [ , ... ] )\n\nParameters"
    },
    {
        "id": 1433,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-catalog.html",
        "content": "Parameters\nFOREIGN  \nApplies to: Databricks SQL Databricks Runtime 13.3 LTS and above  \nSpecifies that the catalog is imported from a CONNECTION.  \nIF NOT EXISTS  \nCreates a catalog with the given name if it does not exist. If a catalog with the same name already exists, nothing will happen.  \ncatalog_name  \nThe name of the catalog to be created.  \nUSING SHARE provider_name . share_name  \nOptionally specifies that the catalog is based on a Delta Sharing share.  \nprovider_name  \nThe name of the Delta Sharing provider who supplied the share.  \nshare_name  \nThe name of the share provided by provider_name.  \nMANAGED LOCATION 'location_path'  \nOptionally specifies the path to a managed storage location for the catalog that is different than the metastore\u2019s root storage location. This path must be defined in an external location configuration, and you must have the CREATE MANAGED STORAGE privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words, 's3://depts/finance' or 's3://depts/finance/product'). Supported in Databricks SQL or on clusters running Databricks Runtime 11.3 LTS and above.  \nSee also Work with managed tables and Create a Unity Catalog metastore.  \nUSING CONNECTION connection_name  \nSpecifies the connection where the source catalog resides.  \ncomment  \nAn optional STRING literal. The description for the catalog.  \nOPTIONS  \nSets connection-type specific parameters needed to identify the catalog at the connection.  \noption  \nThe option key. The key can consist of one or more identifiers separated by a dot, or a STRING literal.  \nOption keys must be unique and are case-sensitive.  \nvalue  \nThe value for the option. The value must be a BOOLEAN, STRING, INTEGER, or DECIMAL constant expression. The value may also be a call to the SECRET SQL function. For example, the value for password may comprise secret('secrets.r.us', 'postgresPassword') instead of entering the literal password."
    },
    {
        "id": 1434,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-catalog.html",
        "content": "Examples\nExamples\n-- Create catalog `customer_cat`. This throws exception if catalog with name customer_cat -- already exists. > CREATE CATALOG customer_cat; -- Create catalog `customer_cat` only if catalog with same name doesn't exist. > CREATE CATALOG IF NOT EXISTS customer_cat; -- Create catalog `customer_cat` only if catalog with same name doesn't exist, with a comment. > CREATE CATALOG IF NOT EXISTS customer_cat COMMENT 'This is customer catalog'; -- Create a catalog from a Delta Sharing share. > CREATE CATALOG customer_cat USING SHARE cdc.vaccinedata; -- Create a catalog with a different managed storage location than the metastore's. > CREATE CATALOG customer_cat MANAGED LOCATION 's3://depts/finance'; -- Create a foreign catalog linked to postgresdb at postgresql_connection > CREATE FOREIGN CATALOG postgresql_catalog USING CONNECTION postgresql_connection OPTIONS (database 'postgresdb');\n\nRelated articles\nRelated articles\nALTER CATALOG  \nDESCRIBE CATALOG  \nDROP CATALOG  \nFederated Queries  \nREFRESH FOREIGN  \nSHOW SHARES IN PROVIDER"
    },
    {
        "id": 1435,
        "url": "https://docs.databricks.com/en/sql/language-manual/security-revoke.html",
        "content": "REVOKE  \nApplies to: Databricks SQL Databricks Runtime  \nRevokes an explicitly granted or denied privilege on a securable object from a principal.  \nNote  \nModifying access to the samples catalog is not supported. This catalog is available to all workspaces, but is read-only.  \nUse REVOKE ON SHARE to revoke access on shares from recipients.  \nSyntax\nSyntax\nREVOKE privilege_types ON securable_object FROM principal privilege_types { ALL PRIVILEGES | privilege_type [, ...] }\n\nParameters"
    },
    {
        "id": 1436,
        "url": "https://docs.databricks.com/en/sql/language-manual/security-revoke.html",
        "content": "Parameters\nprivilege_types  \nThis identifies one or more privileges to be revoked from the principal.  \nALL PRIVILEGES  \nRevoke all privileges applicable to the securable_object. In Unity Catalog, when ALL PRIVILEGES is revoked only the ALL PRIVILEGES privilege itself is revoked. Users retain any other privileges that were granted to them separately.  \nprivilege_type  \nThe specific privilege to be revoked on the securable_object from the principal.  \nsecurable_object  \nThe object on which the privileges are granted to the principal.  \nprincipal  \nA user, service principal, or group from which the privileges are revoked. You must enclose users, service principals, and group names with special characters in backticks ( ` ` ).\n\nExamples\nExamples\n> REVOKE ALL PRIVILEGES ON SCHEMA default FROM `alf@melmak.et`; > REVOKE SELECT ON TABLE t FROM aliens;\n\nRelated articles\nRelated articles\nGRANT  \nREPAIR PRIVILEGES  \nREVOKE ON SHARE  \nSHOW GRANTS"
    },
    {
        "id": 1437,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-aux-resource-mgmt-add-jar.html",
        "content": "ADD JAR  \nApplies to: Databricks Runtime  \nAdds a JAR file to the list of resources. The added JAR file can be listed using LIST JAR.  \nSyntax\nSyntax\nADD [JAR | JARS] file_name [...]\n\nParameters\nParameters\nfile_name  \nThe name of a JAR file to be added. It could be either on a local file system or a distributed file system.\n\nExamples\nExamples\n> ADD JAR /tmp/test.jar; > ADD JAR \"/path/to/some.jar\"; > ADD JAR '/some/other.jar'; > ADD JARS \"/path with space/abc.jar\" \"/path with space/def.jar\";\n\nRelated statements\nRelated statements\nADD ARCHIVE  \nADD FILE  \nLIST ARCHIVE  \nLIST FILE  \nLIST JAR"
    },
    {
        "id": 1438,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-location.html",
        "content": "CREATE EXTERNAL LOCATION  \nApplies to: Databricks SQL Databricks Runtime 10.4 LTS and above Unity Catalog only  \nCreates an external location with the specified name. If a location with the same name already exists, an exception is thrown.  \nFor how-to instructions, see Create an external location to connect cloud storage to Databricks.  \nSyntax\nSyntax\nCREATE EXTERNAL LOCATION [IF NOT EXISTS] location_name URL url_str WITH (STORAGE CREDENTIAL credential_name) [COMMENT comment]  \nAny object name that includes special characters, such as hyphens (-), must be surrounded by backticks ( ` ` ). Object names with underscores (_) don\u2019t require backticks. See Names and Examples.\n\nParameters"
    },
    {
        "id": 1439,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-location.html",
        "content": "Parameters\nlocation_name  \nThe name of the location to be created.  \nIF NOT EXISTS  \nCreates a location with the given name if it does not exist. If a location with the same name already exists, nothing will happen.  \nurl_str  \nA STRING literal with the location of the cloud storage described as an absolute URL. Must be surrounded by single quotes.  \ncredential_name  \nThe named credential used to connect to this location.  \ncomment  \nAn optional description for the location, or NULL. The default is NULL.\n\nExamples"
    },
    {
        "id": 1440,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-location.html",
        "content": "Examples\n-- Create a location accessed using the s3_remote_cred credential > CREATE EXTERNAL LOCATION s3_remote URL 's3://us-east-1/location' WITH (STORAGE CREDENTIAL s3_remote_cred) COMMENT 'Default source for AWS exernal data';  \nNote the backticks ( ` ` ) around the external location and storage credential names in the following example. Backticks are required for object names that include special characters, such as hyphens (-). Object names with underscores (_), such as those in the previous example, don\u2019t require backticks. See Names.  \n-- Create a location accessed using the s3_remote_cred credential > CREATE EXTERNAL LOCATION `s3-remote` URL 's3://us-east-1/location' WITH (STORAGE CREDENTIAL `s3-remote-cred`) COMMENT 'Default source for AWS exernal data';\n\nRelated articles\nRelated articles\nALTER EXTERNAL LOCATION  \nDESCRIBE EXTERNAL LOCATION  \nDROP EXTERNAL LOCATION  \nExternal locations and storage credentials  \nSHOW EXTERNAL LOCATIONS"
    },
    {
        "id": 1441,
        "url": "https://docs.databricks.com/en/sql/language-manual/security-create-group.html",
        "content": "CREATE GROUP  \nApplies to: Databricks SQL Databricks Runtime  \nCreates a workspace-local group with the specified name, optionally including a list of users and groups. Workspace-local groups are not synchronized to the Databricks account and are not compatible with Unity Catalog. For more information, see Manage workspace-local groups (legacy).  \nSyntax\nSyntax\nCREATE GROUP group_principal [ WITH [ USER user_principal [, ...] ] [ GROUP subgroup_principal [, ...] ] ]\n\nParameters\nParameters\ngroup_principal  \nThe name of the workspace-local group to be created.  \nuser_principal  \nA user to include as a member of the group.  \nsubgroup_principal  \nA workspace-local subgroup to include as a member of the group.\n\nExamples\nExamples\n-- Create an empty group. CREATE GROUP humans; -- Create tv_aliens with Alf and Thor as members. CREATE GROUP tv_aliens WITH USER `alf@melmak.et`, `thor@asgaard.et`; -- Create aliens with Hilo and tv_aliens as members. CREATE GROUP aliens WITH USER `hilo@jannus.et` GROUP tv_aliens;\n\nRelated articles"
    },
    {
        "id": 1442,
        "url": "https://docs.databricks.com/en/sql/language-manual/parameters/use_cached_result.html",
        "content": "USE_CACHED_RESULT  \nApplies to: Databricks SQL  \nThe USE_CACHED_RESULT configuration parameter controls whether the session caches results sets which meet certain restrictions. If a query is re-submitted and the underlying tables have not changed, Databricks SQL can re-use the result set, reducing query execution cost.  \nYou can set this parameter at the session level using the SET statement.  \nThe system default for this setting is TRUE.  \nExamples\nExamples\n> SET use_cached_result = true; > SELECT count(1) FROM T; -- Re-use the result from the previous query > SELECT count(1) FROM T;\n\nRelated\nRelated\nRESET  \nSET statement"
    },
    {
        "id": 1443,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-privileges.html",
        "content": "Privileges and securable objects in Unity Catalog  \nApplies to: Databricks SQL Databricks Runtime Unity Catalog only  \nA privilege is a right granted to a principal to operate on a securable object in the metastore. The privilege model and securable objects differ depending on whether you are using a Unity Catalog metastore or the legacy Hive metastore. This article describes the privilege model for the Unity Catalog. If you are using the Hive metastore, see Privileges and securable objects in the Hive metastore  \nNote  \nThis article refers to the Unity Catalog privileges and inheritance model in Privilege Model version 1.0. If you created your Unity Catalog metastore during the public preview (before August 25, 2022), you might be on an earlier privilege model that doesn\u2019t support the current inheritance model. You can upgrade to Privilege Model version 1.0 to get privilege inheritance. See Upgrade to privilege inheritance.  \nSecurable objects"
    },
    {
        "id": 1444,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-privileges.html",
        "content": "Securable objects\nA securable object is an object defined in the Unity Catalog metastore on which privileges can be granted to a principal. To manage privileges on any object, you must be its owner.  \nSyntax  \nsecurable_object { CATALOG [ catalog_name ] | CONNECTION connection_name | CLEAN ROOM clean_room_name | EXTERNAL LOCATION location_name | FUNCTION function_name | METASTORE | SCHEMA schema_name | SHARE share_name | STORAGE CREDENTIAL credential_name | [ TABLE ] table_name | MATERIALIZED VIEW view_name | VIEW view_name | VOLUME volume_name }  \nYou can also specify SERVER instead of CONNECTION and DATABASE instead of SCHEMA.  \nParameters  \nCATALOG catalog_name  \nControls access to the entire data catalog.  \nCLEAN ROOM clean_room_name  \nControls access to a clean room.  \nCONNECTION connection_name  \nControls access to the connection.  \nEXTERNAL LOCATION location_name  \nControls access to an external location.  \nFUNCTION function_name  \nControls access to a user defined function.  \nMATERIALIZED VIEW view_name  \nControls access to a materialized view.  \nMETASTORE  \nControls access to the Unity Catalog metastore attached to the workspace. When you manage privileges on a metastore, you do not include the metastore name in a SQL command. Unity Catalog will grant or revoke the privilege on the metastore attached to your workspace.  \nREGISTERED MODEL  \nControls access to an MLflow registered model.  \nSCHEMA schema_name  \nControls access to a schema.  \nSTORAGE CREDENTIAL credential_name  \nControls access to a storage credential.  \nSHARE share_name  \nControls access on a share to a recipient.  \nTABLE table_name  \nControls access to a managed or external table. If the table cannot be found Databricks raises a TABLE_OR_VIEW_NOT_FOUND error.  \nVIEW view_name  \nControls access to a view. If the view cannot be found Databricks raises a TABLE_OR_VIEW_NOT_FOUND error.  \nVOLUME volume_name  \nControls access to a volume. If the volume cannot be found Databricks raises an error."
    },
    {
        "id": 1445,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-privileges.html",
        "content": "Privilege types\nPrivilege types\nFor a list of privilege types, see Unity Catalog privileges and securable objects.\n\nExamples\nExamples\n-- Grant a privilege to the user alf@melmak.et > GRANT SELECT ON TABLE t TO `alf@melmak.et`; -- Revoke a privilege from the general public group. > REVOKE USE SCHEMA ON SCHEMA some_schema FROM `alf@melmak.et`;\n\nRelated\nRelated\nGRANT  \nPrincipal  \nREVOKE"
    },
    {
        "id": 1446,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html",
        "content": "CREATE FUNCTION (SQL and Python)  \nApplies to: Databricks SQL Databricks Runtime  \nCreates a SQL scalar or table function that takes a set of arguments and returns a scalar value or a set of rows.  \nApplies to: Databricks SQL Databricks Runtime 13.3 LTS and above  \nCreates a Python scalar function that takes a set of arguments and returns a scalar value.  \nPython UDFs require Unity Catalog on serverless or pro SQL warehouses, or a shared or single user Unity Catalog cluster.  \nApplies to: Databricks SQL Databricks Runtime 14.1 and above  \nIn addition to positional parameter invocation, you can also invoke SQL and Python UDF using named parameter invocation.  \nSyntax"
    },
    {
        "id": 1447,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html",
        "content": "Syntax\nCREATE [OR REPLACE] [TEMPORARY] FUNCTION [IF NOT EXISTS] function_name ( [ function_parameter [, ...] ] ) { [ RETURNS data_type ] | RETURNS TABLE [ ( column_spec [, ...]) ] } [ characteristic [...] ] { AS dollar_quoted_string | RETURN { expression | query } } function_parameter parameter_name data_type [DEFAULT default_expression] [COMMENT parameter_comment] column_spec column_name data_type [COMMENT column_comment] characteristic { LANGUAGE { SQL | PYTHON } | [NOT] DETERMINISTIC | COMMENT function_comment | [CONTAINS SQL | READS SQL DATA] }\n\nParameters"
    },
    {
        "id": 1448,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html",
        "content": "Parameters\nOR REPLACE  \nIf specified, the function with the same name and signature (number of parameters and parameter types) is replaced. You cannot replace an existing function with a different signature. This is mainly useful to update the function body and the return type of the function. You cannot specify this parameter with IF NOT EXISTS.  \nTEMPORARY  \nThe scope of the function being created. When you specify TEMPORARY, the created function is valid and visible in the current session. No persistent entry is made in the catalog.  \nIF NOT EXISTS  \nIf specified, creates the function only when it does not exist. The creation of the function succeeds (no error is thrown) if the specified function already exists in the system. You cannot specify this parameter with OR REPLACE.  \nfunction_name  \nA name for the function. For a permanent function, you can optionally qualify the function name with a schema name. If the name is not qualified the permanent function is created in the current schema.  \nfunction_parameter  \nSpecifies a parameter of the function.  \nparameter_name  \nThe parameter name must be unique within the function.  \ndata_type  \nAny supported data type. For Python, data_type is cast to a Python data type according to this language mapping.  \nDEFAULT default_expression  \nApplies to: Databricks SQL Databricks Runtime 10.4 LTS and above  \nAn optional default to be used when a function invocation does not assign an argument to the parameter. default_expression must be castable to data_type. The expression must not reference another parameter or contain a subquery.  \nWhen you specify a default for one parameter, all following parameters must also have a default.  \nDEFAULT is supported for LANGUAGE SQL only.  \nCOMMENT comment  \nAn optional description of the parameter. comment must be a STRING literal.  \nRETURNS data_type  \nThe return data type of the scalar function. For Python UDFs, return values must exactly match the data type as specified in data_type. Otherwise, to prevent unanticipated type conversions, the function will fail.  \nFor SQL UDF this clause is optional. The data type will be derived from the function body if it is not provided.  \nRETURNS TABLE [ (column_spec [,\u2026] ) ]"
    },
    {
        "id": 1449,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html",
        "content": "For SQL UDF this clause is optional. The data type will be derived from the function body if it is not provided.  \nRETURNS TABLE [ (column_spec [,\u2026] ) ]  \nThis clause marks the function as a table function. Optionally it also specifies the signature of the result of the table function. If no column_spec is specified it will be derived from the body of the SQL UDF.  \nRETURNS TABLE is supported for LANGUAGE SQL only.  \ncolumn_name  \nThe column name must be unique within the signature.  \ndata_type  \nAny supported data type.  \nCOMMENT column_comment  \nAn optional description of the column. comment must be a STRING literal.  \nRETURN { expression | query }  \nThe body of the function. For a scalar function, it can either be a query or an expression. For a table function, it can only be a query. The expression cannot contain:  \nAggregate functions  \nWindow functions  \nRanking functions  \nRow producing functions such as explode  \nWithin the body of the function you can refer to parameter by its unqualified name or by qualifying the parameter with the function name.  \nAS dollar_quoted_definition  \ndollar_quoted_definition is the Python function body enclosed by two matching $[tag]$body$[tag]$. tag can be an empty string.  \nExamples:  \n$$ return \u201cHello world\u201d $$ $py$ return \"Hello World\" $py$  \ncharacteristic  \nAll characteristic clauses are optional. You can specify any number of them in any order, but you can specify each clause only once.  \nLANGUAGE SQL or LANGUAGE PYTHON  \nThe language of the function implementation.  \n[NOT] DETERMINISTIC  \nWhether the function is deterministic. A function is deterministic when it returns only one result for a given set of arguments. You may mark a function as DETERMINISTIC when its body is not and vice versa. A reason for this may be to encourage or discourage query optimizations such as constant folding or query caching. If you do not specify ths option it is derived from the function body.  \nCOMMENT function_comment  \nA comment for the function. function_comment must be String literal.  \nCONTAINS SQL or READS SQL DATA"
    },
    {
        "id": 1450,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html",
        "content": "COMMENT function_comment  \nA comment for the function. function_comment must be String literal.  \nCONTAINS SQL or READS SQL DATA  \nWhether a function reads data directly or indirectly from a table or a view. When the function reads SQL data, you cannot specify CONTAINS SQL. If you don\u2019t specify either clause, the property is derived from the function body."
    },
    {
        "id": 1451,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html",
        "content": "Supported Libraries in Python UDFs\nTo use any dependencies, use import <package> within the function body. For example, see the following:  \nCREATE FUNCTION [\u2026] AS $$ import json [... (rest of function definition)] $$  \nDependencies are limited to the standard Python library and the following libraries:  \nPackage  \nVersion  \nbleach  \n4.0.0  \nchardet  \n4.0.0  \ncharset-normalizer  \n2.0.4  \ndefusedxml  \n0.7.1  \ngoogleapis-common-protos  \n1.56.4  \ngrpcio  \n1.47.0  \ngrpcio-status  \n1.47.0  \njmespath  \n0.10.0  \njoblib  \n1.1.0  \nnumpy  \n1.20.3  \npackaging  \n21.3  \npandas  \n1.3.4  \npatsy  \n0.5.2  \nprotobuf  \n4.21.5  \npyarrow  \n7.0.0  \npyparsing  \n3.0.9  \npython-dateutil  \n2.8.2  \npytz  \n2021.3  \nscikit-learn  \n0.24.2\u201d  \nscipy  \n1.7.1\u201d  \nsetuptools  \n65.2.0  \nsix  \n1.16.0  \nthreadpoolctl  \n3.1.0  \nwebencodings  \n0.5.1  \nuser-agents  \n2.2.0  \ncryptography  \n38.0.4"
    },
    {
        "id": 1452,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html",
        "content": "Examples\nCreate and use a SQL scalar function  \nCreate and use a function that uses DEFAULTs  \nCreate a SQL table function  \nReplace a SQL function  \nDescribe a SQL function  \nCreate Python functions  \nCreate and use a SQL scalar function  \n> CREATE VIEW t(c1, c2) AS VALUES (0, 1), (1, 2);  \n-- Create a temporary function with no parameter. > CREATE TEMPORARY FUNCTION hello() RETURNS STRING RETURN 'Hello World!'; > SELECT hello(); Hello World! -- Create a permanent function with parameters. > CREATE FUNCTION area(x DOUBLE, y DOUBLE) RETURNS DOUBLE RETURN x * y; -- Use a SQL function in the SELECT clause of a query. > SELECT area(c1, c2) AS area FROM t; 0.0 2.0 -- Use a SQL function in the WHERE clause of a query. > SELECT * FROM t WHERE area(c1, c2) > 0; 1 2 -- Compose SQL functions. > CREATE FUNCTION square(x DOUBLE) RETURNS DOUBLE RETURN area(x, x); > SELECT c1, square(c1) AS square FROM t; 0 0.0 1 1.0 -- Create a non-deterministic function > CREATE FUNCTION roll_dice() RETURNS INT NOT DETERMINISTIC CONTAINS SQL COMMENT 'Roll a single 6 sided die' RETURN (rand() * 6)::INT + 1; -- Roll a single 6-sided die > SELECT roll_dice(); 3  \nCreate and use a function that uses DEFAULTs"
    },
    {
        "id": 1453,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html",
        "content": "Create and use a function that uses DEFAULTs  \n-- Extend the function to support variable number of sides and dice. -- Use defaults to support a variable number of arguments > DROP FUNCTION roll_dice; > CREATE FUNCTION roll_dice(num_dice INT DEFAULT 1 COMMENT 'number of dice to roll (Default: 1)', num_sides INT DEFAULT 6 COMMENT 'number of sides per die (Default: 6)') RETURNS INT NOT DETERMINISTIC CONTAINS SQL COMMENT 'Roll a number of n-sided dice' RETURN aggregate(sequence(1, roll_dice.num_dice, 1), 0, (acc, x) -> (rand() * roll_dice.num_sides)::int, acc -> acc + roll_dice.num_dice); -- Roll a single 6-sided die still works > SELECT roll_dice(); 3 -- Roll 3 6-sided dice > SELECT roll_dice(3); 15 -- Roll 3 10-sided dice > SELECT roll_dice(3, 10) 21 -- Roll 3 10-sided dice using named parameter invocation > SELECT roll_dice(10 => num_sides, num_dice => 3) 17 -- Create a SQL function with a scalar subquery. > CREATE VIEW scores(player, score) AS VALUES (0, 1), (0, 2), (1, 2), (1, 5); > CREATE FUNCTION avg_score(p INT) RETURNS FLOAT COMMENT 'get an average score of the player' RETURN SELECT AVG(score) FROM scores WHERE player = p; > SELECT c1, avg_score(c1) FROM t; 0 1.5 1 3.5  \nCreate a SQL table function"
    },
    {
        "id": 1454,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html",
        "content": "Create a SQL table function  \n-- Produce all weekdays between two dates > CREATE FUNCTION weekdays(start DATE, end DATE) RETURNS TABLE(day_of_week STRING, day DATE) RETURN SELECT extract(DAYOFWEEK_ISO FROM day), day FROM (SELECT sequence(weekdays.start, weekdays.end)) AS T(days) LATERAL VIEW explode(days) AS day WHERE extract(DAYOFWEEK_ISO FROM day) BETWEEN 1 AND 5; -- Return all weekdays > SELECT weekdays.day_of_week, day FROM weekdays(DATE'2022-01-01', DATE'2022-01-14'); 1 2022-01-03 2 2022-01-04 3 2022-01-05 4 2022-01-06 5 2022-01-07 1 2022-01-10 2 2022-01-11 3 2022-01-12 4 2022-01-13 5 2022-01-14 -- Return weekdays for date ranges originating from a LATERAL correlation > SELECT weekdays.* FROM VALUES (DATE'2020-01-01'), (DATE'2021-01-01'), (DATE'2022-01-01') AS starts(start), LATERAL weekdays(start, start + INTERVAL '7' DAYS); 3 2020-01-01 4 2020-01-02 5 2020-01-03 1 2020-01-06 2 2020-01-07 3 2020-01-08 5 2021-01-01 1 2021-01-04 2 2021-01-05 3 2021-01-06 4 2021-01-07 5 2021-01-08 1 2022-01-03 2 2022-01-04 3 2022-01-05 4 2022-01-06 5 2022-01-07  \nReplace a SQL function"
    },
    {
        "id": 1455,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html",
        "content": "Replace a SQL function  \n-- Replace a SQL scalar function. > CREATE OR REPLACE FUNCTION square(x DOUBLE) RETURNS DOUBLE RETURN x * x; -- Replace a SQL table function. > CREATE OR REPLACE FUNCTION getemps(deptno INT) RETURNS TABLE (name STRING) RETURN SELECT name FROM employee e WHERE e.deptno = getemps.deptno; -- Describe a SQL table function. > DESCRIBE FUNCTION getemps; Function: default.getemps Type: TABLE Input: deptno INT Returns: id INT name STRING  \nNote  \nYou cannot replace an existing function with a different signature.  \nDescribe a SQL function  \n> DESCRIBE FUNCTION hello; Function: hello Type: SCALAR Input: () Returns: STRING > DESCRIBE FUNCTION area; Function: default.area Type: SCALAR Input: x DOUBLE y DOUBLE Returns: DOUBLE > DESCRIBE FUNCTION roll_dice; Function: default.roll_dice Type: SCALAR Input: num_dice INT num_sides INT Returns: INT > DESCRIBE FUNCTION EXTENDED roll_dice; Function: default.roll_dice Type: SCALAR Input: num_dice INT DEFAULT 1 'number of dice to roll (Default: 1)' num_sides INT DEFAULT 6 'number of sides per dice (Default: 6)' Returns: INT Comment: Roll a number of m-sided dice Deterministic: false Data Access: CONTAINS SQL Configs: ... Owner: the.house@always.wins Create Time: Sat Feb 12 09:29:02 PST 2022 Body: aggregate(sequence(1, roll_dice.num_dice, 1), 0, (acc, x) -> (rand() * roll_dice.num_sides)::int, acc -> acc + roll_dice.num_dice)  \nCreate Python functions"
    },
    {
        "id": 1456,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html",
        "content": "Create Python functions  \n\u2014- Hello World-like functionality using Python UDFs > CREATE FUNCTION main.default.greet(s STRING) RETURNS STRING LANGUAGE PYTHON AS $$ def greet(name): return \"Hello \" + name + \"!\" return greet(s) if s else None $$ \u2014- Can import functions from std library and environment > CREATE FUNCTION main.default.isleapyear(year INT) RETURNS BOOLEAN LANGUAGE PYTHON AS $$ import calendar return calendar.isleap(year) if year else None $$ \u2014- Must return the correct type. Otherwise will fail at runtime. > CREATE FUNCTION main.default.a_number() RETURNS INTEGER LANGUAGE PYTHON AS $$ # does not work: return \"10\" # does not work: return 3.14 return 10 $$ \u2014- Deal with exceptions. > CREATE FUNCTION main.default.custom_divide(n1 INT, n2 INT) RETURNS FLOAT LANGUAGE PYTHON AS $$ try: return n1/n2 except ZeroDivisionException: # in case of 0, we can return NULL. return None $$"
    },
    {
        "id": 1457,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-aux-show-columns.html",
        "content": "SHOW COLUMNS  \nApplies to: Databricks SQL Databricks Runtime  \nReturns the list of columns in a table. If the table does not exist, an exception is thrown.  \nSyntax\nSyntax\nSHOW COLUMNS { IN | FROM } table_name [ { IN | FROM } schema_name ]  \nNote  \nKeywords IN and FROM are interchangeable.\n\nParameters\nParameters\ntable_name  \nIdentifies the table. The name must not include a temporal specification.  \nschema_name  \nAn optional alternative means of qualifying the table_name with a schema name. When this parameter is specified then table name should not be qualified with a different schema name.\n\nExamples"
    },
    {
        "id": 1458,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-aux-show-columns.html",
        "content": "Examples\n-- Create `customer` table in the `salessc` schema; > USE SCHEMA salessc; > CREATE TABLE customer( cust_cd INT, name VARCHAR(100), cust_addr STRING); -- List the columns of `customer` table in current schema. > SHOW COLUMNS IN customer; col_name --------- cust_cd name cust_addr -- List the columns of `customer` table in `salessc` schema. > SHOW COLUMNS IN salessc.customer; col_name --------- cust_cd name cust_addr -- List the columns of `customer` table in `salesdb` schema > SHOW COLUMNS IN customer IN salessc; col_name --------- cust_cd name cust_addr\n\nRelated articles\nRelated articles\nDESCRIBE TABLE  \nINFORMATION_SCHEMA.COLUMNS  \nSHOW TABLE"
    },
    {
        "id": 1459,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-parameters.html",
        "content": "Configuration parameters  \nApplies to: Databricks SQL  \nA configuration parameter is a setting which affects the behavior of Databricks SQL outside of the specified SQL syntax.  \nThe effective value of a configuration parameter is derived from the different levels where it is set.  \nConfiguration parameter scopes\nConfiguration parameter scopes\nSystem  \nThe system value is the default value used by Databricks SQL if there is no override.  \nGlobal  \nAn administrator can override the system default value for a parameter using the Configure SQL parameters or SQL Warehouse API. This value is then the initial value seen by any newly established session moving forward.  \nSession  \nA user can override a configuration parameter temporarily within a session scope by using the SET statement. The setting remains in effect for the duration of the session unless the user overrides is again with a SET statement or RESET.\n\nSupported configuration parameters"
    },
    {
        "id": 1460,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-parameters.html",
        "content": "Supported configuration parameters\nThe following is a list of the supported configuration parameters that you can set in Databricks SQL, their system defaults, and at which levels they can be set. Databricks SQL does not support setting Spark configurations that aren\u2019t listed.  \nParameter name  \nDescription  \nSystem default  \nGlobally settable  \nSession settable  \nANSI_MODE  \nAllows you to disable strict ANSI SQL behavior for certain functions and casting rules.  \nTRUE  \nYes  \nYes  \nLEGACY_TIME_PARSER_POLICY  \nControls how date and timestamps are parsed and formatted.  \nEXCEPTION  \nYes  \nYes  \nMAX_FILE_PARTITION_BYTES  \nThe maximum number of bytes to pack into a single partition when reading from file based sources.  \n128m  \nNo  \nYes  \nREAD_ONLY_EXTERNAL_METASTORE  \nControls whether an external metastore is treated as read-only.  \nFALSE  \nYes  \nNo  \nSTATEMENT_TIMEOUT  \nSets a SQL statement timeout in seconds  \n172800 seconds  \nYes  \nYes  \nTIMEZONE  \nSets the local time zone.  \nUTC  \nYes  \nYes  \nUSE_CACHED_RESULT  \nControls whether Databricks SQL caches and reuses results whenever possible.  \nTRUE  \nNo  \nYes  \nThe ANSI_MODE system default value is FALSE for accounts added before Databricks SQL 2022.35."
    },
    {
        "id": 1461,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-parameters.html",
        "content": "Related\nRelated\nANSI_MODE  \nLEGACY_TIME_PARSER_POLICY  \nMAX_FILE_PARTITION_BYTES  \nREAD_ONLY_EXTERNAL_METASTORE  \nSTATEMENT_TIMEOUT  \nTIMEZONE  \nUSE_CACHED_RESULT  \nRESET  \nSET  \nSET TIMEZONE  \nUSE SCHEMA  \nConfigure SQL parameters  \nSQL Warehouses API"
    },
    {
        "id": 1462,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-drop-schema.html",
        "content": "DROP SCHEMA  \nApplies to: Databricks SQL Databricks Runtime  \nDrops a schema and deletes the directory associated with the schema from the file system. An exception is thrown if the schema does not exist in the system. To drop a schema you must be its owner.  \nWhile usage of SCHEMA and DATABASE is interchangeable, SCHEMA is preferred.  \nSyntax\nSyntax\nDROP SCHEMA [ IF EXISTS ] schema_name [ RESTRICT | CASCADE ]\n\nParameters"
    },
    {
        "id": 1463,
        "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-drop-schema.html",
        "content": "Parameters\nIF EXISTS  \nIf specified, no exception is thrown when the schema does not exist.  \nschema_name  \nThe name of an existing schemas in the system. If the name does not exist, an exception is thrown.  \nRESTRICT  \nIf specified, restricts dropping a non-empty schema and is enabled by default.  \nCASCADE  \nIf specified, drops all the associated tables and functions recursively. In Unity Catalog, dropping a schema using CASCADE soft-deletes tables: managed table files will be cleaned up after 30 days, but external files are not deleted. Warning! If the schema is managed by the workspace-level Hive metastore, dropping a schema using CASCADE recursively deletes all files in the specified location, regardless of the table type (managed or external).\n\nExamples\nExamples\n-- Create `inventory_schema` Database > CREATE SCHEMA inventory_schema COMMENT 'This schema is used to maintain Inventory'; -- Drop the schema and its tables > DROP SCHEMA inventory_schema CASCADE; -- Drop the schema using IF EXISTS > DROP SCHEMA IF EXISTS inventory_schema CASCADE;\n\nRelated articles\nRelated articles\nCREATE SCHEMA  \nDESCRIBE SCHEMA  \nSHOW SCHEMAS"
    },
    {
        "id": 1464,
        "url": "https://docs.databricks.com/en/archive/admin-guide/nat-gateway-proxying.html",
        "content": "Proxy traffic through a NAT gateway  \nThis guide illustrates how to configure Databricks to proxy traffic between EC2 instances and another IP address through a NAT gateway. For example, you can point your BI software to a static IP address, which proxies all traffic to a Redshift cluster through a NAT gateway. From the Redshift cluster\u2019s perspective, all instances have a stable public IP address, regardless of the Databricks cluster\u2019s configuration.  \nNote  \nTo proxy Redshift traffic, the Redshift cluster must be launched with an IPv4 elastic IP. An elastic IP is stable and publicly accessible.  \nThis article walks through allocating a new public subnet in your Databricks VPC, adding a NAT gateway inside the subnet, and updating the default route table to make sure specific traffic goes through the NAT gateway.  \nCreate a subnet in the Databricks VPC"
    },
    {
        "id": 1465,
        "url": "https://docs.databricks.com/en/archive/admin-guide/nat-gateway-proxying.html",
        "content": "Create a subnet in the Databricks VPC\nContact Databricks to retrieve the ID of your Databricks VPC.  \nLog in to your AWS VPC console and select Subnet from the left panel.  \nEnter the Databricks VPC ID in the search box to see all subnets included in the Databricks VPC.  \nBy default, Databricks creates one subnet per availability zone (AZ). You must choose an unused CIDR block from the same B-class range, such as a.b.0.0. In the example, three CIDR blocks are used: 10.29.224.0/19, 10.29.192.0/19, 10.29.160.0/19. All IPs in the address range 10.29.0.0 ~ 10.29.159.255 are free. You can choose a new C-class subnet within that range, such as 10.29.100.0/24.  \nClick Create Subnet and enter the following information:  \nName tag: the subnet\u2019s name. Databricks recommends using gateway as part of the name.  \nVPC: the Databricks VPC ID.  \nAvailability Zone: at least one availability zone, depending on your availability needs.  \nCIDR block: an unused CIDR block containing at least one IP address.  \nNote  \nDatabricks allocates CIDR blocks starting from the top of the B-class range. When you choose the CIDR block for the gateway subnet, Databricks recommends that you use a lower range to avoid future conflict if AWS adds more availability zones to the region.  \nThe public subnet needs only a single IP for the NAT gateway.  \nClick Yes, Create."
    },
    {
        "id": 1466,
        "url": "https://docs.databricks.com/en/archive/admin-guide/nat-gateway-proxying.html",
        "content": "Create a NAT gateway in the subnet\nCreate a NAT gateway in the subnet\nSelect NAT gateways from the AWS VPC console\u2019s left panel, and click Create NAT gateway.  \nIn Subnet, select the subnet you just created.  \nChoose any available IP from Elastic IP Allocation ID. If there are no available elastic IPs, first create a new elastic IP.  \nClick Create a NAT gateway.  \nOn the success page, click Edit Route Tables.\n\nAssociate a route table with the gateway subnet\nAssociate a route table with the gateway subnet\nClick Create route table.  \nIn Name tag, enter a name for the route table.  \nIn VPC, select the Databricks VPC ID.  \nClick Yes, Create.  \nSelect the newly-created route table from the list.  \nIn the Routes tab, click Edit, then click Add another route.  \nEnter 0.0.0.0/0 in Destination.  \nIn Target, choose the Databricks VPC\u2019s internet gateway, which starts with igw=.  \nClick Save.  \nClick Edit in the Subnet Associations tab.  \nSelect the NAT gateway\u2019s subnet.  \nClick Save. The subnet is now a public subnet, accessible on the internet."
    },
    {
        "id": 1467,
        "url": "https://docs.databricks.com/en/archive/admin-guide/nat-gateway-proxying.html",
        "content": "Configure routing to an external system\nFollow this procedure to forward all traffic between nodes in the VPC and an external system, such as Redshift, through the NAT gateway.  \nIn the left panel of the AWS VPC console, select Route Tables.  \nEnter the Databricks VPC ID in the search box. The list shows all route tables used by Databricks VPC.  \nOne of the routes is the gateway route table you just created.  \nThe other, labeled Main, is the default route table for all other subnets in the VPC.  \nSelect the Main route table.  \nIn the Routes tab, click Edit.  \nIn Destination, find the elastic IP of the external system. In the image below, the elastic IP is 1.2.3.4.  \nIn Target, select the NAT gateway you created earlier. It starts with nat-.  \nClick Save.  \nAll traffic between all VPC instances and the elastic IP goes through the NAT gateway. From the point of view of the external system, all nodes have the elastic IP\u2019s address.ame IP address as the NAT gateway\u2019s elastic IP. Internal traffic continues using the VPC\u2019s original routing rules."
    },
    {
        "id": 1468,
        "url": "https://docs.databricks.com/en/archive/admin-guide/nat-gateway-proxying.html",
        "content": "Add an S3 endpoint to the VPC\nAdd an S3 endpoint to the VPC\nExternal traffic that goes through the NAT gateway is subject to additional charges (see Amazon VPC pricing). To reduce both costs and latency, you can store Databricks assets in S3 buckets. Traffic between an elastic IP and a S3 bucket is free.  \nNote  \nVPC endpoints do not support cross-region access. When you access an S3 bucket in a different region, traffic is routed through the NAT gateway.  \nIn the left panel of the AWS VPC console, select Endpoints.  \nClick Create Endpoint.  \nIn VPC, select the Databricks VPC ID.  \nIn Service, select the only available S3 service.  \nClick Next Step.  \nSelect the Main route table. This associates the S3 endpoint with all private subnets.  \nClick Create Endpoint.\n\nVerify subnet settings"
    },
    {
        "id": 1469,
        "url": "https://docs.databricks.com/en/archive/admin-guide/nat-gateway-proxying.html",
        "content": "Verify subnet settings\nOn the Subnets page, go to the Route Table tab.  \nEnter the Databricks VPC ID into the search box.  \nVerify the following:  \nThe gateway subnet has the 0.0.0.0/0 route set to the VPC\u2019s internet gateway (igw-xxxxxx).  \nAll other subnets have the 0.0.0.0/0 route set to the NAT gateway (nat-xxxxxx).  \nIf the S3 endpoint is configured correctly, the list includes a route with the target set to vpce-xxxx <target>.  \nImportant  \nThe VPC peering connection to the Databricks control plane VPC (pcx-xxxxx) should remain untouched.\n\nCreate a cluster\nCreate a cluster\nTo test the new configuration, create a new cluster. It automatically uses the new NAT gateway. After the new cluster becomes ready, verify that it works as expected. Contact Databricks if the new cluster fails to start or if you encounter other issues.\n\nDisable the public IP address on the cluster"
    },
    {
        "id": 1470,
        "url": "https://docs.databricks.com/en/archive/admin-guide/nat-gateway-proxying.html",
        "content": "Disable the public IP address on the cluster\nAfter you verify that the NAT gateway proxy works as intended, you can request removal of public-facing IP addresses on newly-created clusters.  \nImportant  \nYou can\u2019t disable the public IP address on existing AWS instances.  \nContact Databricks support and inform them that you would like to disable public IP addresses.  \nAfter Databricks confirms that the configuration change is complete, terminate all clusters in Databricks.  \nWait one hour for AWS resources to be cleaned up, or manually terminate the existing instances in the AWS console. If you skip this step, Databricks might reuse existing instances that still have the previously assigned public IP addresses for any new clusters.  \nLog in to your Databricks deployment and create a new cluster. Wait for the cluster to become ready.  \nTo verify that the cluster has no public IP address, go to the AWS EC2 dashboard and check the configuration of the worker instances:  \nIf the cluster has public IP addresses, or if the cluster does not become ready, contact Databricks."
    },
    {
        "id": 1471,
        "url": "https://docs.databricks.com/en/admin/cloud-configurations/aws/permissions.html",
        "content": "Permissions in cross-account IAM roles  \nThis article lists permissions in the cross-account IAM role and the purpose of each role.  \nThe permissions are different based on how you configure your VPC.  \nIAM permissions for Databricks-managed VPCs  \nIAM permissions for customer-managed VPC  \nIAM permissions for Databricks-managed VPCs"
    },
    {
        "id": 1472,
        "url": "https://docs.databricks.com/en/admin/cloud-configurations/aws/permissions.html",
        "content": "IAM permissions for Databricks-managed VPCs\nDatabricks requires the following list of IAM permissions to operate and manage clusters in an effective manner. This configuration applies only to workspaces that use the default (Databricks-managed) VPC. To create the AWS cross-account role policy for use with the default Databricks-managed VPC, see Create an IAM role for workspace deployment.  \nThe following table lists Databricks IAM cross-account role permissions in the default configuration, the resources that they control, and the purpose for each permission.  \nAWS IAM permission  \nAWS resource  \nPurpose  \nec2:AllocateAddress  \nElastic IP address  \nAllocates an Elastic IP that is associated with the NAT Gateway used in secure cluster connectivity  \nec2:AssignPrivateIpAddresses  \nNetwork Interface  \nAssigns a private IP to EC2 instance.  \nec2:AssociateDhcpOptions  \nDHCP  \nAssociates a set of DHCP options (or no DHCP options) with a VPC.  \nec2:AssociateIamInstanceProfile  \nInstanceProfile  \nAssociate an instance profile with a running EC2 instance. This allows a Databricks pool instance to be used by clusters with different instance profiles throughout its lifetime in the pool.  \nec2:AssociateRouteTable  \nRouteTable  \nAssociates a subnet with a route table.  \nec2:AttachInternetGateway  \nInternetGateway  \nAttaches an Internet gateway to a VPC, enabling connectivity between the Internet and the VPC. This is currently required to connect to S3 buckets and update code for the workers and spark containers.  \nec2:AttachVolume  \nEBS Volume  \nAttaches volume for EBS auto-scaling.  \nec2:AuthorizeSecurityGroupEgress  \nSecurityGroup  \nAdds egress rules to the security groups if required.  \nec2:AuthorizeSecurityGroupIngress  \nSecurityGroup  \nAdds ingress rules to the security groups.  \nec2:CancelSpotInstanceRequests  \nSpotInstance  \nCancels spot instances.  \nec2:CreateDhcpOptions  \nDhcp  \nCreates DHCP options.  \nec2:CreateFleet  \nEC2 Fleet  \nCreates an EC2 fleet (used with Databricks fleet clusters).  \nec2:CreateInternetGateway  \nInternetGateway  \nCreates an Internet Gateway.  \nec2:CreateLaunchTemplate  \nLaunch Template"
    },
    {
        "id": 1473,
        "url": "https://docs.databricks.com/en/admin/cloud-configurations/aws/permissions.html",
        "content": "EC2 Fleet  \nCreates an EC2 fleet (used with Databricks fleet clusters).  \nec2:CreateInternetGateway  \nInternetGateway  \nCreates an Internet Gateway.  \nec2:CreateLaunchTemplate  \nLaunch Template  \nCreates a launch template (used with Databricks fleet clusters).  \nec2:CreateLaunchTemplateVersion  \nLaunch Template  \nCreates a launch template version (used with Databricks fleet clusters).  \nec2:CreateNatGateway  \nNatGateway  \nCreates a NAT gateway.  \nec2:CreateRoute  \nRoute  \nCreates routes during workspace setup.  \nec2:CreateRouteTable  \nRouteTable  \nCreates routes during workspace setup.  \nec2:CreateSecurityGroup  \nSecurityGroup  \nCreates security groups during initial setup  \nec2:CreateSubnet  \nSubnet  \nCreate subnets for the VPC during workspace setup.  \nec2:CreateTags  \nTags  \nAdds tags on Databricks resources.  \nec2:CreateVolume  \nEBS Volume  \nCreates volume.  \nec2:CreateVpc  \nVPC  \nCreates the Databricks-managed VPC.  \nec2:CreateVpcEndpoint  \nVPCEndpoint  \nCreates VPC endpoints as part of configuring the VPC.  \nec2:DeleteDhcpOptions  \nDHCPOptions  \nDeletes DHCPOptions  \nec2:DeleteFleets  \nEC2 Fleet  \nDeletes an EC2 fleet (used with Databricks fleet clusters)  \nec2:DeleteInternetGateway  \nInternetGateway  \nDeletes Internet Gateway during workspace deletion.  \nec2:DeleteLaunchTemplate  \nLaunch Template  \nDeletes a launch template and all its versions.  \nec2:DeleteLaunchTemplateVersions  \nLaunch Template  \nDeletes a version from a launch template (used with Databricks fleet clusters).  \nec2:DeleteNatGateway  \nNatGateway  \nDeletes NAT gateway as needed to setup the secure cluster connectivity relay.  \nec2:DeleteRoute  \nRoute  \nDeletes routes.  \nec2:DeleteRouteTable  \nRouteTable  \nDeletes route table.  \nec2:DeleteSecurityGroup  \nSecurityGroup  \nDeletes security groups during workspace deletion.  \nec2:DeleteSubnet  \nSubnet  \nDeletes subnet.  \nec2:DeleteTags  \nTags  \nRemoves tags from cluster resources to allows Databricks pool instances to be reused by clusters with different tags."
    },
    {
        "id": 1474,
        "url": "https://docs.databricks.com/en/admin/cloud-configurations/aws/permissions.html",
        "content": "ec2:DeleteSubnet  \nSubnet  \nDeletes subnet.  \nec2:DeleteTags  \nTags  \nRemoves tags from cluster resources to allows Databricks pool instances to be reused by clusters with different tags.  \nec2:DeleteVolume  \nEBS Volume  \nDeletes a volume for EBS auto-scaling. See this page.  \nec2:DeleteVpc  \nVPC  \nDeletes the VPC when customers during workspace deletion.  \nec2:DeleteVpcEndpoints  \nVPCEndpoints  \nDeletes the VPC endpoints during workspace deletion.  \nec2:DescribeAvailabilityZones  \nAvailabilityZones  \nGets a list of Availability Zones in a region so that Databricks can deploy resources in that zone.  \nec2:DescribeFleetHistory  \nEC2 Fleet  \nLists events in an EC2 fleet (used with Databricks fleet clusters).  \nec2:DescribeFleetInstances  \nEC2 Fleet  \nLists instances in an EC2 fleet (used with Databricks fleet clusters).  \nec2:DescribeFleets  \nEC2 Fleet  \nDescribes details of an EC2 fleet (used with Databricks fleet clusters).  \nec2:DescribeIamInstanceProfileAssociations  \nInstanceProfile  \nChecks the current instance profile that is set on an EC2 instance so that the right profile is set on a Databricks pool instance before it\u2019s reused by a cluster.  \nec2:DescribeInstanceStatus  \nInstance  \nConfirms that Databricks AWS instances are healthy.  \nec2:DescribeInstances  \nInstance  \nConfirms that Databricks AWS instances are healthy.  \nec2:DescribeInternetGateways  \nInternetGateway  \nDescribes InternetGateway to confirm that Databricks AWS instances have a route to the internet.  \nec2:DescribeLaunchTemplates  \nLaunch Template  \nDeletes a launch template (used with Databricks fleet clusters).  \nec2:DescribeLaunchTemplateVersions  \nLaunch Template  \nDescribes details of launch template versions (used with Databricks fleet clusters).  \nec2:DescribeNatGateways  \nNATGateway  \nDescribes a NAT Gateway to confirm that Databricks AWS instances have a route to the internet in the secure cluster connectivity architecture.  \nec2:DescribePrefixLists  \nPrefixList"
    },
    {
        "id": 1475,
        "url": "https://docs.databricks.com/en/admin/cloud-configurations/aws/permissions.html",
        "content": "ec2:DescribeNatGateways  \nNATGateway  \nDescribes a NAT Gateway to confirm that Databricks AWS instances have a route to the internet in the secure cluster connectivity architecture.  \nec2:DescribePrefixLists  \nPrefixList  \nCreates a prefix list ID to create an outbound security group rule that allows traffic from a VPC so that Databricks can access an AWS service through a gateway VPC endpoint.  \nec2:DescribeReservedInstancesOfferings  \nInstance  \nDescribes Reserved Instance pricing in support of AWS spot instance pricing.  \nec2:DescribeRouteTables  \nRouteTable  \nConfirms that the route tables are set up correctly in the Databricks-managed VPC.  \nec2:DescribeSecurityGroups  \nSecurityGroup  \nConfirms that AWS security groups are set up correctly.  \nec2:DescribeSpotInstanceRequests  \nInstance  \nDescribes spot instances.  \nec2:DescribeSpotPriceHistory  \nSpotInstance  \nDescribes spot instances.  \nec2:DescribeSubnets  \nSubnet  \nConfirms that subnets are setup correctly in Databricks VPC.  \nec2:DescribeVolumes  \nVolume  \nLists volumes.  \nec2:DescribeVpcs  \nVPC  \nConfirm that the workspace\u2019s VPC was set up correctly.  \nec2:DetachInternetGateway  \nInternetGateway  \nDetaches the Databricks created Internet Gateway during workspace deletion.  \nec2:DisassociateIamInstanceProfile  \nInstanceProfile  \nDisassociates an instance profile from an EC2 instance so that xDatabricks pool instances can be used by clusters with different instance profiles.  \nec2:DisassociateRouteTable  \nRouteTable  \nDetaches the Databricks created route table during workspace deletion.  \nec2:GetLaunchTemplateData  \nLaunch Templates  \nGets config of a launch template (used with Databricks fleet clusters).  \nec2:GetSpotPlacementScores  \nAvailability Zones  \nGets list of AZs with best spot capacity for given instance type(s).  \nec2:ModifyFleet  \nEC2 Fleet  \nModifies an EC2 fleet (used with Databricks fleet clusters).  \nec2:ModifyLaunchTemplate  \nLaunch Templates  \nModifies an existing launch template (used with Databricks fleet clusters).  \nec2:ModifyVpcAttribute  \nVPCAttribute  \nConfigures the Databricks-managed VPC."
    },
    {
        "id": 1476,
        "url": "https://docs.databricks.com/en/admin/cloud-configurations/aws/permissions.html",
        "content": "Launch Templates  \nModifies an existing launch template (used with Databricks fleet clusters).  \nec2:ModifyVpcAttribute  \nVPCAttribute  \nConfigures the Databricks-managed VPC.  \nec2:ReleaseAddress  \nAddress  \nDetach the Databricks created address during workspace deletion.  \nec2:ReplaceIamInstanceProfileAssociation  \nInstanceProfile  \nSwaps one instance profile for another on an EC2 instance so that Databricks pool instances can be used by clusters with different instance profiles.  \nec2:RequestSpotInstances  \nSpotInstance  \nRequests spot instances.  \nec2:RevokeSecurityGroupEgress  \nSecurityGroup  \nUpdates Databricks-managed security groups if required.  \nec2:RevokeSecurityGroupIngress  \nSecurityGroup  \nUpdates security groups.  \nec2:RunInstances  \nInstance  \nLaunches AWS instances to create Spark Clusters. Also leveraged during scaling up an existing Spark cluster.  \nec2:TerminateInstances  \nInstance  \nTerminates Spark EC2 nodes during cluster scale down or to terminate a given Spark cluster.  \niam:CreateServiceLinkedRole  \nServiceLinkedRole  \nSets up support for spot instances.  \niam:PutRolePolicy  \nRolePolicy  \nConfigures Databricks to use spot instances."
    },
    {
        "id": 1477,
        "url": "https://docs.databricks.com/en/admin/cloud-configurations/aws/permissions.html",
        "content": "IAM permissions for customer-managed VPC\nIf you use a customer-managed VPC, there\u2019s a smaller set of permissions needed for the cross-account IAM role. This feature requires the Premium plan or above.  \nTo create the AWS cross-account role policy for use with a customer-managed VPC, see Option 2: Customer-managed VPC with default restrictions policy.  \nThe permissions can be scoped down further if needed. To create the AWS cross-account role policy for use with a customer-managed VPC with additional custom restrictions on resources, see Option 3: Customer-managed VPC with custom policy restrictions.  \nThe following table lists Databricks IAM cross-account role permissions for a customer-managed VPC, the resources that they control, and the purpose for each permission.  \nAWS IAM permission  \nAWS resource  \nPurpose  \nec2:AssociateIamInstanceProfile  \nInstanceProfile  \nAssociates an instance profile with a running EC2 instance so that a Databricks pool instance can be used by clusters with different instance profiles throughout its lifetime in the pool.  \nec2:AttachVolume  \nVolume  \nAttaches a volume.  \nec2:AuthorizeSecurityGroupEgress  \nSecurityGroup  \nAdd egress rules to the security groups if required.  \nec2:AuthorizeSecurityGroupIngress  \nSecurityGroup  \nAdds ingress rules to the security groups.  \nec2:CancelSpotInstanceRequests  \nSpotInstance  \nCancels spot instances.  \nec2:CreateFleet  \nEC2 Fleet  \nCreates an EC2 fleet (used with Databricks fleet clusters).  \nec2:CreateLaunchTemplate  \nLaunch Template  \nCreates a launch template (used with Databricks fleet clusters).  \nec2:CreateTags  \nTags  \nAdds tags on Databricks resources.  \nec2:CreateVolume  \nVolume  \nCreates a volume.  \nec2:DeleteFleets  \nEC2 Fleet  \nDeletes an EC2 fleet (used with Databricks fleet clusters)  \nec2:DeleteLaunchTemplate  \nLaunch Template  \nDeletes a launch template and all its versions.  \nec2:DeleteLaunchTemplateVersions  \nLaunch Template  \nDeletes a version from a launch template (used with Databricks fleet clusters).  \nec2:DeleteTags  \nTags  \nRemoves tags from cluster resources so that Databricks pool instances can be reused by clusters with different tags.  \nec2:DeleteVolume"
    },
    {
        "id": 1478,
        "url": "https://docs.databricks.com/en/admin/cloud-configurations/aws/permissions.html",
        "content": "ec2:DeleteTags  \nTags  \nRemoves tags from cluster resources so that Databricks pool instances can be reused by clusters with different tags.  \nec2:DeleteVolume  \nVolume  \nDeletes a volume.  \nec2:DescribeAvailabilityZones  \nAvailabilityZones  \nGets a list of Availability Zones in a region so that Databricks can deploy the resources in that zone.  \nec2:DescribeIamInstanceProfileAssociations  \nInstanceProfile  \nChecks the current instance profile set on an EC2 instance to confirm that the right profile is set on a Databricks pool instance before it\u2019s reused by a cluster.  \nec2:DescribeInstanceStatus  \nInstance  \nConfirms that Databricks AWS instances are healthy.  \nec2:DescribeInstances  \nInstance  \nConfirm that Databricks AWS instances are healthy.  \nec2:DescribeInternetGateways  \nInternetGateway  \nDescribes InternetGateway to confirm that Databricks AWS instances have a route to the internet.  \nec2:DescribeFleetHistory  \nEC2 Fleet  \nLists events in an EC2 fleet (used with Databricks fleet clusters).  \nec2:DescribeFleetInstances  \nEC2 Fleet  \nLists instances in an EC2 fleet (used with Databricks fleet clusters).  \nec2:DescribeFleets  \nEC2 Fleet  \nDescribes details of an EC2 fleet (used with Databricks fleet clusters).  \nec2:DescribeLaunchTemplates  \nLaunch Template  \nDeletes a launch template (used with Databricks fleet clusters).  \nec2:DescribeLaunchTemplateVersions  \nLaunch Template  \nDescribes details of launch template versions (used with Databricks fleet clusters).  \nec2:DescribeNatGateways  \nNATGateway  \nDescribes NATGateway to confirm that Databricks AWS instances have a route to the internet in the secure cluster connectivity architecture.  \nec2:DescribeNetworkAcls  \nNetworkAcl  \nConfirms the correct Network ACL setup.  \nec2:DescribePrefixLists  \nPrefixList  \nGets a list of prefix list IDs to create an outbound security group rule that allows traffic from a VPC to access an AWS service through a gateway VPC endpoint.  \nec2:DescribeReservedInstancesOfferings  \nInstance  \nGets Reserved Instance pricing as the starting point for AWS spot instance pricing.  \nec2:DescribeRouteTables  \nRouteTable"
    },
    {
        "id": 1479,
        "url": "https://docs.databricks.com/en/admin/cloud-configurations/aws/permissions.html",
        "content": "ec2:DescribeReservedInstancesOfferings  \nInstance  \nGets Reserved Instance pricing as the starting point for AWS spot instance pricing.  \nec2:DescribeRouteTables  \nRouteTable  \nConfirms that route tables are set up correctly in the VPC.  \nec2:DescribeSecurityGroups  \nSecurityGroup  \nConfirms that AWS security groups are set up correctly.  \nec2:DescribeSpotInstanceRequests  \nInstance  \nDescribes spot instance.  \nec2:DescribeSpotPriceHistory  \nSpotInstance  \nDescribes spot instances.  \nec2:DescribeSubnets  \nSubnet  \nConfirms that subnets are setup correctly in the VPC.  \nec2:DescribeVolumes  \nVolume  \nList volumes.  \nec2:DescribeVpcAttribute  \nVPC  \nDescribes VPC attributes including but not limited to enableDnsHostnames.  \nec2:DescribeVpcs  \nVPC  \nConfirms that the Databricks workspace VPC was created.  \nec2:DetachVolume  \nVolume  \nDetaches an EBS volume from EC2 instances during cluster shutdown.  \nec2:DisassociateIamInstanceProfile  \nInstanceProfile  \nDisassociates an instance profile from an EC2 instance so that pool instances can be used by clusters with different instance profiles.  \nec2:GetLaunchTemplateData  \nLaunch Templates  \nGets config of a launch template (used with Databricks fleet clusters).  \nec2:ModifyFleet  \nEC2 Fleet  \nModifies an EC2 fleet (used with Databricks fleet clusters).  \nec2:ModifyLaunchTemplate  \nLaunch Templates  \nModifies an existing launch template (used with Databricks fleet clusters).  \nec2:ReplaceIamInstanceProfileAssociation  \nInstanceProfile  \nSwaps one instance profile for another on an EC2 instance so that pool instances can be used by clusters with different instance profiles.  \nec2:RequestSpotInstances  \nSpotInstance  \nRequests spot instances.  \nec2:RevokeSecurityGroupEgress  \nSecurityGroup  \nUpdates Databricks-managed security groups if required  \nec2:RevokeSecurityGroupIngress  \nSecurityGroup  \nUpdates security groups.  \nec2:RunInstances  \nInstance  \nLaunches AWS instances to create Spark Clusters. Also used to scale up an existing Spark cluster.  \nec2:TerminateInstances  \nInstance  \nTerminates Spark EC2 nodes during cluster scale down or to terminate a Spark cluster.  \niam:CreateServiceLinkedRole"
    },
    {
        "id": 1480,
        "url": "https://docs.databricks.com/en/admin/cloud-configurations/aws/permissions.html",
        "content": "ec2:TerminateInstances  \nInstance  \nTerminates Spark EC2 nodes during cluster scale down or to terminate a Spark cluster.  \niam:CreateServiceLinkedRole  \nServiceLinkedRole  \nSets up support for spot instances.  \niam:PutRolePolicy  \nRolePolicy  \nConfigures Databricks to use spot instances."
    },
    {
        "id": 1481,
        "url": "https://docs.databricks.com/en/archive/admin-guide/account-console.html",
        "content": "Access the account console (legacy)  \nImportant  \nThis documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. To view current admin documentation, see Manage your Databricks account.  \nThe account console is where you administer your Databricks account-level configurations. Only the account owner who initially created the Databricks account can log in to the account console. To transfer account owner rights, contact your Databricks account team.  \nNote  \nIf your account is on the E2 version of the platform, instead see Manage your Databricks account. All new Databricks accounts and most existing accounts are now E2.  \nTo access the account console when you are viewing a workspace, click Settings at the lower left and select Manage Account.  \nYou can also go directly to https://accounts.cloud.databricks.com.  \nEnter your account owner email address and password to log in. How you get your initial account owner login ID and password depends on the way you signed up for Databricks. In most cases, you will receive a Welcome email. Follow the link in the email to verify your email address and change your temporary password. When you\u2019ve changed your password, you can open the account console."
    },
    {
        "id": 1482,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Maintenance updates for Databricks Runtime (archived)  \nThis archived page lists maintenance updates issued for Databricks Runtime releases that are no longer supported. To add a maintenance update to an existing cluster, restart the cluster.  \nTo migrate to a supported Databricks Runtime version, see the Databricks Runtime migration guide.  \nImportant  \nThis documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See Databricks Runtime release notes versions and compatibility.  \nNote  \nThis article contains references to the term whitelist, a term that Databricks does not use. When the term is removed from the software, we\u2019ll remove it from this article.  \nDatabricks Runtime releases"
    },
    {
        "id": 1483,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Databricks Runtime releases\nMaintenance updates by release:  \nDatabricks Runtime 15.0  \nDatabricks Runtime 14.0  \nDatabricks Runtime 13.1  \nDatabricks Runtime 12.2 LTS  \nDatabricks Runtime 11.3 LTS  \nDatabricks Runtime 10.4 LTS  \nDatabricks Runtime 9.1 LTS  \nDatabricks Runtime 13.0 (EoS)  \nDatabricks Runtime 12.1 (EoS)  \nDatabricks Runtime 12.0 (EoS)  \nDatabricks Runtime 11.2 (EoS)  \nDatabricks Runtime 11.1 (EoS)  \nDatabricks Runtime 11.0 (EoS)  \nDatabricks Runtime 10.5 (EoS)  \nDatabricks Runtime 10.3 (EoS)  \nDatabricks Runtime 10.2 (EoS)  \nDatabricks Runtime 10.1 (EoS)  \nDatabricks Runtime 10.0 (EoS)  \nDatabricks Runtime 9.0 (EoS)  \nDatabricks Runtime 8.4 (EoS)  \nDatabricks Runtime 8.3 (EoS)  \nDatabricks Runtime 8.2 (EoS)  \nDatabricks Runtime 8.1 (EoS)  \nDatabricks Runtime 8.0 (EoS)  \nDatabricks Runtime 7.6 (EoS)  \nDatabricks Runtime 7.5 (EoS)  \nDatabricks Runtime 7.3 LTS (EoS)  \nDatabricks Runtime 6.4 Extended Support (EoS)  \nDatabricks Runtime 5.5 LTS (EoS)  \nDatabricks Light 2.4 Extended Support  \nDatabricks Runtime 7.4 (EoS)  \nDatabricks Runtime 7.2 (EoS)  \nDatabricks Runtime 7.1 (EoS)  \nDatabricks Runtime 7.0 (EoS)  \nDatabricks Runtime 6.6 (EoS)  \nDatabricks Runtime 6.5 (EoS)  \nDatabricks Runtime 6.3 (EoS)  \nDatabricks Runtime 6.2 (EoS)  \nDatabricks Runtime 6.1 (EoS)  \nDatabricks Runtime 6.0 (EoS)  \nDatabricks Runtime 5.4 ML (EoS)  \nDatabricks Runtime 5.4 (EoS)"
    },
    {
        "id": 1484,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Databricks Runtime 6.1 (EoS)  \nDatabricks Runtime 6.0 (EoS)  \nDatabricks Runtime 5.4 ML (EoS)  \nDatabricks Runtime 5.4 (EoS)  \nDatabricks Runtime 5.3 (EoS)  \nDatabricks Runtime 5.2 (EoS)  \nDatabricks Runtime 5.1 (EoS)  \nDatabricks Runtime 5.0 (EoS)  \nDatabricks Runtime 4.3 (EoS)  \nDatabricks Runtime 4.2 (EoS)  \nDatabricks Runtime 4.1 ML (EoS)  \nDatabricks Runtime 4.1 (EoS)  \nDatabricks Runtime 4.0 (EoS)  \nDatabricks Runtime 3.5 LTS (EoS)  \nDatabricks Runtime 3.4 (EoS)  \nDatabricks Runtime 3.3 (EoS)  \nDatabricks Runtime 3.2 (EoS)  \n2.1.1-db6 (EoS)  \nFor the maintenance updates on supported Databricks Runtime versions, see Databricks Runtime maintenance updates.  \nDatabricks Runtime 15.0  \nSee Databricks Runtime 15.0 (EoS).  \nMay 30, 2024  \n(Behavior change) dbutils.widgets.getAll() is now supported to get all widget values in a notebook.  \nApril 25, 2024  \n[SPARK-47786] SELECT DISTINCT () should not become SELECT DISTINCT struct() (revert to previous behavior)  \n[SPARK-47802][SQL] Revert () from meaning struct() back to meaning *  \n[SPARK-47509][SQL] Block subquery expressions in lambda and higher-order functions  \n[SPARK-47722] Wait until RocksDB background work finish before closing  \n[SPARK-47081][CONNECT][FOLLOW] Improving the usability of the Progress Handler  \n[SPARK-47694][CONNECT] Make max message size configurable on the client side  \n[SPARK-47669][SQL][CONNECT][PYTHON] Add Column.try_cast"
    },
    {
        "id": 1485,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-47694][CONNECT] Make max message size configurable on the client side  \n[SPARK-47669][SQL][CONNECT][PYTHON] Add Column.try_cast  \n[SPARK-47664][PYTHON][CONNECT][Cherry-pick-15.0] Validate the column name with cached schema  \n[SPARK-47818][CONNECT][Cherry-pick-15.0] Introduce plan cache in SparkConnectPlanner to improve performance of Analyze requests  \n[SPARK-47704][SQL] JSON parsing fails with \u201cjava.lang.ClassCastException\u201d when spark.sql.json.enablePartialResults is enabled  \n[SPARK-47755][CONNECT] Pivot should fail when the number of distinct values is too large  \n[SPARK-47713][SQL][CONNECT] Fix a self-join failure  \n[SPARK-47812][CONNECT] Support Serialization of SparkSession for ForEachBatch worker  \n[SPARK-47828][CONNECT][PYTHON] DataFrameWriterV2.overwrite fails with invalid plan  \n[SPARK-47862][PYTHON][CONNECT]Fix generation of proto files  \n[SPARK-47800][SQL] Create new method for identifier to tableIdentifier conversion  \nOperating system security updates.  \nApril 3, 2024  \n(Behavior change) To ensure consistent behavior across compute types, PySpark UDFs on shared clusters now match the behavior of UDFs on no-isolation and assigned clusters. This update includes the following changes that might break existing code:  \nUDFs with a string return type no longer implicitly convert non-string values into string values. Previously, UDFs with a return type of str would wrap the return value with a str() function regardless of the actual data type of the returned value.  \nUDFs with timestamp return types no longer implicitly apply a conversion to timestamp with timezone.  \nThe Spark cluster configurations spark.databricks.sql.externalUDF.* no longer apply to PySpark UDFs on shared clusters."
    },
    {
        "id": 1486,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "The Spark cluster configurations spark.databricks.sql.externalUDF.* no longer apply to PySpark UDFs on shared clusters.  \nThe Spark cluster configuration spark.databricks.safespark.externalUDF.plan.limit no longer affects PySpark UDFs, removing the Public Preview limitation of 5 UDFs per query for PySpark UDFs.  \nThe Spark cluster configuration spark.databricks.safespark.sandbox.size.default.mib no longer applies to PySpark UDFs on shared clusters. Instead, available memory on the system is used. To limit the memory of PySpark UDFs, use spark.databricks.pyspark.udf.isolation.memoryLimit with a minimum value of 100m.  \nThe TimestampNTZ data type is now supported as a clustering column with liquid clustering. See Use liquid clustering for Delta tables.  \n[SPARK-47218][SQL] XML: Ignore commented row tags in XML tokenizer  \n[SPARK-46990][SQL] Fix loading empty Avro files emitted by event-hubs  \n[SPARK-47033][SQL] Fix EXECUTE IMMEDIATE USING does not recognize session variable names  \n[SPARK-47368][SQL] Remove inferTimestampNTZ config check in ParquetRowConverter  \n[SPARK-47561][SQL] Fix analyzer rule order issues about Alias  \n[SPARK-47638][PS][CONNECT] Skip column name validation in PS  \n[BACKPORT][[SPARK-46906]]https://issues.apache.org/jira/browse/SPARK-46906)[SS] Add a check for stateful operator change for streaming  \n[SPARK-47569][SQL] Disallow comparing variant.  \n[SPARK-47241][SQL] Fix rule order issues for ExtractGenerator  \n[SPARK-47218] [SQL] XML: Changed SchemaOfXml to fail on DROPMALFORMED mode  \n[SPARK-47300][SQL] quoteIfNeeded should quote identifier starts with digits"
    },
    {
        "id": 1487,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-47300][SQL] quoteIfNeeded should quote identifier starts with digits  \n[SPARK-47009][SQL][Collation] Enable create table support for collation  \n[SPARK-47322][PYTHON][CONNECT] Make withColumnsRenamed column names duplication handling consistent with withColumnRenamed  \n[SPARK-47544][PYTHON] SparkSession builder method is incompatible with visual studio code intellisense  \n[SPARK-47511][SQL] Canonicalize With expressions by re-assigning IDs  \n[SPARK-47385] Fix tuple encoders with Option inputs.  \n[SPARK-47200][SS] Error class for Foreach batch sink user function error  \n[SPARK-47135][SS] Implement error classes for Kafka data loss exceptions  \n[SPARK-38708][SQL] Upgrade Hive Metastore Client to the 3.1.3 for Hive 3.1  \n[SPARK-47305][SQL] Fix PruneFilters to tag the isStreaming flag of LocalRelation correctly when the plan has both batch and streaming  \n[SPARK-47380][CONNECT] Ensure on the server side that the SparkSession is the same  \nOperating system security updates.  \nDatabricks Runtime 14.0  \nSee Databricks Runtime 14.0 (EoS).  \nFebruary 8, 2024  \n[SPARK-46396] Timestamp inference should not throw exception.  \n[SPARK-46794] Remove subqueries from LogicalRDD constraints.  \n[SPARK-45182] Ignore task completion from old stage after retrying parent-indeterminate stage as determined by checksum.  \n[SPARK-46933] Add query execution time metric to connectors which use JDBCRDD.  \n[SPARK-45957] Avoid generating execution plan for non-executable commands.  \n[SPARK-46861] Avoid Deadlock in DAGScheduler.  \n[SPARK-46930] Add support for a custom prefix for Union type fields in Avro.  \n[SPARK-46941] Can\u2019t insert window group limit node for top-k computation if contains SizeBasedWindowFunction."
    },
    {
        "id": 1488,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-46941] Can\u2019t insert window group limit node for top-k computation if contains SizeBasedWindowFunction.  \n[SPARK-45582] Ensure that store instance is not used after calling commit within output mode streaming aggregation.  \nOperating system security updates.  \nJanuary 31, 2024  \n[SPARK-46541] Fix the ambiguous column reference in self join.  \n[SPARK-46676] dropDuplicatesWithinWatermark should not fail on canonicalization of the plan.  \n[SPARK-46769] Refine timestamp related schema inference.  \n[SPARK-45498] Followup: Ignore task completion from old stage attempts.  \nRevert [SPARK-46769] Refine timestamp related schema inference.  \n[SPARK-46383] Reduce Driver Heap Usage by Reducing the Lifespan of TaskInfo.accumulables().  \n[SPARK-46633] Fix Avro reader to handle zero-length blocks.  \n[SPARK-46677] Fix dataframe[\"*\"] resolution.  \n[SPARK-46684] Fix CoGroup.applyInPandas/Arrow to pass arguments properly.  \n[SPARK-46763] Fix assertion failure in ReplaceDeduplicateWithAggregate for duplicate attributes.  \n[SPARK-46610] Create table should throw exception when no value for a key in options.  \nOperating system security updates.  \nJanuary 17, 2024  \nThe shuffle node of the explain plan returned by a Photon query is updated to add the causedBroadcastJoinBuildOOM=true flag when an out-of-memory error occurs during a shuffle that is part of a broadcast join.  \nTo avoid increased latency when communicating over TLSv1.3, this maintenance release includes a patch to the JDK 8 installation to fix JDK bug JDK-8293562.  \n[SPARK-46394] Fix spark.catalog.listDatabases() issues on schemas with special characters when spark.sql.legacy.keepCommandOutputSchema set to true.  \n[SPARK-46250] Deflake test_parity_listener."
    },
    {
        "id": 1489,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-46250] Deflake test_parity_listener.  \n[SPARK-45814] Make ArrowConverters.createEmptyArrowBatch call close() to avoid memory leak.  \n[SPARK-46173] Skipping trimAll call during date parsing.  \n[SPARK-46484] Make resolveOperators helper functions keep the plan id.  \n[SPARK-46466] Vectorized parquet reader should never do rebase for timestamp ntz.  \n[SPARK-46056] Fix Parquet vectorized read NPE with byteArrayDecimalType default value.  \n[SPARK-46058] Add separate flag for privateKeyPassword.  \n[SPARK-46478] Revert SPARK-43049 to use oracle varchar(255) for string.  \n[SPARK-46132] Support key password for JKS keys for RPC SSL.  \n[SPARK-46417] Do not fail when calling hive.getTable and throwException is false.  \n[SPARK-46261] DataFrame.withColumnsRenamed should keep the dict/map ordering.  \n[SPARK-46370] Fix bug when querying from table after changing column defaults.  \n[SPARK-46609] Avoid exponential explosion in PartitioningPreservingUnaryExecNode.  \n[SPARK-46600] Move shared code between SqlConf and SqlApiConf to SqlApiConfHelper.  \n[SPARK-46538] Fix the ambiguous column reference issue in ALSModel.transform.  \n[SPARK-46337] Make CTESubstitution retain the PLAN_ID_TAG.  \n[SPARK-46602] Propagate allowExisting in view creation when the view/table does not exists.  \n[SPARK-46260] DataFrame.withColumnsRenamed should respect the dict ordering.  \n[SPARK-46145] spark.catalog.listTables does not throw exception when the table or view is not found.  \nDecember 14, 2023  \nFixed an issue where escaped underscores in getColumns operations originating from JDBC or ODBC clients were handled incorrectly and interpreted as wildcards."
    },
    {
        "id": 1490,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "December 14, 2023  \nFixed an issue where escaped underscores in getColumns operations originating from JDBC or ODBC clients were handled incorrectly and interpreted as wildcards.  \n[SPARK-46255] Support complex type -> string conversion.  \n[SPARK-46028] Make Column.__getitem__ accept input column.  \n[SPARK-45920] group by ordinal should be idempotent.  \n[SPARK-45433] Fix CSV/JSON schema inference when timestamps do not match specified timestampFormat.  \n[SPARK-45509] Fix df column reference behavior for Spark Connect.  \nOperating system security updates.  \nNovember 29, 2023  \nInstalled a new package, pyarrow-hotfix to remediate a PyArrow RCE vulnerability.  \nFixed an issue where escaped underscores in getColumns operations originating from JDBC or ODBC clients were wrongly interpreted as wildcards.  \nWhen ingesting CSV data using Auto Loader or Streaming Tables, large CSV files are now splittable and can be processed in parallel during both schema inference and data processing.  \nSpark-snowflake connector is upgraded to 2.12.0.  \n[SPARK-45859] Made UDF objects in ml.functions lazy.  \nRevert [SPARK-45592].  \n[SPARK-45892] Refactor optimizer plan validation to decouple validateSchemaOutput and validateExprIdUniqueness.  \n[SPARK-45592] Fixed correctness issue in AQE with InMemoryTableScanExec.  \n[SPARK-45620] APIs related to Python UDF now use camelCase.  \n[SPARK-44784] Made SBT testing hermetic.  \n[SPARK-45770] Fixed column resolution with DataFrameDropColumns for Dataframe.drop.  \n[SPARK-45544] Integrated SSL support into TransportContext.  \n[SPARK-45730] Improved time constraints for ReloadingX509TrustManagerSuite.  \nOperating system security updates.  \nNovember 10, 2023  \nChanged data feed queries on Unity Catalog Streaming Tables and Materialized Views to display error messages.  \n[SPARK-45545] SparkTransportConf inherits SSLOptions upon creation."
    },
    {
        "id": 1491,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \nNovember 10, 2023  \nChanged data feed queries on Unity Catalog Streaming Tables and Materialized Views to display error messages.  \n[SPARK-45545] SparkTransportConf inherits SSLOptions upon creation.  \n[SPARK-45584] Fixed subquery run failure with TakeOrderedAndProjectExec.  \n[SPARK-45427] Added RPC SSL settings to SSLOptions and SparkTransportConf.  \n[SPARK-45541] Added SSLFactory.  \n[SPARK-45430] FramelessOffsetWindowFunction no longer fails when IGNORE NULLS and offset > rowCount.  \n[SPARK-45429] Added helper classes for SSL RPC communication.  \n[SPARK-44219] Added extra per-rule validations for optimization rewrites.  \n[SPARK-45543] Fixed an issue where InferWindowGroupLimit generated an error if the other window functions haven\u2019t the same window frame as the rank-like functions.  \nOperating system security updates.  \nOctober 23, 2023  \n[SPARK-45426] Added support for ReloadingX509TrustManager.  \n[SPARK-45396] Added doc entry for PySpark.ml.connect module, and added Evaluator to __all__ at ml.connect.  \n[SPARK-45256] Fixed an issue where DurationWriter failed when writing more values than initial capacity.  \n[SPARK-45279] Attached plan_id to all logical plans.  \n[SPARK-45250] Added support for stage-level task resource profile for yarn clusters when dynamic allocation is turned off.  \n[SPARK-45182] Added support for rolling back shuffle map stage so all stage tasks can be retried when the stage output is indeterminate.  \n[SPARK-45419] Avoid reusing rocksdb sst files in a different rocksdb instance by removing file version map entries of larger versions.  \n[SPARK-45386] Fixed an issue where StorageLevel.NONE would incorrectly return 0.  \nOperating system security updates.  \nOctober 13, 2023  \nSnowflake-jdbc dependency upgraded from 3.13.29 to 3.13.33."
    },
    {
        "id": 1492,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \nOctober 13, 2023  \nSnowflake-jdbc dependency upgraded from 3.13.29 to 3.13.33.  \nThe array_insert function is 1-based for positive and negative indexes, while before, it was 0-based for negative indexes. It now inserts a new element at the end of input arrays for the index -1. To restore the previous behavior, set spark.sql.legacy.negativeIndexInArrayInsert to true.  \nDatabricks no longer ignores corrupt files when a CSV schema inference with Auto Loader has enabled ignoreCorruptFiles.  \n[SPARK-45227] Fixed a subtle thread-safety issue with CoarseGrainedExecutorBackend.  \n[SPARK-44658] ShuffleStatus.getMapStatus should return None instead of Some(null).  \n[SPARK-44910] Encoders.bean does not support superclasses with generic type arguments.  \n[SPARK-45346] Parquet schema inference respects case-sensitive flags when merging schema.  \nRevert [SPARK-42946].  \n[SPARK-42205] Updated the JSON protocol to remove Accumulables logging in a task or stage start events.  \n[SPARK-45360] Spark session builder supports initialization from SPARK_REMOTE.  \n[SPARK-45316] Add new parameters ignoreCorruptFiles/ignoreMissingFiles to HadoopRDD and NewHadoopRDD.  \n[SPARK-44909] Skip running the torch distributor log streaming server when it is not available.  \n[SPARK-45084] StateOperatorProgress now uses accurate shuffle partition number.  \n[SPARK-45371] Fixed shading issues in the Spark Connect Scala Client.  \n[SPARK-45178] Fallback to running a single batch for Trigger.AvailableNow with unsupported sources rather than using the wrapper.  \n[SPARK-44840] Make array_insert() 1-based for negative indexes.  \n[SPARK-44551] Edited comments to sync with OSS.  \n[SPARK-45078] The ArrayInsert function now makes explicit casting when the element type does not equal the derived component type.  \n[SPARK-45339] PySpark now logs retry errors."
    },
    {
        "id": 1493,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-45078] The ArrayInsert function now makes explicit casting when the element type does not equal the derived component type.  \n[SPARK-45339] PySpark now logs retry errors.  \n[SPARK-45057] Avoid acquiring read lock when keepReadLock is false.  \n[SPARK-44908] Fixed cross-validator foldCol param functionality.  \nOperating system security updates.  \nDatabricks Runtime 13.1  \nSee Databricks Runtime 13.1 (EoS).  \nNovember 29, 2023  \nFixed an issue where escaped underscores in getColumns operations originating from JDBC or ODBC clients were wrongly interpreted as wildcards.  \n[SPARK-44846] Removed complex grouping expressions after RemoveRedundantAggregates.  \n[SPARK-43802] Fixed an issue where codegen for unhex and unbase64 expressions would fail.  \n[SPARK-43718] Fixed nullability for keys in USING joins.  \nOperating system security updates.  \nNovember 14, 2023  \nPartition filters on Delta Lake streaming queries are pushed down before rate limiting to achieve better utilization.  \nChanged data feed queries on Unity Catalog Streaming Tables and Materialized Views to display error messages.  \n[SPARK-45584] Fixed subquery run failure with TakeOrderedAndProjectExec.  \n[SPARK-45430] FramelessOffsetWindowFunction no longer fails when IGNORE NULLS and offset > rowCount.  \n[SPARK-45543] Fixed an issue where InferWindowGroupLimit caused an issue if the other window functions didn\u2019t have the same window frame as the rank-like functions.  \nOperating system security updates.  \nOctober 24, 2023  \n[SPARK-43799] Added descriptor binary option to PySpark Protobuf API.  \nRevert [SPARK-42946].  \n[SPARK-45346] Parquet schema inference now respects case-sensitive flag when merging a schema.  \nOperating system security updates.  \nOctober 13, 2023  \nSnowflake-jdbc dependency upgraded from 3.13.29 to 3.13.33.  \nNo longer ignoring corrupt files when ignoreCorruptFiles is enabled during CSV schema inference with Auto Loader.  \n[SPARK-44658] ShuffleStatus.getMapStatus returns None instead of Some(null)."
    },
    {
        "id": 1494,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-44658] ShuffleStatus.getMapStatus returns None instead of Some(null).  \n[SPARK-45178] Fallback to running a single batch for Trigger.AvailableNow with unsupported sources rather than using the wrapper.  \n[SPARK-42205] Updated the JSON protocol to remove Accumulables logging in a task or stage start events.  \nOperating system security updates.  \nSeptember 12, 2023  \n[SPARK-44718] Match ColumnVector memory-mode config default to OffHeapMemoryMode config value.  \nSPARK-44878 Turned off strict limit for RocksDB write manager to avoid insertion exception on cache complete.  \nMiscellaneous fixes.  \nAugust 30, 2023  \n[SPARK-44871] Fixed `percentile_disc behavior.  \n[SPARK-44714] Ease restriction of LCA resolution regarding queries.  \n[SPARK-44245] PySpark.sql.dataframe sample() doc tests are now illustrative-only.  \n[SPARK-44818] Fixed race for pending task interrupt issued before taskThread is initialized.  \nOperating system security updates.  \nAugust 15, 2023  \n[SPARK-44485] Optimized TreeNode.generateTreeString.  \n[SPARK-44643] Fixed Row.__repr__ when the row is empty.  \n[SPARK-44504] Maintenance task now cleans up loaded providers on stop error.  \n[SPARK-44479] Fixed protobuf conversion from an empty struct type.  \n[SPARK-44464] Fixed applyInPandasWithStatePythonRunner to output rows that have Null as the first column value.  \nMiscellaneous fixes.  \nJuly 27, 2023  \nFixed an issue where dbutils.fs.ls() returned INVALID_PARAMETER_VALUE.LOCATION_OVERLAP when called for a storage location path which clashed with other external or managed storage location.  \n[SPARK-44199] CacheManager no longer refreshes the fileIndex unnecessarily.  \n[SPARK-44448] Fixed wrong results bug from DenseRankLimitIterator and InferWindowGroupLimit.  \nOperating system security updates.  \nJuly 24, 2023  \nRevert [SPARK-42323]."
    },
    {
        "id": 1495,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-44448] Fixed wrong results bug from DenseRankLimitIterator and InferWindowGroupLimit.  \nOperating system security updates.  \nJuly 24, 2023  \nRevert [SPARK-42323].  \n[SPARK-41848] Fixed task over-schedule issue with TaskResourceProfile.  \n[SPARK-44136] Fixed an issue where StateManager would get materialized in an executor instead of the driver in FlatMapGroupsWithStateExec.  \n[SPARK-44337] Fixed an issue where any field set to Any.getDefaultInstance caused parse errors.  \nOperating system security updates.  \nJune 27, 2023  \nOperating system security updates.  \nJune 15, 2023  \nPhotonized approx_count_distinct.  \nJSON parser in failOnUnknownFields mode now drops the record in DROPMALFORMED mode and fails directly in FAILFAST mode.  \nSnowflake-jdbc library is upgraded to 3.13.29 to address a security issue.  \nThe PubSubRecord attributes field is stored as JSON instead of the string from a Scala map for more straightforward serialization and deserialization.  \nThe EXPLAIN EXTENDED command now returns the result cache eligibility of the query.  \nImprove the performance of incremental updates with SHALLOW CLONE Iceberg and Parquet.  \n[SPARK-43032] Python SQM bug fix.  \n[SPARK-43404]Skip reusing the sst file for the same version of RocksDB state store to avoid the ID mismatch error.  \n[SPARK-43340] Handle missing stack-trace field in eventlogs.  \n[SPARK-43527] Fixed catalog.listCatalogs in PySpark.  \n[SPARK-43541] Propagate all Project tags in resolving of expressions and missing columns.  \n[SPARK-43300] NonFateSharingCache wrapper for Guava Cache.  \n[SPARK-43378] Properly close stream objects in deserializeFromChunkedBuffer.  \n[SPARK-42852] Revert NamedLambdaVariable related changes from EquivalentExpressions.  \n[SPARK-43779] ParseToDate now loads EvalMode in the main thread.  \n[SPARK-43413] Fix IN subquery ListQuery nullability."
    },
    {
        "id": 1496,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-43779] ParseToDate now loads EvalMode in the main thread.  \n[SPARK-43413] Fix IN subquery ListQuery nullability.  \n[SPARK-43889] Add check for column name for __dir__() to filter out error-prone column names.  \n[SPARK-43043] Improved the performance of MapOutputTracker.updateMapOutput  \n[SPARK-43522] Fixed creating struct column name with index of array.  \n[SPARK-43457] Augument user agent with OS, Python and Spark versions.  \n[SPARK-43286] Updated aes_encrypt CBC mode to generate random IVs.  \n[SPARK-42851] Guard EquivalentExpressions.addExpr() with supportedExpression().  \nRevert [SPARK-43183].  \nOperating system security updates.  \nDatabricks Runtime 12.2 LTS  \nSee Databricks Runtime 12.2 LTS.  \nNovember 29, 2023  \nFixed an issue where escaped underscores in getColumns operations originating from JDBC or ODBC clients were wrongly interpreted as wildcards.  \n[SPARK-42205] Removed logging accumulables in Stage and Task start events.  \n[SPARK-44846] Removed complex grouping expressions after RemoveRedundantAggregates.  \n[SPARK-43718] Fixed nullability for keys in USING joins.  \n[SPARK-45544] Integrated SSL support into TransportContext.  \n[SPARK-43973] Structured Streaming UI now displays failed queries correctly.  \n[SPARK-45730] Improved time constraints for ReloadingX509TrustManagerSuite.  \n[SPARK-45859] Made UDF objects in ml.functions lazy.  \nOperating system security updates.  \nNovember 14, 2023  \nPartition filters on Delta Lake streaming queries are pushed down before rate limiting to achieve better utilization.  \n[SPARK-45545] SparkTransportConf inherits SSLOptions upon creation.  \n[SPARK-45427] Added RPC SSL settings to SSLOptions and SparkTransportConf.  \n[SPARK-45584] Fixed subquery run failure with TakeOrderedAndProjectExec.  \n[SPARK-45541] Added SSLFactory."
    },
    {
        "id": 1497,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-45584] Fixed subquery run failure with TakeOrderedAndProjectExec.  \n[SPARK-45541] Added SSLFactory.  \n[SPARK-45430] FramelessOffsetWindowFunction no longer fails when IGNORE NULLS and offset > rowCount.  \n[SPARK-45429] Added helper classes for SSL RPC communication.  \nOperating system security updates.  \nOctober 24, 2023  \n[SPARK-45426] Added support for ReloadingX509TrustManager.  \nMiscellaneous fixes.  \nOctober 13, 2023  \nSnowflake-jdbc dependency upgraded from 3.13.29 to 3.13.33.  \n[SPARK-42553] Ensure at least one time unit after interval.  \n[SPARK-45346] Parquet schema inference respects case sensitive flag when merging schema.  \n[SPARK-45178] Fallback to running a single batch for Trigger.AvailableNow with unsupported sources rather than using the wrapper.  \n[SPARK-45084] StateOperatorProgress to use an accurate, adequate shuffle partition number.  \nSeptember 12, 2023  \n[SPARK-44873] Added support for alter view with nested columns in the Hive client.  \n[SPARK-44718] Match ColumnVector memory-mode config default to OffHeapMemoryMode config value.  \n[SPARK-43799] Added descriptor binary option to PySpark Protobuf API.  \nMiscellaneous fixes.  \nAugust 30, 2023  \n[SPARK-44485] Optimized TreeNode.generateTreeString.  \n[SPARK-44818] Fixed race for pending task interrupt issued before taskThread is initialized.  \n[11.3-13.0][[SPARK-44871]]https://issues.apache.org/jira/browse/SPARK-44871) Fixed percentile_disc behavior.  \n[SPARK-44714] Eased restriction of LCA resolution regarding queries.  \nOperating system security updates.  \nAugust 15, 2023  \n[SPARK-44504] Maintenance task cleans up loaded providers on stop error.  \n[SPARK-44464] Fixed applyInPandasWithStatePythonRunner to output rows that have Null as the first column value.  \nOperating system security updates."
    },
    {
        "id": 1498,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-44464] Fixed applyInPandasWithStatePythonRunner to output rows that have Null as the first column value.  \nOperating system security updates.  \nJuly 29, 2023  \nFixed an issue where dbutils.fs.ls() returned INVALID_PARAMETER_VALUE.LOCATION_OVERLAP when called for a storage location path which clashed with other external or managed storage location.  \n[SPARK-44199] CacheManager no longer refreshes the fileIndex unnecessarily.  \nOperating system security updates.  \nJuly 24, 2023  \n[SPARK-44337] Fixed an issue where any field set to Any.getDefaultInstance caused parse errors.  \n[SPARK-44136] Fixed an issue where StateManager would get materialized in an executor instead of the driver in FlatMapGroupsWithStateExec.  \nOperating system security updates.  \nJune 23, 2023  \nOperating system security updates.  \nJune 15, 2023  \nPhotonized approx_count_distinct.  \nSnowflake-jdbc library is upgraded to 3.13.29 to address a security issue.  \n[SPARK-43779] ParseToDate now loads EvalMode in the main thread.  \n[SPARK-43156][SPARK-43098] Extended scalar subquery count error test with decorrelateInnerQuery turned off.  \nOperating system security updates.  \nJune 2, 2023  \nThe JSON parser in failOnUnknownFields mode drops a record in DROPMALFORMED mode and fails directly in FAILFAST mode.  \nImprove the performance of incremental updates with SHALLOW CLONE Iceberg and Parquet.  \nFixed an issue in Auto Loader where different source file formats were inconsistent when the provided schema did not include inferred partitions. This issue could cause unexpected failures when reading files with missing columns in the inferred partition schema.  \n[SPARK-43404] Skip reusing the sst file for the same version of RocksDB state store to avoid the ID mismatch error.  \n[SPARK-43413][11.3-13.0] Fixed IN subquery ListQuery nullability.  \n[SPARK-43522] Fixed creating struct column name with index of array.  \n[SPARK-43541] Propagate all Project tags in resolving of expressions and missing columns."
    },
    {
        "id": 1499,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-43522] Fixed creating struct column name with index of array.  \n[SPARK-43541] Propagate all Project tags in resolving of expressions and missing columns.  \n[SPARK-43527] Fixed catalog.listCatalogs in PySpark.  \n[SPARK-43123] Internal field metadata no longer leaks to catalogs.  \n[SPARK-43340] Fixed missing stack trace field in eventlogs.  \n[SPARK-42444] DataFrame.drop now handles duplicated columns correctly.  \n[SPARK-42937] PlanSubqueries now sets InSubqueryExec#shouldBroadcast to true.  \n[SPARK-43286] Updated aes_encrypt CBC mode to generate random IVs.  \n[SPARK-43378] Properly close stream objects in deserializeFromChunkedBuffer.  \nMay 17, 2023  \nParquet scans are now robust against OOMs when scanning exceptionally structured files by dynamically adjusting batch size. File metadata is analyzed to preemptively lower batch size and is lowered again on task retries as a final safety net.  \nIf an Avro file was read with just the failOnUnknownFields\\ option or with Auto Loader in the failOnNewColumns\\ schema evolution mode, columns that have different data types would be read as null\\ instead of throwing an error stating that the file cannot be read. These reads now fail and recommend users to use the rescuedDataColumn\\ option.  \nAuto Loader now does the following.  \nCorrectly reads and no longer rescues Integer, Short, and Byte types if one of these data types is provided, but the Avro file suggests one of the other two types.  \nPrevents reading interval types as date or time stamp types to avoid getting corrupt dates.  \nPrevents reading Decimal types with lower precision.  \n[SPARK-43172] Exposes host and token from Spark connect client.  \n[SPARK-43293] __qualified_access_only is ignored in normal columns.  \n[SPARK-43098] Fixed correctness COUNT bug when scalar subquery is grouped by clause.  \n[SPARK-43085] Support for column DEFAULT assignment for multi-part table names.  \n[SPARK-43190] ListQuery.childOutput is now consistent with secondary output."
    },
    {
        "id": 1500,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-43085] Support for column DEFAULT assignment for multi-part table names.  \n[SPARK-43190] ListQuery.childOutput is now consistent with secondary output.  \n[SPARK-43192] Removed user agent charset validation.  \nOperating system security updates.  \nApril 25, 2023  \nIf a Parquet file was read with just the failOnUnknownFields option or with Auto Loader in the failOnNewColumns schema evolution mode, columns that had different data types would be read as null instead of throwing an error stating that the file cannot be read. These reads now fail and recommend users to use the rescuedDataColumn option.  \nAuto Loader now correctly reads and no longer rescues Integer, Short, and Byte types if one of these data types is provided. The Parquet file suggests one of the other two types. When the rescued data column was previously enabled, the data type mismatch would cause columns to be saved even though they were readable.  \n[SPARK-43009] Parameterized sql() with Any constants  \n[SPARK-42406] Terminate Protobuf recursive fields by dropping the field  \n[SPARK-43038] Support the CBC mode by aes_encrypt()/aes_decrypt()  \n[SPARK-42971] Change to print workdir if appDirs is null when worker handle WorkDirCleanup event  \n[SPARK-43018] Fix bug for INSERT commands with timestamp literals  \nOperating system security updates.  \nApril 11, 2023  \nSupport legacy data source formats in the SYNC command.  \nFixes an issue in the %autoreload behavior in notebooks outside of a repo.  \nFixed an issue where Auto Loader schema evolution can go into an infinite fail loop when a new column is detected in the schema of a nested JSON object.  \n[SPARK-42928] Makes resolvePersistentFunction synchronized.  \n[SPARK-42936] Fixes LCan issue when the clause can be resolved directly by its child aggregate.  \n[SPARK-42967] Fixes SparkListenerTaskStart.stageAttemptId when a task starts after the stage is canceled.  \nOperating system security updates.  \nMarch 29, 2023"
    },
    {
        "id": 1501,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-42967] Fixes SparkListenerTaskStart.stageAttemptId when a task starts after the stage is canceled.  \nOperating system security updates.  \nMarch 29, 2023  \nDatabricks SQL now supports specifying default values for columns of Delta Lake tables, either at table creation time or afterward. Subsequent INSERT, UPDATE, DELETE, and MERGE commands can refer to any column\u2019s default value using the explicit DEFAULT keyword. In addition, if any INSERT assignment has an explicit list of fewer columns than the target table, corresponding column default values are substituted for the remaining columns (or NULL if no default is specified).  \nFor example:  \nCREATE TABLE t (first INT, second DATE DEFAULT CURRENT_DATE()); INSERT INTO t VALUES (0, DEFAULT); INSERT INTO t VALUES (1, DEFAULT); SELECT first, second FROM t; \\> 0, 2023-03-28 1, 2023-03-28z  \nAuto Loader now initiates at least one synchronous RocksDB log cleanup for Trigger.AvailableNow streams to check that the checkpoint can get regularly cleaned up for fast-running Auto Loader streams. This can cause some streams to take longer before they shut down, but it will save you storage costs and improve the Auto Loader experience in future runs.  \nYou can now modify a Delta table to add support to table features using DeltaTable.addFeatureSupport(feature_name).  \n[SPARK-42794] Increase the lockAcquireTimeoutMs to 2 minutes for acquiring the RocksDB state store in Structure Streaming  \n[SPARK-42521] Add NULLs for INSERTs with user-specified lists of fewer columns than the target table  \n[SPARK-42702][SPARK-42623] Support parameterized query in subquery and CTE  \n[SPARK-42668] Catch exception while trying to close the compressed stream in HDFSStateStoreProvider stop  \n[SPARK-42403] JsonProtocol should handle null JSON strings  \nMarch 8, 2023  \nThe error message \u201cFailure to initialize configuration\u201d has been improved to provide more context for the customer."
    },
    {
        "id": 1502,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-42403] JsonProtocol should handle null JSON strings  \nMarch 8, 2023  \nThe error message \u201cFailure to initialize configuration\u201d has been improved to provide more context for the customer.  \nThere is a terminology change for adding features to a Delta table using the table property. The preferred syntax is now 'delta.feature.featureName'='supported' instead of 'delta.feature.featureName'='enabled'. For backward compatibility, using 'delta.feature.featureName'='enabled' still works and will continue to work.  \nStarting from this release, it is possible to create/replace a table with an additional table property delta.ignoreProtocolDefaults to ignore protocol-related Spark configs, which includes default reader and writer versions and table features supported by default.  \n[SPARK-42070] Change the default value of the argument of the Mask function from -1 to NULL  \n[SPARK-41793] Incorrect result for window frames defined by a range clause on significant decimals  \n[SPARK-42484] UnsafeRowUtils better error message  \n[SPARK-42516] Always capture the session time zone config while creating views  \n[SPARK-42635] Fix the TimestampAdd expression.  \n[SPARK-42622] Turned off substitution in values  \n[SPARK-42534] Fix DB2Dialect Limit clause  \n[SPARK-42121] Add built-in table-valued functions posexplode, posexplode_outer, json_tuple and stack  \n[SPARK-42045] ANSI SQL mode: Round/Bround should return an error on tiny/small/significant integer overflow  \nOperating system security updates.  \nDatabricks Runtime 11.3 LTS  \nSee Databricks Runtime 11.3 LTS.  \nNovember 29, 2023  \nFixed an issue where escaped underscores in getColumns operations originating from JDBC or ODBC clients were wrongly interpreted as wildcards.  \n[SPARK-43973] Structured Streaming UI now displays failed queries correctly.  \n[SPARK-45730] Improved time constraints for ReloadingX509TrustManagerSuite.  \n[SPARK-45544] Integrated SSL support into TransportContext.  \n[SPARK-45859] Made UDF objects in ml.functions lazy."
    },
    {
        "id": 1503,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-45544] Integrated SSL support into TransportContext.  \n[SPARK-45859] Made UDF objects in ml.functions lazy.  \n[SPARK-43718] Fixed nullability for keys in USING joins.  \n[SPARK-44846] Removed complex grouping expressions after RemoveRedundantAggregates.  \nOperating system security updates.  \nNovember 14, 2023  \nPartition filters on Delta Lake streaming queries are pushed down before rate limiting to achieve better utilization.  \n[SPARK-42205] Removed logging accumulables in Stage and Task start events.  \n[SPARK-45545] SparkTransportConf inherits SSLOptions upon creation.  \nRevert [SPARK-33861].  \n[SPARK-45541] Added SSLFactory.  \n[SPARK-45429] Added helper classes for SSL RPC communication.  \n[SPARK-45584] Fixed subquery run failure with TakeOrderedAndProjectExec.  \n[SPARK-45430] FramelessOffsetWindowFunction no longer fails when IGNORE NULLS and offset > rowCount.  \n[SPARK-45427] Added RPC SSL settings to SSLOptions and SparkTransportConf.  \nOperating system security updates.  \nOctober 24, 2023  \n[SPARK-45426] Added support for ReloadingX509TrustManager.  \nMiscellaneous fixes.  \nOctober 13, 2023  \nSnowflake-jdbc dependency upgraded from 3.13.29 to 3.13.33.  \n[SPARK-45178] Fallback to running a single batch for Trigger.AvailableNow with unsupported sources rather than using the wrapper.  \n[SPARK-45084] StateOperatorProgress to use an accurate, adequate shuffle partition number.  \n[SPARK-45346] Parquet schema inference now respects case-sensitive flag when merging a schema.  \nOperating system security updates.  \nSeptember 10, 2023  \nMiscellaneous fixes.  \nAugust 30, 2023  \n[SPARK-44818] Fixed race for pending task interrupt issued before taskThread is initialized.  \n[SPARK-44871][11.3-13.0] Fixed percentile_disc behavior.  \nOperating system security updates.  \nAugust 15, 2023  \n[SPARK-44485] Optimized TreeNode.generateTreeString."
    },
    {
        "id": 1504,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \nAugust 15, 2023  \n[SPARK-44485] Optimized TreeNode.generateTreeString.  \n[SPARK-44504] Maintenance task cleans up loaded providers on stop error.  \n[SPARK-44464] Fixed applyInPandasWithStatePythonRunner to output rows that have Null as the first column value.  \nOperating system security updates.  \nJuly 27, 2023  \nFixed an issue where dbutils.fs.ls() returned INVALID_PARAMETER_VALUE.LOCATION_OVERLAP when called for a storage location path which clashed with other external or managed storage location.  \n[SPARK-44199] CacheManager no longer refreshes the fileIndex unnecessarily.  \nOperating system security updates.  \nJuly 24, 2023  \n[SPARK-44136] Fixed an issue that StateManager can get materialized in executor instead of driver in FlatMapGroupsWithStateExec.  \nOperating system security updates.  \nJune 23, 2023  \nOperating system security updates.  \nJune 15, 2023  \nPhotonized approx_count_distinct.  \nSnowflake-jdbc library is upgraded to 3.13.29 to address a security issue.  \n[SPARK-43779] ParseToDate now loads EvalMode in the main thread.  \n[SPARK-40862] Support non-aggregated subqueries in RewriteCorrelatedScalarSubquery  \n[SPARK-43156][SPARK-43098] Extended scalar subquery count bug test with decorrelateInnerQuery turned off.  \n[SPARK-43098] Fix correctness COUNT bug when scalar subquery has a group by clause  \nOperating system security updates.  \nJune 2, 2023  \nThe JSON parser in failOnUnknownFields mode drops a record in DROPMALFORMED mode and fails directly in FAILFAST mode.  \nImprove the performance of incremental updates with SHALLOW CLONE Iceberg and Parquet.  \nFixed an issue in Auto Loader where different source file formats were inconsistent when the provided schema did not include inferred partitions. This issue could cause unexpected failures when reading files with missing columns in the inferred partition schema.  \n[SPARK-43404]Skip reusing the sst file for the same version of RocksDB state store to avoid the ID mismatch error."
    },
    {
        "id": 1505,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-43404]Skip reusing the sst file for the same version of RocksDB state store to avoid the ID mismatch error.  \n[SPARK-43527] Fixed catalog.listCatalogs in PySpark.  \n[SPARK-43413][11.3-13.0] Fixed IN subquery ListQuery nullability.  \n[SPARK-43340] Fixed missing stack trace field in eventlogs.  \nDatabricks Runtime 10.4 LTS  \nSee Databricks Runtime 10.4 LTS.  \nNovember 29, 2023  \n[SPARK-45544] Integrated SSL support into TransportContext.  \n[SPARK-45859] Made UDF objects in ml.functions lazy.  \n[SPARK-43718] Fixed nullability for keys in USING joins.  \n[SPARK-45730] Improved time constraints for ReloadingX509TrustManagerSuite.  \n[SPARK-42205] Removed logging accumulables in Stage and Task start events.  \n[SPARK-44846] Removed complex grouping expressions after RemoveRedundantAggregates.  \nOperating system security updates.  \nNovember 14, 2023  \n[SPARK-45541] Added SSLFactory.  \n[SPARK-45545] SparkTransportConf inherits SSLOptions upon creation.  \n[SPARK-45427] Added RPC SSL settings to SSLOptions and SparkTransportConf.  \n[SPARK-45429] Added helper classes for SSL RPC communication.  \n[SPARK-45584] Fixed subquery run failure with TakeOrderedAndProjectExec.  \nRevert [SPARK-33861].  \nOperating system security updates.  \nOctober 24, 2023  \n[SPARK-45426] Added support for ReloadingX509TrustManager.  \nOperating system security updates.  \nOctober 13, 2023  \n[SPARK-45084] StateOperatorProgress to use an accurate, adequate shuffle partition number.  \n[SPARK-45178] Fallback to running a single batch for Trigger.AvailableNow with unsupported sources rather than using the wrapper.  \nOperating system security updates.  \nSeptember 10, 2023  \nMiscellaneous fixes.  \nAugust 30, 2023  \n[SPARK-44818] Fixed race for pending task interrupt issued before taskThread is initialized.  \nOperating system security updates.  \nAugust 15, 2023"
    },
    {
        "id": 1506,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "September 10, 2023  \nMiscellaneous fixes.  \nAugust 30, 2023  \n[SPARK-44818] Fixed race for pending task interrupt issued before taskThread is initialized.  \nOperating system security updates.  \nAugust 15, 2023  \n[SPARK-44504] Maintenance task cleans up loaded providers on stop error.  \n[SPARK-43973] Structured Streaming UI now appears failed queries correctly.  \nOperating system security updates.  \nJune 23, 2023  \nOperating system security updates.  \nJune 15, 2023  \nSnowflake-jdbc library is upgraded to 3.13.29 to address a security issue.  \n[SPARK-43098] Fix correctness COUNT bug when scalar subquery has a group by clause  \n[SPARK-40862] Support non-aggregated subqueries in RewriteCorrelatedScalarSubquery  \n[SPARK-43156][SPARK-43098] Extended scalar subquery count test with decorrelateInnerQuery turned off.  \nOperating system security updates.  \nJune 2, 2023  \nThe JSON parser in failOnUnknownFields mode drops a record in DROPMALFORMED mode and fails directly in FAILFAST mode.  \nFixed an issue in JSON rescued data parsing to prevent UnknownFieldException.  \nFixed an issue in Auto Loader where different source file formats were inconsistent when the provided schema did not include inferred partitions. This issue could cause unexpected failures when reading files with missing columns in the inferred partition schema.  \n[SPARK-43404] Skip reusing the sst file for the same version of RocksDB state store to avoid the ID mismatch error.  \n[SPARK-43413] Fixed IN subquery ListQuery nullability.  \nOperating system security updates.  \nMay 17, 2023  \nParquet scans are now robust against OOMs when scanning exceptionally structured files by dynamically adjusting batch size. File metadata is analyzed to preemptively lower batch size and is lowered again on task retries as a final safety net.  \n[SPARK-41520] Split AND_OR tree pattern to separate AND and OR.  \n[SPARK-43190] ListQuery.childOutput is now consistent with secondary output.  \nOperating system security updates.  \nApril 25, 2023  \n[SPARK-42928] Make resolvePersistentFunction synchronized.  \nOperating system security updates.  \nApril 11, 2023"
    },
    {
        "id": 1507,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \nApril 25, 2023  \n[SPARK-42928] Make resolvePersistentFunction synchronized.  \nOperating system security updates.  \nApril 11, 2023  \nFixed an issue where Auto Loader schema evolution can go into an infinite fail loop when a new column is detected in the schema of a nested JSON object.  \n[SPARK-42937] PlanSubqueries now sets InSubqueryExec#shouldBroadcast to true.  \n[SPARK-42967] Fix SparkListenerTaskStart.stageAttemptId when a task is started after the stage is canceled.  \nMarch 29, 2023  \n[SPARK-42668] Catch exception while trying to close the compressed stream in HDFSStateStoreProvider stop  \n[SPARK-42635] Fix the \u2026  \nOperating system security updates.  \nMarch 14, 2023  \n[SPARK-41162] Fix anti- and semi-join for self-join with aggregations  \n[SPARK-33206] Fix shuffle index cache weight calculation for small index files  \n[SPARK-42484] Improved the UnsafeRowUtils error message  \nMiscellaneous fixes.  \nFebruary 28, 2023  \nSupport generated column for yyyy-MM-dd date_format. This change supports partition pruning for yyyy-MM-dd as a date_format in generated columns.  \nUsers can now read and write specific Delta tables requiring Reader version 3 and Writer version 7, using Databricks Runtime 9.1 LTS or later. To succeed, table features listed in the tables\u2019 protocol must be supported by the current version of Databricks Runtime.  \nSupport generated column for yyyy-MM-dd date_format. This change supports partition pruning for yyyy-MM-dd as a date_format in generated columns.  \nOperating system security updates.  \nFebruary 16, 2023  \n[SPARK-30220] Enable using Exists/In subqueries outside of the Filter node  \nOperating system security updates.  \nJanuary 31, 2023  \nTable types of JDBC tables are now EXTERNAL by default.  \nJanuary 18, 2023"
    },
    {
        "id": 1508,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-30220] Enable using Exists/In subqueries outside of the Filter node  \nOperating system security updates.  \nJanuary 31, 2023  \nTable types of JDBC tables are now EXTERNAL by default.  \nJanuary 18, 2023  \nAzure Synapse connector returns a more descriptive error message when a column name contains not valid characters such as whitespaces or semicolons. In such cases, the following message will be returned: Azure Synapse Analytics failed to run the JDBC query produced by the connector. Check column names do not include not valid characters such as ';' or white space.  \n[SPARK-38277] Clear write batch after RocksDB state store\u2019s commit  \n[SPARK-41199] Fix metrics issue when DSv1 streaming source and DSv2 streaming source are co-used  \n[SPARK-41198] Fix metrics in streaming query having CTE and DSv1 streaming source  \n[SPARK-41339] Close and recreate RocksDB write batch instead of just clearing  \n[SPARK-41732] Apply tree-pattern based pruning for the rule SessionWindowing  \nOperating system security updates.  \nNovember 29, 2022  \nUsers can configure leading and trailing whitespaces\u2019 behavior when writing data using the Redshift connector. The following options have been added to control white space handling:  \ncsvignoreleadingwhitespace, when set to true, removes leading white space from values during writes when tempformat is set to CSV or CSV GZIP. Whitespaces are retained when the config is set to false. By default, the value is true.  \ncsvignoretrailingwhitespace, when set to true, removes trailing white space from values during writes when tempformat is set to CSV or CSV GZIP. Whitespaces are retained when the config is set to false. By default, the value is true.  \nFixed an issue with JSON parsing in Auto Loader when all columns were left as strings (cloudFiles.inferColumnTypes was not set or set to false) and the JSON contained nested objects.  \nOperating system security updates.  \nNovember 15, 2022  \nUpgraded Apache commons-text to 1.10.0."
    },
    {
        "id": 1509,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \nNovember 15, 2022  \nUpgraded Apache commons-text to 1.10.0.  \n[SPARK-40646] JSON parsing for structs, maps, and arrays has been fixed so when a part of a record does not match the schema, the rest of the record can still be parsed correctly instead of returning nulls. To opt-in for the improved behavior, set spark.sql.json.enablePartialResults to true. The flag is turned off by default to preserve the original behavior.  \n[SPARK-40292] Fix column names in arrays_zip function when arrays are referenced from nested structs  \nOperating system security updates.  \nNovember 1, 2022  \nFixed an issue where if a Delta table had a user-defined column named _change_type, but Change data feed was turned off on that table, data in that column would incorrectly fill with NULL values when running MERGE.  \nFixed an issue with Auto Loader where a file can be duplicated in the same micro-batch when allowOverwrites is enabled  \n[SPARK-40697] Add read-side char padding to cover external data files  \n[SPARK-40596] Populate ExecutorDecommission with messages in ExecutorDecommissionInfo  \nOperating system security updates.  \nOctober 18, 2022  \nOperating system security updates.  \nOctober 5, 2022  \n[SPARK-40468] Fix column pruning in CSV when _corrupt_record is selected.  \nOperating system security updates.  \nSeptember 22, 2022  \nUsers can set spark.conf.set(spark.databricks.io.listKeysWithPrefix.azure.enabled, true) to re-enable the built-in listing for Auto Loader on ADLS Gen2. Built-in listing was previously turned off due to performance issues but can have led to increased storage costs for customers.  \n[SPARK-40315] Add hashCode() for Literal of ArrayBasedMapData  \n[SPARK-40213] Support ASCII value conversion for Latin-1 characters  \n[SPARK-40380] Fix constant-folding of InvokeLike to avoid non-serializable literal embedded in the plan  \n[SPARK-38404] Improve CTE resolution when a nested CTE references an outer CTE  \n[SPARK-40089] Fix sorting for some Decimal types"
    },
    {
        "id": 1510,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-38404] Improve CTE resolution when a nested CTE references an outer CTE  \n[SPARK-40089] Fix sorting for some Decimal types  \n[SPARK-39887] RemoveRedundantAliases should keep aliases that make the output of projection nodes unique  \nSeptember 6, 2022  \n[SPARK-40235] Use interruptible lock instead of synchronized in Executor.updateDependencies()  \n[SPARK-40218] GROUPING SETS should preserve the grouping columns  \n[SPARK-39976] ArrayIntersect should handle null in left expression correctly  \n[SPARK-40053] Add assume to dynamic cancel cases which require Python runtime environment  \n[SPARK-35542] Fix: Bucketizer created for multiple columns with parameters splitsArray, inputCols and outputCols can not be loaded after saving it  \n[SPARK-40079] Add Imputer inputCols validation for empty input case  \nAugust 24, 2022  \n[SPARK-39983] Do not cache unserialized broadcast relations on the driver  \n[SPARK-39775] Disable validate default values when parsing Avro schemas  \n[SPARK-39962] Apply projection when group attributes are empty  \n[SPARK-37643] when charVarcharAsString is true, for char datatype predicate query should skip rpadding rule  \nOperating system security updates.  \nAugust 9, 2022  \n[SPARK-39847] Fix race condition in RocksDBLoader.loadLibrary() if the caller thread is interrupted  \n[SPARK-39731] Fix issue in CSV and JSON data sources when parsing dates in \u201cyyyyMMdd\u201d format with CORRECTED time parser policy  \nOperating system security updates.  \nJuly 27, 2022  \n[SPARK-39625] Add Dataset.as(StructType)  \n[SPARK-39689]Support 2-chars lineSep in CSV data source  \n[SPARK-39104] InMemoryRelation#isCachedColumnBuffersLoaded should be thread-safe  \n[SPARK-39570] Inline table should allow expressions with alias  \n[SPARK-39702] Reduce memory overhead of TransportCipher$EncryptedMessage by using a shared byteRawChannel"
    },
    {
        "id": 1511,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-39570] Inline table should allow expressions with alias  \n[SPARK-39702] Reduce memory overhead of TransportCipher$EncryptedMessage by using a shared byteRawChannel  \n[SPARK-39575] add ByteBuffer#rewind after ByteBuffer#get in AvroDeserializer  \n[SPARK-39476] Disable Unwrap cast optimize when casting from Long to Float/ Double or from Integer to Float  \n[SPARK-38868] Don\u2019t propagate exceptions from filter predicate when optimizing outer joins  \nOperating system security updates.  \nJuly 20, 2022  \nMake Delta MERGE operation results consistent when the source is non-deterministic.  \n[SPARK-39355] Single column uses quoted to construct UnresolvedAttribute  \n[SPARK-39548] CreateView Command with a window clause query press a wrong window definition not found issue  \n[SPARK-39419] Fix ArraySort to throw an exception when the comparator returns null  \nTurned off Auto Loader\u2019s use of built-in cloud APIs for directory listing on Azure.  \nOperating system security updates.  \nJuly 5, 2022  \n[SPARK-39376] Hide duplicated columns in star expansion of subquery alias from NATURAL/USING JOIN  \nOperating system security updates.  \nJune 15, 2022  \n[SPARK-39283] Fix deadlock between TaskMemoryManager and UnsafeExternalSorter.SpillableIterator  \n[SPARK-39285] Spark should not check field names when reading files  \n[SPARK-34096] Improve performance for nth_value ignore nulls over offset window  \n[SPARK-36718] Fix the isExtractOnly check in CollapseProject  \nJune 2, 2022  \n[SPARK-39093] Avoid codegen compilation error when dividing year-month intervals or day-time intervals by an integral  \n[SPARK-38990] Avoid NullPointerException when evaluating date_trunc/trunc format as a bound reference  \nOperating system security updates.  \nMay 18, 2022  \nFixes a potential built-in memory leak in Auto Loader.  \n[SPARK-38918] Nested column pruning should filter out attributes that do not belong to the current relation"
    },
    {
        "id": 1512,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \nMay 18, 2022  \nFixes a potential built-in memory leak in Auto Loader.  \n[SPARK-38918] Nested column pruning should filter out attributes that do not belong to the current relation  \n[SPARK-37593] Reduce default page size by LONG_ARRAY_OFFSET if G1GC and ON_HEAP are used  \n[SPARK-39084] Fix df.rdd.isEmpty() by using TaskContext to stop iterator on task completion  \n[SPARK-32268] Add ColumnPruning in injectBloomFilter  \n[SPARK-38974] Filter registered functions with a given database name in list functions  \n[SPARK-38931] Create root dfs directory for RocksDBFileManager with an unknown number of keys on 1st checkpoint  \nOperating system security updates.  \nApril 19, 2022  \nUpgraded Java AWS SDK from version 1.11.655 to 1.12.1899.  \nFixed an issue with notebook-scoped libraries not working in batch streaming jobs.  \n[SPARK-38616] Keep track of SQL query text in Catalyst TreeNode  \nOperating system security updates.  \nApril 6, 2022  \nThe following Spark SQL functions are now available with this release:  \ntimestampadd() and dateadd(): Add a time duration in a specified unit to a time stamp expression.  \ntimestampdiff() and datediff(): Calculate the time difference between two-time stamp expressions in a specified unit.  \nParquet-MR has been upgraded to 1.12.2  \nImproved support for comprehensive schemas in parquet files  \n[SPARK-38631] Uses Java-based implementation for un-tarring at Utils.unpack  \n[SPARK-38509][SPARK-38481] Cherry-pick three timestmapadd/diff changes.  \n[SPARK-38523] Fix referring to the corrupt record column from CSV  \n[SPARK-38237] Allow ClusteredDistribution to require full clustering keys  \n[SPARK-38437] Lenient serialization of datetime from datasource  \n[SPARK-38180] Allow safe up-cast expressions in correlated equality predicates"
    },
    {
        "id": 1513,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-38437] Lenient serialization of datetime from datasource  \n[SPARK-38180] Allow safe up-cast expressions in correlated equality predicates  \n[SPARK-38155] Disallow distinct aggregate in lateral subqueries with unsupported predicates  \nOperating system security updates.  \nDatabricks Runtime 9.1 LTS  \nSee Databricks Runtime 9.1 LTS.  \nNovember 29, 2023  \n[SPARK-45859] Made UDF objects in ml.functions lazy.  \n[SPARK-45544] Integrated SSL support into TransportContext.  \n[SPARK-45730] Improved time constraints for ReloadingX509TrustManagerSuite.  \nOperating system security updates.  \nNovember 14, 2023  \n[SPARK-45545] SparkTransportConf inherits SSLOptions upon creation.  \n[SPARK-45429] Added helper classes for SSL RPC communication.  \n[SPARK-45427] Added RPC SSL settings to SSLOptions and SparkTransportConf.  \n[SPARK-45584] Fixed subquery run failure with TakeOrderedAndProjectExec.  \n[SPARK-45541] Added SSLFactory.  \n[SPARK-42205] Removed logging accumulables in Stage and Task start events.  \nOperating system security updates.  \nOctober 24, 2023  \n[SPARK-45426] Added support for ReloadingX509TrustManager.  \nOperating system security updates.  \nOctober 13, 2023  \nOperating system security updates.  \nSeptember 10, 2023  \nMiscellaneous fixes.  \nAugust 30, 2023  \nOperating system security updates.  \nAugust 15, 2023  \nOperating system security updates.  \nJune 23, 2023  \nSnowflake-jdbc library is upgraded to 3.13.29 to address a security issue.  \nOperating system security updates.  \nJune 15, 2023  \n[SPARK-43098] Fix correctness COUNT bug when scalar subquery has a group by clause  \n[SPARK-43156][SPARK-43098] Extend scalar subquery count bug test with decorrelateInnerQuery turned off.  \n[SPARK-40862] Support non-aggregated subqueries in RewriteCorrelatedScalarSubquery  \nOperating system security updates.  \nJune 2, 2023"
    },
    {
        "id": 1514,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-40862] Support non-aggregated subqueries in RewriteCorrelatedScalarSubquery  \nOperating system security updates.  \nJune 2, 2023  \nThe JSON parser in failOnUnknownFields mode drops a record in DROPMALFORMED mode and fails directly in FAILFAST mode.  \nFixed an issue in JSON rescued data parsing to prevent UnknownFieldException.  \nFixed an issue in Auto Loader where different source file formats were inconsistent when the provided schema did not include inferred partitions. This issue could cause unexpected failures when reading files with missing columns in the inferred partition schema.  \n[SPARK-37520] Add the startswith() and endswith() string functions  \n[SPARK-43413] Fixed IN subquery ListQuery nullability.  \nOperating system security updates.  \nMay 17, 2023  \nOperating system security updates.  \nApril 25, 2023  \nOperating system security updates.  \nApril 11, 2023  \nFixed an issue where Auto Loader schema evolution can go into an infinite fail loop when a new column is detected in the schema of a nested JSON object.  \n[SPARK-42967] Fix SparkListenerTaskStart.stageAttemptId when a task is started after the stage is canceled.  \nMarch 29, 2023  \nOperating system security updates.  \nMarch 14, 2023  \n[SPARK-42484] Improved error message for UnsafeRowUtils.  \nMiscellaneous fixes.  \nFebruary 28, 2023  \nUsers can now read and write specific Delta tables requiring Reader version 3 and Writer version 7, using Databricks Runtime 9.1 LTS or later. To succeed, table features listed in the tables\u2019 protocol must be supported by the current version of Databricks Runtime.  \nOperating system security updates.  \nFebruary 16, 2023  \nOperating system security updates.  \nJanuary 31, 2023  \nTable types of JDBC tables are now EXTERNAL by default.  \nJanuary 18, 2023  \nOperating system security updates.  \nNovember 29, 2022  \nFixed an issue with JSON parsing in Auto Loader when all columns were left as strings (cloudFiles.inferColumnTypes was not set or set to false) and the JSON contained nested objects.  \nOperating system security updates.  \nNovember 15, 2022  \nUpgraded Apache commons-text to 1.10.0.  \nOperating system security updates.  \nMiscellaneous fixes.  \nNovember 1, 2022"
    },
    {
        "id": 1515,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \nNovember 15, 2022  \nUpgraded Apache commons-text to 1.10.0.  \nOperating system security updates.  \nMiscellaneous fixes.  \nNovember 1, 2022  \nFixed an issue where if a Delta table had a user-defined column named _change_type, but Change data feed was turned off on that table, data in that column would incorrectly fill with NULL values when running MERGE.  \nFixed an issue with Auto Loader where a file can be duplicated in the same micro-batch when allowOverwrites is enabled  \n[SPARK-40596] Populate ExecutorDecommission with messages in ExecutorDecommissionInfo  \nOperating system security updates.  \nOctober 18, 2022  \nOperating system security updates.  \nOctober 5, 2022  \nMiscellaneous fixes.  \nOperating system security updates.  \nSeptember 22, 2022  \nUsers can set spark.conf.set(\u201cspark.databricks.io.listKeysWithPrefix.azure.enabled\u201d, \u201ctrue\u201d) to re-enable the built-in listing for Auto Loader on ADLS Gen2. Built-in listing was previously turned off due to performance issues but can have led to increased storage costs for customers.  \n[SPARK-40315] Add hashCode() for Literal of ArrayBasedMapData  \n[SPARK-40089] Fix sorting for some Decimal types  \n[SPARK-39887] RemoveRedundantAliases should keep aliases that make the output of projection nodes unique  \nSeptember 6, 2022  \n[SPARK-40235] Use interruptible lock instead of synchronized in Executor.updateDependencies()  \n[SPARK-35542] Fix: Bucketizer created for multiple columns with parameters splitsArray, inputCols and outputCols can not be loaded after saving it  \n[SPARK-40079] Add Imputer inputCols validation for empty input case  \nAugust 24, 2022  \n[SPARK-39666] Use UnsafeProjection.create to respect spark.sql.codegen.factoryMode in ExpressionEncoder  \n[SPARK-39962] Apply projection when group attributes are empty  \nOperating system security updates.  \nAugust 9, 2022  \nOperating system security updates.  \nJuly 27, 2022  \nMake Delta MERGE operation results consistent when the source is non-deterministic."
    },
    {
        "id": 1516,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-39962] Apply projection when group attributes are empty  \nOperating system security updates.  \nAugust 9, 2022  \nOperating system security updates.  \nJuly 27, 2022  \nMake Delta MERGE operation results consistent when the source is non-deterministic.  \n[SPARK-39689] Support for 2-chars lineSep in CSV data source  \n[SPARK-39575] Added ByteBuffer#rewind after ByteBuffer#get in AvroDeserializer.  \n[SPARK-37392] Fixed the performance error for catalyst optimizer.  \nOperating system security updates.  \nJuly 13, 2022  \n[SPARK-39419] ArraySort throws an exception when the comparator returns null.  \nTurned off Auto Loader\u2019s use of built-in cloud APIs for directory listing on Azure.  \nOperating system security updates.  \nJuly 5, 2022  \nOperating system security updates.  \nMiscellaneous fixes.  \nJune 15, 2022  \n[SPARK-39283] Fix deadlock between TaskMemoryManager and UnsafeExternalSorter.SpillableIterator.  \nJune 2, 2022  \n[SPARK-34554] Implement the copy() method in ColumnarMap.  \nOperating system security updates.  \nMay 18, 2022  \nFixed a potential built-in memory leak in Auto Loader.  \nUpgrade AWS SDK version from 1.11.655 to 1.11.678.  \n[SPARK-38918] Nested column pruning should filter out attributes that do not belong to the current relation  \n[SPARK-39084] Fix df.rdd.isEmpty() by using TaskContext to stop iterator on task completion  \nOperating system security updates.  \nApril 19, 2022  \nOperating system security updates.  \nMiscellaneous fixes.  \nApril 6, 2022  \n[SPARK-38631] Uses Java-based implementation for un-tarring at Utils.unpack  \nOperating system security updates.  \nMarch 22, 2022  \nChanged the current working directory of notebooks on High Concurrency clusters with either table access control or credential passthrough enabled to the user\u2019s home directory. Previously, the active directory was /databricks/driver.  \n[SPARK-38437] Lenient serialization of datetime from datasource  \n[SPARK-38180] Allow safe up-cast expressions in correlated equality predicates"
    },
    {
        "id": 1517,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-38437] Lenient serialization of datetime from datasource  \n[SPARK-38180] Allow safe up-cast expressions in correlated equality predicates  \n[SPARK-38155] Disallow distinct aggregate in lateral subqueries with unsupported predicates  \n[SPARK-27442] Removed a check field when reading or writing data in a parquet.  \nMarch 14, 2022  \n[SPARK-38236] Absolute file paths specified in the create/alter table are treated as relative  \n[SPARK-34069] Interrupt task thread if local property SPARK_JOB_INTERRUPT_ON_CANCEL is set to true.  \nFebruary 23, 2022  \n[SPARK-37859] SQL tables created with JDBC with Spark 3.1 are not readable with Spark 3.2.  \nFebruary 8, 2022  \n[SPARK-27442] Removed a check field when reading or writing data in a parquet.  \nOperating system security updates.  \nFebruary 1, 2022  \nOperating system security updates.  \nJanuary 26, 2022  \nFixed an issue where concurrent transactions on Delta tables could commit in a non-serializable order under certain rare conditions.  \nFixed an issue where the OPTIMIZE command could fail when the ANSI SQL dialect was enabled.  \nJanuary 19, 2022  \nMinor fixes and security enhancements.  \nOperating system security updates.  \nNovember 4, 2021  \nFixed an issue that could cause Structured Streaming streams to fail with an ArrayIndexOutOfBoundsException.  \nFixed a race condition that might cause a query failure with an IOException like java.io.IOException: No FileSystem for scheme or that might cause modifications to sparkContext.hadoopConfiguration to not take effect in queries.  \nThe Apache Spark Connector for Delta Sharing was upgraded to 0.2.0.  \nOctober 20, 2021  \nUpgraded BigQuery connector from 0.18.1 to 0.22.2. This adds support for the BigNumeric type.  \nDatabricks Runtime 13.0 (EoS)  \nSee Databricks Runtime 13.0 (EoS).  \nOctober 13, 2023  \nSnowflake-jdbc dependency upgraded from 3.13.29 to 3.13.33.  \n[SPARK-42553][SQL] Ensure at least one time unit after interval."
    },
    {
        "id": 1518,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "October 13, 2023  \nSnowflake-jdbc dependency upgraded from 3.13.29 to 3.13.33.  \n[SPARK-42553][SQL] Ensure at least one time unit after interval.  \n[SPARK-45178] Fallback to running a single batch for Trigger.AvailableNow with unsupported sources rather than using wrapper.  \n[SPARK-44658][CORE] ShuffleStatus.getMapStatus returns None instead of Some(null).  \n[SPARK-42205][CORE] Remove logging of Accumulables in Task/Stage start events in JsonProtocol.  \nOperating system security updates.  \nSeptember 12, 2023  \n[SPARK-44485][SQL] Optimize TreeNode.generateTreeString.  \n[SPARK-44718][SQL] Match ColumnVector memory-mode config default to OffHeapMemoryMode config value.  \nMiscellaneous bug fixes.  \nAugust 30, 2023  \n[SPARK-44818][Backport] Fixed race for pending task interrupt issued before taskThread is initialized.  \n[SPARK-44714] Ease restriction of LCA resolution regarding queries.  \n[SPARK-44245][PYTHON] pyspark.sql.dataframe sample() doctests is now illustrative-only.  \n[11.3-13.0][[SPARK-44871]]https://issues.apache.org/jira/browse/SPARK-44871)[SQL] Fixed percentile_disc behavior.  \nOperating system security updates.  \nAugust 15, 2023  \n[SPARK-44643][SQL][PYTHON] Fix Row.__repr__ when the row is empty.  \n[SPARK-44504][Backport] Maintenance task cleans up loaded providers on stop error.  \n[SPARK-44479][CONNECT][PYTHON] Fixed protobuf conversion from an empty struct type.  \n[SPARK-44464][SS] Fixed applyInPandasWithStatePythonRunner to output rows that have Null as first column value.  \nMiscellaneous bug fixes.  \nJuly 29, 2023"
    },
    {
        "id": 1519,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-44464][SS] Fixed applyInPandasWithStatePythonRunner to output rows that have Null as first column value.  \nMiscellaneous bug fixes.  \nJuly 29, 2023  \nFixed a bug where dbutils.fs.ls() returned INVALID_PARAMETER_VALUE.LOCATION_OVERLAP when called for a storage location path which clashed with other external or managed storage location.  \n[SPARK-44199] CacheManager no longer refreshes the fileIndex unnecessarily.  \nOperating system security updates.  \nJuly 24, 2023  \n[SPARK-44337][PROTOBUF] Fixed an issue where any field set to Any.getDefaultInstance caused parse errors.  \n[SPARK-44136] [SS] Fixed an issue where StateManager would get materialized in an executor instead of driver in FlatMapGroupsWithStateExec.  \nRevert [SPARK-42323][SQL] Assign name to _LEGACY_ERROR_TEMP_2332.  \nOperating system security updates.  \nJune 23, 2023  \nOperating system security updates.  \nJune 15, 2023  \nPhotonized approx_count_distinct.  \nSnowflake-jdbc library is upgraded to 3.13.29 to address a security issue.  \n[SPARK-43156][SPARK-43098][SQL] Extend scalar subquery count bug test with decorrelateInnerQuery disabled  \n[SPARK-43779][SQL] ParseToDate now loads EvalMode in the main thread.  \n[SPARK-42937][SQL] PlanSubqueries should set InSubqueryExec#shouldBroadcast to true  \nOperating system security updates.  \nJune 2, 2023  \nThe JSON parser in failOnUnknownFields mode drops a record in DROPMALFORMED mode and fails directly in FAILFAST mode.  \nImprove the performance of incremental update with SHALLOW CLONE Iceberg and Parquet.  \nFixed an issue in Auto Loader where different source file formats were inconsistent when the provided schema did not include inferred partitions. This issue could cause unexpected failures when reading files with missing columns in the inferred partition schema.  \n[SPARK-43404][Backport] Skip reusing sst file for same version of RocksDB state store to avoid ID mismatch error."
    },
    {
        "id": 1520,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-43404][Backport] Skip reusing sst file for same version of RocksDB state store to avoid ID mismatch error.  \n[SPARK-43340][CORE] Fixed missing stack trace field in eventlogs.  \n[SPARK-43300][CORE] NonFateSharingCache wrapper for Guava Cache.  \n[SPARK-43378][CORE] Properly close stream objects in deserializeFromChunkedBuffer.  \n[SPARK-16484][SQL] Use 8-bit registers for representing DataSketches.  \n[SPARK-43522][SQL] Fixed creating struct column name with index of array.  \n[SPARK-43413][11.3-13.0][SQL] Fixed IN subquery ListQuery nullability.  \n[SPARK-43043][CORE] Improved MapOutputTracker.updateMapOutput performance.  \n[SPARK-16484][SQL] Added support for DataSketches HllSketch.  \n[SPARK-43123][SQL] Internal field metadata no longer leaks to catalogs.  \n[SPARK-42851][SQL] Guard EquivalentExpressions.addExpr() with supportedExpression().  \n[SPARK-43336][SQL] Casting between Timestamp and TimestampNTZ requires timezone.  \n[SPARK-43286][SQL] Updated aes_encrypt CBC mode to generate random IVs.  \n[SPARK-42852][SQL] Reverted NamedLambdaVariable related changes from EquivalentExpressions.  \n[SPARK-43541][SQL] Propagate all Project tags in resolving of expressions and missing columns..  \n[SPARK-43527][PYTHON] Fixed catalog.listCatalogs in PySpark.  \nOperating system security updates.  \nMay 31, 2023  \nDefault optimized write support for Delta tables registered in Unity Catalog has expanded to include CTAS statements and INSERT operations for partitioned tables. This behavior aligns to defaults on SQL warehouses. See Optimized writes for Delta Lake on Databricks.  \nMay 17, 2023"
    },
    {
        "id": 1521,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "May 17, 2023  \nFixed a regression where _metadata.file_path and _metadata.file_name would return incorrectly formatted strings. For example, now a path with spaces are be represented as s3://test-bucket/some%20directory/some%20data.csv instead of s3://test-bucket/some directory/some data.csv.  \nParquet scans are now robust against OOMs when scanning exceptionally structured files by dynamically adjusting batch size. File metadata is analyzed to preemptively lower batch size and is lowered again on task retries as a final safety net.  \nIf an Avro file was read with just the failOnUnknownFields\\ option or with Auto Loader in the failOnNewColumns\\ schema evolution mode, columns that have different data types would be read as null\\ instead of throwing an error stating that the file cannot be read. These reads now fail and recommend users to use the rescuedDataColumn\\ option.  \nAuto Loader now does the following.  \nCorrectly reads and no longer rescues Integer, Short, Byte types if one of these data types are provided, but the Avro file suggests one of the other two types.  \nPrevents reading interval types as date or timestamp types to avoid getting corrupt dates.  \nPrevents reading Decimal types with lower precision.  \n[SPARK-43172] [CONNECT] Exposes host and token from Spark connect client.  \n[SPARK-43293][SQL] __qualified_access_only is ignored in normal columns.  \n[SPARK-43098][SQL] Fixed correctness COUNT bug when scalar subquery is grouped by clause.  \n[SPARK-43085][SQL] Support for column DEFAULT assignment for multi-part table names.  \n[SPARK-43190][SQL] ListQuery.childOutput is now consistent with secondary output.  \n[SPARK-43192] [CONNECT] Removed user agent charset validation.  \nApril 25, 2023  \nYou can modify a Delta table to add support for a Delta table feature using DeltaTable.addFeatureSupport(feature_name).  \nThe SYNC command now supports legacy data source formats."
    },
    {
        "id": 1522,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "April 25, 2023  \nYou can modify a Delta table to add support for a Delta table feature using DeltaTable.addFeatureSupport(feature_name).  \nThe SYNC command now supports legacy data source formats.  \nFixed a bug where using the Python formatter before running any other commands in a Python notebook could cause the notebook path to be missing from sys.path.  \nDatabricks now supports specifying default values for columns of Delta tables. INSERT, UPDATE, DELETE, and MERGE commands can refer to a column\u2019s default value using the explicit DEFAULT keyword. For INSERT commands with an explicit list of fewer columns than the target table, corresponding column default values are substituted for the remaining columns (or NULL if no default is specified).  \nFixes a bug where the web terminal could not be used to access files in /Workspace for some users.  \nIf a Parquet file was read with just the failOnUnknownFields option or with Auto Loader in the failOnNewColumns schema evolution mode, columns that had different data types would be read as null instead of throwing an error stating that the file cannot be read. These reads now fail and recommend users to use the rescuedDataColumn option.  \nAuto Loader now correctly reads and no longer rescues Integer, Short, Byte types if one of these data types are provided. The Parquet file suggests one of the other two types. When the rescued data column was previously enabled, the data type mismatch would cause columns to be rescued even though they were readable.  \nFixed a bug where Auto Loader schema evolution can go into an infinite fail loop, when a new column is detected in the schema of a nested JSON object.  \n[SPARK-42794][SS] Increase the lockAcquireTimeoutMs to 2 minutes for acquiring the RocksDB state store in Structure Streaming.  \n[SPARK-39221][SQL] Make sensitive information be redacted correctly for thrift server job/stage tab.  \n[SPARK-42971][CORE] Change to print workdir if appDirs is null when worker handle WorkDirCleanup event.  \n[SPARK-42936][SQL] Fix LCA bug when the having clause can be resolved directly by its child aggregate."
    },
    {
        "id": 1523,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-42936][SQL] Fix LCA bug when the having clause can be resolved directly by its child aggregate.  \n[SPARK-43018][SQL] Fix bug for INSERT commands with timestamp literals.  \nRevert [SPARK-42754][SQL][UI] Fix backward compatibility issue in nested SQL run.  \nRevert [SPARK-41498] Propagate metadata through Union.  \n[SPARK-43038][SQL] Support the CBC mode by aes_encrypt()/aes_decrypt().  \n[SPARK-42928][SQL] Make resolvePersistentFunction synchronized.  \n[SPARK-42521][SQL] Add NULL values for INSERT with user-specified lists of fewer columns than the target table.  \n[SPARK-41391][SQL] The output column name of groupBy.agg(count_distinct) was incorrect.  \n[SPARK-42548][SQL] Add ReferenceAllColumns to skip rewriting attributes.  \n[SPARK-42423][SQL] Add metadata column file block start and length.  \n[SPARK-42796][SQL] Support accessing TimestampNTZ columns in CachedBatch.  \n[SPARK-42266][PYTHON] Remove the parent directory in shell.py run when IPython is used.  \n[SPARK-43011][SQL] array_insert should fail with 0 index.  \n[SPARK-41874][CONNECT][PYTHON] Support SameSemantics in Spark Connect.  \n[SPARK-42702][SPARK-42623][SQL] Support parameterized query in subquery and CTE.  \n[SPARK-42967][CORE] Fix SparkListenerTaskStart.stageAttemptId when a task is started after the stage is cancelled.  \nOperating system security updates.  \nDatabricks Runtime 12.1 (EoS)  \nSee Databricks Runtime 12.1 (EoS).  \nJune 23, 2023  \nOperating system security updates.  \nJune 15, 2023  \nPhotonized approx_count_distinct."
    },
    {
        "id": 1524,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "See Databricks Runtime 12.1 (EoS).  \nJune 23, 2023  \nOperating system security updates.  \nJune 15, 2023  \nPhotonized approx_count_distinct.  \nSnowflake-jdbc library is upgraded to 3.13.29 to address a security issue.  \n[SPARK-43779][SQL] ParseToDate now loads EvalMode in the main thread.  \n[SPARK-43156][SPARK-43098][SQL] Extend scalar subquery count bug test with decorrelateInnerQuery disabled  \nOperating system security updates.  \nJune 2, 2023  \nThe JSON parser in failOnUnknownFields mode drops a record in DROPMALFORMED mode and fails directly in FAILFAST mode.  \nImprove the performance of incremental update with SHALLOW CLONE Iceberg and Parquet.  \nFixed an issue in Auto Loader where different source file formats were inconsistent when the provided schema did not include inferred partitions. This issue could cause unexpected failures when reading files with missing columns in the inferred partition schema.  \n[SPARK-43404][Backport] Skip reusing sst file for same version of RocksDB state store to avoid ID mismatch error.  \n[SPARK-43413][11.3-13.0][SQL] Fixed IN subquery ListQuery nullability.  \n[SPARK-43522][SQL] Fixed creating struct column name with index of array.  \n[SPARK-42444][PYTHON] DataFrame.drop now handles duplicated columns properly.  \n[SPARK-43541][SQL] Propagate all Project tags in resolving of expressions and missing columns..  \n[SPARK-43340][CORE] Fixed missing stack trace field in eventlogs.  \n[SPARK-42937][SQL] PlanSubqueries now sets InSubqueryExec#shouldBroadcast to true.  \n[SPARK-43527][PYTHON] Fixed catalog.listCatalogs in PySpark.  \n[SPARK-43378][CORE] Properly close stream objects in deserializeFromChunkedBuffer.  \nMay 17, 2023"
    },
    {
        "id": 1525,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-43378][CORE] Properly close stream objects in deserializeFromChunkedBuffer.  \nMay 17, 2023  \nParquet scans are now robust against OOMs when scanning exceptionally structured files by dynamically adjusting batch size. File metadata is analyzed to preemptively lower batch size and is lowered again on task retries as a final safety net.  \nIf an Avro file was read with just the failOnUnknownFields\\ option or with Auto Loader in the failOnNewColumns\\ schema evolution mode, columns that have different data types would be read as null\\ instead of throwing an error stating that the file cannot be read. These reads now fail and recommend users to use the rescuedDataColumn\\ option.  \nAuto Loader now does the following.  \nCorrectly reads and no longer rescues Integer, Short, Byte types if one of these data types are provided, but the Avro file suggests one of the other two types.  \nPrevents reading interval types as date or timestamp types to avoid getting corrupt dates.  \nPrevents reading Decimal types with lower precision.  \n[SPARK-43098][SQL] Fixed correctness COUNT bug when scalar subquery is grouped by clause.  \n[SPARK-43190][SQL] ListQuery.childOutput is now consistent with secondary output.  \nOperating system security updates.  \nApril 25, 2023  \nIf a Parquet file was read with just the failOnUnknownFields option or with Auto Loader in the failOnNewColumns schema evolution mode, columns that had different data types would be read as null instead of throwing an error stating that the file cannot be read. These reads now fail and recommend users to use the rescuedDataColumn option.  \nAuto Loader now correctly reads and no longer rescues Integer, Short, Byte types if one of these data types are provided. The Parquet file suggests one of the other two types. When the rescued data column was previously enabled, the data type mismatch would cause columns to be rescued even though they were readable.  \n[SPARK-43009][SQL] Parameterized sql() with Any constants.  \n[SPARK-42971][CORE] Change to print workdir if appDirs is null when worker handle WorkDirCleanup event.  \nOperating system security updates.  \nApril 11, 2023"
    },
    {
        "id": 1526,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-42971][CORE] Change to print workdir if appDirs is null when worker handle WorkDirCleanup event.  \nOperating system security updates.  \nApril 11, 2023  \nSupport legacy data source formats in SYNC command.  \nFixes a bug in the %autoreload behavior in notebooks that are outside of a repo.  \nFixed a bug where Auto Loader schema evolution can go into an infinite fail loop, when a new column is detected in the schema of a nested JSON object.  \n[SPARK-42928][SQL] Makes resolvePersistentFunction synchronized.  \n[SPARK-42967][CORE] Fixes SparkListenerTaskStart.stageAttemptId when a task starts after the stage is cancelled.  \nOperating system security updates.  \nMarch 29, 2023  \nAuto Loader now triggers at least one synchronous RocksDB log clean up for Trigger.AvailableNow streams to ensure that the checkpoint can get regularly cleaned up for fast-running Auto Loader streams. This can cause some streams to take longer before they shut down, but will save you storage costs and improve the Auto Loader experience in future runs.  \nYou can now modify a Delta table to add support to table features using DeltaTable.addFeatureSupport(feature_name).  \n[SPARK-42702][SPARK-42623][SQL] Support parameterized query in subquery and CTE  \n[SPARK-41162][SQL] Fix anti- and semi-join for self-join with aggregations  \n[SPARK-42403][CORE] JsonProtocol should handle null JSON strings  \n[SPARK-42668][SS] Catch exception while trying to close compressed stream in HDFSStateStoreProvider abort  \n[SPARK-42794][SS] Increase the lockAcquireTimeoutMs to 2 minutes for acquiring the RocksDB state store in Structure Streaming  \nMarch 14, 2023  \nThere is a terminology change for adding features to a Delta table using the table property. The preferred syntax is now 'delta.feature.featureName'='supported' instead of 'delta.feature.featureName'='enabled'. For backwards compatibility, using 'delta.feature.featureName'='enabled' still works and will continue to work."
    },
    {
        "id": 1527,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-42622][CORE] Disable substitution in values  \n[SPARK-42534][SQL] Fix DB2Dialect Limit clause  \n[SPARK-42635][SQL] Fix the TimestampAdd expression.  \n[SPARK-42516][SQL] Always capture the session time zone config while creating views  \n[SPARK-42484] [SQL] UnsafeRowUtils better error message  \n[SPARK-41793][SQL] Incorrect result for window frames defined by a range clause on large decimals  \nOperating system security updates.  \nFebruary 24, 2023  \nYou can now use a unified set of options (host, port, database, user, password) for connecting to the data sources supported in Query Federation (PostgreSQL, MySQL, Synapse, Snowflake, Redshift, SQL Server). Note that port is optional and uses the default port number for each data source if not provided.  \nExample of PostgreSQL connection configuration  \nCREATE TABLE postgresql_table USING postgresql OPTIONS ( dbtable '<table-name>', host '<host-name>', database '<database-name>', user '<user>', password secret('scope', 'key') );  \nExample of Snowflake connection configuration  \nCREATE TABLE snowflake_table USING snowflake OPTIONS ( dbtable '<table-name>', host '<host-name>', port '<port-number>', database '<database-name>', user secret('snowflake_creds', 'my_username'), password secret('snowflake_creds', 'my_password'), schema '<schema-name>', sfWarehouse '<warehouse-name>' );  \n[SPARK-41989][PYTHON] Avoid breaking logging config from pyspark.pandas  \n[SPARK-42346][SQL] Rewrite distinct aggregates after subquery merge  \n[SPARK-41990][SQL] Use FieldReference.column instead of apply in V1 to V2 filter conversion  \nRevert [SPARK-41848][CORE] Fixing task over-scheduled with TaskResourceProfile"
    },
    {
        "id": 1528,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-41990][SQL] Use FieldReference.column instead of apply in V1 to V2 filter conversion  \nRevert [SPARK-41848][CORE] Fixing task over-scheduled with TaskResourceProfile  \n[SPARK-42162] Introduce MultiCommutativeOp expression as a memory optimization for canonicalizing large trees of commutative expressions  \nOperating system security updates.  \nFebruary 16, 2023  \nSYNC command supports syncing recreated Hive Metastore tables. If a HMS table has been SYNCed previously to Unity Catalog but then dropped and recreated, a subsequent re-sync will work instead of throwing TABLE_ALREADY_EXISTS status code.  \n[SPARK-41219][SQL] IntegralDivide use decimal(1, 0) to represent 0  \n[SPARK-36173][CORE] Support getting CPU number in TaskContext  \n[SPARK-41848][CORE] Fixing task over-scheduled with TaskResourceProfile  \n[SPARK-42286][SQL] Fallback to previous codegen code path for complex expr with CAST  \nJanuary 31, 2023  \nCreating a schema with a defined location now requires the user to have SELECT and MODIFY privileges on ANY FILE.  \n[SPARK-41581][SQL] Assign name to LEGACYERROR_TEMP_1230  \n[SPARK-41996][SQL][SS] Fix kafka test to verify lost partitions to account for slow Kafka operations  \n[SPARK-41580][SQL] Assign name to LEGACYERROR_TEMP_2137  \n[SPARK-41666][PYTHON] Support parameterized SQL by sql()  \n[SPARK-41579][SQL] Assign name to LEGACYERROR_TEMP_1249  \n[SPARK-41573][SQL] Assign name to LEGACYERROR_TEMP_2136  \n[SPARK-41574][SQL] Assign name to LEGACYERROR_TEMP_2009  \n[SPARK-41049][Followup] Fix a code sync regression for ConvertToLocalRelation  \n[SPARK-41576][SQL] Assign name to LEGACYERROR_TEMP_2051"
    },
    {
        "id": 1529,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-41049][Followup] Fix a code sync regression for ConvertToLocalRelation  \n[SPARK-41576][SQL] Assign name to LEGACYERROR_TEMP_2051  \n[SPARK-41572][SQL] Assign name to LEGACYERROR_TEMP_2149  \n[SPARK-41575][SQL] Assign name to LEGACYERROR_TEMP_2054  \nOperating system security updates.  \nDatabricks Runtime 12.0 (EoS)  \nSee Databricks Runtime 12.0 (EoS).  \nJune 15, 2023  \nPhotonized approx_count_distinct.  \nSnowflake-jdbc library is upgraded to 3.13.29 to address a security issue.  \n[SPARK-43156][SPARK-43098][SQL] Extend scalar subquery count bug test with decorrelateInnerQuery disabled  \n[SPARK-43779][SQL] ParseToDate now loads EvalMode in the main thread.  \nOperating system security updates.  \nJune 2, 2023  \nThe JSON parser in failOnUnknownFields mode drops a record in DROPMALFORMED mode and fails directly in FAILFAST mode.  \nImprove the performance of incremental update with SHALLOW CLONE Iceberg and Parquet.  \nFixed an issue in Auto Loader where different source file formats were inconsistent when the provided schema did not include inferred partitions. This issue could cause unexpected failures when reading files with missing columns in the inferred partition schema.  \n[SPARK-42444][PYTHON] DataFrame.drop now handles duplicated columns properly.  \n[SPARK-43404][Backport] Skip reusing sst file for same version of RocksDB state store to avoid ID mismatch error.  \n[11.3-13.0][[SPARK-43413]]https://issues.apache.org/jira/browse/SPARK-43413)[SQL] Fixed IN subquery ListQuery nullability.  \n[SPARK-43527][PYTHON] Fixed catalog.listCatalogs in PySpark.  \n[SPARK-43522][SQL] Fixed creating struct column name with index of array."
    },
    {
        "id": 1530,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-43527][PYTHON] Fixed catalog.listCatalogs in PySpark.  \n[SPARK-43522][SQL] Fixed creating struct column name with index of array.  \n[SPARK-43541][SQL] Propagate all Project tags in resolving of expressions and missing columns..  \n[SPARK-43340][CORE] Fixed missing stack trace field in eventlogs.  \n[SPARK-42937][SQL] PlanSubqueries set InSubqueryExec#shouldBroadcast to true.  \nMay 17, 2023  \nParquet scans are now robust against OOMs when scanning exceptionally structured files by dynamically adjusting batch size. File metadata is analyzed to preemptively lower batch size and is lowered again on task retries as a final safety net.  \nIf an Avro file was read with just the failOnUnknownFields\\ option or with Auto Loader in the failOnNewColumns\\ schema evolution mode, columns that have different data types would be read as null\\ instead of throwing an error stating that the file cannot be read. These reads now fail and recommend users to use the rescuedDataColumn\\ option.  \nAuto Loader now does the following.  \nCorrectly reads and no longer rescues Integer, Short, Byte types if one of these data types are provided, but the Avro file suggests one of the other two types.  \nPrevents reading interval types as date or timestamp types to avoid getting corrupt dates.  \nPrevents reading Decimal types with lower precision.  \n[SPARK-43172] [CONNECT] Exposes host and token from Spark connect client.  \n[SPARK-41520][SQL] Split AND_OR tree pattern to separate AND and OR.  \n[SPARK-43098][SQL] Fixed correctness COUNT bug when scalar subquery is grouped by clause.  \n[SPARK-43190][SQL] ListQuery.childOutput is now consistent with secondary output.  \nOperating system security updates.  \nApril 25, 2023"
    },
    {
        "id": 1531,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-43190][SQL] ListQuery.childOutput is now consistent with secondary output.  \nOperating system security updates.  \nApril 25, 2023  \nIf a Parquet file was read with just the failOnUnknownFields option or with Auto Loader in the failOnNewColumns schema evolution mode, columns that had different data types would be read as null instead of throwing an error stating that the file cannot be read. These reads now fail and recommend users to use the rescuedDataColumn option.  \nAuto Loader now correctly reads and no longer rescues Integer, Short, Byte types if one of these data types are provided. The Parquet file suggests one of the other two types. When the rescued data column was previously enabled, the data type mismatch would cause columns to be rescued even though they were readable.  \n[SPARK-42971][CORE] Change to print workdir if appDirs is null when worker handle WorkDirCleanup event  \nOperating system security updates.  \nApril 11, 2023  \nSupport legacy data source formats in SYNC command.  \nFixes a bug in the %autoreload behavior in notebooks which are outside of a repo.  \nFixed a bug where Auto Loader schema evolution can go into an infinite fail loop, when a new column is detected in the schema of a nested JSON object.  \n[SPARK-42928][SQL] Makes resolvePersistentFunction synchronized.  \n[SPARK-42967][CORE] Fixes SparkListenerTaskStart.stageAttemptId when a task starts after the stage is cancelled.  \nOperating system security updates.  \nMarch 29, 2023  \n[SPARK-42794][SS] Increase the lockAcquireTimeoutMs to 2 minutes for acquiring the RocksDB state store in Structure Streaming  \n[SPARK-41162][SQL] Fix anti- and semi-join for self-join with aggregations  \n[SPARK-42403][CORE] JsonProtocol should handle null JSON strings  \n[SPARK-42668][SS] Catch exception while trying to close compressed stream in HDFSStateStoreProvider abort  \nMiscellaneous bug fixes.  \nMarch 14, 2023  \n[SPARK-42534][SQL] Fix DB2Dialect Limit clause  \n[SPARK-42622][CORE] Disable substitution in values"
    },
    {
        "id": 1532,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Miscellaneous bug fixes.  \nMarch 14, 2023  \n[SPARK-42534][SQL] Fix DB2Dialect Limit clause  \n[SPARK-42622][CORE] Disable substitution in values  \n[SPARK-41793][SQL] Incorrect result for window frames defined by a range clause on large decimals  \n[SPARK-42484] [SQL] UnsafeRowUtils better error message  \n[SPARK-42635][SQL] Fix the TimestampAdd expression.  \n[SPARK-42516][SQL] Always capture the session time zone config while creating views  \nOperating system security updates.  \nFebruary 24, 2023  \nStandardized Connection Options for Query Federation  \nYou can now use a unified set of options (host, port, database, user, password) for connecting to the data sources supported in Query Federation (PostgreSQL, MySQL, Synapse, Snowflake, Redshift, SQL Server). Note that port is optional and will use the default port number for each data source if not provided.  \nExample of PostgreSQL connection configuration  \nCREATE TABLE postgresql_table USING postgresql OPTIONS ( dbtable '<table-name>', host '<host-name>', database '<database-name>', user '<user>', password secret('scope', 'key') );  \nExample of Snowflake connection configuration  \nCREATE TABLE snowflake_table USING snowflake OPTIONS ( dbtable '<table-name>', host '<host-name>', port '<port-number>', database '<database-name>', user secret('snowflake_creds', 'my_username'), password secret('snowflake_creds', 'my_password'), schema '<schema-name>', sfWarehouse '<warehouse-name>' );  \nRevert [SPARK-41848][CORE] Fixing task over-scheduled with TaskResourceProfile  \n[SPARK-42162] Introduce MultiCommutativeOp expression as a memory optimization for canonicalizing large trees of commutative expressions  \n[SPARK-41990][SQL] Use FieldReference.column instead of apply in V1 to V2 filter conversion"
    },
    {
        "id": 1533,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-41990][SQL] Use FieldReference.column instead of apply in V1 to V2 filter conversion  \n[SPARK-42346][SQL] Rewrite distinct aggregates after subquery merge  \nOperating system security updates.  \nFebruary 16, 2023  \nUsers can now read and write certain Delta tables that require Reader version 3 and Writer version 7, by using Databricks Runtime 9.1 or later. To succeed, table features listed in the tables\u2019 protocol must be supported by the current version of Databricks Runtime.  \nSYNC command supports syncing recreated Hive Metastore tables. If a HMS table has been SYNCed previously to Unity Catalog but then dropped and recreated, a subsequent re-sync will work instead of throwing TABLE_ALREADY_EXISTS status code.  \n[SPARK-36173][CORE] Support getting CPU number in TaskContext  \n[SPARK-42286][SQL] Fallback to previous codegen code path for complex expr with CAST  \n[SPARK-41848][CORE] Fixing task over-scheduled with TaskResourceProfile  \n[SPARK-41219][SQL] IntegralDivide use decimal(1, 0) to represent 0  \nJanuary 25, 2023  \n[SPARK-41660][SQL] Only propagate metadata columns if they are used  \n[SPARK-41379][SS][PYTHON] Provide cloned spark session in DataFrame in user function for foreachBatch sink in PySpark  \n[SPARK-41669][SQL] Early pruning in canCollapseExpressions  \nOperating system security updates.  \nJanuary 18, 2023  \nREFRESH FUNCTION SQL command now supports SQL functions and SQL Table functions. For example, the command could be used to refresh a persistent SQL function that was updated in another SQL session.  \nJava Database Connectivity (JDBC) data source v1 now supports LIMIT clause pushdown to improve performance in queries. This feature is enabled by default and can be disabled with spark.databricks.optimizer.jdbcDSv1LimitPushdown.enabled set to false.  \nIn Legacy Table ACLs clusters, creating functions that reference JVM classes now requires the MODIFY_CLASSPATH privilege."
    },
    {
        "id": 1534,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "In Legacy Table ACLs clusters, creating functions that reference JVM classes now requires the MODIFY_CLASSPATH privilege.  \nJava Database Connectivity (JDBC) data source v1 now supports LIMIT clause pushdown to improve performance in queries. This feature is enabled by default and can be disabled with spark.databricks.optimizer.jdbcDSv1LimitPushdown.enabled set to false.  \nAzure Synapse connector now returns a more descriptive error message when a column name contains invalid characters such as whitespaces or semicolons. In such cases, the following message will be returned: Azure Synapse Analytics failed to execute the JDBC query produced by the connector. Make sure column names do not include any invalid characters such as ';' or whitespace.  \nSpark structured streaming now works with format(\u201cdeltasharing\u201d) on a delta sharing table as a source.  \n[SPARK-38277][SS] Clear write batch after RocksDB state store\u2019s commit  \n[SPARK-41733][SQL][SS] Apply tree-pattern based pruning for the rule ResolveWindowTime  \n[SPARK-39591][SS] Async Progress Tracking  \n[SPARK-41339][SQL] Close and recreate RocksDB write batch instead of just clearing  \n[SPARK-41198][SS] Fix metrics in streaming query having CTE and DSv1 streaming source  \n[SPARK-41539][SQL] Remap stats and constraints against output in logical plan for LogicalRDD  \n[SPARK-41732][SQL][SS] Apply tree-pattern based pruning for the rule SessionWindowing  \n[SPARK-41862][SQL] Fix correctness bug related to DEFAULT values in Orc reader  \n[SPARK-41199][SS] Fix metrics issue when DSv1 streaming source and DSv2 streaming source are co-used  \n[SPARK-41261][PYTHON][SS] Fix issue for applyInPandasWithState when the columns of grouping keys are not placed in order from earliest  \nOperating system security updates.  \nMay 17, 2023"
    },
    {
        "id": 1535,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-41261][PYTHON][SS] Fix issue for applyInPandasWithState when the columns of grouping keys are not placed in order from earliest  \nOperating system security updates.  \nMay 17, 2023  \nParquet scans are now robust against OOMs when scanning exceptionally structured files by dynamically adjusting batch size. File metadata is analyzed to preemptively lower batch size and is lowered again on task retries as a final safety net.  \nFixed a regression that caused Databricks jobs to persist after failing to connect to the metastore during cluster initialization.  \n[SPARK-41520][SQL] Split AND_OR tree pattern to separate AND and OR.  \n[SPARK-43190][SQL] ListQuery.childOutput is now consistent with secondary output.  \nOperating system security updates.  \nApril 25, 2023  \nIf a Parquet file was read with just the failOnUnknownFields option or with Auto Loader in the failOnNewColumns schema evolution mode, columns that had different data types would be read as null instead of throwing an error stating that the file cannot be read. These reads now fail and recommend users to use the rescuedDataColumn option.  \nAuto Loader now correctly reads and no longer rescues Integer, Short, Byte types if one of these data types are provided. The Parquet file suggests one of the other two types. When the rescued data column was previously enabled, the data type mismatch would cause columns to be rescued even though they were readable.  \n[SPARK-42937][SQL] PlanSubqueries now sets InSubqueryExec#shouldBroadcast to true.  \nOperating system security updates.  \nApril 11, 2023  \nSupport legacy data source formats in SYNC command.  \nFixes a bug in the %autoreload behavior in notebooks which are outside of a repo.  \nFixed a bug where Auto Loader schema evolution can go into an infinite fail loop, when a new column is detected in the schema of a nested JSON object.  \n[SPARK-42928][SQL] Make resolvePersistentFunction synchronized.  \n[SPARK-42967][CORE] Fix SparkListenerTaskStart.stageAttemptId when a task is started after the stage is cancelled.  \nMarch 29, 2023"
    },
    {
        "id": 1536,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-42967][CORE] Fix SparkListenerTaskStart.stageAttemptId when a task is started after the stage is cancelled.  \nMarch 29, 2023  \n[SPARK-42794][SS] Increase the lockAcquireTimeoutMs to 2 minutes for acquiring the RocksDB state store in Structure Streaming  \n[SPARK-42403][CORE] JsonProtocol should handle null JSON strings  \n[SPARK-42668][SS] Catch exception while trying to close compressed stream in HDFSStateStoreProvider abort  \nOperating system security updates.  \nMarch 14, 2023  \n[SPARK-42635][SQL] Fix the TimestampAdd expression.  \n[SPARK-41793][SQL] Incorrect result for window frames defined by a range clause on large decimals  \n[SPARK-42484] [SQL] UnsafeRowUtils better error message  \n[SPARK-42534][SQL] Fix DB2Dialect Limit clause  \n[SPARK-41162][SQL] Fix anti- and semi-join for self-join with aggregations  \n[SPARK-42516][SQL] Always capture the session time zone config while creating views  \nMiscellaneous bug fixes.  \nFebruary 28, 2023  \nStandardized Connection Options for Query Federation  \nYou can now use a unified set of options (host, port, database, user, password) for connecting to the data sources supported in Query Federation (PostgreSQL, MySQL, Synapse, Snowflake, Redshift, SQL Server). Note that port is optional and uses the default port number for each data source if not provided.  \nExample of PostgreSQL connection configuration  \nCREATE TABLE postgresql_table USING postgresql OPTIONS ( dbtable '<table-name>', host '<host-name>', database '<database-name>', user '<user>', password secret('scope', 'key') );  \nExample of Snowflake connection configuration"
    },
    {
        "id": 1537,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Example of Snowflake connection configuration  \nCREATE TABLE snowflake_table USING snowflake OPTIONS ( dbtable '<table-name>', host '<host-name>', port '<port-number>', database '<database-name>', user secret('snowflake_creds', 'my_username'), password secret('snowflake_creds', 'my_password'), schema '<schema-name>', sfWarehouse '<warehouse-name>' );  \n[SPARK-42286][SQL] Fallback to previous codegen code path for complex expr with CAST  \n[SPARK-41989][PYTHON] Avoid breaking logging config from pyspark.pandas  \n[SPARK-42346][SQL] Rewrite distinct aggregates after subquery merge  \n[SPARK-41360][CORE] Avoid BlockManager re-registration if the executor has been lost  \n[SPARK-42162] Introduce MultiCommutativeOp expression as a memory optimization for canonicalizing large trees of commutative expressions  \n[SPARK-41990][SQL] Use FieldReference.column instead of apply in V1 to V2 filter conversion  \nOperating system security updates.  \nFebruary 16, 2023  \nUsers can now read and write certain Delta tables that require Reader version 3 and Writer version 7, by using Databricks Runtime 9.1 or later. To succeed, table features listed in the tables\u2019 protocol must be supported by the current version of Databricks Runtime.  \nSYNC command supports syncing recreated Hive Metastore tables. If a HMS table has been SYNCed previously to Unity Catalog but then dropped and recreated, a subsequent re-sync will work instead of throwing TABLE_ALREADY_EXISTS status code.  \n[SPARK-41219][SQL] IntegralDivide use decimal(1, 0) to represent 0  \n[SPARK-40382][SQL] Group distinct aggregate expressions by semantically equivalent children in RewriteDistinctAggregates  \nOperating system security updates.  \nJanuary 25, 2023  \n[SPARK-41379][SS][PYTHON] Provide cloned spark session in DataFrame in user function for foreachBatch sink in PySpark"
    },
    {
        "id": 1538,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \nJanuary 25, 2023  \n[SPARK-41379][SS][PYTHON] Provide cloned spark session in DataFrame in user function for foreachBatch sink in PySpark  \n[SPARK-41660][SQL] Only propagate metadata columns if they are used  \n[SPARK-41669][SQL] Early pruning in canCollapseExpressions  \nMiscellaneous bug fixes.  \nJanuary 18, 2023  \nREFRESH FUNCTION SQL command now supports SQL functions and SQL Table functions. For example, the command could be used to refresh a persistent SQL function that was updated in another SQL session.  \nJava Database Connectivity (JDBC) data source v1 now supports LIMIT clause pushdown to improve performance in queries. This feature is enabled by default and can be disabled with spark.databricks.optimizer.jdbcDSv1LimitPushdown.enabled set to false.  \nJava Database Connectivity (JDBC) data source v1 now supports LIMIT clause pushdown to improve performance in queries. This feature is enabled by default and can be disabled with spark.databricks.optimizer.jdbcDSv1LimitPushdown.enabled set to false.  \nAzure Synapse connector now returns a more descriptive error message when a column name contains invalid characters such as whitespaces or semicolons. In such cases, the following message will be returned: Azure Synapse Analytics failed to execute the JDBC query produced by the connector. Make sure column names do not include any invalid characters such as ';' or whitespace.  \n[SPARK-41198][SS] Fix metrics in streaming query having CTE and DSv1 streaming source  \n[SPARK-41862][SQL] Fix correctness bug related to DEFAULT values in Orc reader  \n[SPARK-41539][SQL] Remap stats and constraints against output in logical plan for LogicalRDD  \n[SPARK-39591][SS] Async Progress Tracking  \n[SPARK-41199][SS] Fix metrics issue when DSv1 streaming source and DSv2 streaming source are co-used"
    },
    {
        "id": 1539,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-39591][SS] Async Progress Tracking  \n[SPARK-41199][SS] Fix metrics issue when DSv1 streaming source and DSv2 streaming source are co-used  \n[SPARK-41261][PYTHON][SS] Fix issue for applyInPandasWithState when the columns of grouping keys are not placed in order from earliest  \n[SPARK-41339][SQL] Close and recreate RocksDB write batch instead of just clearing  \n[SPARK-41732][SQL][SS] Apply tree-pattern based pruning for the rule SessionWindowing  \n[SPARK-38277][SS] Clear write batch after RocksDB state store\u2019s commit  \nOperating system security updates.  \nNovember 29, 2022  \nUsers can configure leading and trailing whitespaces\u2019 behavior when writing data using the Redshift connector. The following options have been added to control whitespace handling:  \ncsvignoreleadingwhitespace, when set to true, removes leading whitespace from values during writes when tempformat is set to CSV or CSV GZIP. Whitespaces are retained when the config is set to false. By default, the value is true.  \ncsvignoretrailingwhitespace, when set to true, removes trailing whitespace from values during writes when tempformat is set to CSV or CSV GZIP. Whitespaces are retained when the config is set to false. By default, the value is true.  \nFixed a bug with JSON parsing in Auto Loader when all columns were left as strings (cloudFiles.inferColumnTypes was not set or set to false) and the JSON contained nested objects.  \nUpgrade snowflake-jdbc dependency to version 3.13.22.  \nTable types of JDBC tables are now EXTERNAL by default.  \n[SPARK-40906][SQL] Mode should copy keys before inserting into Map  \nOperating system security updates.  \nNovember 15, 2022  \nTable ACLs and UC Shared clusters now allow the Dataset.toJSON method from python."
    },
    {
        "id": 1540,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \nNovember 15, 2022  \nTable ACLs and UC Shared clusters now allow the Dataset.toJSON method from python.  \n[SPARK-40646] JSON parsing for structs, maps, and arrays has been fixed so when a part of a record does not match the schema, the rest of the record can still be parsed correctly instead of returning nulls. To opt-in for the improved behaviorset spark.sql.json.enablePartialResults to true. The flag is disabled by default to preserve the original behavior  \n[SPARK-40903][SQL] Avoid reordering decimal Add for canonicalization if data type is changed  \n[SPARK-40618][SQL] Fix bug in MergeScalarSubqueries rule with nested subqueries using reference tracking  \n[SPARK-40697][SQL] Add read-side char padding to cover external data files  \nOperating system security updates.  \nNovember 1, 2022  \nStructured Streaming in Unity Catalog now supports refreshing temporary access tokens. Streaming workloads running with Unity Catalog all purpose or jobs clusters no longer fail after the initial token expiry.  \nFixed an issue where if a Delta table had a user-defined column named _change_type, but Change data feed was disabled on that table, data in that column would incorrectly fill with NULL values when running MERGE.  \nFixed an issue where running MERGE and using exactly 99 columns from the source in the condition could result in java.lang.ClassCastException: org.apache.spark.sql.vectorized.ColumnarBatch cannot be cast to org.apache.spark.sql.catalyst.InternalRow.  \nFixed an issue with Auto Loader where a file can be duplicated in the same micro-batch when allowOverwrites is enabled.  \nUpgraded Apache commons-text to 1.10.0.  \n[SPARK-38881][DSTREAMS][KINESIS][PYSPARK] Added Support for CloudWatch MetricsLevel Config  \n[SPARK-40596][CORE] Populate ExecutorDecommission with messages in ExecutorDecommissionInfo  \n[SPARK-40670][SS][PYTHON] Fix NPE in applyInPandasWithState when the input schema has \u201cnon-nullable\u201d column(s)"
    },
    {
        "id": 1541,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-40670][SS][PYTHON] Fix NPE in applyInPandasWithState when the input schema has \u201cnon-nullable\u201d column(s)  \nOperating system security updates.  \nDatabricks Runtime 11.2 (EoS)  \nSee Databricks Runtime 11.2 (EoS).  \nFebruary 28, 2023  \n[SPARK-42286][SQL] Fallback to previous codegen code path for complex expr with CAST  \n[SPARK-42346][SQL] Rewrite distinct aggregates after subquery merge  \nOperating system security updates.  \nFebruary 16, 2023  \nUsers can now read and write certain Delta tables that require Reader version 3 and Writer version 7, by using Databricks Runtime 9.1 or later. To succeed, table features listed in the tables\u2019 protocol must be supported by the current version of Databricks Runtime.  \nSYNC command supports syncing recreated Hive Metastore tables. If a HMS table has been SYNCed previously to Unity Catalog but then dropped and recreated, a subsequent re-sync will work instead of throwing TABLE_ALREADY_EXISTS status code.  \n[SPARK-41219][SQL] IntegralDivide use decimal(1, 0) to represent 0  \nOperating system security updates.  \nJanuary 31, 2023  \nTable types of JDBC tables are now EXTERNAL by default.  \n[SPARK-41379][SS][PYTHON] Provide cloned spark session in DataFrame in user function for foreachBatch sink in PySpark  \nJanuary 18, 2023  \nAzure Synapse connector now returns a more descriptive error message when a column name contains invalid characters such as whitespaces or semicolons. In such cases, the following message will be returned: Azure Synapse Analytics failed to execute the JDBC query produced by the connector. Make sure column names do not include any invalid characters such as ';' or whitespace.  \n[SPARK-41198][SS] Fix metrics in streaming query having CTE and DSv1 streaming source  \n[SPARK-41862][SQL] Fix correctness bug related to DEFAULT values in Orc reader  \n[SPARK-41539][SQL] Remap stats and constraints against output in logical plan for LogicalRDD"
    },
    {
        "id": 1542,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-41539][SQL] Remap stats and constraints against output in logical plan for LogicalRDD  \n[SPARK-41199][SS] Fix metrics issue when DSv1 streaming source and DSv2 streaming source are co-used  \n[SPARK-41339][SQL] Close and recreate RocksDB write batch instead of just clearing  \n[SPARK-41732][SQL][SS] Apply tree-pattern based pruning for the rule SessionWindowing  \n[SPARK-38277][SS] Clear write batch after RocksDB state store\u2019s commit  \nOperating system security updates.  \nNovember 29, 2022  \nUsers can configure leading and trailing whitespaces\u2019 behavior when writing data using the Redshift connector. The following options have been added to control whitespace handling:  \ncsvignoreleadingwhitespace, when set to true, removes leading whitespace from values during writes when tempformat is set to CSV or CSV GZIP. Whitespaces are retained when the config is set to false. By default, the value is true.  \ncsvignoretrailingwhitespace, when set to true, removes trailing whitespace from values during writes when tempformat is set to CSV or CSV GZIP. Whitespaces are retained when the config is set to false. By default, the value is true.  \nFixed a bug with JSON parsing in Auto Loader when all columns were left as strings (cloudFiles.inferColumnTypes was not set or set to false) and the JSON contained nested objects.  \n[SPARK-40906][SQL] Mode should copy keys before inserting into Map  \nOperating system security updates.  \nNovember 15, 2022  \n[SPARK-40646] JSON parsing for structs, maps, and arrays has been fixed so when a part of a record does not match the schema, the rest of the record can still be parsed correctly instead of returning nulls. To opt-in for the improved behavior, set spark.sql.json.enablePartialResults to true. The flag is disabled by default to preserve the original behavior"
    },
    {
        "id": 1543,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-40618][SQL] Fix bug in MergeScalarSubqueries rule with nested subqueries using reference tracking  \n[SPARK-40697][SQL] Add read-side char padding to cover external data files  \nOperating system security updates.  \nNovember 1, 2022  \nUpgraded Apache commons-text to 1.10.0.  \nFixed an issue where if a Delta table had a user-defined column named _change_type, but Change data feed was disabled on that table, data in that column would incorrectly fill with NULL values when running MERGE.  \nFixed an issue where running MERGE and using exactly 99 columns from the source in the condition could result in java.lang.ClassCastException: org.apache.spark.sql.vectorized.ColumnarBatch cannot be cast to org.apache.spark.sql.catalyst.InternalRow.  \nFixed an issue with Auto Loader where a file can be duplicated in the same micro-batch when allowOverwrites is enabled  \n[SPARK-40596][CORE] Populate ExecutorDecommission with messages in ExecutorDecommissionInfo  \nOperating system security updates.  \nOctober 19, 2022  \nFixed an issue with COPY INTO usage with temporary credentials on Unity Catalog enabled clusters / warehouses.  \n[SPARK-40213][SQL] Support ASCII value conversion for Latin-1 characters  \nOperating system security updates.  \nOctober 5, 2022  \nUsers can set spark.conf.set(\u201cspark.databricks.io.listKeysWithPrefix.azure.enabled\u201d, \u201ctrue\u201d) to re-enable native listing for Auto Loader on ADLS Gen2. Native listing was previously turned off due to performance issues, but may have led to an increase in storage costs for customers. This change was rolled out to DBR 10.4 and 9.1 in the previous maintenance update.  \n[SPARK-40315][SQL]Support url encode/decode as built-in function and tidy up url-related functions  \n[SPARK-40156][SQL]url_decode() should the return an error class  \n[SPARK-40169] Don\u2019t pushdown Parquet filters with no reference to data schema  \n[SPARK-40460][SS] Fix streaming metrics when selecting _metadata"
    },
    {
        "id": 1544,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-40169] Don\u2019t pushdown Parquet filters with no reference to data schema  \n[SPARK-40460][SS] Fix streaming metrics when selecting _metadata  \n[SPARK-40468][SQL] Fix column pruning in CSV when corruptrecord is selected  \n[SPARK-40055][SQL] listCatalogs should also return spark_catalog even when spark_catalog implementation is defaultSessionCatalog  \nOperating system security updates.  \nSeptember 22, 2022  \n[SPARK-40315][SQL] Add hashCode() for Literal of ArrayBasedMapData  \n[SPARK-40389][SQL] Decimals can\u2019t upcast as integral types if the cast can overflow  \n[SPARK-40380][SQL] Fix constant-folding of InvokeLike to avoid non-serializable literal embedded in the plan  \n[SPARK-40066][SQL][FOLLOW-UP] Check if ElementAt is resolved before getting its dataType  \n[SPARK-40109][SQL] New SQL function: get()  \n[SPARK-40066][SQL] ANSI mode: always return null on invalid access to map column  \n[SPARK-40089][SQL] Fix sorting for some Decimal types  \n[SPARK-39887][SQL] RemoveRedundantAliases should keep aliases that make the output of projection nodes unique  \n[SPARK-40152][SQL] Fix split_part codegen compilation issue  \n[SPARK-40235][CORE] Use interruptible lock instead of synchronized in Executor.updateDependencies()  \n[SPARK-40212][SQL] SparkSQL castPartValue does not properly handle byte, short, or float  \n[SPARK-40218][SQL] GROUPING SETS should preserve the grouping columns  \n[SPARK-35542][ML] Fix: Bucketizer created for multiple columns with parameters  \n[SPARK-40079] Add Imputer inputCols validation for empty input case  \n[SPARK-39912]SPARK-39828[SQL] Refine CatalogImpl"
    },
    {
        "id": 1545,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-40079] Add Imputer inputCols validation for empty input case  \n[SPARK-39912]SPARK-39828[SQL] Refine CatalogImpl  \nDatabricks Runtime 11.1 (EoS)  \nSee Databricks Runtime 11.1 (EoS).  \nJanuary 31, 2023  \n[SPARK-41379][SS][PYTHON] Provide cloned spark session in DataFrame in user function for foreachBatch sink in PySpark  \nMiscellaneous bug fixes.  \nJanuary 18, 2023  \nAzure Synapse connector now returns a more descriptive error message when a column name contains invalid characters such as whitespaces or semicolons. In such cases, the following message will be returned: Azure Synapse Analytics failed to execute the JDBC query produced by the connector. Make sure column names do not include any invalid characters such as ';' or whitespace.  \n[SPARK-41198][SS] Fix metrics in streaming query having CTE and DSv1 streaming source  \n[SPARK-41862][SQL] Fix correctness bug related to DEFAULT values in Orc reader  \n[SPARK-41199][SS] Fix metrics issue when DSv1 streaming source and DSv2 streaming source are co-used  \n[SPARK-41339][SQL] Close and recreate RocksDB write batch instead of just clearing  \n[SPARK-41732][SQL][SS] Apply tree-pattern based pruning for the rule SessionWindowing  \n[SPARK-38277][SS] Clear write batch after RocksDB state store\u2019s commit  \nOperating system security updates.  \nNovember 29, 2022  \nUsers can configure leading and trailing whitespaces\u2019 behavior when writing data using the Redshift connector. The following options have been added to control whitespace handling:  \ncsvignoreleadingwhitespace, when set to true, removes leading whitespace from values during writes when tempformat is set to CSV or CSV GZIP. Whitespaces are retained when the config is set to false. By default, the value is true."
    },
    {
        "id": 1546,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "csvignoretrailingwhitespace, when set to true, removes trailing whitespace from values during writes when tempformat is set to CSV or CSV GZIP. Whitespaces are retained when the config is set to false. By default, the value is true.  \nFixed a bug with JSON parsing in Auto Loader when all columns were left as strings (cloudFiles.inferColumnTypes was not set or set to false) and the JSON contained nested objects.  \n[SPARK-39650][SS] Fix incorrect value schema in streaming deduplication with backward compatibility  \nOperating system security updates.  \nNovember 15, 2022  \n[SPARK-40646] JSON parsing for structs, maps, and arrays has been fixed so when a part of a record does not match the schema, the rest of record can still be parsed correctly instead of returning nulls.To opt-in for the improved behavior, set spark.sql.json.enablePartialResults to true. The flag is disabled by default to preserve the original behavior  \nOperating system security updates.  \nNovember 1, 2022  \nUpgraded Apache commons-text to 1.10.0.  \nFixed an issue where if a Delta table had a user-defined column named _change_type, but Change data feed was disabled on that table, data in that column would incorrectly fill with NULL values when running MERGE.  \nFixed an issue where running MERGE and using exactly 99 columns from the source in the condition could result in java.lang.ClassCastException: org.apache.spark.sql.vectorized.ColumnarBatch cannot be cast to org.apache.spark.sql.catalyst.InternalRow.  \nFixed an issue with Auto Loader where a file can be duplicated in the same micro-batch when allowOverwrites is enabled  \n[SPARK-40697][SQL] Add read-side char padding to cover external data files  \n[SPARK-40596][CORE] Populate ExecutorDecommission with messages in ExecutorDecommissionInfo  \nOperating system security updates.  \nOctober 18, 2022  \nFixed an issue with COPY INTO usage with temporary credentials on Unity Catalog enabled clusters / warehouses.  \n[SPARK-40213][SQL] Support ASCII value conversion for Latin-1 characters  \nOperating system security updates.  \nOctober 5, 2022"
    },
    {
        "id": 1547,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-40213][SQL] Support ASCII value conversion for Latin-1 characters  \nOperating system security updates.  \nOctober 5, 2022  \nUsers can set spark.conf.set(\u201cspark.databricks.io.listKeysWithPrefix.azure.enabled\u201d, \u201ctrue\u201d) to re-enable native listing for Auto Loader on ADLS Gen2. Native listing was previously turned off due to performance issues, but may have led to an increase in storage costs for customers. This change was rolled out to DBR 10.4 and 9.1 in the previous maintenance update.  \n[SPARK-40169] Don\u2019t pushdown Parquet filters with no reference to data schema  \n[SPARK-40460][SS] Fix streaming metrics when selecting _metadata  \n[SPARK-40468][SQL] Fix column pruning in CSV when corruptrecord is selected  \n[SPARK-40055][SQL] listCatalogs should also return spark_catalog even when spark_catalog implementation is defaultSessionCatalog  \nOperating system security updates.  \nSeptember 22, 2022  \n[SPARK-40315][SQL] Add hashCode() for Literal of ArrayBasedMapData  \n[SPARK-40380][SQL] Fix constant-folding of InvokeLike to avoid non-serializable literal embedded in the plan  \n[SPARK-40089][SQL] Fix sorting for some Decimal types  \n[SPARK-39887][SQL] RemoveRedundantAliases should keep aliases that make the output of projection nodes unique  \n[SPARK-40152][SQL] Fix split_part codegen compilation issue  \nSeptember 6, 2022  \nWe have updated the permission model in Table Access Controls (Table ACLs) so that only MODIFY permissions are needed to change a table\u2019s schema or table properties with ALTER TABLE. Previously, these operations required a user to own the table. Ownership is still required to grant permissions on a table, change its owner, change its location, or rename it. This change makes the permission model for Table ACLs more consistent with Unity Catalog."
    },
    {
        "id": 1548,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-40235][CORE] Use interruptible lock instead of synchronized in Executor.updateDependencies()  \n[SPARK-40212][SQL] SparkSQL castPartValue does not properly handle byte, short, or float  \n[SPARK-40218][SQL] GROUPING SETS should preserve the grouping columns  \n[SPARK-39976][SQL] ArrayIntersect should handle null in left expression correctly  \n[SPARK-40053][CORE][SQL][TESTS] Add assume to dynamic cancel cases which requiring Python runtime environment  \n[SPARK-35542][CORE][ML] Fix: Bucketizer created for multiple columns with parameters splitsArray, inputCols and outputCols can not be loaded after saving it  \n[SPARK-40079][CORE] Add Imputer inputCols validation for empty input case  \nAugust 24, 2022  \nShares, providers, and recipients now support SQL commands to change owners, comment, rename  \n[SPARK-39983][CORE][SQL] Do not cache unserialized broadcast relations on the driver  \n[SPARK-39912][SPARK-39828][SQL] Refine CatalogImpl  \n[SPARK-39775][CORE][AVRO] Disable validate default values when parsing Avro schemas  \n[SPARK-39806] Fixed the issue on queries accessing METADATA struct crash on partitioned tables  \n[SPARK-39867][SQL] Global limit should not inherit OrderPreservingUnaryNode  \n[SPARK-39962][PYTHON][SQL] Apply projection when group attributes are empty  \n[SPARK-39839][SQL] Handle special case of null variable-length Decimal with non-zero offsetAndSize in UnsafeRow structural integrity check  \n[SPARK-39713][SQL] ANSI mode: add suggestion of using try_element_at for INVALID_ARRAY_INDEX error  \n[SPARK-39847][SS] Fix race condition in RocksDBLoader.loadLibrary() if caller thread is interrupted"
    },
    {
        "id": 1549,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-39847][SS] Fix race condition in RocksDBLoader.loadLibrary() if caller thread is interrupted  \n[SPARK-39731][SQL] Fix issue in CSV and JSON data sources when parsing dates in \u201cyyyyMMdd\u201d format with CORRECTED time parser policy  \nOperating system security updates.  \nAugust 10, 2022  \nFor Delta tables with table access control, automatic schema evolution through DML statements such as INSERT and MERGE is now available for all users who have MODIFY permissions on such tables. Additionally, permissions required to perform schema evolution with COPY INTO are now lowered from OWNER to MODIFY for consistency with other commands. These changes make the table ACL security model more consistent with the Unity Catalog security model as well as with other operations such as replacing a table.  \n[SPARK-39889] Enhance the error message of division by 0  \n[SPARK-39795] [SQL] New SQL function: try_to_timestamp  \n[SPARK-39749] Always use plain string representation on casting decimal as string under ANSI mode  \n[SPARK-39625] Rename df.as to df.to  \n[SPARK-39787] [SQL] Use error class in the parsing error of function to_timestamp  \n[SPARK-39625] [SQL] Add Dataset.as(StructType)  \n[SPARK-39689] Support 2-chars lineSep in CSV datasource  \n[SPARK-39579] [SQL][PYTHON][R] Make ListFunctions/getFunction/functionExists compatible with 3 layer namespace  \n[SPARK-39702] [CORE] Reduce memory overhead of TransportCipher$EncryptedMessage by using a shared byteRawChannel  \n[SPARK-39575] [AVRO] add ByteBuffer#rewind after ByteBuffer#get in AvroDeserializer  \n[SPARK-39265] [SQL] Fix test failure when SPARK_ANSI_SQL_MODE is enabled  \n[SPARK-39441] [SQL] Speed up DeduplicateRelations"
    },
    {
        "id": 1550,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-39265] [SQL] Fix test failure when SPARK_ANSI_SQL_MODE is enabled  \n[SPARK-39441] [SQL] Speed up DeduplicateRelations  \n[SPARK-39497] [SQL] Improve the analysis exception of missing map key column  \n[SPARK-39476] [SQL] Disable Unwrap cast optimize when casting from Long to Float/ Double or from Integer to Float  \n[SPARK-39434] [SQL] Provide runtime error query context when array index is out of bounding  \nDatabricks Runtime 11.0 (EoS)  \nSee Databricks Runtime 11.0 (EoS).  \nNovember 29, 2022  \nUsers can configure leading and trailing whitespaces\u2019 behavior when writing data using the Redshift connector. The following options have been added to control whitespace handling:  \ncsvignoreleadingwhitespace, when set to true, removes leading whitespace from values during writes when tempformat is set to CSV or CSV GZIP. Whitespaces are retained when the config is set to false. By default, the value is true.  \ncsvignoretrailingwhitespace, when set to true, removes trailing whitespace from values during writes when tempformat is set to CSV or CSV GZIP. Whitespaces are retained when the config is set to false. By default, the value is true.  \nFixed a bug with JSON parsing in Auto Loader when all columns were left as strings (cloudFiles.inferColumnTypes was not set or set to false) and the JSON contained nested objects.  \n[SPARK-39650][SS] Fix incorrect value schema in streaming deduplication with backward compatibility  \nOperating system security updates.  \nNovember 15, 2022  \n[SPARK-40646] JSON parsing for structs, maps, and arrays has been fixed so when a part of a record does not match the schema, the rest of the record can still be parsed correctly instead of returning nulls. To opt-in for the improved behavior, set spark.sql.json.enablePartialResults to true. The flag is disabled by default to preserve the original behavior.  \nNovember 1, 2022"
    },
    {
        "id": 1551,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "November 1, 2022  \nUpgraded Apache commons-text to 1.10.0.  \nFixed an issue where if a Delta table had a user-defined column named _change_type, but Change data feed was disabled on that table, data in that column would incorrectly fill with NULL values when running MERGE.  \nFixed an issue with Auto Loader where a file can be duplicated in the same micro-batch when allowOverwrites is enabled  \n[SPARK-40697][SQL] Add read-side char padding to cover external data files  \n[SPARK-40596][CORE] Populate ExecutorDecommission with messages in ExecutorDecommissionInfo  \nOperating system security updates.  \nOctober 18, 2022  \n[SPARK-40213][SQL] Support ASCII value conversion for Latin-1 characters  \nOperating system security updates.  \nOctober 5, 2022  \nUsers can set spark.conf.set(\u201cspark.databricks.io.listKeysWithPrefix.azure.enabled\u201d, \u201ctrue\u201d) to re-enable native listing for Auto Loader on ADLS Gen2. Native listing was previously turned off due to performance issues, but may have led to an increase in storage costs for customers. This change was rolled out to DBR 10.4 and 9.1 in the previous maintenance update.  \n[SPARK-40169] Don\u2019t pushdown Parquet filters with no reference to data schema  \n[SPARK-40460][SS] Fix streaming metrics when selecting _metadata  \n[SPARK-40468][SQL] Fix column pruning in CSV when corruptrecord is selected  \nOperating system security updates.  \nSeptember 22, 2022  \n[SPARK-40315][SQL] Add hashCode() for Literal of ArrayBasedMapData  \n[SPARK-40380][SQL] Fix constant-folding of InvokeLike to avoid non-serializable literal embedded in the plan  \n[SPARK-40089][SQL] Fix sorting for some Decimal types  \n[SPARK-39887][SQL] RemoveRedundantAliases should keep aliases that make the output of projection nodes unique  \n[SPARK-40152][SQL] Fix split_part codegen compilation issue  \nSeptember 6, 2022"
    },
    {
        "id": 1552,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-40152][SQL] Fix split_part codegen compilation issue  \nSeptember 6, 2022  \n[SPARK-40235][CORE] Use interruptible lock instead of synchronized in Executor.updateDependencies()  \n[SPARK-40212][SQL] SparkSQL castPartValue does not properly handle byte, short, or float  \n[SPARK-40218][SQL] GROUPING SETS should preserve the grouping columns  \n[SPARK-39976][SQL] ArrayIntersect should handle null in left expression correctly  \n[SPARK-40053][CORE][SQL][TESTS] Add assume to dynamic cancel cases which requiring Python runtime environment  \n[SPARK-35542][CORE][ML] Fix: Bucketizer created for multiple columns with parameters splitsArray, inputCols and outputCols can not be loaded after saving it  \n[SPARK-40079][CORE] Add Imputer inputCols validation for empty input case  \nAugust 24, 2022  \n[SPARK-39983][CORE][SQL] Do not cache unserialized broadcast relations on the driver  \n[SPARK-39775][CORE][AVRO] Disable validate default values when parsing Avro schemas  \n[SPARK-39806] Fixed the issue on queries accessing METADATA struct crash on partitioned tables  \n[SPARK-39867][SQL] Global limit should not inherit OrderPreservingUnaryNode  \n[SPARK-39962][PYTHON][SQL] Apply projection when group attributes are empty  \nOperating system security updates.  \nAugust 9, 2022  \n[SPARK-39713][SQL] ANSI mode: add suggestion of using try_element_at for INVALID_ARRAY_INDEX error  \n[SPARK-39847] Fix race condition in RocksDBLoader.loadLibrary() if caller thread is interrupted  \n[SPARK-39731][SQL] Fix issue in CSV and JSON data sources when parsing dates in \u201cyyyyMMdd\u201d format with CORRECTED time parser policy  \n[SPARK-39889] Enhance the error message of division by 0"
    },
    {
        "id": 1553,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-39889] Enhance the error message of division by 0  \n[SPARK-39795][SQL] New SQL function: try_to_timestamp  \n[SPARK-39749] Always use plain string representation on casting decimal as string under ANSI mode  \n[SPARK-39625][SQL] Add Dataset.to(StructType)  \n[SPARK-39787][SQL] Use error class in the parsing error of function to_timestamp  \nOperating system security updates.  \nJuly 27, 2022  \n[SPARK-39689]Support 2-chars lineSep in CSV datasource  \n[SPARK-39104][SQL] InMemoryRelation#isCachedColumnBuffersLoaded should be thread-safe  \n[SPARK-39702][CORE] Reduce memory overhead of TransportCipher$EncryptedMessage by using a shared byteRawChannel  \n[SPARK-39575][AVRO] add ByteBuffer#rewind after ByteBuffer#get in AvroDeserializer  \n[SPARK-39497][SQL] Improve the analysis exception of missing map key column  \n[SPARK-39441][SQL] Speed up DeduplicateRelations  \n[SPARK-39476][SQL] Disable Unwrap cast optimize when casting from Long to Float/ Double or from Integer to Float  \n[SPARK-39434][SQL] Provide runtime error query context when array index is out of bounding  \n[SPARK-39570][SQL] Inline table should allow expressions with alias  \nOperating system security updates.  \nJuly 13, 2022  \nMake Delta MERGE operation results consistent when source is non-deterministic.  \nFixed an issue for the cloud_files_state TVF when running on non-DBFS paths.  \nDisabled Auto Loader\u2019s use of native cloud APIs for directory listing on Azure.  \n[SPARK-38796][SQL] Update to_number and try_to_number functions to allow PR with positive numbers  \n[SPARK-39272][SQL] Increase the start position of query context by 1"
    },
    {
        "id": 1554,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-39272][SQL] Increase the start position of query context by 1  \n[SPARK-39419][SQL] Fix ArraySort to throw an exception when the comparator returns null  \nOperating system security updates.  \nJuly 5, 2022  \nImprovement on error messages for a range of error classes.  \n[SPARK-39451][SQL] Support casting intervals to integrals in ANSI mode  \n[SPARK-39361] Don\u2019t use Log4J2\u2019s extended throwable conversion pattern in default logging configurations  \n[SPARK-39354][SQL] Ensure show Table or view not found even if there are dataTypeMismatchError related to Filter at the same time  \n[SPARK-38675][CORE] Fix race during unlock in BlockInfoManager  \n[SPARK-39392][SQL] Refine ANSI error messages for try_* function hints  \n[SPARK-39214][SQL][3.3] Improve errors related to CAST  \n[SPARK-37939][SQL] Use error classes in the parsing errors of properties  \n[SPARK-39085][SQL] Move the error message of INCONSISTENT_BEHAVIOR_CROSS_VERSION to error-classes.json  \n[SPARK-39376][SQL] Hide duplicated columns in star expansion of subquery alias from NATURAL/USING JOIN  \n[SPARK-39283][CORE] Fix deadlock between TaskMemoryManager and UnsafeExternalSorter.SpillableIterator  \n[SPARK-39285][SQL] Spark should not check field names when reading files  \nOperating system security updates.  \nDatabricks Runtime 10.5 (EoS)  \nSee Databricks Runtime 10.5 (EoS).  \nNovember 1, 2022  \nFixed an issue where if a Delta table had a user-defined column named _change_type, but Change data feed was disabled on that table, data in that column would incorrectly fill with NULL values when running MERGE.  \n[SPARK-40697][SQL] Add read-side char padding to cover external data files  \n[SPARK-40596][CORE] Populate ExecutorDecommission with messages in ExecutorDecommissionInfo"
    },
    {
        "id": 1555,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-40697][SQL] Add read-side char padding to cover external data files  \n[SPARK-40596][CORE] Populate ExecutorDecommission with messages in ExecutorDecommissionInfo  \nOperating system security updates.  \nOctober 18, 2022  \nOperating system security updates.  \nOctober 5, 2022  \nUsers can set spark.conf.set(\u201cspark.databricks.io.listKeysWithPrefix.azure.enabled\u201d, \u201ctrue\u201d) to re-enable native listing for Auto Loader on ADLS Gen2. Native listing was previously turned off due to performance issues, but may have led to an increase in storage costs for customers. This change was rolled out to DBR 10.4 and 9.1 in the previous maintenance update.  \nreload4j has been upgraded to 1.2.19 to fix vulnerabilities.  \n[SPARK-40460][SS] Fix streaming metrics when selecting _metadata  \n[SPARK-40468][SQL] Fix column pruning in CSV when corruptrecord is selected  \nOperating system security updates.  \nSeptember 22, 2022  \n[SPARK-40315][SQL] Add hashCode() for Literal of ArrayBasedMapData  \n[SPARK-40213][SQL] Support ASCII value conversion for Latin-1 characters  \n[SPARK-40380][SQL] Fix constant-folding of InvokeLike to avoid non-serializable literal embedded in the plan  \n[SPARK-38404][SQL] Improve CTE resolution when a nested CTE references an outer CTE  \n[SPARK-40089][SQL] Fix sorting for some Decimal types  \n[SPARK-39887][SQL] RemoveRedundantAliases should keep aliases that make the output of projection nodes unique  \nOperating system security updates.  \nSeptember 6, 2022  \n[SPARK-40235][CORE] Use interruptible lock instead of synchronized in Executor.updateDependencies()  \n[SPARK-39976][SQL] ArrayIntersect should handle null in left expression correctly  \n[SPARK-40053][CORE][SQL][TESTS] Add assume to dynamic cancel cases which requiring Python runtime environment"
    },
    {
        "id": 1556,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-40053][CORE][SQL][TESTS] Add assume to dynamic cancel cases which requiring Python runtime environment  \n[SPARK-35542][CORE][ML] Fix: Bucketizer created for multiple columns with parameters splitsArray, inputCols and outputCols can not be loaded after saving it  \n[SPARK-40079][CORE] Add Imputer inputCols validation for empty input case  \nAugust 24, 2022  \n[SPARK-39983][CORE][SQL] Do not cache unserialized broadcast relations on the driver  \n[SPARK-39775][CORE][AVRO] Disable validate default values when parsing Avro schemas  \n[SPARK-39806] Fixed the issue on queries accessing METADATA struct crash on partitioned tables  \n[SPARK-39962][PYTHON][SQL] Apply projection when group attributes are empty  \n[SPARK-37643][SQL] when charVarcharAsString is true, for char datatype predicate query should skip rpadding rule  \nOperating system security updates.  \nAugust 9, 2022  \n[SPARK-39847] Fix race condition in RocksDBLoader.loadLibrary() if caller thread is interrupted  \n[SPARK-39731][SQL] Fix issue in CSV and JSON data sources when parsing dates in \u201cyyyyMMdd\u201d format with CORRECTED time parser policy  \nOperating system security updates.  \nJuly 27, 2022  \n[SPARK-39625][SQL] Add Dataset.as(StructType)  \n[SPARK-39689]Support 2-chars lineSep in CSV datasource  \n[SPARK-39104][SQL] InMemoryRelation#isCachedColumnBuffersLoaded should be thread-safe  \n[SPARK-39570][SQL] Inline table should allow expressions with alias  \n[SPARK-39702][CORE] Reduce memory overhead of TransportCipher$EncryptedMessage by using a shared byteRawChannel  \n[SPARK-39575][AVRO] add ByteBuffer#rewind after ByteBuffer#get in AvroDeserializer"
    },
    {
        "id": 1557,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-39575][AVRO] add ByteBuffer#rewind after ByteBuffer#get in AvroDeserializer  \n[SPARK-39476][SQL] Disable Unwrap cast optimize when casting from Long to Float/ Double or from Integer to Float  \nOperating system security updates.  \nJuly 13, 2022  \nMake Delta MERGE operation results consistent when source is non-deterministic.  \n[SPARK-39355][SQL] Single column uses quoted to construct UnresolvedAttribute  \n[SPARK-39548][SQL] CreateView Command with a window clause query hit a wrong window definition not found issue  \n[SPARK-39419][SQL] Fix ArraySort to throw an exception when the comparator returns null  \nDisabled Auto Loader\u2019s use of native cloud APIs for directory listing on Azure.  \nOperating system security updates.  \nJuly 5, 2022  \n[SPARK-39376][SQL] Hide duplicated columns in star expansion of subquery alias from NATURAL/USING JOIN  \nOperating system security updates.  \nJune 15, 2022  \n[SPARK-39283][CORE] Fix deadlock between TaskMemoryManager and UnsafeExternalSorter.SpillableIterator  \n[SPARK-39285][SQL] Spark should not check field names when reading files  \n[SPARK-34096][SQL] Improve performance for nth_value ignore nulls over offset window  \n[SPARK-36718][SQL][FOLLOWUP] Fix the isExtractOnly check in CollapseProject  \nJune 2, 2022  \n[SPARK-39166][SQL] Provide runtime error query context for binary arithmetic when WSCG is off  \n[SPARK-39093][SQL] Avoid codegen compilation error when dividing year-month intervals or day-time intervals by an integral  \n[SPARK-38990][SQL] Avoid NullPointerException when evaluating date_trunc/trunc format as a bound reference  \nOperating system security updates.  \nMay 18, 2022  \nFixes a potential native memory leak in Auto Loader."
    },
    {
        "id": 1558,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \nMay 18, 2022  \nFixes a potential native memory leak in Auto Loader.  \n[SPARK-38868][SQL]Don\u2019t propagate exceptions from filter predicate when optimizing outer joins  \n[SPARK-38796][SQL] Implement the to_number and try_to_number SQL functions according to a new specification  \n[SPARK-38918][SQL] Nested column pruning should filter out attributes that do not belong to the current relation  \n[SPARK-38929][SQL] Improve error messages for cast failures in ANSI  \n[SPARK-38926][SQL] Output types in error messages in SQL style  \n[SPARK-39084][PYSPARK] Fix df.rdd.isEmpty() by using TaskContext to stop iterator on task completion  \n[SPARK-32268][SQL] Add ColumnPruning in injectBloomFilter  \n[SPARK-38908][SQL] Provide query context in runtime error of Casting from String to Number/Date/Timestamp/Boolean  \n[SPARK-39046][SQL] Return an empty context string if TreeNode.origin is wrongly set  \n[SPARK-38974][SQL] Filter registered functions with a given database name in list functions  \n[SPARK-38762][SQL] Provide query context in Decimal overflow errors  \n[SPARK-38931][SS] Create root dfs directory for RocksDBFileManager with unknown number of keys on 1st checkpoint  \n[SPARK-38992][CORE] Avoid using bash -c in ShellBasedGroupsMappingProvider  \n[SPARK-38716][SQL] Provide query context in map key not exists error  \n[SPARK-38889][SQL] Compile boolean column filters to use the bit type for MSSQL data source  \n[SPARK-38698][SQL] Provide query context in runtime error of Divide/Div/Reminder/Pmod  \n[SPARK-38823][SQL] Make NewInstance non-foldable to fix aggregation buffer corruption issue"
    },
    {
        "id": 1559,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-38823][SQL] Make NewInstance non-foldable to fix aggregation buffer corruption issue  \n[SPARK-38809][SS] Implement option to skip null values in symmetric hash implementation of stream-stream joins  \n[SPARK-38676][SQL] Provide SQL query context in runtime error message of Add/Subtract/Multiply  \n[SPARK-38677][PYSPARK] Python MonitorThread should detect deadlock due to blocking I/O  \nOperating system security updates.  \nDatabricks Runtime 10.3 (EoS)  \nSee Databricks Runtime 10.3 (EoS).  \nJuly 27, 2022  \n[SPARK-39689]Support 2-chars lineSep in CSV datasource  \n[SPARK-39104][SQL] InMemoryRelation#isCachedColumnBuffersLoaded should be thread-safe  \n[SPARK-39702][CORE] Reduce memory overhead of TransportCipher$EncryptedMessage by using a shared byteRawChannel  \nOperating system security updates.  \nJuly 20, 2022  \nMake Delta MERGE operation results consistent when source is non-deterministic.  \n[SPARK-39476][SQL] Disable Unwrap cast optimize when casting from Long to Float/ Double or from Integer to Float  \n[SPARK-39548][SQL] CreateView Command with a window clause query hit a wrong window definition not found issue  \n[SPARK-39419][SQL] Fix ArraySort to throw an exception when the comparator returns null  \nOperating system security updates.  \nJuly 5, 2022  \n[SPARK-39376][SQL] Hide duplicated columns in star expansion of subquery alias from NATURAL/USING JOIN  \nOperating system security updates.  \nJune 15, 2022  \n[SPARK-39283][CORE] Fix deadlock between TaskMemoryManager and UnsafeExternalSorter.SpillableIterator  \n[SPARK-39285][SQL] Spark should not check field names when reading files  \n[SPARK-34096][SQL] Improve performance for nth_value ignore nulls over offset window"
    },
    {
        "id": 1560,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-39285][SQL] Spark should not check field names when reading files  \n[SPARK-34096][SQL] Improve performance for nth_value ignore nulls over offset window  \n[SPARK-36718][SQL][FOLLOWUP] Fix the isExtractOnly check in CollapseProject  \nJune 2, 2022  \n[SPARK-38990][SQL] Avoid NullPointerException when evaluating date_trunc/trunc format as a bound reference  \nOperating system security updates.  \nMay 18, 2022  \nFixes a potential native memory leak in Auto Loader.  \n[SPARK-38918][SQL] Nested column pruning should filter out attributes that do not belong to the current relation  \n[SPARK-37593][CORE] Reduce default page size by LONG_ARRAY_OFFSET if G1GC and ON_HEAP are used  \n[SPARK-39084][PYSPARK] Fix df.rdd.isEmpty() by using TaskContext to stop iterator on task completion  \n[SPARK-32268][SQL] Add ColumnPruning in injectBloomFilter  \n[SPARK-38974][SQL] Filter registered functions with a given database name in list functions  \n[SPARK-38889][SQL] Compile boolean column filters to use the bit type for MSSQL data source  \nOperating system security updates.  \nMay 4, 2022  \nUpgraded Java AWS SDK from version 1.11.655 to 1.12.1899.  \nApril 19, 2022  \n[SPARK-38616][SQL] Keep track of SQL query text in Catalyst TreeNode  \nOperating system security updates.  \nApril 6, 2022  \n[SPARK-38631][CORE] Uses Java-based implementation for un-tarring at Utils.unpack  \nOperating system security updates.  \nMarch 22, 2022  \nChanged the current working directory of notebooks on High Concurrency clusters with either table access control or credential passthrough enabled to the user\u2019s home directory. Previously, the working directory was /databricks/driver.  \n[SPARK-38437][SQL] Lenient serialization of datetime from datasource"
    },
    {
        "id": 1561,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-38437][SQL] Lenient serialization of datetime from datasource  \n[SPARK-38180][SQL] Allow safe up-cast expressions in correlated equality predicates  \n[SPARK-38155][SQL] Disallow distinct aggregate in lateral subqueries with unsupported predicates  \n[SPARK-38325][SQL] ANSI mode: avoid potential runtime error in HashJoin.extractKeyExprAt()  \nMarch 14, 2022  \nImproved transaction conflict detection for empty transactions in Delta Lake.  \n[SPARK-38185][SQL] Fix data incorrect if aggregate function is empty  \n[SPARK-38318][SQL] regression when replacing a dataset view  \n[SPARK-38236][SQL] Absolute file paths specified in create/alter table are treated as relative  \n[SPARK-35937][SQL] Extracting date field from timestamp should work in ANSI mode  \n[SPARK-34069][SQL] Kill barrier tasks should respect SPARK_JOB_INTERRUPT_ON_CANCEL  \n[SPARK-37707][SQL] Allow store assignment between TimestampNTZ and Date/Timestamp  \nFebruary 23, 2022  \n[SPARK-27442][SQL] Remove check field name when reading/writing data in parquet  \nDatabricks Runtime 10.2 (EoS)  \nSee Databricks Runtime 10.2 (EoS).  \nJune 15, 2022  \n[SPARK-39283][CORE] Fix deadlock between TaskMemoryManager and UnsafeExternalSorter.SpillableIterator  \n[SPARK-39285][SQL] Spark should not check field names when reading files  \n[SPARK-34096][SQL] Improve performance for nth_value ignore nulls over offset window  \nJune 2, 2022  \n[SPARK-38918][SQL] Nested column pruning should filter out attributes that do not belong to the current relation  \n[SPARK-38990][SQL] Avoid NullPointerException when evaluating date_trunc/trunc format as a bound reference  \nOperating system security updates.  \nMay 18, 2022"
    },
    {
        "id": 1562,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-38990][SQL] Avoid NullPointerException when evaluating date_trunc/trunc format as a bound reference  \nOperating system security updates.  \nMay 18, 2022  \nFixes a potential native memory leak in Auto Loader.  \n[SPARK-39084][PYSPARK] Fix df.rdd.isEmpty() by using TaskContext to stop iterator on task completion  \n[SPARK-38889][SQL] Compile boolean column filters to use the bit type for MSSQL data source  \n[SPARK-38931][SS] Create root dfs directory for RocksDBFileManager with unknown number of keys on 1st checkpoint  \nOperating system security updates.  \nMay 4, 2022  \nUpgraded Java AWS SDK from version 1.11.655 to 1.12.1899.  \nApril 19, 2022  \nOperating system security updates.  \nMiscellaneous bug fixes.  \nApril 6, 2022  \n[SPARK-38631][CORE] Uses Java-based implementation for un-tarring at Utils.unpack  \nOperating system security updates.  \nMarch 22, 2022  \nChanged the current working directory of notebooks on High Concurrency clusters with either table access control or credential passthrough enabled to the user\u2019s home directory. Previously, the working directory was /databricks/driver.  \n[SPARK-38437][SQL] Lenient serialization of datetime from datasource  \n[SPARK-38180][SQL] Allow safe up-cast expressions in correlated equality predicates  \n[SPARK-38155][SQL] Disallow distinct aggregate in lateral subqueries with unsupported predicates  \n[SPARK-38325][SQL] ANSI mode: avoid potential runtime error in HashJoin.extractKeyExprAt()  \nMarch 14, 2022  \nImproved transaction conflict detection for empty transactions in Delta Lake.  \n[SPARK-38185][SQL] Fix data incorrect if aggregate function is empty  \n[SPARK-38318][SQL] regression when replacing a dataset view  \n[SPARK-38236][SQL] Absolute file paths specified in create/alter table are treated as relative"
    },
    {
        "id": 1563,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-38318][SQL] regression when replacing a dataset view  \n[SPARK-38236][SQL] Absolute file paths specified in create/alter table are treated as relative  \n[SPARK-35937][SQL] Extracting date field from timestamp should work in ANSI mode  \n[SPARK-34069][SQL] Kill barrier tasks should respect SPARK_JOB_INTERRUPT_ON_CANCEL  \n[SPARK-37707][SQL] Allow store assignment between TimestampNTZ and Date/Timestamp  \nFebruary 23, 2022  \n[SPARK-37577][SQL] Fix ClassCastException: ArrayType cannot be cast to StructType for Generate Pruning  \nFebruary 8, 2022  \n[SPARK-27442][SQL] Remove check field name when reading/writing data in parquet.  \nOperating system security updates.  \nFebruary 1, 2022  \nOperating system security updates.  \nJanuary 26, 2022  \nFixed a bug where concurrent transactions on Delta tables could commit in a non-serializable order under certain rare conditions.  \nFixed a bug where the OPTIMIZE command could fail when the ANSI SQL dialect was enabled.  \nJanuary 19, 2022  \nIntroduced support for inlining temporary credentials to COPY INTO for loading the source data without requiring SQL ANY_FILE permissions  \nBug fixes and security enhancements.  \nDecember 20, 2021  \nFixed a rare bug with Parquet column index based filtering.  \nDatabricks Runtime 10.1 (EoS)  \nSee Databricks Runtime 10.1 (EoS).  \nJune 15, 2022  \n[SPARK-39283][CORE] Fix deadlock between TaskMemoryManager and UnsafeExternalSorter.SpillableIterator  \n[SPARK-39285][SQL] Spark should not check field names when reading files  \n[SPARK-34096][SQL] Improve performance for nth_value ignore nulls over offset window  \nJune 2, 2022  \nOperating system security updates.  \nMay 18, 2022  \nFixes a potential native memory leak in Auto Loader.  \n[SPARK-39084][PYSPARK] Fix df.rdd.isEmpty() by using TaskContext to stop iterator on task completion"
    },
    {
        "id": 1564,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fixes a potential native memory leak in Auto Loader.  \n[SPARK-39084][PYSPARK] Fix df.rdd.isEmpty() by using TaskContext to stop iterator on task completion  \n[SPARK-38889][SQL] Compile boolean column filters to use the bit type for MSSQL data source  \nOperating system security updates.  \nApril 19, 2022  \n[SPARK-37270][SQL] Fix push foldable into CaseWhen branches if elseValue is empty  \nOperating system security updates.  \nApril 6, 2022  \n[SPARK-38631][CORE] Uses Java-based implementation for un-tarring at Utils.unpack  \nOperating system security updates.  \nMarch 22, 2022  \n[SPARK-38437][SQL] Lenient serialization of datetime from datasource  \n[SPARK-38180][SQL] Allow safe up-cast expressions in correlated equality predicates  \n[SPARK-38155][SQL] Disallow distinct aggregate in lateral subqueries with unsupported predicates  \n[SPARK-38325][SQL] ANSI mode: avoid potential runtime error in HashJoin.extractKeyExprAt()  \nMarch 14, 2022  \nImproved transaction conflict detection for empty transactions in Delta Lake.  \n[SPARK-38185][SQL] Fix data incorrect if aggregate function is empty  \n[SPARK-38318][SQL] regression when replacing a dataset view  \n[SPARK-38236][SQL] Absolute file paths specified in create/alter table are treated as relative  \n[SPARK-35937][SQL] Extracting date field from timestamp should work in ANSI mode  \n[SPARK-34069][SQL] Kill barrier tasks should respect SPARK_JOB_INTERRUPT_ON_CANCEL  \n[SPARK-37707][SQL] Allow store assignment between TimestampNTZ and Date/Timestamp  \nFebruary 23, 2022  \n[SPARK-37577][SQL] Fix ClassCastException: ArrayType cannot be cast to StructType for Generate Pruning  \nFebruary 8, 2022"
    },
    {
        "id": 1565,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "February 23, 2022  \n[SPARK-37577][SQL] Fix ClassCastException: ArrayType cannot be cast to StructType for Generate Pruning  \nFebruary 8, 2022  \n[SPARK-27442][SQL] Remove check field name when reading/writing data in parquet.  \nOperating system security updates.  \nFebruary 1, 2022  \nOperating system security updates.  \nJanuary 26, 2022  \nFixed a bug where concurrent transactions on Delta tables could commit in a non-serializable order under certain rare conditions.  \nFixed a bug where the OPTIMIZE command could fail when the ANSI SQL dialect was enabled.  \nJanuary 19, 2022  \nIntroduced support for inlining temporary credentials to COPY INTO for loading the source data without requiring SQL ANY_FILE permissions  \nFixed an out of memory issue with query result caching under certain conditions.  \nFixed an issue with USE DATABASE when a user switches the current catalog to a non-default catalog.  \nBug fixes and security enhancements.  \nOperating system security updates.  \nDecember 20, 2021  \nFixed a rare bug with Parquet column index based filtering.  \nDatabricks Runtime 10.0 (EoS)  \nSee Databricks Runtime 10.0 (EoS).  \nApril 19, 2022  \n[SPARK-37270][SQL] Fix push foldable into CaseWhen branches if elseValue is empty  \nOperating system security updates.  \nApril 6, 2022  \n[SPARK-38631][CORE] Uses Java-based implementation for un-tarring at Utils.unpack  \nOperating system security updates.  \nMarch 22, 2022  \n[SPARK-38437][SQL] Lenient serialization of datetime from datasource  \n[SPARK-38180][SQL] Allow safe up-cast expressions in correlated equality predicates  \n[SPARK-38155][SQL] Disallow distinct aggregate in lateral subqueries with unsupported predicates  \n[SPARK-38325][SQL] ANSI mode: avoid potential runtime error in HashJoin.extractKeyExprAt()  \nMarch 14, 2022  \nImproved transaction conflict detection for empty transactions in Delta Lake.  \n[SPARK-38185][SQL] Fix data incorrect if aggregate function is empty"
    },
    {
        "id": 1566,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "March 14, 2022  \nImproved transaction conflict detection for empty transactions in Delta Lake.  \n[SPARK-38185][SQL] Fix data incorrect if aggregate function is empty  \n[SPARK-38318][SQL] regression when replacing a dataset view  \n[SPARK-38236][SQL] Absolute file paths specified in create/alter table are treated as relative  \n[SPARK-35937][SQL] Extracting date field from timestamp should work in ANSI mode  \n[SPARK-34069][SQL] Kill barrier tasks should respect SPARK_JOB_INTERRUPT_ON_CANCEL  \n[SPARK-37707][SQL] Allow store assignment between TimestampNTZ and Date/Timestamp  \nFebruary 23, 2022  \n[SPARK-37577][SQL] Fix ClassCastException: ArrayType cannot be cast to StructType for Generate Pruning  \nFebruary 8, 2022  \n[SPARK-27442][SQL] Remove check field name when reading/writing data in parquet.  \n[SPARK-36905][SQL] Fix reading hive views without explicit column names  \n[SPARK-37859][SQL] Fix issue that SQL tables created with JDBC with Spark 3.1 are not readable with 3.2  \nOperating system security updates.  \nFebruary 1, 2022  \nOperating system security updates.  \nJanuary 26, 2022  \nFixed a bug where concurrent transactions on Delta tables could commit in a non-serializable order under certain rare conditions.  \nFixed a bug where the OPTIMIZE command could fail when the ANSI SQL dialect was enabled.  \nJanuary 19, 2022  \nBug fixes and security enhancements.  \nOperating system security updates.  \nDecember 20, 2021  \nFixed a rare bug with Parquet column index based filtering.  \nNovember 9, 2021  \nIntroduced additional configuration flags to enable fine grained control of ANSI behaviors.  \nNovember 4, 2021  \nFixed a bug that could cause Structured Streaming streams to fail with an ArrayIndexOutOfBoundsException  \nFixed a race condition that might cause a query failure with an IOException like java.io.IOException: No FileSystem for scheme or that might cause modifications to sparkContext.hadoopConfiguration to not take effect in queries."
    },
    {
        "id": 1567,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fixed a race condition that might cause a query failure with an IOException like java.io.IOException: No FileSystem for scheme or that might cause modifications to sparkContext.hadoopConfiguration to not take effect in queries.  \nThe Apache Spark Connector for Delta Sharing was upgraded to 0.2.0.  \nNovember 30, 2021  \nFixed an issue with timestamp parsing where a timezone string without a colon was considered invalid.  \nFixed an out of memory issue with query result caching under certain conditions.  \nFixed an issue with USE DATABASE when a user switches the current catalog to a non-default catalog.  \nDatabricks Runtime 9.0 (EoS)  \nSee Databricks Runtime 9.0 (EoS).  \nFebruary 8, 2022  \nOperating system security updates.  \nFebruary 1, 2022  \nOperating system security updates.  \nJanuary 26, 2022  \nFixed a bug where the OPTIMIZE command could fail when the ANSI SQL dialect was enabled.  \nJanuary 19, 2022  \nBug fixes and security enhancements.  \nOperating system security updates.  \nNovember 4, 2021  \nFixed a bug that could cause Structured Streaming streams to fail with an ArrayIndexOutOfBoundsException  \nFixed a race condition that might cause a query failure with an IOException like java.io.IOException: No FileSystem for scheme or that might cause modifications to sparkContext.hadoopConfiguration to not take effect in queries.  \nThe Apache Spark Connector for Delta Sharing was upgraded to 0.2.0.  \nSeptember 22, 2021  \nFixed a bug in cast Spark array with null to string  \nSeptember 15, 2021  \nFixed a race condition that might cause a query failure with an IOException like java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_x_piecey of broadcast_x.  \nSeptember 8, 2021  \nAdded support for schema name (databaseName.schemaName.tableName format) as the target table name for Azure Synapse Connector.  \nAdded geometry and geography JDBC types support for Spark SQL.  \n[SPARK-33527][SQL] Extended the function of decode to be consistent with mainstream databases."
    },
    {
        "id": 1568,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Added geometry and geography JDBC types support for Spark SQL.  \n[SPARK-33527][SQL] Extended the function of decode to be consistent with mainstream databases.  \n[SPARK-36532][CORE][3.1] Fixed deadlock in CoarseGrainedExecutorBackend.onDisconnected to avoid executorsconnected to prevent executor shutdown hang.  \nAugust 25, 2021  \nSQL Server driver library was upgraded to 9.2.1.jre8.  \nSnowflake connector was upgraded to 2.9.0.  \nFixed broken link to best trial notebook on AutoML experiment page.  \nDatabricks Runtime 8.4 (EoS)  \nSee Databricks Runtime 8.4 (EoS).  \nJanuary 19, 2022  \nOperating system security updates.  \nNovember 4, 2021  \nFixed a bug that could cause Structured Streaming streams to fail with an ArrayIndexOutOfBoundsException  \nFixed a race condition that might cause a query failure with an IOException like java.io.IOException: No FileSystem for scheme or that might cause modifications to sparkContext.hadoopConfiguration to not take effect in queries.  \nThe Apache Spark Connector for Delta Sharing was upgraded to 0.2.0.  \nSeptember 22, 2021  \nSpark JDBC driver was upgraded to 2.6.19.1030  \n[SPARK-36734][SQL] Upgrade ORC to 1.5.1  \nSeptember 15, 2021  \nFixed a race condition that might cause a query failure with an IOException like java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_x_piecey of broadcast_x.  \nOperating system security updates.  \nSeptember 8, 2021  \n[SPARK-36532][CORE][3.1] Fixed deadlock in CoarseGrainedExecutorBackend.onDisconnected to avoid executorsconnected to prevent executor shutdown hang.  \nAugust 25, 2021  \nSQL Server driver library was upgraded to 9.2.1.jre8.  \nSnowflake connector was upgraded to 2.9.0.  \nFixes a bug in credential passthrough caused by the new Parquet prefetch optimization, where user\u2019s passthrough credential might not be found during file access.  \nAugust 11, 2021"
    },
    {
        "id": 1569,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fixes a bug in credential passthrough caused by the new Parquet prefetch optimization, where user\u2019s passthrough credential might not be found during file access.  \nAugust 11, 2021  \nFixes a RocksDB incompatibility problem that prevents older Databricks Runtime 8.4. This fixes forward compatibility for Auto Loader, COPY INTO, and stateful streaming applications.  \nFixes a bug in Auto Loader with S3 paths when using Auto Loader without a path option.  \nFixes a bug that misconfigured AWS STS endpoints as Amazon Kinesis endpoints for the Kinesis source.  \nFixes a bug when using Auto Loader to read CSV files with mismatching header files. If column names do not match, the column would be filled in with nulls. Now, if a schema is provided, it assumes the schema is the same and will only save column mismatches if rescued data columns are enabled.  \nAdds a new option called externalDataSource into the Azure Synapse connector to remove the CONTROL permission requirement on the database for PolyBase reading.  \nJuly 29, 2021  \n[SPARK-36034][BUILD] Rebase datetime in pushed down filters to Parquet  \n[SPARK-36163][BUILD] Propagate correct JDBC properties in JDBC connector provider and add connectionProvider option  \nDatabricks Runtime 8.3 (EoS)  \nSee Databricks Runtime 8.3 (EoS).  \nJanuary 19, 2022  \nOperating system security updates.  \nNovember 4, 2021  \nFixed a bug that could cause Structured Streaming streams to fail with an ArrayIndexOutOfBoundsException  \nFixed a race condition that might cause a query failure with an IOException like java.io.IOException: No FileSystem for scheme or that might cause modifications to sparkContext.hadoopConfiguration to not take effect in queries.  \nSeptember 22, 2021  \nSpark JDBC driver was upgraded to 2.6.19.1030  \nSeptember 15, 2021  \nFixed a race condition that might cause a query failure with an IOException like java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_x_piecey of broadcast_x.  \nOperating system security updates.  \nSeptember 8, 2021"
    },
    {
        "id": 1570,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \nSeptember 8, 2021  \n[SPARK-35700][SQL][WARMFIX] Read char/varchar orc table when created and written by external systems.  \n[SPARK-36532][CORE][3.1] Fixed deadlock in CoarseGrainedExecutorBackend.onDisconnected to avoid executorsconnected to prevent executor shutdown hang.  \nAugust 25, 2021  \nSQL Server driver library was upgraded to 9.2.1.jre8.  \nSnowflake connector was upgraded to 2.9.0.  \nFixes a bug in credential passthrough caused by the new Parquet prefetch optimization, where user\u2019s passthrough credential might not be found during file access.  \nAugust 11, 2021  \nFixes a bug that misconfigured AWS STS endpoints as Amazon Kinesis endpoints for the Kinesis source.  \nFixes a bug when using Auto Loader to read CSV files with mismatching header files. If column names do not match, the column would be filled in with nulls. Now, if a schema is provided, it assumes the schema is the same and will only save column mismatches if rescued data columns are enabled.  \nJuly 29, 2021  \nUpgrade Databricks Snowflake Spark connector to 2.9.0-spark-3.1  \n[SPARK-36034][BUILD] Rebase datetime in pushed down filters to Parquet  \n[SPARK-36163][BUILD] Propagate correct JDBC properties in JDBC connector provider and add connectionProvider option  \nJuly 14, 2021  \nFixed an issue when using column names with dots in Azure Synapse connector.  \nIntroduced database.schema.table format for Synapse Connector.  \nAdded support to provide databaseName.schemaName.tableName format as the target table instead of only schemaName.tableName or tableName.  \nJune 15, 2021  \nFixed a NoSuchElementException bug in Delta Lake optimized writes that can happen when writing large amounts of data and encountering executor losses  \nAdds SQL CREATE GROUP, DROP GROUP, ALTER GROUP, SHOW GROUPS, and SHOW USERS commands. For details, see Security statements and Show statements.  \nDatabricks Runtime 8.2 (EoS)  \nSee Databricks Runtime 8.2 (EoS)."
    },
    {
        "id": 1571,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Databricks Runtime 8.2 (EoS)  \nSee Databricks Runtime 8.2 (EoS).  \nSeptember 22, 2021  \nOperating system security updates.  \nSeptember 15, 2021  \nFixed a race condition that might cause a query failure with an IOException like java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_x_piecey of broadcast_x.  \nSeptember 8, 2021  \n[SPARK-35700][SQL][WARMFIX] Read char/varchar orc table when created and written by external systems.  \n[SPARK-36532][CORE][3.1] Fixed deadlock in CoarseGrainedExecutorBackend.onDisconnected to avoid executorsconnected to prevent executor shutdown hang.  \nAugust 25, 2021  \nSnowflake connector was upgraded to 2.9.0.  \nAugust 11, 2021  \nFixes a bug that misconfigured AWS STS endpoints as Amazon Kinesis endpoints for the Kinesis source.  \n[SPARK-36034][SQL] Rebase datetime in pushed down filters to parquet.  \nJuly 29, 2021  \nUpgrade Databricks Snowflake Spark connector to 2.9.0-spark-3.1  \n[SPARK-36163][BUILD] Propagate correct JDBC properties in JDBC connector provider and add connectionProvider option  \nJuly 14, 2021  \nFixed an issue when using column names with dots in Azure Synapse connector.  \nIntroduced database.schema.table format for Synapse Connector.  \nAdded support to provide databaseName.schemaName.tableName format as the target table instead of only schemaName.tableName or tableName.  \nFixed a bug that prevents users from time traveling to older available versions with Delta tables.  \nJune 15, 2021  \nFixes a NoSuchElementException bug in Delta Lake optimized writes that can happen when writing large amounts of data and encountering executor losses  \nJune 7, 2021  \nDisable a list of pushdown predicates (StartsWith, EndsWith, Contains, Not(EqualTo()), and DataType) for AWS Glue Catalog since they are not supported in Glue yet.  \nMay 26, 2021  \nUpdated Python with security patch to fix Python security vulnerability (CVE-2021-3177)."
    },
    {
        "id": 1572,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "May 26, 2021  \nUpdated Python with security patch to fix Python security vulnerability (CVE-2021-3177).  \nApril 30, 2021  \nOperating system security updates.  \n[SPARK-35227][BUILD] Update the resolver for spark-packages in SparkSubmit  \n[SPARK-34245][CORE] Ensure Master removes executors that failed to send finished state  \nFixed an OOM issue when Auto Loader reports Structured Streaming progress metrics.  \nDatabricks Runtime 8.1 (EoS)  \nSee Databricks Runtime 8.1 (EoS).  \nSeptember 22, 2021  \nOperating system security updates.  \nSeptember 15, 2021  \nFixed a race condition that might cause a query failure with an IOException like java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_x_piecey of broadcast_x.  \nSeptember 8, 2021  \n[SPARK-35700][SQL][WARMFIX] Read char/varchar orc table when created and written by external systems.  \n[SPARK-36532][CORE][3.1] Fixed deadlock in CoarseGrainedExecutorBackend.onDisconnected to avoid executorsconnected to prevent executor shutdown hang.  \nAugust 25, 2021  \nSnowflake connector was upgraded to 2.9.0.  \nAugust 11, 2021  \nFixes a bug that misconfigured AWS STS endpoints as Amazon Kinesis endpoints for the Kinesis source.  \n[SPARK-36034][SQL] Rebase datetime in pushed down filters to parquet.  \nJuly 29, 2021  \nUpgrade Databricks Snowflake Spark connector to 2.9.0-spark-3.1  \n[SPARK-36163][BUILD] Propagate correct JDBC properties in JDBC connector provider and add connectionProvider option  \nJuly 14, 2021  \nFixed an issue when using column names with dots in Azure Synapse connector.  \nFixed a bug that prevents users from time traveling to older available versions with Delta tables.  \nJune 15, 2021  \nFixes a NoSuchElementException bug in Delta Lake optimized writes that can happen when writing large amounts of data and encountering executor losses  \nJune 7, 2021"
    },
    {
        "id": 1573,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "June 15, 2021  \nFixes a NoSuchElementException bug in Delta Lake optimized writes that can happen when writing large amounts of data and encountering executor losses  \nJune 7, 2021  \nDisable a list of pushdown predicates (StartsWith, EndsWith, Contains, Not(EqualTo()), and DataType) for AWS Glue Catalog since they are not supported in Glue yet.  \nMay 26, 2021  \nUpdated Python with security patch to fix Python security vulnerability (CVE-2021-3177).  \nApril 30, 2021  \nOperating system security updates.  \n[SPARK-35227][BUILD] Update the resolver for spark-packages in SparkSubmit  \nFixed an OOM issue when Auto Loader reports Structured Streaming progress metrics.  \nApril 27, 2021  \n[SPARK-34245][CORE] Ensure Master removes executors that failed to send finished state  \n[SPARK-34856][SQL] ANSI mode: Allow casting complex types as string type  \n[SPARK-35014] Fix the PhysicalAggregation pattern to not rewrite foldable expressions  \n[SPARK-34769][SQL] AnsiTypeCoercion: return narrowest convertible type among TypeCollection  \n[SPARK-34614][SQL] ANSI mode: Casting String to Boolean will throw exception on parse error  \n[SPARK-33794][SQL] ANSI mode: Fix NextDay expression to throw runtime IllegalArgumentException when receiving invalid input under  \nDatabricks Runtime 8.0 (EoS)  \nSee Databricks Runtime 8.0 (EoS).  \nSeptember 15, 2021  \nFixed a race condition that might cause a query failure with an IOException like java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_x_piecey of broadcast_x.  \nAugust 25, 2021  \nSnowflake connector was upgraded to 2.9.0.  \nAugust 11, 2021  \nFixes a bug that misconfigured AWS STS endpoints as Amazon Kinesis endpoints for the Kinesis source.  \n[SPARK-36034][SQL] Rebase datetime in pushed down filters to parquet.  \nJuly 29, 2021"
    },
    {
        "id": 1574,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-36034][SQL] Rebase datetime in pushed down filters to parquet.  \nJuly 29, 2021  \n[SPARK-36163][BUILD] Propagate correct JDBC properties in JDBC connector provider and add connectionProvider option  \nJuly 14, 2021  \nFixed an issue when using column names with dots in Azure Synapse connector.  \nFixed a bug that prevents users from time traveling to older available versions with Delta tables.  \nJune 7, 2021  \nDisable a list of pushdown predicates (StartsWith, EndsWith, Contains, Not(EqualTo()), and DataType) for AWS Glue Catalog since they are not supported in Glue yet.  \nMay 26, 2021  \nUpdated Python with security patch to fix Python security vulnerability (CVE-2021-3177).  \nApril 30, 2021  \nOperating system security updates.  \n[SPARK-35227][BUILD] Update the resolver for spark-packages in SparkSubmit  \n[SPARK-34245][CORE] Ensure Master removes executors that failed to send finished state  \nMarch 24, 2021  \n[SPARK-34681][SQL] Fix bug for full outer shuffled hash join when building left side with non-equal condition  \n[SPARK-34534] Fix blockIds order when use FetchShuffleBlocks to fetch blocks  \n[SPARK-34613][SQL] Fix view does not capture disable hint config  \nDisk caching is enabled by default on i3en instances.  \nMarch 9, 2021  \n[SPARK-34543][SQL] Respect the spark.sql.caseSensitive config while resolving partition spec in v1 SET LOCATION  \n[SPARK-34392][SQL] Support ZoneOffset +h:mm in DateTimeUtils. getZoneId  \n[UI] Fix the href link of Spark DAG Visualization  \n[SPARK-34436][SQL] DPP support LIKE ANY/ALL expression  \nDatabricks Runtime 7.6 (EoS)  \nSee Databricks Runtime 7.6 (EoS).  \nAugust 11, 2021  \nFixes a bug that misconfigured AWS STS endpoints as Amazon Kinesis endpoints for the Kinesis source."
    },
    {
        "id": 1575,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "See Databricks Runtime 7.6 (EoS).  \nAugust 11, 2021  \nFixes a bug that misconfigured AWS STS endpoints as Amazon Kinesis endpoints for the Kinesis source.  \n[SPARK-36034][SQL] Rebase datetime in pushed down filters to parquet.  \nJuly 29, 2021  \n[SPARK-32998][BUILD] Add ability to override default remote repos with internal repos only  \nJuly 14, 2021  \nFixed a bug that prevents users from time traveling to older available versions with Delta tables.  \nMay 26, 2021  \nUpdated Python with security patch to fix Python security vulnerability (CVE-2021-3177).  \nApril 30, 2021  \nOperating system security updates.  \n[SPARK-35227][BUILD] Update the resolver for spark-packages in SparkSubmit  \n[SPARK-34245][CORE] Ensure Master removes executors that failed to send finished state  \nMarch 24, 2021  \n[SPARK-34768][SQL] Respect the default input buffer size in Univocity  \n[SPARK-34534] Fix blockIds order when use FetchShuffleBlocks to fetch blocks  \nDisk caching is enabled by default on i3en instances.  \nMarch 9, 2021  \n(Azure only) Fixed an Auto Loader bug that can cause NullPointerException when using Databricks Runtime 7.6 to run an old Auto Loader stream created in Databricks Runtime 7.2  \n[UI] Fix the href link of Spark DAG Visualization  \nUnknown leaf-node SparkPlan is not handled correctly in SizeInBytesOnlyStatsSparkPlanVisitor  \nRestore the output schema of SHOW DATABASES  \n[Delta][8.0, 7.6] Fixed calculation bug in file size auto-tuning logic  \nDisable staleness check for Delta table files in disk cache  \n[SQL] Use correct dynamic pruning build key when range join hint is present  \nDisable char type support in non-SQL code path  \nAvoid NPE in DataFrameReader.schema  \nFix NPE when EventGridClient response has no entity  \nFix a read closed stream bug in Azure Auto Loader  \n[SQL] Do not generate shuffle partition number advice when AOS is enabled  \nFebruary 24, 2021"
    },
    {
        "id": 1576,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fix NPE when EventGridClient response has no entity  \nFix a read closed stream bug in Azure Auto Loader  \n[SQL] Do not generate shuffle partition number advice when AOS is enabled  \nFebruary 24, 2021  \nUpgraded the Spark BigQuery connector to v0.18, which introduces various bug fixes and support for Arrow and Avro iterators.  \nFixed a correctness issue that caused Spark to return incorrect results when the Parquet file\u2019s decimal precision and scale are different from the Spark schema.  \nFixed reading failure issue on Microsoft SQL Server tables that contain spatial data types, by adding geometry and geography JDBC types support for Spark SQL.  \nIntroduced a new configuration spark.databricks.hive.metastore.init.reloadFunctions.enabled. This configuration controls the built in Hive initialization. When set to true, Databricks reloads all functions from all databases that users have into FunctionRegistry. This is the default behavior in Hive Metastore. When set to false, Databricks disables this process for optimization.  \n[SPARK-34212] Fixed issues related to reading decimal data from Parquet files.  \n[SPARK-34260][SQL] Fix UnresolvedException when creating temp view twice.  \nDatabricks Runtime 7.5 (EoS)  \nSee Databricks Runtime 7.5 (EoS).  \nMay 26, 2021  \nUpdated Python with security patch to fix Python security vulnerability (CVE-2021-3177).  \nApril 30, 2021  \nOperating system security updates.  \n[SPARK-35227][BUILD] Update the resolver for spark-packages in SparkSubmit  \n[SPARK-34245][CORE] Ensure Master removes executors that failed to send finished state  \nMarch 24, 2021  \n[SPARK-34768][SQL] Respect the default input buffer size in Univocity  \n[SPARK-34534] Fix blockIds order when use FetchShuffleBlocks to fetch blocks  \nDisk caching is enabled by default on i3en instances.  \nMarch 9, 2021  \n(Azure only) Fixed an Auto Loader bug that can cause NullPointerException when using Databricks Runtime 7.5 to run an old Auto Loader stream created in Databricks Runtime 7.2."
    },
    {
        "id": 1577,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "(Azure only) Fixed an Auto Loader bug that can cause NullPointerException when using Databricks Runtime 7.5 to run an old Auto Loader stream created in Databricks Runtime 7.2.  \n[UI] Fix the href link of Spark DAG Visualization  \nUnknown leaf-node SparkPlan is not handled correctly in SizeInBytesOnlyStatsSparkPlanVisitor  \nRestore the output schema of SHOW DATABASES  \nDisable staleness check for Delta table files in disk cache  \n[SQL] Use correct dynamic pruning build key when range join hint is present  \nDisable char type support in non-SQL code path  \nAvoid NPE in DataFrameReader.schema  \nFix NPE when EventGridClient response has no entity  \nFix a read closed stream bug in Azure Auto Loader  \nFebruary 24, 2021  \nUpgraded the Spark BigQuery connector to v0.18, which introduces various bug fixes and support for Arrow and Avro iterators.  \nFixed a correctness issue that caused Spark to return incorrect results when the Parquet file\u2019s decimal precision and scale are different from the Spark schema.  \nFixed reading failure issue on Microsoft SQL Server tables that contain spatial data types, by adding geometry and geography JDBC types support for Spark SQL.  \nIntroduced a new configuration spark.databricks.hive.metastore.init.reloadFunctions.enabled. This configuration controls the built in Hive initialization. When set to true, Databricks reloads all functions from all databases that users have into FunctionRegistry. This is the default behavior in Hive Metastore. When set to false, Databricks disables this process for optimization.  \n[SPARK-34212] Fixed issues related to reading decimal data from Parquet files.  \n[SPARK-34260][SQL] Fix UnresolvedException when creating temp view twice.  \nFebruary 4, 2021  \nFixed a regression that prevents the incremental execution of a query that sets a global limit such as SELECT * FROM table LIMIT nrows. The regression was experienced by users running queries via ODBC/JDBC with Arrow serialization enabled.  \nIntroduced write time checks to the Hive client to prevent the corruption of metadata in the Hive metastore for Delta tables."
    },
    {
        "id": 1578,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Introduced write time checks to the Hive client to prevent the corruption of metadata in the Hive metastore for Delta tables.  \nFixed a regression that caused DBFS FUSE to fail to start when cluster environment variable configurations contain invalid bash syntax.  \nJanuary 20, 2021  \nFixed a regression in the January 12, 2021 maintenance release that can cause an incorrect AnalysisException and say the column is ambiguous in a self join. This regression happens when a user joins a DataFrame with its derived DataFrame (a so-called self-join) with the following conditions:  \nThese two DataFrames have common columns, but the output of the self join does not have common columns. For example, df.join(df.select($\"col\" as \"new_col\"), cond)  \nThe derived DataFrame excludes some columns via select, groupBy, or window.  \nThe join condition or the following transformation after the joined Dataframe refers to the non-common columns. For example, df.join(df.drop(\"a\"), df(\"a\") === 1)  \nJanuary 12, 2021  \nUpgrade Azure Storage SDK from 2.3.8 to 2.3.9.  \n[SPARK-33593][SQL] Vector reader got incorrect data with binary partition value  \n[SPARK-33480][SQL] updates the error message of char/varchar table insertion length check  \nDatabricks Runtime 7.3 LTS (EoS)  \nSee Databricks Runtime 7.3 LTS (EoS).  \nSeptember 10, 2023  \nMiscellaneous bug fixes.  \nAugust 30, 2023  \nOperating system security updates.  \nAugust 15, 2023  \nOperating system security updates.  \nJune 23, 2023  \nSnowflake-jdbc library is upgraded to 3.13.29 to address a security issue.  \nOperating system security updates.  \nJune 15, 2023  \n[SPARK-43413][SQL] Fix IN subquery ListQuery nullability.  \nOperating system security updates.  \nJune 2, 2023  \nFixed an issue in Auto Loader where different source file formats were inconsistent when the provided schema did not include inferred partitions. This issue could cause unexpected failures when reading files with missing columns in the inferred partition schema."
    },
    {
        "id": 1579,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fixed an issue in Auto Loader where different source file formats were inconsistent when the provided schema did not include inferred partitions. This issue could cause unexpected failures when reading files with missing columns in the inferred partition schema.  \nMay 17, 2023  \nOperating system security updates.  \nApril 25, 2023  \nOperating system security updates.  \nApril 11, 2023  \n[SPARK-42967][CORE] Fix SparkListenerTaskStart.stageAttemptId when a task is started after the stage is cancelled.  \nMiscellaneous bug fixes.  \nMarch 29, 2023  \nOperating system security updates.  \nMarch 14, 2023  \nMiscellaneous bug fixes.  \nFebruary 28, 2023  \nOperating system security updates.  \nFebruary 16, 2023  \nOperating system security updates.  \nJanuary 31, 2023  \nTable types of JDBC tables are now EXTERNAL by default.  \nJanuary 18, 2023  \nOperating system security updates.  \nNovember 29, 2022  \nMiscellaneous bug fixes.  \nNovember 15, 2022  \nUpgraded Apache commons-text to 1.10.0.  \nOperating system security updates.  \nMiscellaneous bug fixes.  \nNovember 1, 2022  \n[SPARK-38542][SQL] UnsafeHashedRelation should serialize numKeys out  \nOctober 18, 2022  \nOperating system security updates.  \nOctober 5, 2022  \nMiscellaneous bug fixes.  \nOperating system security updates.  \nSeptember 22, 2022  \n[SPARK-40089][SQL] Fix sorting for some Decimal types  \nSeptember 6, 2022  \n[SPARK-35542][CORE][ML] Fix: Bucketizer created for multiple columns with parameters splitsArray, inputCols and outputCols can not be loaded after saving it  \n[SPARK-40079][CORE] Add Imputer inputCols validation for empty input case  \nAugust 24, 2022  \n[SPARK-39962][PYTHON][SQL] Apply projection when group attributes are empty  \nOperating system security updates.  \nAugust 9, 2022  \nOperating system security updates.  \nJuly 27, 2022  \nMake Delta MERGE operation results consistent when source is non-deterministic.  \nOperating system security updates.  \nMiscellaneous bug fixes.  \nJuly 13, 2022  \n[SPARK-32680][SQL] Don\u2019t Preprocess V2 CTAS with Unresolved Query  \nDisabled Auto Loader\u2019s use of native cloud APIs for directory listing on Azure.  \nOperating system security updates.  \nJuly 5, 2022  \nOperating system security updates."
    },
    {
        "id": 1580,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Disabled Auto Loader\u2019s use of native cloud APIs for directory listing on Azure.  \nOperating system security updates.  \nJuly 5, 2022  \nOperating system security updates.  \nMiscellaneous bug fixes.  \nJune 2, 2022  \n[SPARK-38918][SQL] Nested column pruning should filter out attributes that do not belong to the current relation  \nOperating system security updates.  \nMay 18, 2022  \nUpgrade AWS SDK version from 1.11.655 to 1.11.678.  \nOperating system security updates.  \nMiscellaneous bug fixes.  \nApril 19, 2022  \nOperating system security updates.  \nMiscellaneous bug fixes.  \nApril 6, 2022  \nOperating system security updates.  \nMiscellaneous bug fixes.  \nMarch 14, 2022  \nRemove vulnerable classes from log4j 1.2.17 jar  \nMiscellaneous bug fixes.  \nFebruary 23, 2022  \n[SPARK-37859][SQL] Do not check for metadata during schema comparison  \nFebruary 8, 2022  \nUpgrade Ubuntu JDK to 1.8.0.312.  \nOperating system security updates.  \nFebruary 1, 2022  \nOperating system security updates.  \nJanuary 26, 2022  \nFixed a bug where the OPTIMIZE command could fail when the ANSI SQL dialect was enabled.  \nJanuary 19, 2022  \nConda defaults channel is removed from 7.3 ML LTS  \nOperating system security updates.  \nDecember 7, 2021  \nOperating system security updates.  \nNovember 4, 2021  \nFixed a bug that could cause Structured Streaming streams to fail with an ArrayIndexOutOfBoundsException  \nFixed a race condition that might cause a query failure with an IOException like java.io.IOException: No FileSystem for scheme or that might cause modifications to sparkContext.hadoopConfiguration to not take effect in queries.  \nSeptember 15, 2021  \nFixed a race condition that might cause a query failure with an IOException like java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_x_piecey of broadcast_x.  \nOperating system security updates.  \nSeptember 8, 2021  \n[SPARK-35700][SQL][WARMFIX] Read char/varchar orc table when created and written by external systems."
    },
    {
        "id": 1581,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \nSeptember 8, 2021  \n[SPARK-35700][SQL][WARMFIX] Read char/varchar orc table when created and written by external systems.  \n[SPARK-36532][CORE][3.1] Fixed deadlock in CoarseGrainedExecutorBackend.onDisconnected to avoid executorsconnected to prevent executor shutdown hang.  \nAugust 25, 2021  \nSnowflake connector was upgraded to 2.9.0.  \nJuly 29, 2021  \n[SPARK-36034][BUILD] Rebase datetime in pushed down filters to Parquet  \n[SPARK-34508][BUILD] Skip HiveExternalCatalogVersionsSuite if network is down  \nJuly 14, 2021  \nIntroduced database.schema.table format for Azure Synapse connector.  \nAdded support to provide databaseName.schemaName.tableName format as the target table instead of only schemaName.tableName or tableName.  \nFixed a bug that prevents users from time traveling to older available versions with Delta tables.  \nJune 15, 2021  \nFixes a NoSuchElementException bug in Delta Lake optimized writes that can happen when writing large amounts of data and encountering executor losses  \nUpdated Python with security patch to fix Python security vulnerability (CVE-2021-3177).  \nApril 30, 2021  \nOperating system security updates.  \n[SPARK-35227][BUILD] Update the resolver for spark-packages in SparkSubmit  \n[SPARK-34245][CORE] Ensure Master removes executors that failed to send finished state  \n[SPARK-35045][SQL] Add an internal option to control input buffer in univocity  \nMarch 24, 2021  \n[SPARK-34768][SQL] Respect the default input buffer size in Univocity  \n[SPARK-34534] Fix blockIds order when use FetchShuffleBlocks to fetch blocks  \n[SPARK-33118][SQL]CREATE TEMPORARY TABLE fails with location  \nDisk caching is enabled by default on i3en instances.  \nMarch 9, 2021  \nThe updated Azure Blob File System driver for Azure Data Lake Storage Gen2 is now enabled by default. It brings multiple stability improvements.  \nFix path separator on Windows for databricks-connect get-jar-dir"
    },
    {
        "id": 1582,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "March 9, 2021  \nThe updated Azure Blob File System driver for Azure Data Lake Storage Gen2 is now enabled by default. It brings multiple stability improvements.  \nFix path separator on Windows for databricks-connect get-jar-dir  \n[UI] Fix the href link of Spark DAG Visualization  \n[DBCONNECT] Add support for FlatMapCoGroupsInPandas in Databricks Connect 7.3  \nRestore the output schema of SHOW DATABASES  \n[SQL] Use correct dynamic pruning build key when range join hint is present  \nDisable staleness check for Delta table files in disk cache  \n[SQL] Do not generate shuffle partition number advice when AOS is enable  \nFebruary 24, 2021  \nUpgraded the Spark BigQuery connector to v0.18, which introduces various bug fixes and support for Arrow and Avro iterators.  \nFixed a correctness issue that caused Spark to return incorrect results when the Parquet file\u2019s decimal precision and scale are different from the Spark schema.  \nFixed reading failure issue on Microsoft SQL Server tables that contain spatial data types, by adding geometry and geography JDBC types support for Spark SQL.  \nIntroduced a new configuration spark.databricks.hive.metastore.init.reloadFunctions.enabled. This configuration controls the built in Hive initialization. When set to true, Databricks reloads all functions from all databases that users have into FunctionRegistry. This is the default behavior in Hive Metastore. When set to false, Databricks disables this process for optimization.  \n[SPARK-34212] Fixed issues related to reading decimal data from Parquet files.  \n[SPARK-33579][UI] Fix executor blank page behind proxy.  \n[SPARK-20044][UI] Support Spark UI behind front-end reverse proxy using a path prefix.  \n[SPARK-33277][PYSPARK][SQL] Use ContextAwareIterator to stop consuming after the task ends.  \nFebruary 4, 2021  \nFixed a regression that prevents the incremental execution of a query that sets a global limit such as SELECT * FROM table LIMIT nrows. The regression was experienced by users running queries via ODBC/JDBC with Arrow serialization enabled."
    },
    {
        "id": 1583,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fixed a regression that caused DBFS FUSE to fail to start when cluster environment variable configurations contain invalid bash syntax.  \nJanuary 20, 2021  \nFixed a regression in the January 12, 2021 maintenance release that can cause an incorrect AnalysisException and say the column is ambiguous in a self join. This regression happens when a user joins a DataFrame with its derived DataFrame (a so-called self-join) with the following conditions:  \nThese two DataFrames have common columns, but the output of the self join does not have common columns. For example, df.join(df.select($\"col\" as \"new_col\"), cond)  \nThe derived DataFrame excludes some columns via select, groupBy, or window.  \nThe join condition or the following transformation after the joined Dataframe refers to the non-common columns. For example, df.join(df.drop(\"a\"), df(\"a\") === 1)  \nJanuary 12, 2021  \nOperating system security updates.  \n[SPARK-33593][SQL] Vector reader got incorrect data with binary partition value  \n[SPARK-33677][SQL] Skip LikeSimplification rule if pattern contains any escapeChar  \n[SPARK-33592][ML][PYTHON] Pyspark ML Validator params in estimatorParamMaps may be lost after saving and reloading  \n[SPARK-33071][SPARK-33536][SQL] Avoid changing dataset_id of LogicalPlan in join() to not break DetectAmbiguousSelfJoin  \nDecember 8, 2020  \n[SPARK-33587][CORE] Kill the executor on nested fatal errors  \n[SPARK-27421][SQL] Fix filter for int column and value class java.lang.String when pruning partition column  \n[SPARK-33316][SQL] Support user provided nullable Avro schema for non-nullable catalyst schema in Avro writing  \nSpark Jobs launched using Databricks Connect could hang indefinitely with Executor$TaskRunner.$anonfun$copySessionState in executor stack trace  \nOperating system security updates.  \nNovember 20, 2020"
    },
    {
        "id": 1584,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Spark Jobs launched using Databricks Connect could hang indefinitely with Executor$TaskRunner.$anonfun$copySessionState in executor stack trace  \nOperating system security updates.  \nNovember 20, 2020  \n[SPARK-33404][SQL][3.0] Fix incorrect results in date_trunc expression  \n[SPARK-33339][PYTHON] Pyspark application will hang due to non Exception error  \n[SPARK-33183][SQL][HOTFIX] Fix Optimizer rule EliminateSorts and add a physical rule to remove redundant sorts  \n[SPARK-33371][PYTHON][3.0] Update setup.py and tests for Python 3.9  \n[SPARK-33391][SQL] element_at with CreateArray not respect one based index.  \n[SPARK-33306][SQL]Timezone is needed when cast date to string  \n[SPARK-33260][SQL] Fix incorrect results from SortExec when sortOrder is Stream  \nNovember 5, 2020  \nFix ABFS and WASB locking with regard to UserGroupInformation.getCurrentUser().  \nFix an infinite loop bug when Avro reader reads the MAGIC bytes.  \nAdd support for the USAGE privilege.  \nPerformance improvements for privilege checking in table access control.  \nOctober 13, 2020  \nOperating system security updates.  \nYou can read and write from DBFS using the FUSE mount at /dbfs/ when on a high concurrency credential passthrough enabled cluster. Regular mounts are supported but mounts that need passthrough credentials are not supported yet.  \n[SPARK-32999][SQL] Use Utils.getSimpleName to avoid hitting Malformed class name in TreeNode  \n[SPARK-32585][SQL] Support scala enumeration in ScalaReflection  \nFixed listing directories in FUSE mount that contain file names with invalid XML characters  \nFUSE mount no longer uses ListMultipartUploads  \nSeptember 29, 2020  \n[SPARK-32718][SQL] Remove unnecessary keywords for interval units  \n[SPARK-32635][SQL] Fix foldable propagation"
    },
    {
        "id": 1585,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "September 29, 2020  \n[SPARK-32718][SQL] Remove unnecessary keywords for interval units  \n[SPARK-32635][SQL] Fix foldable propagation  \nAdd a new config spark.shuffle.io.decoder.consolidateThreshold. Set the config value to Long.MAX_VALUE to skip the consolidation of netty FrameBuffers, which prevents java.lang.IndexOutOfBoundsException in corner cases.  \nApril 25, 2023  \nOperating system security updates.  \nApril 11, 2023  \nMiscellaneous bug fixes.  \nMarch 29, 2023  \nMiscellaneous bug fixes.  \nMarch 14, 2023  \nOperating system security updates.  \nFebruary 28, 2023  \nOperating system security updates.  \nFebruary 16, 2023  \nOperating system security updates.  \nJanuary 31, 2023  \nMiscellaneous bug fixes.  \nJanuary 18, 2023  \nOperating system security updates.  \nNovember 29, 2022  \nOperating system security updates.  \nNovember 15, 2022  \nOperating system security updates.  \nMiscellaneous bug fixes.  \nNovember 1, 2022  \nOperating system security updates.  \nOctober 18, 2022  \nOperating system security updates.  \nOctober 5, 2022  \nOperating system security updates.  \nAugust 24, 2022  \nOperating system security updates.  \nAugust 9, 2022  \nOperating system security updates.  \nJuly 27, 2022  \nOperating system security updates.  \nJuly 5, 2022  \nOperating system security updates.  \nJune 2, 2022  \nOperating system security updates.  \nMay 18, 2022  \nOperating system security updates.  \nApril 19, 2022  \nOperating system security updates.  \nMiscellaneous bug fixes.  \nApril 6, 2022  \nOperating system security updates.  \nMiscellaneous bug fixes.  \nMarch 14, 2022  \nMiscellaneous bug fixes.  \nFebruary 23, 2022  \nMiscellaneous bug fixes.  \nFebruary 8, 2022  \nUpgrade Ubuntu JDK to 1.8.0.312.  \nOperating system security updates.  \nFebruary 1, 2022  \nOperating system security updates.  \nJanuary 19, 2022  \nOperating system security updates.  \nSeptember 22, 2021  \nOperating system security updates.  \nApril 30, 2021  \nOperating system security updates.  \n[SPARK-35227][BUILD] Update the resolver for spark-packages in SparkSubmit  \nJanuary 12, 2021  \nOperating system security updates.  \nDecember 8, 2020  \n[SPARK-27421][SQL] Fix filter for int column and value class java.lang.String when pruning partition column  \nOperating system security updates.  \nNovember 20, 2020  \nNovember 3, 2020"
    },
    {
        "id": 1586,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-27421][SQL] Fix filter for int column and value class java.lang.String when pruning partition column  \nOperating system security updates.  \nNovember 20, 2020  \nNovember 3, 2020  \nUpgraded Java version from 1.8.0_252 to 1.8.0_265.  \nFix ABFS and WASB locking with regard to UserGroupInformation.getCurrentUser()  \nOctober 13, 2020  \nOperating system security updates.  \nDatabricks Runtime 6.4 Extended Support (EoS)  \nSee Databricks Runtime 6.4 (EoS) and Databricks Runtime 6.4 Extended Support (EoS).  \nJuly 5, 2022  \nOperating system security updates.  \nMiscellaneous bug fixes.  \nJune 2, 2022  \nOperating system security updates.  \nMay 18, 2022  \nOperating system security updates.  \nApril 19, 2022  \nOperating system security updates.  \nMiscellaneous bug fixes.  \nApril 6, 2022  \nOperating system security updates.  \nMiscellaneous bug fixes.  \nMarch 14, 2022  \nRemove vulnerable classes from log4j 1.2.17 jar  \nMiscellaneous bug fixes.  \nFebruary 23, 2022  \nMiscellaneous bug fixes.  \nFebruary 8, 2022  \nUpgrade Ubuntu JDK to 1.8.0.312.  \nOperating system security updates.  \nFebruary 1, 2022  \nOperating system security updates.  \nJanuary 26, 2022  \nFixed a bug where the OPTIMIZE command could fail when the ANSI SQL dialect was enabled.  \nJanuary 19, 2022  \nOperating system security updates.  \nDecember 8, 2021  \nOperating system security updates.  \nSeptember 22, 2021  \nOperating system security updates.  \nJune 15, 2021  \n[SPARK-35576][SQL] Redact the sensitive info in the result of Set command  \nJune 7, 2021  \nAdd a new config called spark.sql.maven.additionalRemoteRepositories, a comma-delimited string config of the optional additional remote maven mirror. The value defaults to https://maven-central.storage-download.googleapis.com/maven2/.  \nApril 30, 2021  \nOperating system security updates.  \n[SPARK-35227][BUILD] Update the resolver for spark-packages in SparkSubmit  \nApril 6, 2021  \nFixed retries added to the S3 client to resolve connection reset issues.  \nMarch 24, 2021"
    },
    {
        "id": 1587,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-35227][BUILD] Update the resolver for spark-packages in SparkSubmit  \nApril 6, 2021  \nFixed retries added to the S3 client to resolve connection reset issues.  \nMarch 24, 2021  \nDisk caching is enabled by default on i3en instances.  \nMarch 9, 2021  \nPort HADOOP-17215 to the Azure Blob File System driver (Support for conditional overwrite).  \nFix path separator on Windows for databricks-connect get-jar-dir  \nAdded support for Hive metastore versions 2.3.5, 2.3.6, and 2.3.7  \nArrow \u201ctotalResultsCollected\u201d reported incorrectly after spill  \nFebruary 24, 2021  \nIntroduced a new configuration spark.databricks.hive.metastore.init.reloadFunctions.enabled. This configuration controls the built in Hive initialization. When set to true, Databricks reloads all functions from all databases that users have into FunctionRegistry. This is the default behavior in Hive Metastore. When set to false, Databricks disables this process for optimization.  \nFebruary 4, 2021  \nFixed a regression that prevents the incremental execution of a query that sets a global limit such as SELECT * FROM table LIMIT nrows. The regression was experienced by users running queries via ODBC/JDBC with Arrow serialization enabled.  \nFixed a regression that caused DBFS FUSE to fail to start when cluster environment variable configurations contain invalid bash syntax.  \nJanuary 12, 2021  \nOperating system security updates.  \nDecember 8, 2020  \n[SPARK-27421][SQL] Fix filter for int column and value class java.lang.String when pruning partition column  \n[SPARK-33183][SQL] Fix Optimizer rule EliminateSorts and add a physical rule to remove redundant sorts  \n[Runtime 6.4 ML GPU] We previously installed an incorrect version (2.7.8-1+cuda11.1) of NCCL. This release corrects it to 2.4.8-1+cuda10.0 that is compatible with CUDA 10.0.  \nOperating system security updates.  \nNovember 20, 2020  \n[SPARK-33260][SQL] Fix incorrect results from SortExec when sortOrder is Stream"
    },
    {
        "id": 1588,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \nNovember 20, 2020  \n[SPARK-33260][SQL] Fix incorrect results from SortExec when sortOrder is Stream  \n[SPARK-32635][SQL] Fix foldable propagation  \nNovember 3, 2020  \nUpgraded Java version from 1.8.0_252 to 1.8.0_265.  \nFix ABFS and WASB locking with regard to UserGroupInformation.getCurrentUser()  \nFix an infinite loop bug of Avro reader when reading the MAGIC bytes.  \nOctober 13, 2020  \nOperating system security updates.  \n[SPARK-32999][SQL][2.4] Use Utils.getSimpleName to avoid hitting Malformed class name in TreeNode  \nFixed listing directories in FUSE mount that contain file names with invalid XML characters  \nFUSE mount no longer uses ListMultipartUploads  \nSeptember 24, 2020  \nFixed a previous limitation where passthrough on standard cluster would still restrict the filesystem implementation user uses. Now users would be able to access local filesystems without restrictions.  \nOperating system security updates.  \nSeptember 8, 2020  \nA new parameter was created for Azure Synapse Analytics, maxbinlength. This parameter is used to control the column length of BinaryType columns, and is translated as VARBINARY(maxbinlength). It can be set using .option(\"maxbinlength\", n), where 0 < n <= 8000.  \nUpdate Azure Storage SDK to 8.6.4 and enable TCP keep alive on connections made by the WASB driver  \nAugust 25, 2020  \nFixed ambiguous attribute resolution in self-merge  \nAugust 18, 2020  \n[SPARK-32431][SQL] Check duplicate nested columns in read from in-built datasources  \nFixed a race condition in the SQS connector when using Trigger.Once.  \nAugust 11, 2020  \n[SPARK-28676][CORE] Avoid Excessive logging from ContextCleaner  \nAugust 3, 2020  \nYou can now use the LDA transform function on a passthrough-enabled cluster.  \nOperating system security updates.  \nJuly 7, 2020  \nUpgraded Java version from 1.8.0_232 to 1.8.0_252.  \nApril 21, 2020"
    },
    {
        "id": 1589,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \nJuly 7, 2020  \nUpgraded Java version from 1.8.0_232 to 1.8.0_252.  \nApril 21, 2020  \n[SPARK-31312][SQL] Cache Class instance for the UDF instance in HiveFunctionWrapper  \nApril 7, 2020  \nTo resolve an issue with pandas udf not working with PyArrow 0.15.0 and above, we added an environment variable (ARROW_PRE_0_15_IPC_FORMAT=1) to enable support for those versions of PyArrow. See the instructions in [SPARK-29367].  \nMarch 10, 2020  \nOptimized autoscaling is now used by default on interactive clusters on the Security plan.  \nThe Snowflake connector (spark-snowflake_2.11) included in Databricks Runtime is updated to version 2.5.9. snowflake-jdbc is updated to version 3.12.0.  \nDatabricks Runtime 5.5 LTS (EoS)  \nSee Databricks Runtime 5.5 LTS (EoS) and Databricks Runtime 5.5 Extended Support (EoS).  \nDecember 8, 2021  \nOperating system security updates.  \nSeptember 22, 2021  \nOperating system security updates.  \nAugust 25, 2021  \nDowngraded some previously upgraded python packages in 5.5 ML Extended Support Release to maintain better parity with 5.5 ML LTS (now deprecated). See [_]/release-notes/runtime/5.5xml.md) for the updated differences between the two versions.  \nJune 15, 2021  \n[SPARK-35576][SQL] Redact the sensitive info in the result of Set command  \nJune 7, 2021  \nAdd a new config called spark.sql.maven.additionalRemoteRepositories, a comma-delimited string config of the optional additional remote maven mirror. The value defaults to https://maven-central.storage-download.googleapis.com/maven2/.  \nApril 30, 2021  \nOperating system security updates.  \n[SPARK-35227][BUILD] Update the resolver for spark-packages in SparkSubmit  \nApril 6, 2021  \nFixed retries added to the S3 client to resolve connection reset issues."
    },
    {
        "id": 1590,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \n[SPARK-35227][BUILD] Update the resolver for spark-packages in SparkSubmit  \nApril 6, 2021  \nFixed retries added to the S3 client to resolve connection reset issues.  \nMarch 24, 2021  \nDisk caching is enabled by default on i3en instances.  \nMarch 9, 2021  \nPort HADOOP-17215 to the Azure Blob File System driver (Support for conditional overwrite).  \nFebruary 24, 2021  \nIntroduced a new configuration spark.databricks.hive.metastore.init.reloadFunctions.enabled. This configuration controls the built in Hive initialization. When set to true, Databricks reloads all functions from all databases that users have into FunctionRegistry. This is the default behavior in Hive Metastore. When set to false, Databricks disables this process for optimization.  \nJanuary 12, 2021  \nOperating system security updates.  \nFix for [HADOOP-17130].  \nDecember 8, 2020  \n[SPARK-27421][SQL] Fix filter for int column and value class java.lang.String when pruning partition column  \nOperating system security updates.  \nNovember 20, 2020  \n[SPARK-33260][SQL] Fix incorrect results from SortExec when sortOrder is Stream  \n[SPARK-32635][SQL] Fix foldable propagation  \nOctober 29, 2020  \nUpgraded Java version from 1.8.0_252 to 1.8.0_265.  \nFix ABFS and WASB locking with regard to UserGroupInformation.getCurrentUser()  \nFix an infinite loop bug of Avro reader when reading the MAGIC bytes.  \nOctober 13, 2020  \nOperating system security updates.  \n[SPARK-32999][SQL][2.4] Use Utils.getSimpleName to avoid hitting Malformed class name in TreeNode  \nSeptember 24, 2020  \nOperating system security updates.  \nSeptember 8, 2020  \nA new parameter was created for Azure Synapse Analytics, maxbinlength. This parameter is used to control the column length of BinaryType columns, and is translated as VARBINARY(maxbinlength). It can be set using .option(\"maxbinlength\", n), where 0 < n <= 8000.  \nAugust 18, 2020"
    },
    {
        "id": 1591,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "August 18, 2020  \n[SPARK-32431][SQL] Check duplicate nested columns in read from in-built datasources  \nFixed a race condition in the SQS connector when using Trigger.Once.  \nAugust 11, 2020  \n[SPARK-28676][CORE] Avoid Excessive logging from ContextCleaner  \nAugust 3, 2020  \nOperating system security updates  \nJuly 7, 2020  \nUpgraded Java version from 1.8.0_232 to 1.8.0_252.  \nApril 21, 2020  \n[SPARK-31312][SQL] Cache Class instance for the UDF instance in HiveFunctionWrapper  \nApril 7, 2020  \nTo resolve an issue with pandas udf not working with PyArrow 0.15.0 and above, we added an environment variable (ARROW_PRE_0_15_IPC_FORMAT=1) to enable support for those versions of PyArrow. See the instructions in [SPARK-29367].  \nMarch 25, 2020  \nThe Snowflake connector (spark-snowflake_2.11) included in Databricks Runtime is updated to version 2.5.9. snowflake-jdbc is updated to version 3.12.0.  \nMarch 10, 2020  \nJob output, such as log output emitted to stdout, is subject to a 20MB size limit. If the total output has a larger size, the run will be canceled and marked as failed. To avoid encountering this limit, you can prevent stdout from being returned from the driver to by setting the spark.databricks.driver.disableScalaOutput Spark configuration to true. By default the flag value is false. The flag controls cell output for Scala JAR jobs and Scala notebooks. If the flag is enabled, Spark does not return job execution results to the client. The flag does not affect the data that is written in the cluster\u2019s log files. Setting this flag is recommended only for automated clusters for JAR jobs, because it will disable notebook results.  \nFebruary 18, 2020  \n[SPARK-24783][SQL] spark.sql.shuffle.partitions=0 should throw exception"
    },
    {
        "id": 1592,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "February 18, 2020  \n[SPARK-24783][SQL] spark.sql.shuffle.partitions=0 should throw exception  \nCredential passthrough with ADLS Gen2 has a performance degradation due to incorrect thread local handling when ADLS client prefetching is enabled. This release disables ADLS Gen2 prefetching when credential passthrough is enabled until we have a proper fix.  \nJanuary 28, 2020  \nFixed a bug in S3AFileSystem, whereby fs.isDirectory(path) or fs.getFileStatus(path).isDirectory() could sometimes incorrectly return false. The bug would manifest on paths for which aws s3 list-objects-v2 --prefix path/ --max-keys 1 --delimiter / responds with no keys or common prefixes, but isTruncated = true. This might happen for directories under which many objects were deleted and versioning was enabled.  \n[SPARK-30447][SQL] Constant propagation nullability issue.  \nJanuary 14, 2020  \nUpgraded Java version from 1.8.0_222 to 1.8.0_232.  \nNovember 19, 2019  \n[SPARK-29743] [SQL] sample should set needCopyResult to true if its child\u2019s needCopyResult is true  \nR version was unintendedly upgraded to 3.6.1 from 3.6.0. We downgraded it back to 3.6.0.  \nNovember 5, 2019  \nUpgraded Java version from 1.8.0_212 to 1.8.0_222.  \nOctober 23, 2019  \n[SPARK-29244][CORE] Prevent freed page in BytesToBytesMap free again  \nOctober 8, 2019  \nServer side changes to allow Simba Apache Spark ODBC driver to reconnect and continue after a connection failure during fetching results (requires Simba Apache Spark ODBC driver version 2.6.10).  \nFixed an issue affecting using Optimize command with table ACL enabled clusters.  \nFixed an issue where pyspark.ml libraries would fail due to Scala UDF forbidden error on table ACL enabled clusters.  \nFixed NullPointerException when checking error code in the WASB client.  \nSeptember 24, 2019  \nImproved stability of Parquet writer."
    },
    {
        "id": 1593,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fixed NullPointerException when checking error code in the WASB client.  \nSeptember 24, 2019  \nImproved stability of Parquet writer.  \nFixed the problem that Thrift query cancelled before it starts executing may stuck in STARTED state.  \nSeptember 10, 2019  \nAdd thread safe iterator to BytesToBytesMap  \n[SPARK-27992][SPARK-28881]Allow Python to join with connection thread to propagate errors  \nFixed a bug affecting certain global aggregation queries.  \nImproved credential redaction.  \n[SPARK-27330][SS] support task abort in foreach writer  \n[SPARK-28642]Hide credentials in SHOW CREATE TABLE  \n[SPARK-28699][SQL] Disable using radix sort for ShuffleExchangeExec in repartition case  \nAugust 27, 2019  \n[SPARK-20906][SQL]Allow user-specified schema in the API toavro_ with schema registry  \n[SPARK-27838][SQL] Support user provided non-nullable avro schema for nullable catalyst schema without any null record  \nImprovement on Delta Lake time travel  \nFixed an issue affecting certain transform expression  \nSupports broadcast variables when Process Isolation is enabled  \nAugust 13, 2019  \nDelta streaming source should check the latest protocol of a table  \n[SPARK-28260]Add CLOSED state to ExecutionState  \n[SPARK-28489][SS]Fix a bug that KafkaOffsetRangeCalculator.getRanges may drop offsets  \nJuly 30, 2019  \n[SPARK-28015][SQL] Check stringToDate() consumes entire input for the yyyy and yyyy-[m]m formats  \n[SPARK-28308][CORE] CalendarInterval sub-second part should be padded before parsing  \n[SPARK-27485]EnsureRequirements.reorder should handle duplicate expressions gracefully  \n[SPARK-28355][CORE][PYTHON] Use Spark conf for threshold at which UDF is compressed by broadcast  \nDatabricks Light 2.4 Extended Support  \nSee Databricks Light 2.4 (EoS) and Databricks Light 2.4 Extended Support (EoS).  \nDatabricks Runtime 7.4 (EoS)  \nSee Databricks Runtime 7.4 (EoS)."
    },
    {
        "id": 1594,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Databricks Runtime 7.4 (EoS)  \nSee Databricks Runtime 7.4 (EoS).  \nApril 30, 2021  \nOperating system security updates.  \n[SPARK-35227][BUILD] Update the resolver for spark-packages in SparkSubmit  \n[SPARK-34245][CORE] Ensure Master removes executors that failed to send finished state  \n[SPARK-35045][SQL] Add an internal option to control input buffer in univocity and a configuration for CSV input buffer size  \nMarch 24, 2021  \n[SPARK-34768][SQL] Respect the default input buffer size in Univocity  \n[SPARK-34534] Fix blockIds order when use FetchShuffleBlocks to fetch blocks  \nDisk caching is enabled by default on i3en instances.  \nMarch 9, 2021  \nThe updated Azure Blob File System driver for Azure Data Lake Storage Gen2 is now enabled by default. It brings multiple stability improvements.  \n[ES-67926][UI] Fix the href link of Spark DAG Visualization  \n[ES-65064] Restore the output schema of SHOW DATABASES  \n[SC-70522][SQL] Use correct dynamic pruning build key when range join hint is present  \n[SC-35081] Disable staleness check for Delta table files in disk cache  \n[SC-70640] Fix NPE when EventGridClient response has no entity  \n[SC-70220][SQL] Do not generate shuffle partition number advice when AOS is enabled  \nFebruary 24, 2021  \nUpgraded the Spark BigQuery connector to v0.18, which introduces various bug fixes and support for Arrow and Avro iterators.  \nFixed a correctness issue that caused Spark to return incorrect results when the Parquet file\u2019s decimal precision and scale are different from the Spark schema.  \nFixed reading failure issue on Microsoft SQL Server tables that contain spatial data types, by adding geometry and geography JDBC types support for Spark SQL."
    },
    {
        "id": 1595,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fixed reading failure issue on Microsoft SQL Server tables that contain spatial data types, by adding geometry and geography JDBC types support for Spark SQL.  \nIntroduced a new configuration spark.databricks.hive.metastore.init.reloadFunctions.enabled. This configuration controls the built in Hive initialization. When set to true, Databricks reloads all functions from all databases that users have into FunctionRegistry. This is the default behavior in Hive Metastore. When set to false, Databricks disables this process for optimization.  \n[SPARK-34212] Fixed issues related to reading decimal data from Parquet files.  \n[SPARK-33579][UI] Fix executor blank page behind proxy.  \n[SPARK-20044][UI] Support Spark UI behind front-end reverse proxy using a path prefix.  \n[SPARK-33277][PYSPARK][SQL] Use ContextAwareIterator to stop consuming after the task ends.  \nFebruary 4, 2021  \nFixed a regression that prevents the incremental execution of a query that sets a global limit such as SELECT * FROM table LIMIT nrows. The regression was experienced by users running queries via ODBC/JDBC with Arrow serialization enabled.  \nFixed a regression that caused DBFS FUSE to fail to start when cluster environment variable configurations contain invalid bash syntax.  \nJanuary 20, 2021  \nFixed a regression in the January 12, 2021 maintenance release that can cause an incorrect AnalysisException and say the column is ambiguous in a self join. This regression happens when a user joins a DataFrame with its derived DataFrame (a so-called self-join) with the following conditions:  \nThese two DataFrames have common columns, but the output of the self join does not have common columns. For example, df.join(df.select($\"col\" as \"new_col\"), cond)  \nThe derived DataFrame excludes some columns via select, groupBy, or window.  \nThe join condition or the following transformation after the joined Dataframe refers to the non-common columns. For example, df.join(df.drop(\"a\"), df(\"a\") === 1)"
    },
    {
        "id": 1596,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "January 12, 2021  \nOperating system security updates.  \n[SPARK-33593][SQL] Vector reader got incorrect data with binary partition value  \n[SPARK-33677][SQL] Skip LikeSimplification rule if pattern contains any escapeChar  \n[SPARK-33071][SPARK-33536][SQL] Avoid changing dataset_id of LogicalPlan in join() to not break DetectAmbiguousSelfJoin  \nDecember 8, 2020  \n[SPARK-33587][CORE] Kill the executor on nested fatal errors  \n[SPARK-27421][SQL] Fix filter for int column and value class java.lang.String when pruning partition column  \n[SPARK-33316][SQL] Support user provided nullable Avro schema for non-nullable catalyst schema in Avro writing  \nOperating system security updates.  \nNovember 20, 2020  \n[SPARK-33404][SQL][3.0] Fix incorrect results in date_trunc expression  \n[SPARK-33339][PYTHON] Pyspark application will hang due to non Exception error  \n[SPARK-33183][SQL][HOTFIX] Fix Optimizer rule EliminateSorts and add a physical rule to remove redundant sorts  \n[SPARK-33371][PYTHON][3.0] Update setup.py and tests for Python 3.9  \n[SPARK-33391][SQL] element_at with CreateArray not respect one based index.  \n[SPARK-33306][SQL]Timezone is needed when cast date to string  \n[SPARK-33260][SQL] Fix incorrect results from SortExec when sortOrder is Stream  \n[SPARK-33272][SQL] prune the attributes mapping in QueryPlan.transformUpWithNewOutput  \nDatabricks Runtime 7.2 (EoS)  \nSee Databricks Runtime 7.2 (EoS).  \nFebruary 4, 2021  \nFixed a regression that prevents the incremental execution of a query that sets a global limit such as SELECT * FROM table LIMIT nrows. The regression was experienced by users running queries via ODBC/JDBC with Arrow serialization enabled."
    },
    {
        "id": 1597,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fixed a regression that caused DBFS FUSE to fail to start when cluster environment variable configurations contain invalid bash syntax.  \nJanuary 20, 2021  \nFixed a regression in the January 12, 2021 maintenance release that can cause an incorrect AnalysisException and say the column is ambiguous in a self join. This regression happens when a user joins a DataFrame with its derived DataFrame (a so-called self-join) with the following conditions:  \nThese two DataFrames have common columns, but the output of the self join does not have common columns. For example, df.join(df.select($\"col\" as \"new_col\"), cond)  \nThe derived DataFrame excludes some columns via select, groupBy, or window.  \nThe join condition or the following transformation after the joined Dataframe refers to the non-common columns. For example, df.join(df.drop(\"a\"), df(\"a\") === 1)  \nJanuary 12, 2021  \nOperating system security updates.  \n[SPARK-33593][SQL] Vector reader got incorrect data with binary partition value  \n[SPARK-33677][SQL] Skip LikeSimplification rule if pattern contains any escapeChar  \n[SPARK-33071][SPARK-33536][SQL] Avoid changing dataset_id of LogicalPlan in join() to not break DetectAmbiguousSelfJoin  \nDecember 8, 2020  \n[SPARK-27421][SQL] Fix filter for int column and value class java.lang.String when pruning partition column  \n[SPARK-33404][SQL] Fix incorrect results in date_trunc expression  \n[SPARK-33339][PYTHON] Pyspark application will hang due to non Exception error  \n[SPARK-33183][SQL] Fix Optimizer rule EliminateSorts and add a physical rule to remove redundant sorts  \n[SPARK-33391][SQL] element_at with CreateArray not respect one based index.  \nOperating system security updates.  \nNovember 20, 2020  \n[SPARK-33306][SQL]Timezone is needed when cast date to string"
    },
    {
        "id": 1598,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Operating system security updates.  \nNovember 20, 2020  \n[SPARK-33306][SQL]Timezone is needed when cast date to string  \n[SPARK-33260][SQL] Fix incorrect results from SortExec when sortOrder is Stream  \nNovember 3, 2020  \nUpgraded Java version from 1.8.0_252 to 1.8.0_265.  \nFix ABFS and WASB locking with regard to UserGroupInformation.getCurrentUser()  \nFix an infinite loop bug of Avro reader when reading the MAGIC bytes.  \nOctober 13, 2020  \nOperating system security updates.  \n[SPARK-32999][SQL] Use Utils.getSimpleName to avoid hitting Malformed class name in TreeNode  \nFixed listing directories in FUSE mount that contain file names with invalid XML characters  \nFUSE mount no longer uses ListMultipartUploads  \nSeptember 29, 2020  \n[SPARK-28863][SQL][WARMFIX] Introduce AlreadyOptimized to prevent reanalysis of V1FallbackWriters  \n[SPARK-32635][SQL] Fix foldable propagation  \nAdd a new config spark.shuffle.io.decoder.consolidateThreshold. Set the config value to Long.MAX_VALUE to skip the consolidation of netty FrameBuffers, which prevents java.lang.IndexOutOfBoundsException in corner cases.  \nSeptember 24, 2020  \n[SPARK-32764][SQL] -0.0 should be equal to 0.0  \n[SPARK-32753][SQL] Only copy tags to node with no tags when transforming plans  \n[SPARK-32659][SQL] Fix the data issue of inserted Dynamic Partition Pruning on non-atomic type  \nOperating system security updates.  \nSeptember 8, 2020  \nA new parameter was created for Azure Synapse Analytics, maxbinlength. This parameter is used to control the column length of BinaryType columns, and is translated as VARBINARY(maxbinlength). It can be set using .option(\"maxbinlength\", n), where 0 < n <= 8000.  \nDatabricks Runtime 7.1 (EoS)"
    },
    {
        "id": 1599,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Databricks Runtime 7.1 (EoS)  \nSee Databricks Runtime 7.1 (EoS).  \nFebruary 4, 2021  \nFixed a regression that caused DBFS FUSE to fail to start when cluster environment variable configurations contain invalid bash syntax.  \nJanuary 20, 2021  \nFixed a regression in the January 12, 2021 maintenance release that can cause an incorrect AnalysisException and say the column is ambiguous in a self join. This regression happens when a user joins a DataFrame with its derived DataFrame (a so-called self-join) with the following conditions:  \nThese two DataFrames have common columns, but the output of the self join does not have common columns. For example, df.join(df.select($\"col\" as \"new_col\"), cond)  \nThe derived DataFrame excludes some columns via select, groupBy, or window.  \nThe join condition or the following transformation after the joined Dataframe refers to the non-common columns. For example, df.join(df.drop(\"a\"), df(\"a\") === 1)  \nJanuary 12, 2021  \nOperating system security updates.  \n[SPARK-33593][SQL] Vector reader got incorrect data with binary partition value  \n[SPARK-33677][SQL] Skip LikeSimplification rule if pattern contains any escapeChar  \n[SPARK-33071][SPARK-33536][SQL] Avoid changing dataset_id of LogicalPlan in join() to not break DetectAmbiguousSelfJoin  \nDecember 8, 2020  \n[SPARK-27421][SQL] Fix filter for int column and value class java.lang.String when pruning partition column  \nSpark Jobs launched using Databricks Connect could hang indefinitely with Executor$TaskRunner.$anonfun$copySessionState in executor stack trace  \nOperating system security updates.  \nNovember 20, 2020  \n[SPARK-33404][SQL][3.0] Fix incorrect results in date_trunc expression  \n[SPARK-33339][PYTHON] Pyspark application will hang due to non Exception error"
    },
    {
        "id": 1600,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-33404][SQL][3.0] Fix incorrect results in date_trunc expression  \n[SPARK-33339][PYTHON] Pyspark application will hang due to non Exception error  \n[SPARK-33183][SQL][HOTFIX] Fix Optimizer rule EliminateSorts and add a physical rule to remove redundant sorts  \n[SPARK-33371][PYTHON][3.0] Update setup.py and tests for Python 3.9  \n[SPARK-33391][SQL] element_at with CreateArray not respect one based index.  \n[SPARK-33306][SQL]Timezone is needed when cast date to string  \nNovember 3, 2020  \nUpgraded Java version from 1.8.0_252 to 1.8.0_265.  \nFix ABFS and WASB locking with regard to UserGroupInformation.getCurrentUser()  \nFix an infinite loop bug of Avro reader when reading the MAGIC bytes.  \nOctober 13, 2020  \nOperating system security updates.  \n[SPARK-32999][SQL] Use Utils.getSimpleName to avoid hitting Malformed class name in TreeNode  \nFixed listing directories in FUSE mount that contain file names with invalid XML characters  \nFUSE mount no longer uses ListMultipartUploads  \nSeptember 29, 2020  \n[SPARK-28863][SQL][WARMFIX] Introduce AlreadyOptimized to prevent reanalysis of V1FallbackWriters  \n[SPARK-32635][SQL] Fix foldable propagation  \nAdd a new config spark.shuffle.io.decoder.consolidateThreshold. Set the config value to Long.MAX_VALUE to skip the consolidation of netty FrameBuffers, which prevents java.lang.IndexOutOfBoundsException in corner cases.  \nSeptember 24, 2020  \n[SPARK-32764][SQL] -0.0 should be equal to 0.0  \n[SPARK-32753][SQL] Only copy tags to node with no tags when transforming plans  \n[SPARK-32659][SQL] Fix the data issue of inserted Dynamic Partition Pruning on non-atomic type  \nOperating system security updates."
    },
    {
        "id": 1601,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-32659][SQL] Fix the data issue of inserted Dynamic Partition Pruning on non-atomic type  \nOperating system security updates.  \nSeptember 8, 2020  \nA new parameter was created for Azure Synapse Analytics, maxbinlength. This parameter is used to control the column length of BinaryType columns, and is translated as VARBINARY(maxbinlength). It can be set using .option(\"maxbinlength\", n), where 0 < n <= 8000.  \nAugust 25, 2020  \n[SPARK-32159][SQL] Fix integration between Aggregator[Array[_], _, _] and UnresolvedMapObjects  \n[SPARK-32559][SQL] Fix the trim logic in UTF8String.toInt/toLong, which didn\u2019t handle non-ASCII characters correctly  \n[SPARK-32543][R] Remove arrow::as_tibble usage in SparkR  \n[SPARK-32091][CORE] Ignore timeout error when removing blocks on the lost executor  \nFixed an issue affecting Azure Synapse connector with MSI credentials  \nFixed ambiguous attribute resolution in self-merge  \nAugust 18, 2020  \n[SPARK-32594][SQL] Fix serialization of dates inserted to Hive tables  \n[SPARK-32237][SQL] Resolve hint in CTE  \n[SPARK-32431][SQL] Check duplicate nested columns in read from in-built datasources  \n[SPARK-32467][UI] Avoid encoding URL twice on https redirect  \nFixed a race condition in the SQS connector when using Trigger.Once.  \nAugust 11, 2020  \n[SPARK-32280][SPARK-32372][SQL] ResolveReferences.dedupRight should only rewrite attributes for ancestor nodes of the conflict plan  \n[SPARK-32234][SQL] Spark SQL commands are failing on selecting the ORC tables  \nAugust 3, 2020  \nYou can now use the LDA transform function on a passthrough-enabled cluster.  \nDatabricks Runtime 7.0 (EoS)  \nSee Databricks Runtime 7.0 (EoS).  \nFebruary 4, 2021"
    },
    {
        "id": 1602,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "You can now use the LDA transform function on a passthrough-enabled cluster.  \nDatabricks Runtime 7.0 (EoS)  \nSee Databricks Runtime 7.0 (EoS).  \nFebruary 4, 2021  \nFixed a regression that caused DBFS FUSE to fail to start when cluster environment variable configurations contain invalid bash syntax.  \nJanuary 20, 2021  \nFixed a regression in the January 12, 2021 maintenance release that can cause an incorrect AnalysisException and say the column is ambiguous in a self join. This regression happens when a user joins a DataFrame with its derived DataFrame (a so-called self-join) with the following conditions:  \nThese two DataFrames have common columns, but the output of the self join does not have common columns. For example, df.join(df.select($\"col\" as \"new_col\"), cond)  \nThe derived DataFrame excludes some columns via select, groupBy, or window.  \nThe join condition or the following transformation after the joined Dataframe refers to the non-common columns. For example, df.join(df.drop(\"a\"), df(\"a\") === 1)  \nJanuary 12, 2021  \nOperating system security updates.  \n[SPARK-33593][SQL] Vector reader got incorrect data with binary partition value  \n[SPARK-33677][SQL] Skip LikeSimplification rule if pattern contains any escapeChar  \n[SPARK-33071][SPARK-33536][SQL] Avoid changing dataset_id of LogicalPlan in join() to not break DetectAmbiguousSelfJoin  \nDecember 8, 2020  \n[SPARK-27421][SQL] Fix filter for int column and value class java.lang.String when pruning partition column  \n[SPARK-33404][SQL] Fix incorrect results in date_trunc expression  \n[SPARK-33339][PYTHON] Pyspark application will hang due to non Exception error  \n[SPARK-33183][SQL] Fix Optimizer rule EliminateSorts and add a physical rule to remove redundant sorts"
    },
    {
        "id": 1603,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-33339][PYTHON] Pyspark application will hang due to non Exception error  \n[SPARK-33183][SQL] Fix Optimizer rule EliminateSorts and add a physical rule to remove redundant sorts  \n[SPARK-33391][SQL] element_at with CreateArray not respect one based index.  \nOperating system security updates.  \nNovember 20, 2020  \n[SPARK-33306][SQL]Timezone is needed when cast date to string  \nNovember 3, 2020  \nUpgraded Java version from 1.8.0_252 to 1.8.0_265.  \nFix ABFS and WASB locking with regard to UserGroupInformation.getCurrentUser()  \nFix an infinite loop bug of Avro reader when reading the MAGIC bytes.  \nOctober 13, 2020  \nOperating system security updates.  \n[SPARK-32999][SQL] Use Utils.getSimpleName to avoid hitting Malformed class name in TreeNode  \nFixed listing directories in FUSE mount that contain file names with invalid XML characters  \nFUSE mount no longer uses ListMultipartUploads  \nSeptember 29, 2020  \n[SPARK-28863][SQL][WARMFIX] Introduce AlreadyOptimized to prevent reanalysis of V1FallbackWriters  \n[SPARK-32635][SQL] Fix foldable propagation  \nAdd a new config spark.shuffle.io.decoder.consolidateThreshold. Set the config value to Long.MAX_VALUE to skip the consolidation of netty FrameBuffers, which prevents java.lang.IndexOutOfBoundsException in corner cases.  \nSeptember 24, 2020  \n[SPARK-32764][SQL] -0.0 should be equal to 0.0  \n[SPARK-32753][SQL] Only copy tags to node with no tags when transforming plans  \n[SPARK-32659][SQL] Fix the data issue of inserted Dynamic Partition Pruning on non-atomic type  \nOperating system security updates.  \nSeptember 8, 2020"
    },
    {
        "id": 1604,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-32659][SQL] Fix the data issue of inserted Dynamic Partition Pruning on non-atomic type  \nOperating system security updates.  \nSeptember 8, 2020  \nA new parameter was created for Azure Synapse Analytics, maxbinlength. This parameter is used to control the column length of BinaryType columns, and is translated as VARBINARY(maxbinlength). It can be set using .option(\"maxbinlength\", n), where 0 < n <= 8000.  \nAugust 25, 2020  \n[SPARK-32159][SQL] Fix integration between Aggregator[Array[_], _, _] and UnresolvedMapObjects  \n[SPARK-32559][SQL] Fix the trim logic in UTF8String.toInt/toLong, which didn\u2019t handle non-ASCII characters correctly  \n[SPARK-32543][R] Remove arrow::as_tibble usage in SparkR  \n[SPARK-32091][CORE] Ignore timeout error when removing blocks on the lost executor  \nFixed an issue affecting Azure Synapse connector with MSI credentials  \nFixed ambiguous attribute resolution in self-merge  \nAugust 18, 2020  \n[SPARK-32594][SQL] Fix serialization of dates inserted to Hive tables  \n[SPARK-32237][SQL] Resolve hint in CTE  \n[SPARK-32431][SQL] Check duplicate nested columns in read from in-built datasources  \n[SPARK-32467][UI] Avoid encoding URL twice on https redirect  \nFixed a race condition in the SQS connector when using Trigger.Once.  \nAugust 11, 2020  \n[SPARK-32280][SPARK-32372][SQL] ResolveReferences.dedupRight should only rewrite attributes for ancestor nodes of the conflict plan  \n[SPARK-32234][SQL] Spark SQL commands are failing on selecting the ORC tables  \nYou can now use the LDA transform function on a passthrough-enabled cluster.  \nDatabricks Runtime 6.6 (EoS)  \nSee Databricks Runtime 6.6 (EoS).  \nNovember 20, 2020"
    },
    {
        "id": 1605,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "You can now use the LDA transform function on a passthrough-enabled cluster.  \nDatabricks Runtime 6.6 (EoS)  \nSee Databricks Runtime 6.6 (EoS).  \nNovember 20, 2020  \n[SPARK-33260][SQL] Fix incorrect results from SortExec when sortOrder is Stream  \n[SPARK-32635][SQL] Fix foldable propagation  \nNovember 3, 2020  \nUpgraded Java version from 1.8.0_252 to 1.8.0_265.  \nFix ABFS and WASB locking with regard to UserGroupInformation.getCurrentUser()  \nFix an infinite loop bug of Avro reader when reading the MAGIC bytes.  \nOctober 13, 2020  \nOperating system security updates.  \n[SPARK-32999][SQL][2.4] Use Utils.getSimpleName to avoid hitting Malformed class name in TreeNode  \nFixed listing directories in FUSE mount that contain file names with invalid XML characters  \nFUSE mount no longer uses ListMultipartUploads  \nSeptember 24, 2020  \nOperating system security updates.  \nSeptember 8, 2020  \nA new parameter was created for Azure Synapse Analytics, maxbinlength. This parameter is used to control the column length of BinaryType columns, and is translated as VARBINARY(maxbinlength). It can be set using .option(\"maxbinlength\", n), where 0 < n <= 8000.  \nUpdate Azure Storage SDK to 8.6.4 and enable TCP keep alive on connections made by the WASB driver  \nAugust 25, 2020  \nFixed ambiguous attribute resolution in self-merge  \nAugust 18, 2020  \n[SPARK-32431][SQL] Check duplicate nested columns in read from in-built datasources  \nFixed a race condition in the SQS connector when using Trigger.Once.  \nAugust 11, 2020  \n[SPARK-28676][CORE] Avoid Excessive logging from ContextCleaner  \n[SPARK-31967][UI] Downgrade to vis.js 4.21.0 to fix Jobs UI loading time regression  \nAugust 3, 2020  \nYou can now use the LDA transform function on a passthrough-enabled cluster.  \nOperating system security updates."
    },
    {
        "id": 1606,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "August 3, 2020  \nYou can now use the LDA transform function on a passthrough-enabled cluster.  \nOperating system security updates.  \nDatabricks Runtime 6.5 (EoS)  \nSee Databricks Runtime 6.5 (EoS).  \nSeptember 24, 2020  \nFixed a previous limitation where passthrough on standard cluster would still restrict the filesystem implementation user uses. Now users would be able to access local filesystems without restrictions.  \nOperating system security updates.  \nSeptember 8, 2020  \nA new parameter was created for Azure Synapse Analytics, maxbinlength. This parameter is used to control the column length of BinaryType columns, and is translated as VARBINARY(maxbinlength). It can be set using .option(\"maxbinlength\", n), where 0 < n <= 8000.  \nUpdate Azure Storage SDK to 8.6.4 and enable TCP keep alive on connections made by the WASB driver  \nAugust 25, 2020  \nFixed ambiguous attribute resolution in self-merge  \nAugust 18, 2020  \n[SPARK-32431][SQL] Check duplicate nested columns in read from in-built datasources  \nFixed a race condition in the SQS connector when using Trigger.Once.  \nAugust 11, 2020  \n[SPARK-28676][CORE] Avoid Excessive logging from ContextCleaner  \nAugust 3, 2020  \nYou can now use the LDA transform function on a passthrough-enabled cluster.  \nOperating system security updates.  \nJuly 7, 2020  \nUpgraded Java version from 1.8.0_242 to 1.8.0_252.  \nApril 21, 2020  \n[SPARK-31312][SQL] Cache Class instance for the UDF instance in HiveFunctionWrapper  \nDatabricks Runtime 6.3 (EoS)  \nSee Databricks Runtime 6.3 (EoS).  \nJuly 7, 2020  \nUpgraded Java version from 1.8.0_232 to 1.8.0_252.  \nApril 21, 2020  \n[SPARK-31312][SQL] Cache Class instance for the UDF instance in HiveFunctionWrapper  \nApril 7, 2020"
    },
    {
        "id": 1607,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "April 21, 2020  \n[SPARK-31312][SQL] Cache Class instance for the UDF instance in HiveFunctionWrapper  \nApril 7, 2020  \nTo resolve an issue with pandas udf not working with PyArrow 0.15.0 and above, we added an environment variable (ARROW_PRE_0_15_IPC_FORMAT=1) to enable support for those versions of PyArrow. See the instructions in [SPARK-29367].  \nMarch 10, 2020  \nThe Snowflake connector (spark-snowflake_2.11) included in Databricks Runtime is updated to version 2.5.9. snowflake-jdbc is updated to version 3.12.0.  \nFebruary 18, 2020  \nCredential passthrough with ADLS Gen2 has a performance degradation due to incorrect thread local handling when ADLS client prefetching is enabled. This release disables ADLS Gen2 prefetching when credential passthrough is enabled until we have a proper fix.  \nFebruary 11, 2020  \nFixed a bug in our S3 client (S3AFileSystem.java), whereby fs.isDirectory(path) or fs.getFileStatus(path).isDirectory() could sometimes incorrectly return false. The bug would manifest on paths for which aws s3 list-objects-v2 --prefix path/ --max-keys 1 --delimiter / responds with no keys or common prefixes, but isTruncated = true. This might happen for directories under which many objects were deleted and versioning was enabled.  \n[SPARK-24783][SQL] spark.sql.shuffle.partitions=0 should throw exception  \n[SPARK-30447][SQL] Constant propagation nullability issue  \n[SPARK-28152][SQL] Add a legacy conf for old MsSqlServerDialect numeric mapping  \nAllowlisted the overwrite function so that the MLModels extends MLWriter could call the function.  \nDatabricks Runtime 6.2 (EoS)  \nSee Databricks Runtime 6.2 (EoS).  \nApril 21, 2020  \n[SPARK-31312][SQL] Cache Class instance for the UDF instance in HiveFunctionWrapper"
    },
    {
        "id": 1608,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "See Databricks Runtime 6.2 (EoS).  \nApril 21, 2020  \n[SPARK-31312][SQL] Cache Class instance for the UDF instance in HiveFunctionWrapper  \nApril 7, 2020  \nTo resolve an issue with pandas udf not working with PyArrow 0.15.0 and above, we added an environment variable (ARROW_PRE_0_15_IPC_FORMAT=1) to enable support for those versions of PyArrow. See the instructions in [SPARK-29367].  \nMarch 25, 2020  \nJob output, such as log output emitted to stdout, is subject to a 20MB size limit. If the total output has a larger size, the run will be canceled and marked as failed. To avoid encountering this limit, you can prevent stdout from being returned from the driver to by setting the spark.databricks.driver.disableScalaOutput Spark configuration to true. By default the flag value is false. The flag controls cell output for Scala JAR jobs and Scala notebooks. If the flag is enabled, Spark does not return job execution results to the client. The flag does not affect the data that is written in the cluster\u2019s log files. Setting this flag is recommended only for automated clusters for JAR jobs, because it will disable notebook results.  \nMarch 10, 2020  \nThe Snowflake connector (spark-snowflake_2.11) included in Databricks Runtime is updated to version 2.5.9. snowflake-jdbc is updated to version 3.12.0.  \nFebruary 18, 2020  \n[SPARK-24783][SQL] spark.sql.shuffle.partitions=0 should throw exception  \nCredential passthrough with ADLS Gen2 has a performance degradation due to incorrect thread local handling when ADLS client prefetching is enabled. This release disables ADLS Gen2 prefetching when credential passthrough is enabled until we have a proper fix.  \nJanuary 28, 2020"
    },
    {
        "id": 1609,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "January 28, 2020  \nFixed a bug in S3AFileSystem, whereby fs.isDirectory(path) or fs.getFileStatus(path).isDirectory() could sometimes incorrectly return false. The bug would manifest on paths for which aws s3 list-objects-v2 --prefix path/ --max-keys 1 --delimiter / responds with no keys or common prefixes, but isTruncated = true. This might happen for directories under which many objects were deleted and versioning was enabled.  \nAllowlisted ML Model Writers\u2019 overwrite function for clusters enabled for credential passthrough, so that model save can use overwrite mode on credential passthrough clusters.  \n[SPARK-30447][SQL] Constant propagation nullability issue.  \n[SPARK-28152][SQL] Add a legacy conf for old MsSqlServerDialect numeric mapping.  \nJanuary 14, 2020  \nUpgraded Java version from 1.8.0_222 to 1.8.0_232.  \nDecember 10, 2019  \n[SPARK-29904][SQL] Parse timestamps in microsecond precision by JSON/CSV data sources.  \nDatabricks Runtime 6.1 (EoS)  \nSee Databricks Runtime 6.1 (EoS).  \nApril 7, 2020  \nTo resolve an issue with pandas udf not working with PyArrow 0.15.0 and above, we added an environment variable (ARROW_PRE_0_15_IPC_FORMAT=1) to enable support for those versions of PyArrow. See the instructions in [SPARK-29367].  \nMarch 25, 2020"
    },
    {
        "id": 1610,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "March 25, 2020  \nJob output, such as log output emitted to stdout, is subject to a 20MB size limit. If the total output has a larger size, the run will be canceled and marked as failed. To avoid encountering this limit, you can prevent stdout from being returned from the driver to by setting the spark.databricks.driver.disableScalaOutput Spark configuration to true. By default the flag value is false. The flag controls cell output for Scala JAR jobs and Scala notebooks. If the flag is enabled, Spark does not return job execution results to the client. The flag does not affect the data that is written in the cluster\u2019s log files. Setting this flag is recommended only for automated clusters for JAR jobs, because it will disable notebook results.  \nMarch 10, 2020  \nThe Snowflake connector (spark-snowflake_2.11) included in Databricks Runtime is updated to version 2.5.9. snowflake-jdbc is updated to version 3.12.0.  \nFebruary 18, 2020  \n[SPARK-24783][SQL] spark.sql.shuffle.partitions=0 should throw exception  \nCredential passthrough with ADLS Gen2 has a performance degradation due to incorrect thread local handling when ADLS client prefetching is enabled. This release disables ADLS Gen2 prefetching when credential passthrough is enabled until we have a proper fix.  \nJanuary 28, 2020  \nFixed a bug in S3AFileSystem, whereby fs.isDirectory(path) or fs.getFileStatus(path).isDirectory() could sometimes incorrectly return false. The bug would manifest on paths for which aws s3 list-objects-v2 --prefix path/ --max-keys 1 --delimiter / responds with no keys or common prefixes, but isTruncated = true. This might happen for directories under which many objects were deleted and versioning was enabled.  \n[SPARK-30447][SQL] Constant propagation nullability issue.  \n[SPARK-28152][SQL] Add a legacy conf for old MsSqlServerDialect numeric mapping.  \nJanuary 14, 2020  \nUpgraded Java version from 1.8.0_222 to 1.8.0_232."
    },
    {
        "id": 1611,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "January 14, 2020  \nUpgraded Java version from 1.8.0_222 to 1.8.0_232.  \nNovember 7, 2019  \n[SPARK-29743][SQL] sample should set needCopyResult to true if its child\u2019s needCopyResult is true.  \nSecrets referenced from Spark configuration properties and environment variables in Public Preview. See Use a secret in a Spark configuration property or environment variable.  \nNovember 5, 2019  \nFixed a bug in DBFS FUSE to handle mount points having // in its path.  \n[SPARK-29081] Replace calls to SerializationUtils.clone on properties with a faster implementation  \n[SPARK-29244][CORE] Prevent freed page in BytesToBytesMap free again  \n(6.1 ML) Library mkl version 2019.4 was installed unintentionally. We downgraded it to mkl version 2019.3 to match Anaconda Distribution 2019.03.  \nDatabricks Runtime 6.0 (EoS)  \nSee Databricks Runtime 6.0 (EoS).  \nMarch 25, 2020  \nJob output, such as log output emitted to stdout, is subject to a 20MB size limit. If the total output has a larger size, the run will be canceled and marked as failed. To avoid encountering this limit, you can prevent stdout from being returned from the driver to by setting the spark.databricks.driver.disableScalaOutput Spark configuration to true. By default the flag value is false. The flag controls cell output for Scala JAR jobs and Scala notebooks. If the flag is enabled, Spark does not return job execution results to the client. The flag does not affect the data that is written in the cluster\u2019s log files. Setting this flag is recommended only for automated clusters for JAR jobs, because it will disable notebook results.  \nFebruary 18, 2020  \nCredential passthrough with ADLS Gen2 has a performance degradation due to incorrect thread local handling when ADLS client prefetching is enabled. This release disables ADLS Gen2 prefetching when credential passthrough is enabled until we have a proper fix.  \nFebruary 11, 2020  \n[SPARK-24783][SQL] spark.sql.shuffle.partitions=0 should throw exception  \nJanuary 28, 2020"
    },
    {
        "id": 1612,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "February 11, 2020  \n[SPARK-24783][SQL] spark.sql.shuffle.partitions=0 should throw exception  \nJanuary 28, 2020  \nFixed a bug in S3AFileSystem, whereby fs.isDirectory(path) or fs.getFileStatus(path).isDirectory() could sometimes incorrectly return false. The bug would manifest on paths for which aws s3 list-objects-v2 --prefix path/ --max-keys 1 --delimiter / responds with no keys or common prefixes, but isTruncated = true. This might happen for directories under which many objects were deleted and versioning was enabled.  \n[SPARK-30447][SQL] Constant propagation nullability issue.  \n[SPARK-28152][SQL] Add a legacy conf for old MsSqlServerDialect numeric mapping.  \nJanuary 14, 2020  \nUpgraded Java version from 1.8.0_222 to 1.8.0_232.  \nNovember 19, 2019  \n[SPARK-29743] [SQL] sample should set needCopyResult to true if its child\u2019s needCopyResult is true  \nNovember 5, 2019  \nDBFS FUSE supports S3 mounts with canned ACL.  \ndbutils.tensorboard.start() now supports TensorBoard 2.0 (if installed manually).  \nFixed a bug in DBFS FUSE to handle mount points having // in its path.  \n[SPARK-29081]Replace calls to SerializationUtils.clone on properties with a faster implementation  \nOctober 23, 2019  \n[SPARK-29244][CORE] Prevent freed page in BytesToBytesMap free again  \nOctober 8, 2019  \nServer side changes to allow Simba Apache Spark ODBC driver to reconnect and continue after a connection failure during fetching results (requires Simba Apache Spark ODBC driver version 2.6.10).  \nFixed an issue affecting using Optimize command with table ACL enabled clusters.  \nFixed an issue where pyspark.ml libraries would fail due to Scala UDF forbidden error on table ACL enabled clusters.  \nFixed NullPointerException when checking error code in the WASB client.  \nDatabricks Runtime 5.4 ML (EoS)"
    },
    {
        "id": 1613,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fixed NullPointerException when checking error code in the WASB client.  \nDatabricks Runtime 5.4 ML (EoS)  \nSee Databricks Runtime 5.4 for ML (EoS).  \nJune 18, 2019  \nImproved handling of MLflow active runs in Hyperopt integration  \nImproved messages in Hyperopt  \nUpdated package Marchkdown from 3.1 to 3.1.1  \nDatabricks Runtime 5.4 (EoS)  \nSee Databricks Runtime 5.4 (EoS).  \nNovember 19, 2019  \n[SPARK-29743] [SQL] sample should set needCopyResult to true if its child\u2019s needCopyResult is true  \nOctober 8, 2019  \nServer side changes to allow Simba Apache Spark ODBC driver to reconnect and continue after a connection failure during fetching results (requires Simba Apache Spark ODBC driver update to version 2.6.10).  \nFixed NullPointerException when checking error code in the WASB client.  \nSeptember 10, 2019  \nAdd thread safe iterator to BytesToBytesMap  \nFixed a bug affecting certain global aggregation queries.  \n[SPARK-27330][SS] support task abort in foreach writer  \n[SPARK-28642]Hide credentials in SHOW CREATE TABLE  \n[SPARK-28699][SQL] Disable using radix sort for ShuffleExchangeExec in repartition case  \n[SPARK-28699][CORE] Fix a corner case for aborting indeterminate stage  \nAugust 27, 2019  \nFixed an issue affecting certain transform expressions  \nAugust 13, 2019  \nDelta streaming source should check the latest protocol of a table  \n[SPARK-28489][SS]Fix a bug that KafkaOffsetRangeCalculator.getRanges may drop offsets  \nJuly 30, 2019  \n[SPARK-28015][SQL] Check stringToDate() consumes entire input for the yyyy and yyyy-[m]m formats  \n[SPARK-28308][CORE] CalendarInterval sub-second part should be padded before parsing  \n[SPARK-27485]EnsureRequirements.reorder should handle duplicate expressions gracefully  \nJuly 2, 2019  \nUpgraded snappy-java from 1.1.7.1 to 1.1.7.3.  \nJune 18, 2019"
    },
    {
        "id": 1614,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "July 2, 2019  \nUpgraded snappy-java from 1.1.7.1 to 1.1.7.3.  \nJune 18, 2019  \nImproved handling of MLflow active runs in MLlib integration  \nImproved Databricks Advisor message related to using disk caching  \nFixed a bug affecting using higher order functions  \nFixed a bug affecting Delta metadata queries  \nDatabricks Runtime 5.3 (EoS)  \nSee Databricks Runtime 5.3 (EoS).  \nNovember 7, 2019  \n[SPARK-29743][SQL] sample should set needCopyResult to true if its child\u2019s needCopyResult is true  \nOctober 8, 2019  \nServer side changes to allow Simba Apache Spark ODBC driver to reconnect and continue after a connection failure during fetching results (requires Simba Apache Spark ODBC driver update to version 2.6.10).  \nFixed NullPointerException when checking error code in the WASB client.  \nSeptember 10, 2019  \nAdd thread safe iterator to BytesToBytesMap  \nFixed a bug affecting certain global aggregation queries.  \n[SPARK-27330][SS] support task abort in foreach writer  \n[SPARK-28642]Hide credentials in SHOW CREATE TABLE  \n[SPARK-28699][SQL] Disable using radix sort for ShuffleExchangeExec in repartition case  \n[SPARK-28699][CORE] Fix a corner case for aborting indeterminate stage  \nAugust 27, 2019  \nFixed an issue affecting certain transform expressions  \nAugust 13, 2019  \nDelta streaming source should check the latest protocol of a table  \n[SPARK-28489][SS]Fix a bug that KafkaOffsetRangeCalculator.getRanges may drop offsets  \nJuly 30, 2019  \n[SPARK-28015][SQL] Check stringToDate() consumes entire input for the yyyy and yyyy-[m]m formats  \n[SPARK-28308][CORE] CalendarInterval sub-second part should be padded before parsing  \n[SPARK-27485]EnsureRequirements.reorder should handle duplicate expressions gracefully  \nJune 18, 2019  \nImproved Databricks Advisor message related to using disk caching  \nFixed a bug affecting using higher order functions  \nFixed a bug affecting Delta metadata queries  \nMay 28, 2019"
    },
    {
        "id": 1615,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "June 18, 2019  \nImproved Databricks Advisor message related to using disk caching  \nFixed a bug affecting using higher order functions  \nFixed a bug affecting Delta metadata queries  \nMay 28, 2019  \nImproved the stability of Delta  \nTolerate IOExceptions when reading Delta LAST_CHECKPOINT file  \nAdded recovery to failed library installation  \nMay 7, 2019  \nPort HADOOP-15778 (ABFS: Fix client side throttling for read) to Azure Data Lake Storage Gen2 connector  \nPort HADOOP-16040 (ABFS: Bug fix for tolerateOobAppends configuration) to Azure Data Lake Storage Gen2 connector  \nFixed a bug affecting table ACLs  \nRenamed fs.s3a.requesterPays.enabled to fs.s3a.requester-pays.enabled  \nFixed a race condition when loading a Delta log checksum file  \nFixed Delta conflict detection logic to not identify \u201cinsert + overwrite\u201d as pure \u201cappend\u201d operation  \nFixed a bug affecting Amazon Kinesis connector  \nEnsure that disk caching is not disabled when table ACLs are enabled  \n[SPARK-27494][SS] Null keys/values don\u2019t work in Kafka source v2  \n[SPARK-27446][R] Use existing spark conf if available.  \n[SPARK-27454][SPARK-27454][ML][SQL] Spark image datasource fail when encounter some illegal images  \n[SPARK-27160][SQL] Fix DecimalType when building orc filters  \n[SPARK-27338][CORE] Fix deadlock between UnsafeExternalSorter and TaskMemoryManager  \nDatabricks Runtime 5.2 (EoS)  \nSee Databricks Runtime 5.2 (EoS).  \nSeptember 10, 2019  \nAdd thread safe iterator to BytesToBytesMap  \nFixed a bug affecting certain global aggregation queries.  \n[SPARK-27330][SS] support task abort in foreach writer  \n[SPARK-28642]Hide credentials in SHOW CREATE TABLE  \n[SPARK-28699][SQL] Disable using radix sort for ShuffleExchangeExec in repartition case  \n[SPARK-28699][CORE] Fix a corner case for aborting indeterminate stage  \nAugust 27, 2019  \nFixed an issue affecting certain transform expressions  \nAugust 13, 2019"
    },
    {
        "id": 1616,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-28699][CORE] Fix a corner case for aborting indeterminate stage  \nAugust 27, 2019  \nFixed an issue affecting certain transform expressions  \nAugust 13, 2019  \nDelta streaming source should check the latest protocol of a table  \n[SPARK-28489][SS]Fix a bug that KafkaOffsetRangeCalculator.getRanges may drop offsets  \nJuly 30, 2019  \n[SPARK-28015][SQL] Check stringToDate() consumes entire input for the yyyy and yyyy-[m]m formats  \n[SPARK-28308][CORE] CalendarInterval sub-second part should be padded before parsing  \n[SPARK-27485]EnsureRequirements.reorder should handle duplicate expressions gracefully  \nJuly 2, 2019  \nTolerate IOExceptions when reading Delta LAST_CHECKPOINT file  \nJune 18, 2019  \nImproved Databricks Advisor message related to using disk cache  \nFixed a bug affecting using higher order functions  \nFixed a bug affecting Delta metadata queries  \nMay 28, 2019  \nAdded recovery to failed library installation  \nMay 7, 2019  \nPort HADOOP-15778 (ABFS: Fix client side throttling for read) to Azure Data Lake Storage Gen2 connector  \nPort HADOOP-16040 (ABFS: Bug fix for tolerateOobAppends configuration) to Azure Data Lake Storage Gen2 connector  \nFixed a race condition when loading a Delta log checksum file  \nFixed Delta conflict detection logic to not identify \u201cinsert + overwrite\u201d as pure \u201cappend\u201d operation  \nFixed a bug affecting Amazon Kinesis connector  \nEnsure that disk caching is not disabled when table ACLs are enabled  \n[SPARK-27494][SS] Null keys/values don\u2019t work in Kafka source v2  \n[SPARK-27454][SPARK-27454][ML][SQL] Spark image datasource fail when encounter some illegal images  \n[SPARK-27160][SQL] Fix DecimalType when building orc filters  \n[SPARK-27338][CORE] Fix deadlock between UnsafeExternalSorter and TaskMemoryManager  \nMarch 26, 2019  \nAvoid embedding platform-dependent offsets literally in whole-stage generated code"
    },
    {
        "id": 1617,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-27338][CORE] Fix deadlock between UnsafeExternalSorter and TaskMemoryManager  \nMarch 26, 2019  \nAvoid embedding platform-dependent offsets literally in whole-stage generated code  \n[SPARK-26665][CORE] Fix a bug that BlockTransferService.fetchBlockSync may hang forever.  \n[SPARK-27134][SQL] array_distinct function does not work correctly with columns containing array of array.  \n[SPARK-24669][SQL] Invalidate tables in case of DROP DATABASE CASCADE.  \n[SPARK-26572][SQL] fix aggregate codegen result evaluation.  \nFixed a bug affecting certain PythonUDFs.  \nFebruary 26, 2019  \n[SPARK-26864][SQL] Query may return incorrect result when python udf is used as a left-semi join condition.  \n[SPARK-26887][PYTHON] Create datetime.date directly instead of creating datetime64 as intermediate data.  \nFixed a bug affecting JDBC/ODBC server.  \nFixed a bug affecting PySpark.  \nExclude the hidden files when building HadoopRDD.  \nFixed a bug in Delta that caused serialization issues.  \nFebruary 12, 2019  \nFixed an issue affecting using Delta with Azure ADLS Gen2 mount points.  \nFixed an issue that Spark low level network protocol may be broken when sending large RPC error messages with encryption enabled (in HIPAA compliance features) or when spark.network.crypto.enabled is set to true).  \nJanuary 30, 2019  \nFixed the StackOverflowError when putting skew join hint on cached relation.  \nFixed the inconsistency between a SQL cache\u2019s cached RDD and its physical plan, which causes incorrect result.  \n[SPARK-26706][SQL] Fix illegalNumericPrecedence for ByteType.  \n[SPARK-26709][SQL] OptimizeMetadataOnlyQuery does not handle empty records correctly.  \nCSV/JSON data sources should avoid globbing paths when inferring schema.  \nFixed constraint inference on Window operator.  \nFixed an issue affecting installing egg libraries with clusters having table ACL enabled.  \nDatabricks Runtime 5.1 (EoS)"
    },
    {
        "id": 1618,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fixed constraint inference on Window operator.  \nFixed an issue affecting installing egg libraries with clusters having table ACL enabled.  \nDatabricks Runtime 5.1 (EoS)  \nSee Databricks Runtime 5.1 (EoS).  \nAugust 13, 2019  \nDelta streaming source should check the latest protocol of a table  \n[SPARK-28489][SS]Fix a bug that KafkaOffsetRangeCalculator.getRanges may drop offsets  \nJuly 30, 2019  \n[SPARK-28015][SQL] Check stringToDate() consumes entire input for the yyyy and yyyy-[m]m formats  \n[SPARK-28308][CORE] CalendarInterval sub-second part should be padded before parsing  \n[SPARK-27485]EnsureRequirements.reorder should handle duplicate expressions gracefully  \nJuly 2, 2019  \nTolerate IOExceptions when reading Delta LAST_CHECKPOINT file  \nJune 18, 2019  \nFixed a bug affecting using higher order functions  \nFixed a bug affecting Delta metadata queries  \nMay 28, 2019  \nAdded recovery to failed library installation  \nMay 7, 2019  \nPort HADOOP-15778 (ABFS: Fix client side throttling for read) to Azure Data Lake Storage Gen2 connector  \nPort HADOOP-16040 (ABFS: Bug fix for tolerateOobAppends configuration) to Azure Data Lake Storage Gen2 connector  \nFixed a race condition when loading a Delta log checksum file  \nFixed Delta conflict detection logic to not identify \u201cinsert + overwrite\u201d as pure \u201cappend\u201d operation  \n[SPARK-27494][SS] Null keys/values don\u2019t work in Kafka source v2  \n[SPARK-27454][SPARK-27454][ML][SQL] Spark image datasource fail when encounter some illegal images  \n[SPARK-27160][SQL] Fix DecimalType when building orc filters  \n[SPARK-27338][CORE] Fix deadlock between UnsafeExternalSorter and TaskMemoryManager  \nMarch 26, 2019  \nAvoid embedding platform-dependent offsets literally in whole-stage generated code  \nFixed a bug affecting certain PythonUDFs.  \nFebruary 26, 2019"
    },
    {
        "id": 1619,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "March 26, 2019  \nAvoid embedding platform-dependent offsets literally in whole-stage generated code  \nFixed a bug affecting certain PythonUDFs.  \nFebruary 26, 2019  \n[SPARK-26864][SQL] Query may return incorrect result when python udf is used as a left-semi join condition.  \nFixed a bug affecting JDBC/ODBC server.  \nExclude the hidden files when building HadoopRDD.  \nFebruary 12, 2019  \nFixed an issue affecting installing egg libraries with clusters having table ACL enabled.  \nFixed the inconsistency between a SQL cache\u2019s cached RDD and its physical plan, which causes incorrect result.  \n[SPARK-26706][SQL] Fix illegalNumericPrecedence for ByteType.  \n[SPARK-26709][SQL] OptimizeMetadataOnlyQuery does not handle empty records correctly.  \nFixed constraint inference on Window operator.  \nFixed an issue that Spark low level network protocol may be broken when sending large RPC error messages with encryption enabled (in HIPAA compliance features) or when spark.network.crypto.enabled is set to true).  \nJanuary 30, 2019  \nFixed an issue that can cause df.rdd.count() with UDT to return incorrect answer for certain cases.  \nFixed an issue affecting installing wheelhouses.  \n[SPARK-26267]Retry when detecting incorrect offsets from Kafka.  \nFixed a bug that affects multiple file stream sources in a streaming query.  \nFixed the StackOverflowError when putting skew join hint on cached relation.  \nFixed the inconsistency between a SQL cache\u2019s cached RDD and its physical plan, which causes incorrect result.  \nJanuary 8, 2019  \nFixed issue that causes the error org.apache.spark.sql.expressions.Window.rangeBetween(long,long) is not whitelisted.  \n[SPARK-26352]join reordering should not change the order of output attributes.  \n[SPARK-26366]ReplaceExceptWithFilter should consider NULL as False.  \nStability improvement for Delta Lake.  \nDelta Lake is enabled.  \nDatabricks IO Cache is enabled for the IO Cache Accelerated instance type.  \nDatabricks Runtime 5.0 (EoS)  \nSee Databricks Runtime 5.0 (EoS).  \nJune 18, 2019"
    },
    {
        "id": 1620,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Databricks IO Cache is enabled for the IO Cache Accelerated instance type.  \nDatabricks Runtime 5.0 (EoS)  \nSee Databricks Runtime 5.0 (EoS).  \nJune 18, 2019  \nFixed a bug affecting using higher order functions  \nMay 7, 2019  \nFixed a race condition when loading a Delta log checksum file  \nFixed Delta conflict detection logic to not identify \u201cinsert + overwrite\u201d as pure \u201cappend\u201d operation  \n[SPARK-27494][SS] Null keys/values don\u2019t work in Kafka source v2  \n[SPARK-27454][SPARK-27454][ML][SQL] Spark image datasource fail when encounter some illegal images  \n[SPARK-27160][SQL] Fix DecimalType when building orc filters  \n[SPARK-27338][CORE] Fix deadlock between UnsafeExternalSorter and TaskMemoryManager  \nMarch 26, 2019  \nAvoid embedding platform-dependent offsets literally in whole-stage generated code  \nFixed a bug affecting certain PythonUDFs.  \nMarch 12, 2019  \n[SPARK-26864][SQL] Query may return incorrect result when python udf is used as a left-semi join condition.  \nFebruary 26, 2019  \nFixed a bug affecting JDBC/ODBC server.  \nExclude the hidden files when building HadoopRDD.  \nFebruary 12, 2019  \nFixed the inconsistency between a SQL cache\u2019s cached RDD and its physical plan, which causes incorrect result.  \n[SPARK-26706][SQL] Fix illegalNumericPrecedence for ByteType.  \n[SPARK-26709][SQL] OptimizeMetadataOnlyQuery does not handle empty records correctly.  \nFixed constraint inference on Window operator.  \nFixed an issue that Spark low level network protocol may be broken when sending large RPC error messages with encryption enabled (in HIPAA compliance features) or when spark.network.crypto.enabled is set to true).  \nJanuary 30, 2019  \nFixed an issue that can cause df.rdd.count() with UDT to return incorrect answer for certain cases.  \n[SPARK-26267]Retry when detecting incorrect offsets from Kafka."
    },
    {
        "id": 1621,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "January 30, 2019  \nFixed an issue that can cause df.rdd.count() with UDT to return incorrect answer for certain cases.  \n[SPARK-26267]Retry when detecting incorrect offsets from Kafka.  \nFixed a bug that affects multiple file stream sources in a streaming query.  \nFixed the StackOverflowError when putting skew join hint on cached relation.  \nFixed the inconsistency between a SQL cache\u2019s cached RDD and its physical plan, which causes incorrect result.  \nJanuary 8, 2019  \nFixed issue that caused the error org.apache.spark.sql.expressions.Window.rangeBetween(long,long) is not whitelisted.  \n[SPARK-26352]join reordering should not change the order of output attributes.  \n[SPARK-26366]ReplaceExceptWithFilter should consider NULL as False.  \nStability improvement for Delta Lake.  \nDelta Lake is enabled.  \nDatabricks IO Cache is enabled for the IO Cache Accelerated instance type.  \nDecember 18, 2018  \n[SPARK-26293]Cast exception when having Python UDF in subquery  \nFixed an issue affecting certain queries using Join and Limit.  \nRedacted credentials from RDD names in Spark UI  \nDecember 6, 2018  \nFixed an issue that caused incorrect query result when using orderBy followed immediately by groupBy with group-by key as the leading part of the sort-by key.  \nUpgraded Snowflake Connector for Spark from 2.4.9.2-spark_2.4_pre_release to 2.4.10.  \nOnly ignore corrupt files after one or more retries when spark.sql.files.ignoreCorruptFiles or spark.sql.files.ignoreMissingFiles flag is enabled.  \nFixed an issue affecting certain self union queries.  \nFixed a bug with the thrift server where sessions are sometimes leaked when cancelled.  \n[SPARK-26307]Fixed CTAS when INSERT a partitioned table using Hive SerDe.  \n[SPARK-26147]Python UDFs in join condition fail even when using columns from only one side of join  \n[SPARK-26211]Fix InSet for binary, and struct and array with null.  \n[SPARK-26181]the hasMinMaxStats method of ColumnStatsMap is not correct.  \nFixed an issue affecting installing Python Wheels in environments without Internet access."
    },
    {
        "id": 1622,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-26181]the hasMinMaxStats method of ColumnStatsMap is not correct.  \nFixed an issue affecting installing Python Wheels in environments without Internet access.  \nNovember 20, 2018  \nFixed an issue that caused a notebook not usable after cancelling a streaming query.  \nFixed an issue affecting certain queries using window functions.  \nFixed an issue affecting a stream from Delta with multiple schema changes.  \nFixed an issue affecting certain aggregation queries with Left Semi/Anti joins.  \nFixed an issue affecting reading timestamp columns from Redshift.  \nDatabricks Runtime 4.3 (EoS)  \nSee Databricks Runtime 4.3 (EoS).  \nApril 9, 2019  \n[SPARK-26665][CORE] Fix a bug that can cause BlockTransferService.fetchBlockSync to hang forever.  \n[SPARK-24669][SQL] Invalidate tables in case of DROP DATABASE CASCADE.  \nMarch 12, 2019  \nFixed a bug affecting code generation.  \nFixed a bug affecting Delta.  \nFebruary 26, 2019  \nFixed a bug affecting JDBC/ODBC server.  \nFebruary 12, 2019  \n[SPARK-26709][SQL] OptimizeMetadataOnlyQuery does not handle empty records correctly.  \nExcluding the hidden files when building HadoopRDD.  \nFixed Parquet Filter Conversion for IN predicate when its value is empty.  \nFixed an issue that Spark low level network protocol may be broken when sending large RPC error messages with encryption enabled (in HIPAA compliance features) or when spark.network.crypto.enabled is set to true).  \nJanuary 30, 2019  \nFixed an issue that can cause df.rdd.count() with UDT to return incorrect answer for certain cases.  \nFixed the inconsistency between a SQL cache\u2019s cached RDD and its physical plan, which causes incorrect result.  \nJanuary 8, 2019  \nFixed the issue that causes the error org.apache.spark.sql.expressions.Window.rangeBetween(long,long) is not whitelisted.  \nRedacted credentials from RDD names in Spark UI  \n[SPARK-26352]join reordering should not change the order of output attributes.  \n[SPARK-26366]ReplaceExceptWithFilter should consider NULL as False.  \nDelta Lake is enabled."
    },
    {
        "id": 1623,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-26352]join reordering should not change the order of output attributes.  \n[SPARK-26366]ReplaceExceptWithFilter should consider NULL as False.  \nDelta Lake is enabled.  \nDatabricks IO Cache is enabled for the IO Cache Accelerated instance type.  \nDecember 18, 2018  \n[SPARK-25002]Avro: revise the output record namespace.  \nFixed an issue affecting certain queries using Join and Limit.  \n[SPARK-26307]Fixed CTAS when INSERT a partitioned table using Hive SerDe.  \nOnly ignore corrupt files after one or more retries when spark.sql.files.ignoreCorruptFiles or spark.sql.files.ignoreMissingFiles flag is enabled.  \n[SPARK-26181]the hasMinMaxStats method of ColumnStatsMap is not correct.  \nFixed an issue affecting installing Python Wheels in environments without Internet access.  \nFixed a performance issue in query analyzer.  \nFixed an issue in PySpark that caused DataFrame actions failed with \u201cconnection refused\u201d error.  \nFixed an issue affecting certain self union queries.  \nNovember 20, 2018  \n[SPARK-17916][SPARK-25241]Fix empty string being parsed as null when nullValue is set.  \n[SPARK-25387]Fix for NPE caused by bad CSV input.  \nFixed an issue affecting certain aggregation queries with Left Semi/Anti joins.  \nFixed an issue affecting reading timestamp columns from Redshift.  \nNovember 6, 2018  \n[SPARK-25741]Long URLs are not rendered properly in web UI.  \n[SPARK-25714]Fix Null Handling in the Optimizer rule BooleanSimplification.  \nFixed an issue affecting temporary objects cleanup in Synapse Analytics connector.  \n[SPARK-25816]Fix attribute resolution in nested extractors.  \nOctober 9, 2018  \nFixed a bug affecting the output of running SHOW CREATE TABLE on Delta tables.  \nFixed a bug affecting Union operation.  \nSeptember 25, 2018  \n[SPARK-25368][SQL] Incorrect constraint inference returns wrong result.  \n[SPARK-25402][SQL] Null handling in BooleanSimplification.  \nFixed NotSerializableException in Avro data source.  \nSeptember 11, 2018"
    },
    {
        "id": 1624,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-25402][SQL] Null handling in BooleanSimplification.  \nFixed NotSerializableException in Avro data source.  \nSeptember 11, 2018  \n[SPARK-25214][SS] Fix the issue that Kafka v2 source may return duplicated records when failOnDataLoss=false.  \n[SPARK-24987][SS] Fix Kafka consumer leak when no new offsets for articlePartition.  \nFilter reduction should handle null value correctly.  \nImproved stability of execution engine.  \nAugust 28, 2018  \nFixed a bug in Delta Lake Delete command that would incorrectly delete the rows where the condition evaluates to null.  \n[SPARK-25142]Add error messages when Python worker could not open socket in _load_from_socket.  \nAugust 23, 2018  \n[SPARK-23935]mapEntry throws org.codehaus.commons.compiler.CompileException.  \nFixed nullable map issue in Parquet reader.  \n[SPARK-25051][SQL] FixNullability should not stop on AnalysisBarrier.  \n[SPARK-25081]Fixed a bug where ShuffleExternalSorter may access a released memory page when spilling fails to allocate memory.  \nFixed an interaction between Databricks Delta and Pyspark which could cause transient read failures.  \n[SPARK-25084]\u201ddistribute by\u201d on multiple columns (wrap in brackets) may lead to codegen issue.  \n[SPARK-25096]Loosen nullability if the cast is force-nullable.  \nLowered the default number of threads used by the Delta Lake Optimize command, reducing memory overhead and committing data faster.  \n[SPARK-25114]Fix RecordBinaryComparator when subtraction between two words is divisible by Integer.MAX_VALUE.  \nFixed secret manager redaction when command partially succeed.  \nDatabricks Runtime 4.2 (EoS)  \nSee Databricks Runtime 4.2 (EoS).  \nFebruary 26, 2019  \nFixed a bug affecting JDBC/ODBC server.  \nFebruary 12, 2019  \n[SPARK-26709][SQL] OptimizeMetadataOnlyQuery does not handle empty records correctly.  \nExcluding the hidden files when building HadoopRDD."
    },
    {
        "id": 1625,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "February 12, 2019  \n[SPARK-26709][SQL] OptimizeMetadataOnlyQuery does not handle empty records correctly.  \nExcluding the hidden files when building HadoopRDD.  \nFixed Parquet Filter Conversion for IN predicate when its value is empty.  \nFixed an issue that Spark low level network protocol may be broken when sending large RPC error messages with encryption enabled (in HIPAA compliance features) or when spark.network.crypto.enabled is set to true).  \nJanuary 30, 2019  \nFixed an issue that can cause df.rdd.count() with UDT to return incorrect answer for certain cases.  \nJanuary 8, 2019  \nFixed issue that causes the error org.apache.spark.sql.expressions.Window.rangeBetween(long,long) is not whitelisted.  \nRedacted credentials from RDD names in Spark UI  \n[SPARK-26352]join reordering should not change the order of output attributes.  \n[SPARK-26366]ReplaceExceptWithFilter should consider NULL as False.  \nDelta Lake is enabled.  \nDatabricks IO Cache is enabled for the IO Cache Accelerated instance type.  \nDecember 18, 2018  \n[SPARK-25002]Avro: revise the output record namespace.  \nFixed an issue affecting certain queries using Join and Limit.  \n[SPARK-26307]Fixed CTAS when INSERT a partitioned table using Hive SerDe.  \nOnly ignore corrupt files after one or more retries when spark.sql.files.ignoreCorruptFiles or spark.sql.files.ignoreMissingFiles flag is enabled.  \n[SPARK-26181]the hasMinMaxStats method of ColumnStatsMap is not correct.  \nFixed an issue affecting installing Python Wheels in environments without Internet access.  \nFixed a performance issue in query analyzer.  \nFixed an issue in PySpark that caused DataFrame actions failed with \u201cconnection refused\u201d error.  \nFixed an issue affecting certain self union queries.  \nNovember 20, 2018  \n[SPARK-17916][SPARK-25241]Fix empty string being parsed as null when nullValue is set.  \nFixed an issue affecting certain aggregation queries with Left Semi/Anti joins.  \nFixed an issue affecting reading timestamp columns from Redshift.  \nNovember 6, 2018"
    },
    {
        "id": 1626,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fixed an issue affecting certain aggregation queries with Left Semi/Anti joins.  \nFixed an issue affecting reading timestamp columns from Redshift.  \nNovember 6, 2018  \n[SPARK-25741]Long URLs are not rendered properly in web UI.  \n[SPARK-25714]Fix Null Handling in the Optimizer rule BooleanSimplification.  \nOctober 9, 2018  \nFixed a bug affecting the output of running SHOW CREATE TABLE on Delta tables.  \nFixed a bug affecting Union operation.  \nSeptember 25, 2018  \n[SPARK-25368][SQL] Incorrect constraint inference returns wrong result.  \n[SPARK-25402][SQL] Null handling in BooleanSimplification.  \nFixed NotSerializableException in Avro data source.  \nSeptember 11, 2018  \n[SPARK-25214][SS] Fix the issue that Kafka v2 source may return duplicated records when failOnDataLoss=false.  \n[SPARK-24987][SS] Fix Kafka consumer leak when no new offsets for articlePartition.  \nFilter reduction should handle null value correctly.  \nAugust 28, 2018  \nFixed a bug in Delta Lake Delete command that would incorrectly delete the rows where the condition evaluates to null.  \nAugust 23, 2018  \nFixed NoClassDefError for Delta Snapshot  \n[SPARK-23935]mapEntry throws org.codehaus.commons.compiler.CompileException.  \n[SPARK-24957][SQL] Average with decimal followed by aggregation returns wrong result. The incorrect results of AVERAGE might be returned. The CAST added in the Average operator will be bypassed if the result of Divide is the same type which it is casted to.  \n[SPARK-25081]Fixed a bug where ShuffleExternalSorter may access a released memory page when spilling fails to allocate memory.  \nFixed an interaction between Databricks Delta and Pyspark which could cause transient read failures.  \n[SPARK-25114]Fix RecordBinaryComparator when subtraction between two words is divisible by Integer.MAX_VALUE.  \n[SPARK-25084]\u201ddistribute by\u201d on multiple columns (wrap in brackets) may lead to codegen issue."
    },
    {
        "id": 1627,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "[SPARK-25084]\u201ddistribute by\u201d on multiple columns (wrap in brackets) may lead to codegen issue.  \n[SPARK-24934][SQL] Explicitly allowlist supported types in upper/lower bounds for in-memory partition pruning. When complex data types are used in query filters against cached data, Spark always returns an empty result set. The in-memory stats-based pruning generates incorrect results, because null is set for upper/lower bounds for complex types. The fix is to not use in-memory stats-based pruning for complex types.  \nFixed secret manager redaction when command partially succeed.  \nFixed nullable map issue in Parquet reader.  \nAugust 2, 2018  \nAdded writeStream.table API in Python.  \nFixed an issue affecting Delta checkpointing.  \n[SPARK-24867][SQL] Add AnalysisBarrier to DataFrameWriter. SQL cache is not being used when using DataFrameWriter to write a DataFrame with UDF. This is a regression caused by the changes we made in AnalysisBarrier, since not all the Analyzer rules are idempotent.  \nFixed an issue that could cause mergeInto command to produce incorrect results.  \nImproved stability on accessing Azure Data Lake Storage Gen1.  \n[SPARK-24809]Serializing LongHashedRelation in executor may result in data error.  \n[SPARK-24878][SQL] Fix reverse function for array type of primitive type containing null.  \nJuly 11, 2018  \nFixed a bug in query execution that would cause aggregations on decimal columns with different precisions to return incorrect results in some cases.  \nFixed a NullPointerException bug that was thrown during advanced aggregation operations like grouping sets.  \nDatabricks Runtime 4.1 ML (EoS)  \nSee Databricks Runtime 4.1 ML (EoS).  \nJuly 31, 2018  \nAdded Azure Synapse Analytics to ML Runtime 4.1  \nFixed a bug that could cause incorrect query results when the name of a partition column used in a predicate differs from the case of that column in the schema of the table.  \nFixed a bug affecting Spark SQL execution engine.  \nFixed a bug affecting code generation."
    },
    {
        "id": 1628,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fixed a bug affecting Spark SQL execution engine.  \nFixed a bug affecting code generation.  \nFixed a bug (java.lang.NoClassDefFoundError) affecting Delta Lake.  \nImproved error handling in Delta Lake.  \nFixed a bug that caused incorrect data skipping statistics to be collected for string columns 32 characters or greater.  \nDatabricks Runtime 4.1 (EoS)  \nSee Databricks Runtime 4.1 (EoS).  \nJanuary 8, 2019  \n[SPARK-26366]ReplaceExceptWithFilter should consider NULL as False.  \nDelta Lake is enabled.  \nDecember 18, 2018  \n[SPARK-25002]Avro: revise the output record namespace.  \nFixed an issue affecting certain queries using Join and Limit.  \n[SPARK-26307]Fixed CTAS when INSERT a partitioned table using Hive SerDe.  \nOnly ignore corrupt files after one or more retries when spark.sql.files.ignoreCorruptFiles or spark.sql.files.ignoreMissingFiles flag is enabled.  \nFixed an issue affecting installing Python Wheels in environments without Internet access.  \nFixed an issue in PySpark that caused DataFrame actions failed with \u201cconnection refused\u201d error.  \nFixed an issue affecting certain self union queries.  \nNovember 20, 2018  \n[SPARK-17916][SPARK-25241]Fix empty string being parsed as null when nullValue is set.  \nFixed an issue affecting certain aggregation queries with Left Semi/Anti joins.  \nNovember 6, 2018  \n[SPARK-25741]Long URLs are not rendered properly in web UI.  \n[SPARK-25714]Fix Null Handling in the Optimizer rule BooleanSimplification.  \nOctober 9, 2018  \nFixed a bug affecting the output of running SHOW CREATE TABLE on Delta tables.  \nFixed a bug affecting Union operation.  \nSeptember 25, 2018  \n[SPARK-25368][SQL] Incorrect constraint inference returns wrong result.  \n[SPARK-25402][SQL] Null handling in BooleanSimplification.  \nFixed NotSerializableException in Avro data source.  \nSeptember 11, 2018  \n[SPARK-25214][SS] Fix the issue that Kafka v2 source may return duplicated records when failOnDataLoss=false."
    },
    {
        "id": 1629,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "September 11, 2018  \n[SPARK-25214][SS] Fix the issue that Kafka v2 source may return duplicated records when failOnDataLoss=false.  \n[SPARK-24987][SS] Fix Kafka consumer leak when no new offsets for articlePartition.  \nFilter reduction should handle null value correctly.  \nAugust 28, 2018  \nFixed a bug in Delta Lake Delete command that would incorrectly delete the rows where the condition evaluates to null.  \n[SPARK-25084]\u201ddistribute by\u201d on multiple columns (wrap in brackets) may lead to codegen issue.  \n[SPARK-25114]Fix RecordBinaryComparator when subtraction between two words is divisible by Integer.MAX_VALUE.  \nAugust 23, 2018  \nFixed NoClassDefError for Delta Snapshot.  \n[SPARK-24957][SQL] Average with decimal followed by aggregation returns wrong result. The incorrect results of AVERAGE might be returned. The CAST added in the Average operator will be bypassed if the result of Divide is the same type which it is casted to.  \nFixed nullable map issue in Parquet reader.  \n[SPARK-24934][SQL] Explicitly allowlist supported types in upper/lower bounds for in-memory partition pruning. When complex data types are used in query filters against cached data, Spark always returns an empty result set. The in-memory stats-based pruning generates incorrect results, because null is set for upper/lower bounds for complex types. The fix is to not use in-memory stats-based pruning for complex types.  \n[SPARK-25081]Fixed a bug where ShuffleExternalSorter may access a released memory page when spilling fails to allocate memory.  \nFixed an interaction between Databricks Delta and Pyspark which could cause transient read failures.  \nFixed secret manager redaction when command partially succeed  \nAugust 2, 2018  \n[SPARK-24613][SQL] Cache with UDF could not be matched with subsequent dependent caches. Wraps the logical plan with a AnalysisBarrier for execution plan compilation in CacheManager, in order to avoid the plan being analyzed again. This is also a regression of Spark 2.3."
    },
    {
        "id": 1630,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fixed a Synapse Analytics connector issue affecting timezone conversion for writing DateType data.  \nFixed an issue affecting Delta checkpointing.  \nFixed an issue that could cause mergeInto command to produce incorrect results.  \n[SPARK-24867][SQL] Add AnalysisBarrier to DataFrameWriter. SQL cache is not being used when using DataFrameWriter to write a DataFrame with UDF. This is a regression caused by the changes we made in AnalysisBarrier, since not all the Analyzer rules are idempotent.  \n[SPARK-24809]Serializing LongHashedRelation in executor may result in data error.  \nJuly 11, 2018  \nFixed a bug in query execution that would cause aggregations on decimal columns with different precisions to return incorrect results in some cases.  \nFixed a NullPointerException bug that was thrown during advanced aggregation operations like grouping sets.  \nJune 28, 2018  \nFixed a bug that could cause incorrect query results when the name of a partition column used in a predicate differs from the case of that column in the schema of the table.  \nMay 29, 2018  \nFixed a bug affecting Spark SQL execution engine.  \nFixed a bug affecting code generation.  \nFixed a bug (java.lang.NoClassDefFoundError) affecting Delta Lake.  \nImproved error handling in Delta Lake.  \nMay 15, 2018  \nFixed a bug that caused incorrect data skipping statistics to be collected for string columns 32 characters or greater.  \nDatabricks Runtime 4.0 (EoS)  \nSee Databricks Runtime 4.0 (EoS).  \nNovember 6, 2018  \n[SPARK-25714]Fix Null Handling in the Optimizer rule BooleanSimplification.  \nOctober 9, 2018  \nFixed a bug affecting Union operation.  \nSeptember 25, 2018  \n[SPARK-25368][SQL] Incorrect constraint inference returns wrong result.  \n[SPARK-25402][SQL] Null handling in BooleanSimplification.  \nFixed NotSerializableException in Avro data source.  \nSeptember 11, 2018  \nFilter reduction should handle null value correctly.  \nAugust 28, 2018  \nFixed a bug in Delta Lake Delete command that would incorrectly delete the rows where the condition evaluates to null.  \nAugust 23, 2018"
    },
    {
        "id": 1631,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "September 11, 2018  \nFilter reduction should handle null value correctly.  \nAugust 28, 2018  \nFixed a bug in Delta Lake Delete command that would incorrectly delete the rows where the condition evaluates to null.  \nAugust 23, 2018  \nFixed nullable map issue in Parquet reader.  \nFixed secret manager redaction when command partially succeed  \nFixed an interaction between Databricks Delta and Pyspark which could cause transient read failures.  \n[SPARK-25081]Fixed a bug where ShuffleExternalSorter may access a released memory page when spilling fails to allocate memory.  \n[SPARK-25114]Fix RecordBinaryComparator when subtraction between two words is divisible by Integer.MAX_VALUE.  \nAugust 2, 2018  \n[SPARK-24452]Avoid possible overflow in int add or multiple.  \n[SPARK-24588]Streaming join should require HashClusteredPartitioning from children.  \nFixed an issue that could cause mergeInto command to produce incorrect results.  \n[SPARK-24867][SQL] Add AnalysisBarrier to DataFrameWriter. SQL cache is not being used when using DataFrameWriter to write a DataFrame with UDF. This is a regression caused by the changes we made in AnalysisBarrier, since not all the Analyzer rules are idempotent.  \n[SPARK-24809]Serializing LongHashedRelation in executor may result in data error.  \nJune 28, 2018  \nFixed a bug that could cause incorrect query results when the name of a partition column used in a predicate differs from the case of that column in the schema of the table.  \nMay 31, 2018  \nFixed a bug affecting Spark SQL execution engine.  \nImproved error handling in Delta Lake.  \nMay 17, 2018  \nBug fixes for Databricks secret management.  \nImproved stability on reading data stored in Azure Data Lake Store.  \nFixed a bug affecting RDD caching.  \nFixed a bug affecting Null-safe Equal in Spark SQL.  \nApril 24, 2018  \nUpgraded Azure Data Lake Store SDK from 2.0.11 to 2.2.8 to improve the stability of access to Azure Data Lake Store.  \nFixed a bug affecting the insertion of overwrites to partitioned Hive tables when spark.databricks.io.hive.fastwriter.enabled is false."
    },
    {
        "id": 1632,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fixed a bug affecting the insertion of overwrites to partitioned Hive tables when spark.databricks.io.hive.fastwriter.enabled is false.  \nFixed an issue that failed task serialization.  \nImproved Delta Lake stability.  \nMarch 14, 2018  \nPrevent unnecessary metadata updates when writing into Delta Lake.  \nFixed an issue caused by a race condition that could, in rare circumstances, lead to loss of some output files.  \nDatabricks Runtime 3.5 LTS (EoS)  \nSee Databricks Runtime 3.5 LTS (EoS).  \nNovember 7, 2019  \n[SPARK-29743][SQL] sample should set needCopyResult to true if its child\u2019s needCopyResult is true  \nOctober 8, 2019  \nServer side changes to allow Simba Apache Spark ODBC driver to reconnect and continue after a connection failure during fetching results (requires Simba Apache Spark ODBC driver update to version 2.6.10).  \nSeptember 10, 2019  \n[SPARK-28699][SQL] Disable using radix sort for ShuffleExchangeExec in repartition case  \nApril 9, 2019  \n[SPARK-26665][CORE] Fix a bug that can cause BlockTransferService.fetchBlockSync to hang forever.  \nFebruary 12, 2019  \nFixed an issue that Spark low level network protocol may be broken when sending large RPC error messages with encryption enabled (in HIPAA compliance features) or when spark.network.crypto.enabled is set to true).  \nJanuary 30, 2019  \nFixed an issue that can cause df.rdd.count() with UDT to return incorrect answer for certain cases.  \nDecember 18, 2018  \nOnly ignore corrupt files after one or more retries when spark.sql.files.ignoreCorruptFiles or spark.sql.files.ignoreMissingFiles flag is enabled.  \nFixed an issue affecting certain self union queries.  \nNovember 20, 2018  \n[SPARK-25816]Fixed attribute resolution in nested extractors.  \nNovember 6, 2018  \n[SPARK-25714]Fix Null Handling in the Optimizer rule BooleanSimplification.  \nOctober 9, 2018  \nFixed a bug affecting Union operation.  \nSeptember 25, 2018  \n[SPARK-25402][SQL] Null handling in BooleanSimplification."
    },
    {
        "id": 1633,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "October 9, 2018  \nFixed a bug affecting Union operation.  \nSeptember 25, 2018  \n[SPARK-25402][SQL] Null handling in BooleanSimplification.  \nFixed NotSerializableException in Avro data source.  \nSeptember 11, 2018  \nFilter reduction should handle null value correctly.  \nAugust 28, 2018  \nFixed a bug in Delta Lake Delete command that would incorrectly delete the rows where the condition evaluates to null.  \n[SPARK-25114]Fix RecordBinaryComparator when subtraction between two words is divisible by Integer.MAX_VALUE.  \nAugust 23, 2018  \n[SPARK-24809]Serializing LongHashedRelation in executor may result in data error.  \nFixed nullable map issue in Parquet reader.  \n[SPARK-25081]Fixed a bug where ShuffleExternalSorter may access a released memory page when spilling fails to allocate memory.  \nFixed an interaction between Databricks Delta and Pyspark which could cause transient read failures.  \nJune 28, 2018  \nFixed a bug that could cause incorrect query results when the name of a partition column used in a predicate differs from the case of that column in the schema of the table.  \nJune 28, 2018  \nFixed a bug that could cause incorrect query results when the name of a partition column used in a predicate differs from the case of that column in the schema of the table.  \nMay 31, 2018  \nFixed a bug affecting Spark SQL execution engine.  \nImproved error handling in Delta Lake.  \nMay 17, 2018  \nImproved stability on reading data stored in Azure Data Lake Store.  \nFixed a bug affecting RDD caching.  \nFixed a bug affecting Null-safe Equal in Spark SQL.  \nFixed a bug affecting certain aggregations in streaming queries.  \nApril 24, 2018  \nUpgraded Azure Data Lake Store SDK from 2.0.11 to 2.2.8 to improve the stability of access to Azure Data Lake Store.  \nFixed a bug affecting the insertion of overwrites to partitioned Hive tables when spark.databricks.io.hive.fastwriter.enabled is false.  \nFixed an issue that failed task serialization.  \nMarch 09, 2018  \nFixed an issue caused by a race condition that could, in rare circumstances, lead to loss of some output files.  \nMarch 01, 2018"
    },
    {
        "id": 1634,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Fixed an issue that failed task serialization.  \nMarch 09, 2018  \nFixed an issue caused by a race condition that could, in rare circumstances, lead to loss of some output files.  \nMarch 01, 2018  \nImproved the efficiency of handling streams that can take a long time to stop.  \nFixed an issue affecting Python autocomplete.  \nApplied Ubuntu security patches.  \nFixed an issue affecting certain queries using Python UDFs and window functions.  \nFixed an issue affecting the use of UDFs on a cluster with table access control enabled.  \nJanuary 29, 2018  \nFixed an issue affecting the manipulation of tables stored in Azure Blob storage.  \nFixed aggregation after dropDuplicates on empty DataFrame.  \nDatabricks Runtime 3.4 (EoS)  \nSee Databricks Runtime 3.4 (EoS).  \nMay 31, 2018  \nFixed a bug affecting Spark SQL execution engine.  \nImproved error handling in Delta Lake.  \nMay 17, 2018  \nImproved stability on reading data stored in Azure Data Lake Store.  \nFixed a bug affecting RDD caching.  \nFixed a bug affecting Null-safe Equal in Spark SQL.  \nApril 24, 2018  \nFixed a bug affecting the insertion of overwrites to partitioned Hive tables when spark.databricks.io.hive.fastwriter.enabled is false.  \nMarch 09, 2018  \nFixed an issue caused by a race condition that could, in rare circumstances, lead to loss of some output files.  \nDecember 13, 2017  \nFixed an issue affecting UDFs in Scala.  \nFixed an issue affecting the use of Data Skipping Index on data source tables stored in non-DBFS paths.  \nDecember 07, 2017  \nImproved shuffle stability.  \nDatabricks Runtime 3.3 (EoS)  \nSee Databricks Runtime 3.3 (EoS).  \nMay 31, 2018  \nFixed a bug affecting Spark SQL execution engine.  \nApril 24, 2018  \nFixed a bug affecting the insertion of overwrites to partitioned Hive tables when spark.databricks.io.hive.fastwriter.enabled is false.  \nMarch 12, 2018  \nFixed an issue caused by a race condition that could, in rare circumstances, lead to loss of some output files.  \nJanuary 29, 2018  \nFixed an issue affecting UDFs in Scala.  \nOctober 11, 2017  \nImproved shuffle stability.  \nDatabricks Runtime 3.2 (EoS)"
    },
    {
        "id": 1635,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "January 29, 2018  \nFixed an issue affecting UDFs in Scala.  \nOctober 11, 2017  \nImproved shuffle stability.  \nDatabricks Runtime 3.2 (EoS)  \nSee Databricks Runtime 3.2 (EoS).  \nMarch 30, 2018  \nFixed an issue caused by a race condition that could, in rare circumstances, lead to loss of some output files.  \nSeptember 13, 2017  \nFixed an issue affecting the use of spark_submit_task with Databricks jobs.  \nSeptember 06, 2017  \nFixed an issue affecting the performance of certain window functions.  \n2.1.1-db6 (EoS)  \nSee 2.1.1-db6 cluster image (EoS).  \nMay 31, 2018  \nFixed a bug affecting Spark SQL execution engine.  \nMarch 30, 2018  \nFixed an issue caused by a race condition that could, in rare circumstances, lead to loss of some output files.  \n2.1.1-db4 (EoS)  \nSee 2.1.1-db4 cluster image (EoS).  \nMay 31, 2018  \nFixed a bug affecting Spark SQL execution engine.  \nMarch 30, 2018  \nFixed an issue caused by a race condition that could, in rare circumstances, lead to loss of some output files."
    },
    {
        "id": 1636,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/maintenance-updates-archive.html",
        "content": "Unsupported Databricks Runtime releases\nUnsupported Databricks Runtime releases\nFor the original release notes, follow the link below the subheading."
    },
    {
        "id": 1637,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/2.0.2-db1.html",
        "content": "2.0.2-db1 cluster image  \nDatabricks released this image in November, 2016.  \nImportant  \nThis release has been deprecated. For more information about the Databricks Runtime deprecation policy and schedule, see Databricks support lifecycles.  \nThe following release notes provide information about the Spark 2.0.2-db1 cluster image powered by Apache Spark.  \nApache Spark"
    },
    {
        "id": 1638,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/2.0.2-db1.html",
        "content": "Apache Spark\n2.0.2-db1 cluster image includes Apache Spark 2.0.2. Apache Spark 2.0.2 contains stability fixes, Apache Kafka support for Structured Streaming, and improved metrics for Structured Streaming. For more information, please see Apache Spark 2.0.2 release notes. 2.0.2-db1 cluster image also includes the following extra bug fixes and improvements:  \nSPARK-18280 [CORE]: Fix potential deadlock in StandaloneSchedulerBackend.dead.  \nSPARK-17703 [SQL]: Add unnamed version of addReferenceObj for minor objects.  \nSPARK-18137 [SQL]: Fix RewriteDistinctAggregates UnresolvedException when a UDAF has a foldable TypeCheck.  \nSPARK-17919 [R]: Make timeout to RBackend configurable in SparkR.\n\nChanges and Improvements"
    },
    {
        "id": 1639,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/2.0.2-db1.html",
        "content": "Changes and Improvements\nNote  \nStarting from this version, users can enable Spark Session Isolation when creating clusters. With Spark Session Isolation, different notebooks attached to a cluster are in different sessions with isolated runtime configurations and current database setting.  \nOperating system upgraded to Ubuntu 16.04.1 LTS from Ubuntu 15.10.  \nJava upgraded to 1.8.0_111 from 1.8.0_66-internal.  \nPython upgraded to 2.7.12 from 2.7.10.  \nMost pre-installed Python libraries upgraded. For the list of Python libraries and their version, see Pre-installed Python Libraries.  \nFixed issues around unicode characters in Python notebooks  \nPerformance improvement (better pipelining) on scanning files in S3.  \nPerformance improvement on queries using percentile_approx.  \nUsers can set spark.databricks.session.share to false to enable spark session isolation. With Spark Session Isolation, different notebooks attached to a cluster are in different sessions with isolated runtime configurations and current database setting. For details, see Spark Session Isolation.  \nDatabricks redirects executor Log4j logs to stderr. Users can access stderr on the executor page and every worker page of the cluster. Worker pages will not show the log4j link and executors will not write logs to log4j files."
    },
    {
        "id": 1640,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/2.0.2-db1.html",
        "content": "System Environment\nOperating System: Ubuntu 16.04.1 LTS  \nJava: 1.8.0_111  \nScala: 2.10.6 (Scala 2.10 cluster version)/2.11.8 (Scala 2.11 cluster version)  \nPython: 2.7.12  \nR: R version 3.2.3 (2015-12-10)  \nPre-installed Python Libraries  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nansi2html  \n1.1.1  \nargparse  \n1.2.1  \nboto  \n2.42.0  \nboto3  \n1.4.1  \nbotocore  \n1.4.70  \nbrewer2mpl  \n1.4.1  \ncertifi  \n2016.2.28  \ncffi  \n1.7.0  \nchardet  \n2.3.0  \ncolorama  \n0.3.7  \nconfigobj  \n5.0.6  \ncryptography  \n1.5  \ncycler  \n0.10.0  \nCython  \n0.24.1  \ndecorator  \n4.0.10  \ndocutils  \n0.12  \nenum34  \n1.1.6  \net-xmlfile  \n1.0.1  \nfreetype-py  \n1.0.2  \nfuncsigs  \n1.0.2  \nfusepy  \n2.0.4  \nfutures  \n3.0.5  \nggplot  \n0.6.8  \nhtml5lib  \n0.999  \nidna  \n2.1  \nipaddress  \n1.0.16  \nipython  \n2.2.0  \nipython-genutils  \n0.1.0  \njdcal  \n1.2  \nJinja2  \n2.8  \njmespath  \n0.9.0  \nllvmlite  \n0.13.0  \nlxml  \n3.6.4  \nMarkupSafe  \n0.23  \nmatplotlib  \n1.5.3  \nmpld3  \n0.2  \nmsgpack-python  \n0.4.7  \nndg-httpsclient  \n0.3.3  \nnumba  \n0.28.1  \nnumpy  \n1.11.1  \nopenpyxl  \n2.3.2  \npandas  \n0.18.1  \npathlib2  \n2.1.0  \npatsy  \n0.4.1  \npexpect  \n4.0.1  \npickleshare  \n0.7.4  \nPillow  \n3.3.1  \npip  \n8.1.2  \npkg_resources  \n0.0.0  \nply  \n3.9  \nprompt-toolkit  \n1.0.7  \npsycopg2"
    },
    {
        "id": 1641,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/2.0.2-db1.html",
        "content": "pickleshare  \n0.7.4  \nPillow  \n3.3.1  \npip  \n8.1.2  \npkg_resources  \n0.0.0  \nply  \n3.9  \nprompt-toolkit  \n1.0.7  \npsycopg2  \n2.6.2  \nptyprocess  \n0.5.1  \npy4j  \n0.10.3  \npyasn1  \n0.1.9  \npycparser  \n2.14  \nPygments  \n2.1.3  \nPyGObject  \n3.20.0  \npyOpenSSL  \n16.0.0  \npyparsing  \n2.1.4  \npypng  \n0.0.18  \nPython  \n2.7.12  \npython-dateutil  \n2.5.3  \npython-geohash  \n0.8.5  \npytz  \n2016.6.1  \nrequests  \n2.11.1  \ns3transfer  \n0.1.9  \nscikit-learn  \n0.17.1  \nscipy  \n0.18.1  \nscour  \n0.32  \nseaborn  \n0.7.1  \nsetuptools  \n28.6.0  \nsimplejson  \n3.8.2  \nsimples3  \n1.0  \nsingledispatch  \n3.4.0.3  \nsix  \n1.10.0  \nstatsmodels  \n0.6.1  \ntraitlets  \n4.3.0  \nurllib3  \n1.19.1  \nvirtualenv  \n15.0.1  \nwcwidth  \n0.1.7  \nwheel  \n0.30.0a0  \nwsgiref  \n0.1.2  \nPre-installed R Libraries  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nabind  \n1.4-3  \nassertthat  \n0.1  \nbase  \n3.2.3  \nBH  \n1.60.0-2  \nbitops  \n1.0-6  \nboot  \n1.3-17  \nbrew  \n1.0-6  \ncar  \n2.1-3  \ncaret  \n6.0-71  \nchron  \n2.3-47  \nclass  \n7.3-14  \ncluster  \n2.0.5  \ncodetools  \n0.2-14  \ncolorspace  \n1.2-4  \ncompiler  \n3.2.3  \ncrayon  \n1.3.1  \ncurl  \n2.2  \ndata.table  \n1.9.6  \ndatasets  \n3.2.3  \nDBI  \n0.5-1  \ndevtools  \n1.12.0  \ndichromat  \n2.0-0  \ndigest  \n0.6.9  \ndoMC  \n1.3.4  \ndplyr  \n0.5.0  \nforeach  \n1.4.3  \nforeign  \n0.8-66  \ngbm  \n2.1.1"
    },
    {
        "id": 1642,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/2.0.2-db1.html",
        "content": "2.0-0  \ndigest  \n0.6.9  \ndoMC  \n1.3.4  \ndplyr  \n0.5.0  \nforeach  \n1.4.3  \nforeign  \n0.8-66  \ngbm  \n2.1.1  \nggplot2  \n2.1.0  \ngit2r  \n0.15.0  \nglmnet  \n2.0-5  \ngraphics  \n3.2.3  \ngrDevices  \n3.2.3  \ngrid  \n3.2.3  \ngsubfn  \n0.6-6  \ngtable  \n0.1.2  \nh2o  \n3.10.0.8  \nhttr  \n1.2.1  \nhwriter  \n1.3.2  \nhwriterPlus  \n1.0-3  \niterators  \n1.0.8  \njsonlite  \n1.1  \nKernSmooth  \n2.23-15  \nlabeling  \n0.3  \nlattice  \n0.20-34  \nlazyeval  \n0.2.0  \nlittler  \n0.3.0  \nlme4  \n1.1-12  \nlubridate  \n1.6.0  \nmagrittr  \n1.5  \nmapproj  \n1.2-4  \nmaps  \n3.0.2  \nMASS  \n7.3-45  \nMatrix  \n1.2-7.1  \nMatrixModels  \n0.4-1  \nmemoise  \n1.0.0  \nmethods  \n3.2.3  \nmgcv  \n1.8-11  \nmime  \n0.5  \nminqa  \n1.2.4  \nmulticore  \n0.2  \nmunsell  \n0.4.2  \nmvtnorm  \n1.0-5  \nnlme  \n3.1-124  \nnloptr  \n1.0.4  \nnnet  \n7.3-12  \nopenssl  \n0.9.4  \nparallel  \n3.2.3  \npbkrtest  \n0.4-6  \npkgKitten  \n0.1.3  \nplyr  \n1.8.4  \npraise  \n1.0.0  \npROC  \n1.8  \nproto  \n0.3-10  \nquantreg  \n5.29  \nR.methodsS3  \n1.7.1  \nR.oo  \n1.20.0  \nR.utils  \n2.4.0  \nR6  \n2.2.0  \nrandomForest  \n4.6-12  \nRColorBrewer  \n1.1-2  \nRcpp  \n0.12.7  \nRcppEigen  \n0.3.2.9.0  \nRCurl  \n1.95-4.8  \nreshape2  \n1.4.2  \nRODBC  \n1.3-12  \nroxygen2  \n5.0.1  \nrpart"
    },
    {
        "id": 1643,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/2.0.2-db1.html",
        "content": "RcppEigen  \n0.3.2.9.0  \nRCurl  \n1.95-4.8  \nreshape2  \n1.4.2  \nRODBC  \n1.3-12  \nroxygen2  \n5.0.1  \nrpart  \n4.1-10  \nRserve  \n1.7-3  \nRSQLite  \n1.0.0  \nrstudioapi  \n0.6  \nscales  \n0.3.0  \nsp  \n1.0-15  \nSparkR  \n2.0.2  \nSparseM  \n1.72  \nspatial  \n7.3-11  \nsplines  \n3.2.3  \nsqldf  \n0.4-10  \nstatmod  \n1.4.26  \nstats  \n3.2.3  \nstats4  \n3.2.3  \nstringi  \n1.0-1  \nstringr  \n1.0.0  \nsurvival  \n2.38-3  \ntcltk  \n3.2.3  \nTeachingDemos  \n2.10  \ntestthat  \n1.0.2  \ntibble  \n1.2  \ntools  \n3.2.3  \nutils  \n3.2.3  \nwhisker  \n0.3-2  \nwithr  \n1.0.2"
    },
    {
        "id": 1644,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "Databricks Runtime 11.2 (EoS)  \nNote  \nSupport for this Databricks Runtime version has ended. For the end-of-support date, see End-of-support history. For all supported Databricks Runtime versions, see Databricks Runtime release notes versions and compatibility.  \nThe following release notes provide information about Databricks Runtime 11.2, powered by Apache Spark 3.3.0. Databricks released these images in September 2022.  \nNew features and improvements"
    },
    {
        "id": 1645,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "New features and improvements\nChange to ALTER TABLE permissions in table access controls (table ACLs)  \nDynamic pruning for MERGE INTO  \nImproved conflict detection in Delta with dynamic file pruning  \nNew Open Source Integrations card in DSE/SQL homepages  \nCONVERT TO DELTA partition detection improvements  \nDatabricks Runtime Graviton images now support JDK 11 (Public Preview)  \nTable schemas now support default values for columns  \nNew H3 geospatial functions  \nNew Databricks Runtime dependency  \nBring your own key: Git credentials encryption  \nSQL: New aggregate function any_value  \nDatabricks Utilities file system commands allowed on more cluster types  \nCREATE privileges can now be granted on metastores for Unity Catalog  \nOptimized writes for unpartitioned tables in Photon-enabled clusters  \nPhoton support for more data sources  \nSQL: ALTER SHARE now supports START VERSION  \nChange to ALTER TABLE permissions in table access controls (table ACLs)  \nNow, users only need MODIFY permissions to change a table\u2019s schema or properties with ALTER TABLE. Ownership is still required to grant permissions on a table, change its owner and location, or rename it. This change makes the permission model for table ACLs consistent with Unity Catalog. See ALTER TABLE.  \nDynamic pruning for MERGE INTO  \nWhen using Photon-enabled compute, MERGE INTO now uses dynamic file and partition pruning when appropriate to improve performance, for example when a small source table is merged into a larger target table.  \nImproved conflict detection in Delta with dynamic file pruning  \nWhen checking for potential conflicts during commits, conflict detection now considers files that are pruned by dynamic file pruning, but would not have been pruned by static filters. This results in a decreased number of failed transactions.  \nNew Open Source Integrations card in DSE/SQL homepages  \nIntroducing the new \u2018Open Source Integrations\u2019 card in the DSE/SQL homepages that displays open source integration options such as Delta Live Tables and dbt core.  \nCONVERT TO DELTA partition detection improvements  \nCONVERT TO DELTA automatically infers the partition schema for Parquet tables registered to the Hive metastore or Unity Catalog, eliminating the need to provide the PARTITIONED BY clause.  \nCONVERT TO DELTA leverages partition information from the metastore to discover files for a Parquet table instead of listing the entire base path, ensuring dropped partitions do not get added to the Delta table.  \nSee Convert to Delta Lake."
    },
    {
        "id": 1646,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "CONVERT TO DELTA leverages partition information from the metastore to discover files for a Parquet table instead of listing the entire base path, ensuring dropped partitions do not get added to the Delta table.  \nSee Convert to Delta Lake.  \nDatabricks Runtime Graviton images now support JDK 11 (Public Preview)  \nStarting with Databricks Runtime 11.2, Databricks Runtime Graviton images now support Java Development Kit (JDK) 11.  \nWhen you create a cluster, you can specify that the cluster uses JDK 11 (for both the driver and executor). To do this, add the following environment variable to Advanced Options > Spark > Environment Variables:  \nJNAME=zulu11-ca-arm64  \nTable schemas now support default values for columns  \nTable schemas now support setting default values for columns. INSERT, UPDATE, and DELETE commands for these columns can refer to these values with the DEFAULT keyword. For example, CREATE TABLE t (id INT, data INT DEFAULT 42) USING PARQUET followed by INSERT INTO t VALUES (1, DEFAULT) will append the row (1, 42). This behavior is supported for CSV, JSON, Orc, and Parquet data sources.  \nNew H3 geospatial functions  \nYou can now use 28 new built-in H3 expressions for geospatial processing in Photon-enabled clusters, available in SQL, Scala, and Python. See H3 geospatial functions.  \nNew Databricks Runtime dependency  \nDatabricks Runtime now depends on the H3 Java library version 3.7.0.  \nBring your own key: Git credentials encryption  \nYou can use AWS Key Management Service to encrypt a Git personal access token (PAT) or other Git credential.  \nSee Set up Databricks Git folders (Repos)  \nSQL: New aggregate function any_value  \nThe new any_value aggregate function returns any random value of expr for a group of rows. See the any_value aggregate function.  \nDatabricks Utilities file system commands allowed on more cluster types  \ndbutils.fs commands (except for mount-related commands) are now allowed on user-isolation clusters with Unity Catalog, as well as legacy table ACL clusters when the user has ANY FILE permissions.  \nCREATE privileges can now be granted on metastores for Unity Catalog"
    },
    {
        "id": 1647,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "CREATE privileges can now be granted on metastores for Unity Catalog  \nCREATE CATALOG, CREATE EXTERNAL LOCATION, CREATE SHARE, CREATE RECIPIENT, and CREATE PROVIDER privileges can now be granted on Unity Catalog metastores.  \nOptimized writes for unpartitioned tables in Photon-enabled clusters  \nUnity Catalog managed tables now automatically persist files of a well-tuned size from unpartitioned tables to improve query speed and optimize performance.  \nPhoton support for more data sources  \nPhoton now supports more data sources, including CSV and Avro, and is also compatible with cached data frames. Previously, scanning these data sources meant that the entire query could not be photonized, regardless of the query\u2019s operators or expressions. Now, queries scanning these data sources can be photonized, unlocking significant latency and TCO improvements.  \nThis feature is enabled by default by the spark.databricks.photon.photonRowToColumnar.enabled configuration.  \nLimitations:  \nSchemas with nested types are unsupported (i.e arrays, maps, and structs) in this release.  \nORC, RDD, Kinesis, Kafka, and EventHub sources are unsupported in this release.  \nSQL: ALTER SHARE now supports START VERSION  \nThe ALTER SHARE command now supports START VERSION, which which allows providers to share data starting from a specific table version. See ALTER SHARE."
    },
    {
        "id": 1648,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "Library upgrades\nUpgraded Python libraries:  \ndistlib from 0.3.4 to 0.3.5  \nfilelock from 3.7.1 to 3.8.0  \nplotly from 5.8.2 to 5.9.0  \nprotobuf from 4.21.2 to 4.21.5  \nUpgraded R libraries:  \nbroom from 0.8.0 to 1.0.0  \nbslib from 0.3.1 to 0.4.0  \ncallr from 3.7.0 to 3.7.1  \ncaret from 6.0-92 to 6.0-93  \ndbplyr from 2.2.0 to 2.2.1  \ndevtools from 2.4.3 to 2.4.4  \nevaluate from 0.15 to 0.16  \nfarver from 2.1.0 to 2.1.1  \nfontawesome from 0.2.2 to 0.3.0  \nfuture from 1.26.1 to 1.27.0  \ngenerics from 0.1.2 to 0.1.3  \ngert from 1.6.0 to 1.7.0  \nglobals from 0.15.1 to 0.16.0  \ngooglesheets4 from 1.0.0 to 1.0.1  \nhardhat from 1.1.0 to 1.2.0  \nhtmltools from 0.5.2 to 0.5.3  \nparallelly from 1.32.0 to 1.32.1  \npillar from 1.7.0 to 1.8.0  \npkgload from 1.2.4 to 1.3.0  \nprocessx from 3.6.1 to 3.7.0  \nRcpp from 1.0.8.3 to 1.0.9  \nrecipes from 0.2.0 to 1.0.1  \nrlang from 1.0.2 to 1.0.4  \nroxygen2 from 7.2.0 to 7.2.1  \nRSQLite from 2.2.14 to 2.2.15  \nsass from 0.4.1 to 0.4.2  \nshiny from 1.7.1 to 1.7.2  \nstringi from 1.7.6 to 1.7.8  \ntibble from 3.1.7 to 3.1.8"
    },
    {
        "id": 1649,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "shiny from 1.7.1 to 1.7.2  \nstringi from 1.7.6 to 1.7.8  \ntibble from 3.1.7 to 3.1.8  \ntidyverse from 1.3.1 to 1.3.2  \ntimeDate from 3043.102 to 4021.104  \nxfun from 0.31 to 0.32  \nUpgraded Java libraries:  \norg.apache.orc.orc-core from 1.7.4 to 1.7.5  \norg.apache.orc.orc-mapreduce from 1.7.4 to 1.7.5  \norg.apache.orc.orc-shims from 1.7.4 to 1.7.5"
    },
    {
        "id": 1650,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "Apache Spark\nDatabricks Runtime 11.2 includes Apache Spark 3.3.0. This release includes all Spark fixes and improvements included in Databricks Runtime 11.1 (EoS), as well as the following additional bug fixes and improvements made to Spark:  \n[SPARK-40151] [WARMFIX][SC-109002][SC-108809][SQL] Return wider ANSI interval types from the percentile functions  \n[SPARK-40054] [SQL] Restore the error handling syntax of try_cast()  \n[SPARK-39489] [CORE] Improve event logging JsonProtocol performance by using Jackson instead of Json4s  \n[SPARK-39319] [CORE][SQL] Make query contexts as a part of SparkThrowable  \n[SPARK-40085] [SQL] Use INTERNAL_ERROR error class instead of IllegalStateException to indicate bugs  \n[SPARK-40001] [SQL] Make NULL writes to JSON DEFAULT columns write \u2018null\u2019 to storage  \n[SPARK-39635] [SQL] Support driver metrics in DS v2 custom metric API  \n[SPARK-39184] [SQL] Handle undersized result array in date and timestamp sequences  \n[SPARK-40019] [SQL] Refactor comment of ArrayType\u2019s containsNull and refactor the misunderstanding logics in collectionOperator\u2019s expression about containsNull  \n[SPARK-39989] [SQL] Support estimate column statistics if it is foldable expression  \n[SPARK-39926] [SQL] Fix bug in column DEFAULT support for non-vectorized Parquet scans  \n[SPARK-40052] [SQL] Handle direct byte buffers in VectorizedDeltaBinaryPackedReader  \n[SPARK-40044] [SQL] Fix the target interval type in cast overflow errors  \n[SPARK-39835] [SQL] Fix EliminateSorts remove global sort below the local sort  \n[SPARK-40002] [SQL] Don\u2019t push down limit through window using ntile"
    },
    {
        "id": 1651,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "[SPARK-39835] [SQL] Fix EliminateSorts remove global sort below the local sort  \n[SPARK-40002] [SQL] Don\u2019t push down limit through window using ntile  \n[SPARK-39976] [SQL] ArrayIntersect should handle null in left expression correctly  \n[SPARK-39985] [SQL] Enable implicit DEFAULT column values in inserts from DataFrames  \n[SPARK-39776] [SQL] JOIN verbose string should add Join type  \n[SPARK-38901] [SQL] DS V2 supports push down misc functions  \n[SPARK-40028] [SQL][FollowUp] Improve examples of string functions  \n[SPARK-39983] [CORE][SQL] Do not cache unserialized broadcast relations on the driver  \n[SPARK-39812] [SQL] Simplify code which construct AggregateExpression with toAggregateExpression  \n[SPARK-40028] [SQL] Add binary examples for string expressions  \n[SPARK-39981] [SQL] Throw the exception QueryExecutionErrors.castingCauseOverflowErrorInTableInsert in Cast  \n[SPARK-40007] [PYTHON][SQL] Add \u2018mode\u2019 to functions  \n[SPARK-40008] [SQL] Support casting of integrals to ANSI intervals  \n[SPARK-40003] [PYTHON][SQL] Add \u2018median\u2019 to functions  \n[SPARK-39952] [SQL] SaveIntoDataSourceCommand should recache result relation  \n[SPARK-39951] [SQL] Update Parquet V2 columnar check for nested fields  \n[SPARK-33236] [shuffle] Backport to DBR 11.x: Enable Push-based shuffle service to store state in NM level DB for work preserving restart  \n[SPARK-39836] [SQL] Simplify V2ExpressionBuilder by extract common method.  \n[SPARK-39873] [SQL] Remove OptimizeLimitZero and merge it into EliminateLimits"
    },
    {
        "id": 1652,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "[SPARK-39873] [SQL] Remove OptimizeLimitZero and merge it into EliminateLimits  \n[SPARK-39961] [SQL] DS V2 push-down translate Cast if the cast is safe  \n[SPARK-39872] [SQL] Change to use BytePackerForLong#unpack8Values with Array input api in VectorizedDeltaBinaryPackedReader  \n[SPARK-39858] [SQL] Remove unnecessary AliasHelper or PredicateHelper for some rules  \n[SPARK-39900] [SQL] Address partial or negated condition in binary format\u2019s predicate pushdown  \n[SPARK-39904] [SQL] Rename inferDate to prefersDate and clarify semantics of the option in CSV data source  \n[SPARK-39958] [SQL] Add warning log when unable to load custom metric object  \n[SPARK-39932] [SQL] WindowExec should clear the final partition buffer  \n[SPARK-37194] [SQL] Avoid unnecessary sort in v1 write if it\u2019s not dynamic partition  \n[SPARK-39902] [SQL] Add Scan details to spark plan scan node in SparkUI  \n[SPARK-39865] [SQL] Show proper error messages on the overflow errors of table insert  \n[SPARK-39940] [SS] Refresh catalog table on streaming query with DSv1 sink  \n[SPARK-39827] [SQL] Use the error class ARITHMETIC_OVERFLOW on int overflow in add_months()  \n[SPARK-39914] [SQL] Add DS V2 Filter to V1 Filter conversion  \n[SPARK-39857] [SQL] Manual DBR 11.x backport; V2ExpressionBuilder uses the wrong LiteralValue data type for In predicate #43454  \n[SPARK-39840] [SQL][PYTHON] Factor PythonArrowInput out as a symmetry to PythonArrowOutput  \n[SPARK-39651] [SQL] Prune filter condition if compare with rand is deterministic"
    },
    {
        "id": 1653,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "[SPARK-39651] [SQL] Prune filter condition if compare with rand is deterministic  \n[SPARK-39877] [PYTHON] Add unpivot to PySpark DataFrame API  \n[SPARK-39909] [SQL] Organize the check of push down information for JDBCV2Suite  \n[SPARK-39834] [SQL][SS] Include the origin stats and constraints for LogicalRDD if it comes from DataFrame  \n[SPARK-39849] [SQL] Dataset.as(StructType) fills missing new columns with null value  \n[SPARK-39860] [SQL] More expressions should extend Predicate  \n[SPARK-39823] [SQL][PYTHON] Rename Dataset.as as Dataset.to and add DataFrame.to in PySpark  \n[SPARK-39918] [SQL][MINOR] Replace the wording \u201cun-comparable\u201d with \u201cincomparable\u201d in error message  \n[SPARK-39857] [SQL][3.3] V2ExpressionBuilder uses the wrong LiteralValue data type for In predicate  \n[SPARK-39862] [SQL] Manual backport for PR 43654 targeting DBR 11.x: Update SQLConf.DEFAULT_COLUMN_ALLOWED_PROVIDERS to allow/deny ALTER TABLE \u2026 ADD COLUMN commands separately.  \n[SPARK-39844] [SQL] Manual backport for PR 43652 targeting DBR 11.x  \n[SPARK-39899] [SQL] Fix passing of message parameters to InvalidUDFClassException  \n[SPARK-39890] [SQL] Make TakeOrderedAndProjectExec inherit AliasAwareOutputOrdering  \n[SPARK-39809] [PYTHON] Support CharType in PySpark  \n[SPARK-38864] [SQL] Add unpivot / melt to Dataset  \n[SPARK-39864] [SQL] Lazily register ExecutionListenerBus  \n[SPARK-39808] [SQL] Support aggregate function MODE"
    },
    {
        "id": 1654,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "[SPARK-39864] [SQL] Lazily register ExecutionListenerBus  \n[SPARK-39808] [SQL] Support aggregate function MODE  \n[SPARK-39875] [SQL] Change protected method in final class to private or package-visible  \n[SPARK-39731] [SQL] Fix issue in CSV and JSON data sources when parsing dates in \u201cyyyyMMdd\u201d format with CORRECTED time parser policy  \n[SPARK-39805] [SS] Deprecate Trigger.Once and Promote Trigger.AvailableNow  \n[SPARK-39784] [SQL] Put Literal values on the right side of the data source filter after translating Catalyst Expression to data source filter  \n[SPARK-39672] [SQL][3.1] Fix removing project before filter with correlated subquery  \n[SPARK-39552] [SQL] Unify v1 and v2 DESCRIBE TABLE  \n[SPARK-39810] [SQL] Catalog.tableExists should handle nested namespace  \n[SPARK-37287] [SQL] Pull out dynamic partition and bucket sort from FileFormatWriter  \n[SPARK-39469] [SQL] Infer date type for CSV schema inference  \n[SPARK-39148] [SQL] DS V2 aggregate push down can work with OFFSET or LIMIT  \n[SPARK-39818] [SQL] Fix bug in ARRAY, STRUCT, MAP types with DEFAULT values with NULL field(s)  \n[SPARK-39792] [SQL] Add DecimalDivideWithOverflowCheck for decimal average  \n[SPARK-39798] [SQL] Replcace toSeq.toArray with .toArray[Any] in constructor of GenericArrayData  \n[SPARK-39759] [SQL] Implement listIndexes in JDBC (H2 dialect)  \n[SPARK-39385] [SQL] Supports push down REGR_AVGX and REGR_AVGY  \n[SPARK-39787] [SQL] Use error class in the parsing error of function to_timestamp"
    },
    {
        "id": 1655,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "[SPARK-39787] [SQL] Use error class in the parsing error of function to_timestamp  \n[SPARK-39760] [PYTHON] Support Varchar in PySpark  \n[SPARK-39557] [SQL] Manual backport to DBR 11.x: Support ARRAY, STRUCT, MAP types as DEFAULT values  \n[SPARK-39758] [SQL][3.3] Fix NPE from the regexp functions on invalid patterns  \n[SPARK-39749] [SQL] ANSI SQL mode: Use plain string representation on casting Decimal to String  \n[SPARK-39704] [SQL] Implement createIndex & dropIndex & indexExists in JDBC (H2 dialect)  \n[SPARK-39803] [SQL] Use LevenshteinDistance instead of StringUtils.getLevenshteinDistance  \n[SPARK-39339] [SQL] Support TimestampNTZ type in JDBC data source  \n[SPARK-39781] [SS] Add support for providing max_open_files to rocksdb state store provider  \n[SPARK-39719] [R] Implement databaseExists/getDatabase in SparkR support 3L namespace  \n[SPARK-39751] [SQL] Rename hash aggregate key probes metric  \n[SPARK-39772] [SQL] namespace should be null when database is null in the old constructors  \n[SPARK-39625] [SPARK-38904][SQL] Add Dataset.as(StructType)  \n[SPARK-39384] [SQL] Compile built-in linear regression aggregate functions for JDBC dialect  \n[SPARK-39720] [R] Implement tableExists/getTable in SparkR for 3L namespace  \n[SPARK-39744] [SQL] Add the REGEXP_INSTR function  \n[SPARK-39716] [R] Make currentDatabase/setCurrentDatabase/listCatalogs in SparkR support 3L namespace"
    },
    {
        "id": 1656,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "[SPARK-39716] [R] Make currentDatabase/setCurrentDatabase/listCatalogs in SparkR support 3L namespace  \n[SPARK-39788] [SQL] Rename catalogName to dialectName for JdbcUtils  \n[SPARK-39647] [CORE] Register the executor with ESS before registering the BlockManager  \n[SPARK-39754] [CORE][SQL] Remove unused import or unnecessary {}  \n[SPARK-39706] [SQL] Set missing column with defaultValue as constant in ParquetColumnVector  \n[SPARK-39699] [SQL] Make CollapseProject smarter about collection creation expressions  \n[SPARK-39737] [SQL] PERCENTILE_CONT and PERCENTILE_DISC should support aggregate filter  \n[SPARK-39579] [SQL][PYTHON][R] Make ListFunctions/getFunction/functionExists compatible with 3 layer namespace  \n[SPARK-39627] [SQL] JDBC V2 pushdown should unify the compile API  \n[SPARK-39748] [SQL][SS] Include the origin logical plan for LogicalRDD if it comes from DataFrame  \n[SPARK-39385] [SQL] Translate linear regression aggregate functions for pushdown  \n[SPARK-39695] [SQL] Add the REGEXP_SUBSTR function  \n[SPARK-39667] [SQL] Add another workaround when there is not enough memory to build and broadcast the table  \n[SPARK-39666] [ES-337834][SQL] Use UnsafeProjection.create to respect spark.sql.codegen.factoryMode in ExpressionEncoder  \n[SPARK-39643] [SQL] Prohibit subquery expressions in DEFAULT values  \n[SPARK-38647] [SQL] Add SupportsReportOrdering mix in interface for Scan (DataSourceV2)  \n[SPARK-39497] [SQL] Improve the analysis exception of missing map key column"
    },
    {
        "id": 1657,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "[SPARK-39497] [SQL] Improve the analysis exception of missing map key column  \n[SPARK-39661] [SQL] Avoid creating unnecessary SLF4J Logger  \n[SPARK-39713] [SQL] ANSI mode: add suggestion of using try_element_at for INVALID_ARRAY_INDEX error  \n[SPARK-38899] [SQL]DS V2 supports push down datetime functions  \n[SPARK-39638] [SQL] Change to use ConstantColumnVector to store partition columns in OrcColumnarBatchReader  \n[SPARK-39653] [SQL] Clean up ColumnVectorUtils#populate(WritableColumnVector, InternalRow, int) from ColumnVectorUtils  \n[SPARK-39231] [SQL] Use ConstantColumnVector instead of On/OffHeapColumnVector to store partition columns in VectorizedParquetRecordReader  \n[SPARK-39547] [SQL] V2SessionCatalog should not throw NoSuchDatabaseException in loadNamspaceMetadata  \n[SPARK-39447] [SQL] Avoid AssertionError in AdaptiveSparkPlanExec.doExecuteBroadcast  \n[SPARK-39492] [SQL] Rework MISSING_COLUMN  \n[SPARK-39679] [SQL] TakeOrderedAndProjectExec should respect child output ordering  \n[SPARK-39606] [SQL] Use child stats to estimate order operator  \n[SPARK-39611] [PYTHON][PS] Fix wrong aliases in array_ufunc  \n[SPARK-39656] [SQL][3.3] Fix wrong namespace in DescribeNamespaceExec  \n[SPARK-39675] [SQL] Switch \u2018spark.sql.codegen.factoryMode\u2019 configuration from testing purpose to internal purpose  \n[SPARK-39139] [SQL] DS V2 supports push down DS V2 UDF  \n[SPARK-39434] [SQL] Provide runtime error query context when array index is out of bounding"
    },
    {
        "id": 1658,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "[SPARK-39434] [SQL] Provide runtime error query context when array index is out of bounding  \n[SPARK-39479] [SQL] DS V2 supports push down math functions(non ANSI)  \n[SPARK-39618] [SQL] Add the REGEXP_COUNT function  \n[SPARK-39553] [CORE] Multi-thread unregister shuffle shouldn\u2019t throw NPE when using Scala 2.13  \n[SPARK-38755] [PYTHON][3.3] Add file to address missing pandas general functions  \n[SPARK-39444] [SQL] Add OptimizeSubqueries into nonExcludableRules list  \n[SPARK-39316] [SQL] Merge PromotePrecision and CheckOverflow into decimal binary arithmetic  \n[SPARK-39505] [UI] Escape log content rendered in UI  \n[SPARK-39448] [SQL] Add ReplaceCTERefWithRepartition into nonExcludableRules list  \n[SPARK-37961] [SQL] Override maxRows/maxRowsPerPartition for some logical operators  \n[SPARK-35223] Revert Add IssueNavigationLink  \n[SPARK-39633] [SQL] Support timestamp in seconds for TimeTravel using Dataframe options  \n[SPARK-38796] [SQL] Update documentation for number format strings with the {try_}to_number functions  \n[SPARK-39650] [SS] Fix incorrect value schema in streaming deduplication with backward compatibility  \n[SPARK-39636] [CORE][UI] Fix multiple bugs in JsonProtocol, impacting off heap StorageLevels and Task/Executor ResourceRequests  \n[SPARK-39432] [SQL] Return ELEMENT_AT_BY_INDEX_ZERO from element_at(*, 0)  \n[SPARK-39349] Add a centralized CheckError method for QA of error path  \n[SPARK-39453] [SQL] DS V2 supports push down misc non-aggregate functions(non ANSI)"
    },
    {
        "id": 1659,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "[SPARK-39453] [SQL] DS V2 supports push down misc non-aggregate functions(non ANSI)  \n[SPARK-38978] [SQL] DS V2 supports push down OFFSET operator  \n[SPARK-39567] [SQL] Support ANSI intervals in the percentile functions  \n[SPARK-39383] [SQL] Support DEFAULT columns in ALTER TABLE ALTER COLUMNS to V2 data sources  \n[SPARK-39396] [SQL] Fix LDAP login exception \u2018error code 49 - invalid credentials\u2019  \n[SPARK-39548] [SQL] CreateView Command with a window clause query hit a wrong window definition not found issue  \n[SPARK-39575] [AVRO] add ByteBuffer#rewind after ByteBuffer#get in Avr\u2026  \n[SPARK-39543] The option of DataFrameWriterV2 should be passed to storage properties if fallback to v1  \n[SPARK-39564] [SS] Expose the information of catalog table to the logical plan in streaming query  \n[SPARK-39582] [SQL] Fix \u201cSince\u201d marker for array_agg  \n[SPARK-39388] [SQL] Reuse orcSchema when push down Orc predicates  \n[SPARK-39511] [SQL] Enhance push down local limit 1 for right side of left semi/anti join if join condition is empty  \n[SPARK-38614] [SQL] Don\u2019t push down limit through window that\u2019s using percent_rank  \n[SPARK-39551] [SQL] Add AQE invalid plan check  \n[SPARK-39383] [SQL] Support DEFAULT columns in ALTER TABLE ADD COLUMNS to V2 data sources  \n[SPARK-39538] [SQL] Avoid creating unnecessary SLF4J Logger  \n[SPARK-39383] [SQL] Manual backport to DBR 11.x: Refactor DEFAULT column support to skip passing the primary Analyzer around  \n[SPARK-39397] [SQL] Relax AliasAwareOutputExpression to support alias with expression"
    },
    {
        "id": 1660,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "[SPARK-39397] [SQL] Relax AliasAwareOutputExpression to support alias with expression  \n[SPARK-39496] [SQL] Handle null struct in Inline.eval  \n[SPARK-39545] [SQL] Override concat method for ExpressionSet in Scala 2.13 to improve the performance  \n[SPARK-39340] [SQL] DS v2 agg pushdown should allow dots in the name of top-level columns  \n[SPARK-39488] [SQL] Simplify the error handling of TempResolvedColumn  \n[SPARK-38846] [SQL] Add explicit data mapping between Teradata Numeric Type and Spark DecimalType  \n[SPARK-39520] [SQL] Override -- method for ExpressionSet in Scala 2.13  \n[SPARK-39470] [SQL] Support cast of ANSI intervals to decimals  \n[SPARK-39477] [SQL] Remove \u201cNumber of queries\u201d info from the golden files of SQLQueryTestSuite  \n[SPARK-39419] [SQL] Fix ArraySort to throw an exception when the comparator returns null  \n[SPARK-39061] [SQL] Set nullable correctly for Inline output attributes  \n[SPARK-39320] [SQL] Support aggregate function MEDIAN  \n[SPARK-39261] [CORE] Improve newline formatting for error messages  \n[SPARK-39355] [SQL] Single column uses quoted to construct UnresolvedAttribute  \n[SPARK-39351] [SQL] SHOW CREATE TABLE should redact properties  \n[SPARK-37623] [SQL] Support ANSI Aggregate Function: regr_intercept  \n[SPARK-39374] [SQL] Improve error message for user specified column list  \n[SPARK-39255] [SQL][3.3] Improve error messages  \n[SPARK-39321] [SQL] Refactor TryCast to use RuntimeReplaceable  \n[SPARK-39406] [PYTHON] Accept NumPy array in createDataFrame"
    },
    {
        "id": 1661,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "[SPARK-39321] [SQL] Refactor TryCast to use RuntimeReplaceable  \n[SPARK-39406] [PYTHON] Accept NumPy array in createDataFrame  \n[SPARK-39267] [SQL] Clean up dsl unnecessary symbol  \n[SPARK-39171] [SQL] Unify the Cast expression  \n[SPARK-28330] [SQL] Support ANSI SQL: result offset clause in query expression  \n[SPARK-39203] [SQL] Rewrite table location to absolute URI based on database URI  \n[SPARK-39313] [SQL] toCatalystOrdering should fail if V2Expression can not be translated  \n[SPARK-39301] [SQL][PYTHON] Leverage LocalRelation and respect Arrow batch size in createDataFrame with Arrow optimization  \n[SPARK-39400] [SQL] spark-sql should remove hive resource dir in all case"
    },
    {
        "id": 1662,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "System environment\nOperating System: Ubuntu 20.04.4 LTS  \nJava: Zulu 8.56.0.21-CA-linux64  \nScala: 2.12.14  \nPython: 3.9.5  \nR: 4.1.3  \nDelta Lake: 2.1.0  \nInstalled Python libraries  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nAntergos Linux  \n2015.10 (ISO-Rolling)  \nargon2-cffi  \n20.1.0  \nasync-generator  \n1.10  \nattrs  \n21.2.0  \nbackcall  \n0.2.0  \nbackports.entry-points-selectable  \n1.1.1  \nblack  \n22.3.0  \nbleach  \n4.0.0  \nboto3  \n1.21.18  \nbotocore  \n1.24.18  \ncertifi  \n2021.10.8  \ncffi  \n1.14.6  \nchardet  \n4.0.0  \ncharset-normalizer  \n2.0.4  \nclick  \n8.0.3  \ncryptography  \n3.4.8  \ncycler  \n0.10.0  \nCython  \n0.29.24  \ndbus-python  \n1.2.16  \ndebugpy  \n1.4.1  \ndecorator  \n5.1.0  \ndefusedxml  \n0.7.1  \ndistlib  \n0.3.5  \ndistro-info  \n0.23ubuntu1  \nentrypoints  \n0.3  \nfacets-overview  \n1.0.0  \nfilelock  \n3.8.0  \nidna  \n3.2  \nipykernel  \n6.12.1  \nipython  \n7.32.0  \nipython-genutils  \n0.2.0  \nipywidgets  \n7.7.0  \njedi  \n0.18.0  \nJinja2  \n2.11.3  \njmespath  \n0.10.0  \njoblib  \n1.0.1  \njsonschema  \n3.2.0  \njupyter-client  \n6.1.12  \njupyter-core  \n4.8.1  \njupyterlab-pygments  \n0.1.2  \njupyterlab-widgets  \n1.0.0  \nkiwisolver  \n1.3.1  \nMarkupSafe  \n2.0.1  \nmatplotlib  \n3.4.3  \nmatplotlib-inline  \n0.1.2  \nmistune  \n0.8.4  \nmypy-extensions  \n0.4.3  \nnbclient  \n0.5.3  \nnbconvert  \n6.1.0"
    },
    {
        "id": 1663,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "matplotlib-inline  \n0.1.2  \nmistune  \n0.8.4  \nmypy-extensions  \n0.4.3  \nnbclient  \n0.5.3  \nnbconvert  \n6.1.0  \nnbformat  \n5.1.3  \nnest-asyncio  \n1.5.1  \nnotebook  \n6.4.5  \nnumpy  \n1.20.3  \npackaging  \n21.0  \npandas  \n1.3.4  \npandocfilters  \n1.4.3  \nparso  \n0.8.2  \npathspec  \n0.9.0  \npatsy  \n0.5.2  \npexpect  \n4.8.0  \npickleshare  \n0.7.5  \nPillow  \n8.4.0  \npip  \n21.2.4  \nplatformdirs  \n2.5.2  \nplotly  \n5.9.0  \nprometheus-client  \n0.11.0  \nprompt-toolkit  \n3.0.20  \nprotobuf  \n4.21.5  \npsutil  \n5.8.0  \npsycopg2  \n2.9.3  \nptyprocess  \n0.7.0  \npyarrow  \n7.0.0  \npycparser  \n2.20  \nPygments  \n2.10.0  \nPyGObject  \n3.36.0  \npyodbc  \n4.0.31  \npyparsing  \n3.0.4  \npyrsistent  \n0.18.0  \npython-apt  \n2.0.0+ubuntu0.20.4.7  \npython-dateutil  \n2.8.2  \npytz  \n2021.3  \npyzmq  \n22.2.1  \nrequests  \n2.26.0  \nrequests-unixsocket  \n0.2.0  \ns3transfer  \n0.5.2  \nscikit-learn  \n0.24.2  \nscipy  \n1.7.1  \nseaborn  \n0.11.2  \nSend2Trash  \n1.8.0  \nsetuptools  \n58.0.4  \nsix  \n1.16.0  \nssh-import-id  \n5.10  \nstatsmodels  \n0.12.2  \ntenacity  \n8.0.1  \nterminado  \n0.9.4  \ntestpath  \n0.5.0  \nthreadpoolctl  \n2.2.0  \ntokenize-rt  \n4.2.1  \ntomli  \n2.0.1  \ntornado  \n6.1  \ntraitlets  \n5.1.0  \ntyping-extensions  \n3.10.0.2  \nunattended-upgrades  \n0.1  \nurllib3  \n1.26.7  \nvirtualenv  \n20.8.0  \nwcwidth"
    },
    {
        "id": 1664,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "tornado  \n6.1  \ntraitlets  \n5.1.0  \ntyping-extensions  \n3.10.0.2  \nunattended-upgrades  \n0.1  \nurllib3  \n1.26.7  \nvirtualenv  \n20.8.0  \nwcwidth  \n0.2.5  \nwebencodings  \n0.5.1  \nwheel  \n0.37.0  \nwidgetsnbextension  \n3.6.0  \nInstalled R libraries  \nR libraries are installed from the Microsoft CRAN snapshot on 2022-08-15.  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \naskpass  \n1.1  \nassertthat  \n0.2.1  \nbackports  \n1.4.1  \nbase  \n4.1.3  \nbase64enc  \n0.1-3  \nbit  \n4.0.4  \nbit64  \n4.0.5  \nblob  \n1.2.3  \nboot  \n1.3-28  \nbrew  \n1.0-7  \nbrio  \n1.1.3  \nbroom  \n1.0.0  \nbslib  \n0.4.0  \ncachem  \n1.0.6  \ncallr  \n3.7.1  \ncaret  \n6.0-93  \ncellranger  \n1.1.0  \nchron  \n2.3-57  \nclass  \n7.3-20  \ncli  \n3.3.0  \nclipr  \n0.8.0  \ncluster  \n2.1.3  \ncodetools  \n0.2-18  \ncolorspace  \n2.0-3  \ncommonmark  \n1.8.0  \ncompiler  \n4.1.3  \nconfig  \n0.3.1  \ncpp11  \n0.4.2  \ncrayon  \n1.5.1  \ncredentials  \n1.3.2  \ncurl  \n4.3.2  \ndata.table  \n1.14.2  \ndatasets  \n4.1.3  \nDBI  \n1.1.3  \ndbplyr  \n2.2.1  \ndesc  \n1.4.1  \ndevtools  \n2.4.4  \ndiffobj  \n0.3.5  \ndigest  \n0.6.29  \ndownlit  \n0.4.2  \ndplyr  \n1.0.9  \ndtplyr  \n1.2.1  \ne1071  \n1.7-11  \nellipsis  \n0.3.2  \nevaluate  \n0.16  \nfansi  \n1.0.3  \nfarver  \n2.1.1  \nfastmap  \n1.1.0  \nfontawesome  \n0.3.0  \nforcats  \n0.5.1  \nforeach  \n1.5.2  \nforeign  \n0.8-82  \nforge  \n0.2.0  \nfs  \n1.5.2  \nfuture  \n1.27.0  \nfuture.apply  \n1.9.0"
    },
    {
        "id": 1665,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "forcats  \n0.5.1  \nforeach  \n1.5.2  \nforeign  \n0.8-82  \nforge  \n0.2.0  \nfs  \n1.5.2  \nfuture  \n1.27.0  \nfuture.apply  \n1.9.0  \ngargle  \n1.2.0  \ngenerics  \n0.1.3  \ngert  \n1.7.0  \nggplot2  \n3.3.6  \ngh  \n1.3.0  \ngitcreds  \n0.1.1  \nglmnet  \n4.1-4  \nglobals  \n0.16.0  \nglue  \n1.6.2  \ngoogledrive  \n2.0.0  \ngooglesheets4  \n1.0.1  \ngower  \n1.0.0  \ngraphics  \n4.1.3  \ngrDevices  \n4.1.3  \ngrid  \n4.1.3  \ngridExtra  \n2.3  \ngsubfn  \n0.7  \ngtable  \n0.3.0  \nhardhat  \n1.2.0  \nhaven  \n2.5.0  \nhighr  \n0.9  \nhms  \n1.1.1  \nhtmltools  \n0.5.3  \nhtmlwidgets  \n1.5.4  \nhttpuv  \n1.6.5  \nhttr  \n1.4.3  \nids  \n1.0.1  \nini  \n0.3.1  \nipred  \n0.9-13  \nisoband  \n0.2.5  \niterators  \n1.0.14  \njquerylib  \n0.1.4  \njsonlite  \n1.8.0  \nKernSmooth  \n2.23-20  \nknitr  \n1.39  \nlabeling  \n0.4.2  \nlater  \n1.3.0  \nlattice  \n0.20-45  \nlava  \n1.6.10  \nlifecycle  \n1.0.1  \nlistenv  \n0.8.0  \nlubridate  \n1.8.0  \nmagrittr  \n2.0.3  \nmarkdown  \n1.1  \nMASS  \n7.3-56  \nMatrix  \n1.4-1  \nmemoise  \n2.0.1  \nmethods  \n4.1.3  \nmgcv  \n1.8-40  \nmime  \n0.12  \nminiUI  \n0.1.1.1  \nModelMetrics  \n1.2.2.2  \nmodelr  \n0.1.8  \nmunsell  \n0.5.0  \nnlme  \n3.1-157  \nnnet  \n7.3-17  \nnumDeriv  \n2016.8-1.1  \nopenssl  \n2.0.2  \nparallel  \n4.1.3  \nparallelly  \n1.32.1  \npillar  \n1.8.0  \npkgbuild"
    },
    {
        "id": 1666,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "nnet  \n7.3-17  \nnumDeriv  \n2016.8-1.1  \nopenssl  \n2.0.2  \nparallel  \n4.1.3  \nparallelly  \n1.32.1  \npillar  \n1.8.0  \npkgbuild  \n1.3.1  \npkgconfig  \n2.0.3  \npkgdown  \n2.0.6  \npkgload  \n1.3.0  \nplogr  \n0.2.0  \nplyr  \n1.8.7  \npraise  \n1.0.0  \nprettyunits  \n1.1.1  \npROC  \n1.18.0  \nprocessx  \n3.7.0  \nprodlim  \n2019.11.13  \nprofvis  \n0.3.7  \nprogress  \n1.2.2  \nprogressr  \n0.10.1  \npromises  \n1.2.0.1  \nproto  \n1.0.0  \nproxy  \n0.4-27  \nps  \n1.7.1  \npurrr  \n0.3.4  \nr2d3  \n0.2.6  \nR6  \n2.5.1  \nragg  \n1.2.2  \nrandomForest  \n4.7-1.1  \nrappdirs  \n0.3.3  \nrcmdcheck  \n1.4.0  \nRColorBrewer  \n1.1-3  \nRcpp  \n1.0.9  \nRcppEigen  \n0.3.3.9.2  \nreadr  \n2.1.2  \nreadxl  \n1.4.0  \nrecipes  \n1.0.1  \nrematch  \n1.0.1  \nrematch2  \n2.1.2  \nremotes  \n2.4.2  \nreprex  \n2.0.1  \nreshape2  \n1.4.4  \nrlang  \n1.0.4  \nrmarkdown  \n2.14  \nRODBC  \n1.3-19  \nroxygen2  \n7.2.1  \nrpart  \n4.1.16  \nrprojroot  \n2.0.3  \nRserve  \n1.8-11  \nRSQLite  \n2.2.15  \nrstudioapi  \n0.13  \nrversions  \n2.1.1  \nrvest  \n1.0.2  \nsass  \n0.4.2  \nscales  \n1.2.0  \nselectr  \n0.4-2  \nsessioninfo  \n1.2.2  \nshape  \n1.4.6  \nshiny  \n1.7.2  \nsourcetools  \n0.1.7  \nsparklyr  \n1.7.7  \nSparkR  \n3.3.0  \nspatial  \n7.3-11  \nsplines  \n4.1.3  \nsqldf  \n0.4-11"
    },
    {
        "id": 1667,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "sourcetools  \n0.1.7  \nsparklyr  \n1.7.7  \nSparkR  \n3.3.0  \nspatial  \n7.3-11  \nsplines  \n4.1.3  \nsqldf  \n0.4-11  \nSQUAREM  \n2021.1  \nstats  \n4.1.3  \nstats4  \n4.1.3  \nstringi  \n1.7.8  \nstringr  \n1.4.0  \nsurvival  \n3.2-13  \nsys  \n3.4  \nsystemfonts  \n1.0.4  \ntcltk  \n4.1.3  \ntestthat  \n3.1.4  \ntextshaping  \n0.3.6  \ntibble  \n3.1.8  \ntidyr  \n1.2.0  \ntidyselect  \n1.1.2  \ntidyverse  \n1.3.2  \ntimeDate  \n4021.104  \ntinytex  \n0.40  \ntools  \n4.1.3  \ntzdb  \n0.3.0  \nurlchecker  \n1.0.1  \nusethis  \n2.1.6  \nutf8  \n1.2.2  \nutils  \n4.1.3  \nuuid  \n1.1-0  \nvctrs  \n0.4.1  \nviridisLite  \n0.4.0  \nvroom  \n1.5.7  \nwaldo  \n0.4.0  \nwhisker  \n0.4  \nwithr  \n2.5.0  \nxfun  \n0.32  \nxml2  \n1.3.3  \nxopen  \n1.0.0  \nxtable  \n1.8-4  \nyaml  \n2.3.5  \nzip  \n2.2.0  \nInstalled Java and Scala libraries (Scala 2.12 cluster version)  \nGroup ID  \nArtifact ID  \nVersion  \nantlr  \nantlr  \n2.7.7  \ncom.amazonaws  \namazon-kinesis-client  \n1.12.0  \ncom.amazonaws  \naws-java-sdk-autoscaling  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudformation  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudfront  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudhsm  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudsearch  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudtrail  \n1.12.189  \ncom.amazonaws"
    },
    {
        "id": 1668,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "aws-java-sdk-cloudsearch  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudtrail  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudwatch  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cloudwatchmetrics  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-codedeploy  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cognitoidentity  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-cognitosync  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-config  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-core  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-datapipeline  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-directconnect  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-directory  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-dynamodb  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-ec2  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-ecs  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-efs  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-elasticache  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-elasticbeanstalk  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-elasticloadbalancing  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-elastictranscoder  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-emr  \n1.12.189"
    },
    {
        "id": 1669,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "com.amazonaws  \naws-java-sdk-elastictranscoder  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-emr  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-glacier  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-glue  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-iam  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-importexport  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-kinesis  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-kms  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-lambda  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-logs  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-machinelearning  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-opsworks  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-rds  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-redshift  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-route53  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-s3  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-ses  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-simpledb  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-simpleworkflow  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-sns  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-sqs  \n1.12.189  \ncom.amazonaws"
    },
    {
        "id": 1670,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "aws-java-sdk-sns  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-sqs  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-ssm  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-storagegateway  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-sts  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-support  \n1.12.189  \ncom.amazonaws  \naws-java-sdk-swf-libraries  \n1.11.22  \ncom.amazonaws  \naws-java-sdk-workspaces  \n1.12.189  \ncom.amazonaws  \njmespath-java  \n1.12.189  \ncom.chuusai  \nshapeless_2.12  \n2.3.3  \ncom.clearspring.analytics  \nstream  \n2.9.6  \ncom.databricks  \nRserve  \n1.8-3  \ncom.databricks  \njets3t  \n0.7.1-0  \ncom.databricks.scalapb  \ncompilerplugin_2.12  \n0.4.15-10  \ncom.databricks.scalapb  \nscalapb-runtime_2.12  \n0.4.15-10  \ncom.esotericsoftware  \nkryo-shaded  \n4.0.2  \ncom.esotericsoftware  \nminlog  \n1.3.0  \ncom.fasterxml  \nclassmate  \n1.3.4  \ncom.fasterxml.jackson.core  \njackson-annotations  \n2.13.3  \ncom.fasterxml.jackson.core  \njackson-core  \n2.13.3  \ncom.fasterxml.jackson.core  \njackson-databind  \n2.13.3  \ncom.fasterxml.jackson.dataformat  \njackson-dataformat-cbor  \n2.13.3  \ncom.fasterxml.jackson.datatype  \njackson-datatype-joda  \n2.13.3  \ncom.fasterxml.jackson.datatype  \njackson-datatype-jsr310  \n2.13.3  \ncom.fasterxml.jackson.module  \njackson-module-paranamer"
    },
    {
        "id": 1671,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "com.fasterxml.jackson.datatype  \njackson-datatype-jsr310  \n2.13.3  \ncom.fasterxml.jackson.module  \njackson-module-paranamer  \n2.13.3  \ncom.fasterxml.jackson.module  \njackson-module-scala_2.12  \n2.13.3  \ncom.github.ben-manes.caffeine  \ncaffeine  \n2.3.4  \ncom.github.fommil  \njniloader  \n1.1  \ncom.github.fommil.netlib  \ncore  \n1.1.2  \ncom.github.fommil.netlib  \nnative_ref-java  \n1.1  \ncom.github.fommil.netlib  \nnative_ref-java-natives  \n1.1  \ncom.github.fommil.netlib  \nnative_system-java  \n1.1  \ncom.github.fommil.netlib  \nnative_system-java-natives  \n1.1  \ncom.github.fommil.netlib  \nnetlib-native_ref-linux-x86_64-natives  \n1.1  \ncom.github.fommil.netlib  \nnetlib-native_system-linux-x86_64-natives  \n1.1  \ncom.github.luben  \nzstd-jni  \n1.5.2-1  \ncom.github.wendykierp  \nJTransforms  \n3.1  \ncom.google.code.findbugs  \njsr305  \n3.0.0  \ncom.google.code.gson  \ngson  \n2.8.6  \ncom.google.crypto.tink  \ntink  \n1.6.1  \ncom.google.flatbuffers  \nflatbuffers-java  \n1.12.0  \ncom.google.guava  \nguava  \n15.0  \ncom.google.protobuf  \nprotobuf-java  \n2.6.1  \ncom.h2database  \nh2  \n2.0.204  \ncom.helger  \nprofiler  \n1.1.1  \ncom.jcraft  \njsch  \n0.1.50  \ncom.jolbox  \nbonecp  \n0.8.0.RELEASE  \ncom.lihaoyi  \nsourcecode_2.12  \n0.1.9  \ncom.microsoft.azure  \nazure-data-lake-store-sdk"
    },
    {
        "id": 1672,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "com.jolbox  \nbonecp  \n0.8.0.RELEASE  \ncom.lihaoyi  \nsourcecode_2.12  \n0.1.9  \ncom.microsoft.azure  \nazure-data-lake-store-sdk  \n2.3.9  \ncom.ning  \ncompress-lzf  \n1.1  \ncom.sun.mail  \njavax.mail  \n1.5.2  \ncom.tdunning  \njson  \n1.8  \ncom.thoughtworks.paranamer  \nparanamer  \n2.8  \ncom.trueaccord.lenses  \nlenses_2.12  \n0.4.12  \ncom.twitter  \nchill-java  \n0.10.0  \ncom.twitter  \nchill_2.12  \n0.10.0  \ncom.twitter  \nutil-app_2.12  \n7.1.0  \ncom.twitter  \nutil-core_2.12  \n7.1.0  \ncom.twitter  \nutil-function_2.12  \n7.1.0  \ncom.twitter  \nutil-jvm_2.12  \n7.1.0  \ncom.twitter  \nutil-lint_2.12  \n7.1.0  \ncom.twitter  \nutil-registry_2.12  \n7.1.0  \ncom.twitter  \nutil-stats_2.12  \n7.1.0  \ncom.typesafe  \nconfig  \n1.2.1  \ncom.typesafe.scala-logging  \nscala-logging_2.12  \n3.7.2  \ncom.uber  \nh3  \n3.7.0  \ncom.univocity  \nunivocity-parsers  \n2.9.1  \ncom.zaxxer  \nHikariCP  \n4.0.3  \ncommons-cli  \ncommons-cli  \n1.5.0  \ncommons-codec  \ncommons-codec  \n1.15  \ncommons-collections  \ncommons-collections  \n3.2.2  \ncommons-dbcp  \ncommons-dbcp  \n1.4  \ncommons-fileupload  \ncommons-fileupload  \n1.3.3  \ncommons-httpclient  \ncommons-httpclient  \n3.1  \ncommons-io  \ncommons-io  \n2.11.0  \ncommons-lang  \ncommons-lang  \n2.6  \ncommons-logging  \ncommons-logging  \n1.1.3  \ncommons-pool  \ncommons-pool  \n1.5.4  \ndev.ludovic.netlib  \narpack  \n2.2.1  \ndev.ludovic.netlib  \nblas  \n2.2.1  \ndev.ludovic.netlib  \nlapack"
    },
    {
        "id": 1673,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "1.5.4  \ndev.ludovic.netlib  \narpack  \n2.2.1  \ndev.ludovic.netlib  \nblas  \n2.2.1  \ndev.ludovic.netlib  \nlapack  \n2.2.1  \nhadoop3  \njets3t-0.7  \nliball_deps_2.12  \ninfo.ganglia.gmetric4j  \ngmetric4j  \n1.0.10  \nio.airlift  \naircompressor  \n0.21  \nio.delta  \ndelta-sharing-spark_2.12  \n0.5.0  \nio.dropwizard.metrics  \nmetrics-core  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-graphite  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-healthchecks  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jetty9  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jmx  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-json  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jvm  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-servlets  \n4.1.1  \nio.netty  \nnetty-all  \n4.1.74.Final  \nio.netty  \nnetty-buffer  \n4.1.74.Final  \nio.netty  \nnetty-codec  \n4.1.74.Final  \nio.netty  \nnetty-common  \n4.1.74.Final  \nio.netty  \nnetty-handler  \n4.1.74.Final  \nio.netty  \nnetty-resolver  \n4.1.74.Final  \nio.netty  \nnetty-tcnative-classes  \n2.0.48.Final  \nio.netty  \nnetty-transport  \n4.1.74.Final  \nio.netty  \nnetty-transport-classes-epoll  \n4.1.74.Final  \nio.netty  \nnetty-transport-classes-kqueue  \n4.1.74.Final  \nio.netty  \nnetty-transport-native-epoll-linux-aarch_64  \n4.1.74.Final  \nio.netty  \nnetty-transport-native-epoll-linux-x86_64"
    },
    {
        "id": 1674,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "netty-transport-native-epoll-linux-aarch_64  \n4.1.74.Final  \nio.netty  \nnetty-transport-native-epoll-linux-x86_64  \n4.1.74.Final  \nio.netty  \nnetty-transport-native-kqueue-osx-aarch_64  \n4.1.74.Final  \nio.netty  \nnetty-transport-native-kqueue-osx-x86_64  \n4.1.74.Final  \nio.netty  \nnetty-transport-native-unix-common  \n4.1.74.Final  \nio.prometheus  \nsimpleclient  \n0.7.0  \nio.prometheus  \nsimpleclient_common  \n0.7.0  \nio.prometheus  \nsimpleclient_dropwizard  \n0.7.0  \nio.prometheus  \nsimpleclient_pushgateway  \n0.7.0  \nio.prometheus  \nsimpleclient_servlet  \n0.7.0  \nio.prometheus.jmx  \ncollector  \n0.12.0  \njakarta.annotation  \njakarta.annotation-api  \n1.3.5  \njakarta.servlet  \njakarta.servlet-api  \n4.0.3  \njakarta.validation  \njakarta.validation-api  \n2.0.2  \njakarta.ws.rs  \njakarta.ws.rs-api  \n2.1.6  \njavax.activation  \nactivation  \n1.1.1  \njavax.annotation  \njavax.annotation-api  \n1.3.2  \njavax.el  \njavax.el-api  \n2.2.4  \njavax.jdo  \njdo-api  \n3.0.1  \njavax.transaction  \njta  \n1.1  \njavax.transaction  \ntransaction-api  \n1.1  \njavax.xml.bind  \njaxb-api  \n2.2.11  \njavolution  \njavolution  \n5.5.1  \njline  \njline  \n2.14.6  \njoda-time  \njoda-time  \n2.10.13  \nmvn  \nhadoop3  \nliball_deps_2.12  \nnet.java.dev.jna  \njna  \n5.8.0  \nnet.razorvine  \npickle  \n1.2  \nnet.sf.jpam  \njpam  \n1.1  \nnet.sf.opencsv  \nopencsv  \n2.3"
    },
    {
        "id": 1675,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "net.java.dev.jna  \njna  \n5.8.0  \nnet.razorvine  \npickle  \n1.2  \nnet.sf.jpam  \njpam  \n1.1  \nnet.sf.opencsv  \nopencsv  \n2.3  \nnet.sf.supercsv  \nsuper-csv  \n2.2.0  \nnet.snowflake  \nsnowflake-ingest-sdk  \n0.9.6  \nnet.snowflake  \nsnowflake-jdbc  \n3.13.14  \nnet.snowflake  \nspark-snowflake_2.12  \n2.10.0-spark_3.2  \nnet.sourceforge.f2j  \narpack_combined_all  \n0.1  \norg.acplt.remotetea  \nremotetea-oncrpc  \n1.1.2  \norg.antlr  \nST4  \n4.0.4  \norg.antlr  \nantlr-runtime  \n3.5.2  \norg.antlr  \nantlr4-runtime  \n4.8  \norg.antlr  \nstringtemplate  \n3.2.1  \norg.apache.ant  \nant  \n1.9.2  \norg.apache.ant  \nant-jsch  \n1.9.2  \norg.apache.ant  \nant-launcher  \n1.9.2  \norg.apache.arrow  \narrow-format  \n7.0.0  \norg.apache.arrow  \narrow-memory-core  \n7.0.0  \norg.apache.arrow  \narrow-memory-netty  \n7.0.0  \norg.apache.arrow  \narrow-vector  \n7.0.0  \norg.apache.avro  \navro  \n1.11.0  \norg.apache.avro  \navro-ipc  \n1.11.0  \norg.apache.avro  \navro-mapred  \n1.11.0  \norg.apache.commons  \ncommons-collections4  \n4.4  \norg.apache.commons  \ncommons-compress  \n1.21  \norg.apache.commons  \ncommons-crypto  \n1.1.0  \norg.apache.commons  \ncommons-lang3  \n3.12.0  \norg.apache.commons  \ncommons-math3  \n3.6.1  \norg.apache.commons  \ncommons-text  \n1.9  \norg.apache.curator  \ncurator-client  \n2.13.0  \norg.apache.curator  \ncurator-framework  \n2.13.0  \norg.apache.curator  \ncurator-recipes  \n2.13.0  \norg.apache.derby  \nderby  \n10.14.2.0  \norg.apache.hadoop  \nhadoop-client-api"
    },
    {
        "id": 1676,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "curator-framework  \n2.13.0  \norg.apache.curator  \ncurator-recipes  \n2.13.0  \norg.apache.derby  \nderby  \n10.14.2.0  \norg.apache.hadoop  \nhadoop-client-api  \n3.3.2-databricks  \norg.apache.hadoop  \nhadoop-client-runtime  \n3.3.2  \norg.apache.hive  \nhive-beeline  \n2.3.9  \norg.apache.hive  \nhive-cli  \n2.3.9  \norg.apache.hive  \nhive-jdbc  \n2.3.9  \norg.apache.hive  \nhive-llap-client  \n2.3.9  \norg.apache.hive  \nhive-llap-common  \n2.3.9  \norg.apache.hive  \nhive-serde  \n2.3.9  \norg.apache.hive  \nhive-shims  \n2.3.9  \norg.apache.hive  \nhive-storage-api  \n2.7.2  \norg.apache.hive.shims  \nhive-shims-0.23  \n2.3.9  \norg.apache.hive.shims  \nhive-shims-common  \n2.3.9  \norg.apache.hive.shims  \nhive-shims-scheduler  \n2.3.9  \norg.apache.httpcomponents  \nhttpclient  \n4.5.13  \norg.apache.httpcomponents  \nhttpcore  \n4.4.14  \norg.apache.ivy  \nivy  \n2.5.0  \norg.apache.logging.log4j  \nlog4j-1.2-api  \n2.17.2  \norg.apache.logging.log4j  \nlog4j-api  \n2.17.2  \norg.apache.logging.log4j  \nlog4j-core  \n2.17.2  \norg.apache.logging.log4j  \nlog4j-slf4j-impl  \n2.17.2  \norg.apache.mesos  \nmesos-shaded-protobuf  \n1.4.0  \norg.apache.orc  \norc-core  \n1.7.5  \norg.apache.orc  \norc-mapreduce  \n1.7.5  \norg.apache.orc  \norc-shims  \n1.7.5  \norg.apache.parquet  \nparquet-column  \n1.12.0-databricks-0004  \norg.apache.parquet  \nparquet-common  \n1.12.0-databricks-0004  \norg.apache.parquet"
    },
    {
        "id": 1677,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "org.apache.parquet  \nparquet-column  \n1.12.0-databricks-0004  \norg.apache.parquet  \nparquet-common  \n1.12.0-databricks-0004  \norg.apache.parquet  \nparquet-encoding  \n1.12.0-databricks-0004  \norg.apache.parquet  \nparquet-format-structures  \n1.12.0-databricks-0004  \norg.apache.parquet  \nparquet-hadoop  \n1.12.0-databricks-0004  \norg.apache.parquet  \nparquet-jackson  \n1.12.0-databricks-0004  \norg.apache.thrift  \nlibfb303  \n0.9.3  \norg.apache.thrift  \nlibthrift  \n0.12.0  \norg.apache.xbean  \nxbean-asm9-shaded  \n4.20  \norg.apache.yetus  \naudience-annotations  \n0.5.0  \norg.apache.zookeeper  \nzookeeper  \n3.6.2  \norg.apache.zookeeper  \nzookeeper-jute  \n3.6.2  \norg.checkerframework  \nchecker-qual  \n3.5.0  \norg.codehaus.jackson  \njackson-core-asl  \n1.9.13  \norg.codehaus.jackson  \njackson-mapper-asl  \n1.9.13  \norg.codehaus.janino  \ncommons-compiler  \n3.0.16  \norg.codehaus.janino  \njanino  \n3.0.16  \norg.datanucleus  \ndatanucleus-api-jdo  \n4.2.4  \norg.datanucleus  \ndatanucleus-core  \n4.1.17  \norg.datanucleus  \ndatanucleus-rdbms  \n4.1.19  \norg.datanucleus  \njavax.jdo  \n3.2.0-m3  \norg.eclipse.jetty  \njetty-client  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-continuation  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-http  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-io  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-jndi  \n9.4.46.v20220331"
    },
    {
        "id": 1678,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "org.eclipse.jetty  \njetty-io  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-jndi  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-plus  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-proxy  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-security  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-server  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-servlet  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-servlets  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-util  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-util-ajax  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-webapp  \n9.4.46.v20220331  \norg.eclipse.jetty  \njetty-xml  \n9.4.46.v20220331  \norg.eclipse.jetty.websocket  \nwebsocket-api  \n9.4.46.v20220331  \norg.eclipse.jetty.websocket  \nwebsocket-client  \n9.4.46.v20220331  \norg.eclipse.jetty.websocket  \nwebsocket-common  \n9.4.46.v20220331  \norg.eclipse.jetty.websocket  \nwebsocket-server  \n9.4.46.v20220331  \norg.eclipse.jetty.websocket  \nwebsocket-servlet  \n9.4.46.v20220331  \norg.fusesource.leveldbjni  \nleveldbjni-all  \n1.8  \norg.glassfish.hk2  \nhk2-api  \n2.6.1  \norg.glassfish.hk2  \nhk2-locator  \n2.6.1  \norg.glassfish.hk2  \nhk2-utils  \n2.6.1  \norg.glassfish.hk2  \nosgi-resource-locator  \n1.0.3  \norg.glassfish.hk2.external  \naopalliance-repackaged  \n2.6.1"
    },
    {
        "id": 1679,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "2.6.1  \norg.glassfish.hk2  \nosgi-resource-locator  \n1.0.3  \norg.glassfish.hk2.external  \naopalliance-repackaged  \n2.6.1  \norg.glassfish.hk2.external  \njakarta.inject  \n2.6.1  \norg.glassfish.jersey.containers  \njersey-container-servlet  \n2.34  \norg.glassfish.jersey.containers  \njersey-container-servlet-core  \n2.34  \norg.glassfish.jersey.core  \njersey-client  \n2.34  \norg.glassfish.jersey.core  \njersey-common  \n2.34  \norg.glassfish.jersey.core  \njersey-server  \n2.34  \norg.glassfish.jersey.inject  \njersey-hk2  \n2.34  \norg.hibernate.validator  \nhibernate-validator  \n6.1.0.Final  \norg.javassist  \njavassist  \n3.25.0-GA  \norg.jboss.logging  \njboss-logging  \n3.3.2.Final  \norg.jdbi  \njdbi  \n2.63.1  \norg.jetbrains  \nannotations  \n17.0.0  \norg.joda  \njoda-convert  \n1.7  \norg.jodd  \njodd-core  \n3.5.2  \norg.json4s  \njson4s-ast_2.12  \n3.7.0-M11  \norg.json4s  \njson4s-core_2.12  \n3.7.0-M11  \norg.json4s  \njson4s-jackson_2.12  \n3.7.0-M11  \norg.json4s  \njson4s-scalap_2.12  \n3.7.0-M11  \norg.lz4  \nlz4-java  \n1.8.0  \norg.mariadb.jdbc  \nmariadb-java-client  \n2.7.4  \norg.mlflow  \nmlflow-spark  \n1.27.0  \norg.objenesis  \nobjenesis  \n2.5.1  \norg.postgresql  \npostgresql  \n42.3.3  \norg.roaringbitmap  \nRoaringBitmap  \n0.9.25  \norg.roaringbitmap  \nshims  \n0.9.25  \norg.rocksdb  \nrocksdbjni  \n6.24.2  \norg.rosuda.REngine  \nREngine"
    },
    {
        "id": 1680,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "0.9.25  \norg.roaringbitmap  \nshims  \n0.9.25  \norg.rocksdb  \nrocksdbjni  \n6.24.2  \norg.rosuda.REngine  \nREngine  \n2.1.0  \norg.scala-lang  \nscala-compiler_2.12  \n2.12.14  \norg.scala-lang  \nscala-library_2.12  \n2.12.14  \norg.scala-lang  \nscala-reflect_2.12  \n2.12.14  \norg.scala-lang.modules  \nscala-collection-compat_2.12  \n2.4.3  \norg.scala-lang.modules  \nscala-parser-combinators_2.12  \n1.1.2  \norg.scala-lang.modules  \nscala-xml_2.12  \n1.2.0  \norg.scala-sbt  \ntest-interface  \n1.0  \norg.scalacheck  \nscalacheck_2.12  \n1.14.2  \norg.scalactic  \nscalactic_2.12  \n3.0.8  \norg.scalanlp  \nbreeze-macros_2.12  \n1.2  \norg.scalanlp  \nbreeze_2.12  \n1.2  \norg.scalatest  \nscalatest_2.12  \n3.0.8  \norg.slf4j  \njcl-over-slf4j  \n1.7.36  \norg.slf4j  \njul-to-slf4j  \n1.7.36  \norg.slf4j  \nslf4j-api  \n1.7.36  \norg.spark-project.spark  \nunused  \n1.0.0  \norg.threeten  \nthreeten-extra  \n1.5.0  \norg.tukaani  \nxz  \n1.8  \norg.typelevel  \nalgebra_2.12  \n2.0.1  \norg.typelevel  \ncats-kernel_2.12  \n2.1.1  \norg.typelevel  \nmacro-compat_2.12  \n1.1.1  \norg.typelevel  \nspire-macros_2.12  \n0.17.0  \norg.typelevel  \nspire-platform_2.12  \n0.17.0  \norg.typelevel  \nspire-util_2.12  \n0.17.0  \norg.typelevel  \nspire_2.12"
    },
    {
        "id": 1681,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/11.2.html",
        "content": "org.typelevel  \nspire-platform_2.12  \n0.17.0  \norg.typelevel  \nspire-util_2.12  \n0.17.0  \norg.typelevel  \nspire_2.12  \n0.17.0  \norg.wildfly.openssl  \nwildfly-openssl  \n1.0.7.Final  \norg.xerial  \nsqlite-jdbc  \n3.8.11.2  \norg.xerial.snappy  \nsnappy-java  \n1.1.8.4  \norg.yaml  \nsnakeyaml  \n1.24  \noro  \noro  \n2.0.8  \npl.edu.icm  \nJLargeArrays  \n1.5  \nsoftware.amazon.ion  \nion-java  \n1.0.2  \nstax  \nstax-api  \n1.0.1"
    },
    {
        "id": 1682,
        "url": "https://docs.databricks.com/en/archive/repos/git-version-control-legacy.html",
        "content": "Git version control for notebooks (legacy)  \nImportant  \nLegacy notebook Git integration support was removed on January 31st, 2024. Databricks recommends that you use Databricks Git folders to sync your work in Databricks with a remote Git repository.  \nThis article describes how to set up Git version control for notebooks (legacy feature). You can also use the Databricks CLI or Workspace API to import and export notebooks and to perform Git operations in your local development environment.  \nEnable and disable Git versioning\nEnable and disable Git versioning\nBy default version control is enabled. To toggle this setting:  \nGo to Settings > Workspace settings.  \nIn the Advanced section, deselect the Notebook Git Versioning toggle.\n\nConfigure version control\nConfigure version control\nTo configure version control, create access credentials in your Git provider, then add those credentials to Databricks.\n\nWork with notebook versions"
    },
    {
        "id": 1683,
        "url": "https://docs.databricks.com/en/archive/repos/git-version-control-legacy.html",
        "content": "Work with notebook versions\nYou work with notebook versions in the history panel. Open the history panel by clicking in the right sidebar.  \nNote  \nYou cannot modify a notebook while the history panel is open.  \nLink a notebook to GitHub  \nClick in the right sidebar. The Git status bar displays Git: Not linked.  \nClick Git: Not linked.  \nThe Git Preferences dialog appears. The first time you open your notebook, the Status is Unlink, because the notebook is not in GitHub.  \nIn the Status field, click Link.  \nIn the Link field, paste the URL of the GitHub repository.  \nClick the Branch drop-down and select a branch or type the name of a new branch.  \nIn the Path in Git folder field, specify where in the repository to store your file.  \nPython notebooks have the suggested default file extension .py. If you use .ipynb, your notebook will save in iPython notebook format. If the file already exists on GitHub, you can directly copy and paste the URL of the file.  \nClick Save to finish linking your notebook. If this file did not previously exist, a prompt with the option Save this file to your GitHub repo displays.  \nType a message and click Save.  \nSave a notebook to GitHub  \nWhile the changes that you make to your notebook are saved automatically to the Databricks version history, changes do not automatically persist to GitHub.  \nClick in the right sidebar to open the history panel.  \nClick Save Now to save your notebook to GitHub. The Save Notebook Version dialog appears.  \nOptionally, enter a message to describe your change.  \nMake sure that Also commit to Git is selected.  \nClick Save.  \nRevert or update a notebook to a version from GitHub  \nOnce you link a notebook, Databricks syncs your history with Git every time you re-open the history panel. Versions that sync to Git have commit hashes as part of the entry.  \nClick in the right sidebar to open the history panel.  \nChoose an entry in the history panel. Databricks displays that version.  \nClick Restore this version.  \nClick Confirm to confirm that you want to restore that version.  \nUnlink a notebook  \nClick in the right sidebar to open the history panel.  \nThe Git status bar displays Git: Synced.  \nClick Git: Synced.  \nIn the Git Preferences dialog, click Unlink.  \nClick Save."
    },
    {
        "id": 1684,
        "url": "https://docs.databricks.com/en/archive/repos/git-version-control-legacy.html",
        "content": "Unlink a notebook  \nClick in the right sidebar to open the history panel.  \nThe Git status bar displays Git: Synced.  \nClick Git: Synced.  \nIn the Git Preferences dialog, click Unlink.  \nClick Save.  \nClick Confirm to confirm that you want to unlink the notebook from version control."
    },
    {
        "id": 1685,
        "url": "https://docs.databricks.com/en/archive/repos/git-version-control-legacy.html",
        "content": "Use branches\nYou can work on any branch of your repository and create new branches inside Databricks.  \nCreate a branch  \nClick in the right sidebar to open the history panel.  \nClick the Git status bar to open the GitHub panel.  \nClick the Branch dropdown.  \nEnter a branch name.  \nSelect the Create Branch option at the bottom of the dropdown. The parent branch is indicated. You always branch from your current selected branch.  \nCreate a pull request  \nClick in the right sidebar to open the history panel.  \nClick the Git status bar to open the GitHub panel.  \nClick Create PR. GitHub opens to a pull request page for the branch.  \nRebase a branch  \nYou can also rebase your branch inside Databricks. The Rebase link displays if new commits are available in the parent branch. Only rebasing on top of the default branch of the parent repository is supported.  \nFor example, assume that you are working on databricks/reference-apps. You fork it into your own account (for example, brkyvz) and start working on a branch called my-branch. If a new update is pushed to databricks:master, then the Rebase button displays, and you will be able to pull the changes into your branch brkyvz:my-branch.  \nRebasing works a little differently in Databricks. Assume the following branch structure:  \nAfter a rebase, the branch structure looks like:  \nWhat\u2019s different here is that Commits C5 and C6 do not apply on top of C4. They appear as local changes in your notebook. Merge conflicts show up as follows:  \nYou can then commit to GitHub once again using the Save Now button.  \nWhat happens if someone branched off from my branch that I just rebased?  \nIf your branch (for example, branch-a) was the base for another branch (branch-b), and you rebase, you need not worry! Once a user also rebases branch-b, everything will work out. The best practice in this situation is to use separate branches for separate notebooks.  \nBest practices for code reviews  \nDatabricks supports Git branching.  \nYou can link a notebook to any branch in a repository. Databricks recommends using a separate branch for each notebook."
    },
    {
        "id": 1686,
        "url": "https://docs.databricks.com/en/archive/repos/git-version-control-legacy.html",
        "content": "Best practices for code reviews  \nDatabricks supports Git branching.  \nYou can link a notebook to any branch in a repository. Databricks recommends using a separate branch for each notebook.  \nDuring development, you can link a notebook to a fork of a repository or to a non-default branch in the main repository. To integrate your changes upstream, you can use the Create PR link in the Git Preferences dialog in Databricks to create a GitHub pull request. The Create PR link displays only if you\u2019re not working on the default branch of the parent repository."
    },
    {
        "id": 1687,
        "url": "https://docs.databricks.com/en/archive/repos/git-version-control-legacy.html",
        "content": "Troubleshooting\nTroubleshooting\nIf you receive errors related to syncing GitHub history, verify the following:  \nYou can only link a notebook to an initialized Git repository that isn\u2019t empty. Test the URL in a web browser.  \nThe GitHub personal access token must be active.  \nTo use a private GitHub repository, you must have permission to read the repository.  \nIf a notebook is linked to a GitHub branch that is renamed, the change is not automatically reflected in Databricks. You must re-link the notebook to the branch manually.\n\nMigrate to Databricks Git folders\nMigrate to Databricks Git folders\nUsers that need to migrate to Databricks Git folders from the legacy Git version control can use the following guide:  \nSwitching to Databricks Git folders from Legacy Git integration"
    },
    {
        "id": 1688,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "1.6.3-db1 cluster image  \nDatabricks released this update in November, 2016.  \nImportant  \nThis release has been deprecated. For more information about the Databricks Runtime deprecation policy and schedule, see Databricks support lifecycles.  \nThe following release notes provide information about the Spark 1.6.3-db1 cluster image powered by Apache Spark.  \nApache Spark\nApache Spark\nIn this release, Spark refers to Apache Spark 1.6.3. For more information, please see Apache Spark 1.6.3 release notes.\n\nChanges and Improvements"
    },
    {
        "id": 1689,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "Changes and Improvements\nOperating system upgraded to Ubuntu 16.04.1 LTS from Ubuntu 15.10.  \nPython upgraded to 2.7.12 from 2.7.10.  \nMost pre-installed Python libraries upgraded. For the list of Python libraries and their versions, see Pre-installed Python Libraries.  \nFixed issues around unicode characters in Python notebooks  \nFor Hadoop 2 version, Apache Hadoop upgraded to 2.7.3 from 2.7.2.  \nFor Hadoop 2 version, the default value of mapreduce.fileoutputcommitter.algorithm.version changed to 2 from 1.\n\nSystem Environment"
    },
    {
        "id": 1690,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "System Environment\nOperating System: Ubuntu 16.04.1 LTS  \nJava: 1.8.0_91  \nScala: 2.10.6  \nPython: 2.7.12  \nR: R version 3.2.3 (2015-12-10)  \nPre-installed Python Libraries  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nansi2html  \n1.1.1  \nargparse  \n1.2.1  \nboto  \n2.42.0  \nboto3  \n1.4.1  \nbotocore  \n1.4.70  \nbrewer2mpl  \n1.4.1  \ncertifi  \n2016.2.28  \ncffi  \n1.7.0  \nchardet  \n2.3.0  \ncolorama  \n0.3.7  \nconfigobj  \n5.0.6  \ncryptography  \n1.5  \ncycler  \n0.10.0  \nCython  \n0.24.1  \ndecorator  \n4.0.10  \ndocutils  \n0.12  \nenum34  \n1.1.6  \net-xmlfile  \n1.0.1  \nfreetype-py  \n1.0.2  \nfuncsigs  \n1.0.2  \nfusepy  \n2.0.4  \nfutures  \n3.0.5  \nggplot  \n0.6.8  \nhtml5lib  \n0.999  \nidna  \n2.1  \nipaddress  \n1.0.16  \nipython  \n2.2.0  \nipython-genutils  \n0.1.0  \njdcal  \n1.2  \nJinja2  \n2.8  \njmespath  \n0.9.0  \nllvmlite  \n0.13.0  \nlxml  \n3.6.4  \nMarkupSafe  \n0.23  \nmatplotlib  \n1.5.3  \nmpld3  \n0.2  \nmsgpack-python  \n0.4.7  \nndg-httpsclient  \n0.3.3  \nnumba  \n0.28.1  \nnumpy  \n1.11.1  \nopenpyxl  \n2.3.2  \npandas  \n0.18.1  \npathlib2  \n2.1.0  \npatsy  \n0.4.1  \npexpect  \n4.0.1  \npickleshare  \n0.7.4  \nPillow  \n3.3.1  \npip  \n8.1.2  \npkg_resources  \n0.0.0  \nply  \n3.9  \nprompt-toolkit  \n1.0.7  \npsycopg2  \n2.6.2  \nptyprocess  \n0.5.1  \npy4j  \n0.10.3  \npyasn1"
    },
    {
        "id": 1691,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "0.0.0  \nply  \n3.9  \nprompt-toolkit  \n1.0.7  \npsycopg2  \n2.6.2  \nptyprocess  \n0.5.1  \npy4j  \n0.10.3  \npyasn1  \n0.1.9  \npycparser  \n2.14  \nPygments  \n2.1.3  \nPyGObject  \n3.20.0  \npyOpenSSL  \n16.0.0  \npyparsing  \n2.1.4  \npypng  \n0.0.18  \nPython  \n2.7.12  \npython-dateutil  \n2.5.3  \npython-geohash  \n0.8.5  \npytz  \n2016.6.1  \nrequests  \n2.11.1  \ns3transfer  \n0.1.9  \nscikit-learn  \n0.17.1  \nscipy  \n0.18.1  \nscour  \n0.32  \nseaborn  \n0.7.1  \nsetuptools  \n28.6.0  \nsimplejson  \n3.8.2  \nsimples3  \n1.0  \nsingledispatch  \n3.4.0.3  \nsix  \n1.10.0  \nstatsmodels  \n0.6.1  \ntraitlets  \n4.3.0  \nurllib3  \n1.19.1  \nvirtualenv  \n15.0.1  \nwcwidth  \n0.1.7  \nwheel  \n0.30.0a0  \nwsgiref  \n0.1.2  \nPre-installed R Libraries  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nabind  \n1.4-3  \nassertthat  \n0.1  \nbase  \n3.2.3  \nBH  \n1.60.0-2  \nbitops  \n1.0-6  \nboot  \n1.3-17  \nbrew  \n1.0-6  \ncar  \n2.1-3  \ncaret  \n6.0-73  \nchron  \n2.3-47  \nclass  \n7.3-14  \ncluster  \n2.0.5  \ncodetools  \n0.2-14  \ncolorspace  \n1.2-4  \ncompiler  \n3.2.3  \ncrayon  \n1.3.1  \ncurl  \n2.2  \ndata.table  \n1.9.6  \ndatasets  \n3.2.3  \nDBI  \n0.5-1  \ndevtools  \n1.12.0  \ndichromat  \n2.0-0  \ndigest  \n0.6.9  \ndoMC  \n1.3.4  \ndplyr  \n0.5.0  \nforeach  \n1.4.3  \nforeign  \n0.8-66  \ngbm  \n2.1.1  \nggplot2  \n2.2.0  \ngit2r  \n0.15.0  \nglmnet  \n2.0-5"
    },
    {
        "id": 1692,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "foreach  \n1.4.3  \nforeign  \n0.8-66  \ngbm  \n2.1.1  \nggplot2  \n2.2.0  \ngit2r  \n0.15.0  \nglmnet  \n2.0-5  \ngraphics  \n3.2.3  \ngrDevices  \n3.2.3  \ngrid  \n3.2.3  \ngsubfn  \n0.6-6  \ngtable  \n0.1.2  \nh2o  \n3.10.0.8  \nhttr  \n1.2.1  \nhwriter  \n1.3.2  \nhwriterPlus  \n1.0-3  \niterators  \n1.0.8  \njsonlite  \n1.1  \nKernSmooth  \n2.23-15  \nlabeling  \n0.3  \nlattice  \n0.20-34  \nlazyeval  \n0.2.0  \nlittler  \n0.3.0  \nlme4  \n1.1-12  \nlubridate  \n1.6.0  \nmagrittr  \n1.5  \nmapproj  \n1.2-4  \nmaps  \n3.0.2  \nMASS  \n7.3-45  \nMatrix  \n1.2-7.1  \nMatrixModels  \n0.4-1  \nmemoise  \n1.0.0  \nmethods  \n3.2.3  \nmgcv  \n1.8-11  \nmime  \n0.5  \nminqa  \n1.2.4  \nModelMetrics  \n1.1.0  \nmulticore  \n0.2  \nmunsell  \n0.4.2  \nmvtnorm  \n1.0-5  \nnlme  \n3.1-124  \nnloptr  \n1.0.4  \nnnet  \n7.3-12  \nopenssl  \n0.9.5  \nparallel  \n3.2.3  \npbkrtest  \n0.4-6  \npkgKitten  \n0.1.3  \nplyr  \n1.8.4  \npraise  \n1.0.0  \npROC  \n1.8  \nproto  \n0.3-10  \nquantreg  \n5.29  \nR.methodsS3  \n1.7.1  \nR.oo  \n1.21.0  \nR.utils  \n2.5.0  \nR6  \n2.2.0  \nrandomForest  \n4.6-12  \nRColorBrewer  \n1.1-2  \nRcpp  \n0.12.7  \nRcppEigen  \n0.3.2.9.0  \nRCurl  \n1.95-4.8  \nreshape2  \n1.4.2  \nRODBC  \n1.3-12  \nroxygen2  \n5.0.1  \nrpart  \n4.1-10  \nRserve  \n1.7-3  \nRSQLite"
    },
    {
        "id": 1693,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "1.95-4.8  \nreshape2  \n1.4.2  \nRODBC  \n1.3-12  \nroxygen2  \n5.0.1  \nrpart  \n4.1-10  \nRserve  \n1.7-3  \nRSQLite  \n1.0.0  \nrstudioapi  \n0.6  \nscales  \n0.4.1  \nsp  \n1.0-15  \nSparkR  \n1.6.3  \nSparseM  \n1.74  \nspatial  \n7.3-11  \nsplines  \n3.2.3  \nsqldf  \n0.4-10  \nstatmod  \n1.4.27  \nstats  \n3.2.3  \nstats4  \n3.2.3  \nstringi  \n1.0-1  \nstringr  \n1.0.0  \nsurvival  \n2.38-3  \ntcltk  \n3.2.3  \nTeachingDemos  \n2.10  \ntestthat  \n1.0.2  \ntibble  \n1.2  \ntools  \n3.2.3  \nutils  \n3.2.3  \nwhisker  \n0.3-2  \nwithr  \n1.0.2  \nPre-installed Java and Scala libraries (Hadoop 1 cluster version)  \nGroup ID  \nArtifact ID  \nVersion  \nantlr  \nantlr  \n2.7.7  \nasm  \nasm  \n3.1  \ncom.amazonaws  \namazon-sqs-java-messaging-lib  \n1.0.0  \ncom.amazonaws  \naws-java-sdk  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-autoscaling  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cloudformation  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cloudfront  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cloudhsm  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cloudsearch  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cloudtrail  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cloudwatch  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cloudwatchmetrics  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-codedeploy  \n1.9.40  \ncom.amazonaws"
    },
    {
        "id": 1694,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "aws-java-sdk-cloudwatchmetrics  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-codedeploy  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cognitoidentity  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cognitosync  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-config  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-core  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-datapipeline  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-directconnect  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-directory  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-dynamodb  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-ec2  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-ecs  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-efs  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-elasticache  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-elasticbeanstalk  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-elasticloadbalancing  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-elastictranscoder  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-emr  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-glacier  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-iam  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-importexport  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-kinesis  \n1.9.40"
    },
    {
        "id": 1695,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "1.9.40  \ncom.amazonaws  \naws-java-sdk-importexport  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-kinesis  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-kms  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-lambda  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-logs  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-machinelearning  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-opsworks  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-rds  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-redshift  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-route53  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-s3  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-ses  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-simpledb  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-simpleworkflow  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-sns  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-sqs  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-ssm  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-storagegateway  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-sts  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-support  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-swf-libraries  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-workspaces  \n1.9.40"
    },
    {
        "id": 1696,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "com.amazonaws  \naws-java-sdk-swf-libraries  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-workspaces  \n1.9.40  \ncom.clearspring.analytics  \nstream  \n2.7.0  \ncom.databricks  \nRserve  \n1.8-3  \ncom.databricks  \njets3t  \n0.7.1-0  \ncom.databricks  \nspark-avro_2.10  \n2.0.1  \ncom.databricks  \nspark-csv_2.10  \n1.3.2  \ncom.databricks  \nspark-redshift_2.10  \n0.6.0  \ncom.databricks.scalapb  \ncompilerplugin_2.10  \n0.4.15-9  \ncom.databricks.scalapb  \nscalapb-runtime_2.10  \n0.4.15-9  \ncom.esotericsoftware.kryo  \nkryo  \n2.21  \ncom.esotericsoftware.minlog  \nminlog  \n1.2  \ncom.esotericsoftware.reflectasm  \nreflectasm  \n1.07  \ncom.fasterxml  \nclassmate  \n1.0.0  \ncom.fasterxml.jackson.core  \njackson-annotations  \n2.4.5  \ncom.fasterxml.jackson.core  \njackson-core  \n2.4.5  \ncom.fasterxml.jackson.core  \njackson-databind  \n2.4.5  \ncom.fasterxml.jackson.datatype  \njackson-datatype-joda  \n2.4.5  \ncom.fasterxml.jackson.module  \njackson-module-scala_2.10  \n2.4.5  \ncom.github.fommil.netlib  \ncore  \n1.1.2  \ncom.github.rwl  \njtransforms  \n2.4.0  \ncom.github.scopt  \nscopt_2.10  \n3.3.0  \ncom.google.api-client  \ngoogle-api-client  \n1.22.0  \ncom.google.apis  \ngoogle-api-services-admin-directory  \ndirectory_v1-rev69-1.22.0  \ncom.google.code.findbugs  \njsr305  \n2.0.1  \ncom.google.guava  \nguava  \n15.0  \ncom.google.http-client"
    },
    {
        "id": 1697,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "directory_v1-rev69-1.22.0  \ncom.google.code.findbugs  \njsr305  \n2.0.1  \ncom.google.guava  \nguava  \n15.0  \ncom.google.http-client  \ngoogle-http-client  \n1.22.0  \ncom.google.http-client  \ngoogle-http-client-jackson2  \n1.22.0  \ncom.google.oauth-client  \ngoogle-oauth-client  \n1.22.0  \ncom.google.protobuf  \nprotobuf-java  \n2.6.1  \ncom.googlecode.javaewah  \nJavaEWAH  \n0.3.2  \ncom.h2database  \nh2  \n1.3.174  \ncom.jcraft  \njsch  \n0.1.50  \ncom.jolbox  \nbonecp  \n0.8.0.RELEASE  \ncom.mchange  \nc3p0  \n0.9.5.1  \ncom.mchange  \nmchange-commons-java  \n0.2.10  \ncom.ning  \ncompress-lzf  \n1.0.3  \ncom.sun.istack  \nistack-commons-runtime  \n2.16  \ncom.sun.jersey  \njersey-core  \n1.9  \ncom.sun.jersey  \njersey-json  \n1.9  \ncom.sun.jersey  \njersey-server  \n1.9  \ncom.sun.mail  \ngimap  \n1.5.2  \ncom.sun.mail  \njavax.mail  \n1.5.2  \ncom.sun.xml.bind  \njaxb-core  \n2.2.7  \ncom.sun.xml.bind  \njaxb-impl  \n2.2.7  \ncom.sun.xml.fastinfoset  \nFastInfoset  \n1.2.12  \ncom.thoughtworks.paranamer  \nparanamer  \n2.6  \ncom.trueaccord.lenses  \nlenses_2.10  \n0.3  \ncom.twitter  \nchill-java  \n0.5.0  \ncom.twitter  \nchill_2.10  \n0.5.0  \ncom.twitter  \nparquet-hadoop-bundle  \n1.6.0  \ncom.twitter  \nutil-app_2.10  \n6.23.0  \ncom.twitter  \nutil-core_2.10  \n6.23.0  \ncom.twitter  \nutil-jvm_2.10  \n6.23.0  \ncom.typesafe  \nconfig  \n1.2.1  \ncom.typesafe  \nscalalogging-slf4j_2.10"
    },
    {
        "id": 1698,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "6.23.0  \ncom.twitter  \nutil-jvm_2.10  \n6.23.0  \ncom.typesafe  \nconfig  \n1.2.1  \ncom.typesafe  \nscalalogging-slf4j_2.10  \n1.1.0  \ncom.typesafe.akka  \nakka-actor_2.10  \n2.3.11  \ncom.typesafe.akka  \nakka-remote_2.10  \n2.3.11  \ncom.typesafe.akka  \nakka-slf4j_2.10  \n2.3.11  \ncom.univocity  \nunivocity-parsers  \n1.5.1  \ncom.zaxxer  \nHikariCP  \n2.4.1  \ncommons-beanutils  \ncommons-beanutils  \n1.9.2  \ncommons-beanutils  \ncommons-beanutils-core  \n1.8.0  \ncommons-cli  \ncommons-cli  \n1.2  \ncommons-codec  \ncommons-codec  \n1.8  \ncommons-collections  \ncommons-collections  \n3.2.2  \ncommons-configuration  \ncommons-configuration  \n1.6  \ncommons-dbcp  \ncommons-dbcp  \n1.4  \ncommons-digester  \ncommons-digester  \n1.8.1  \ncommons-el  \ncommons-el  \n1.0  \ncommons-fileupload  \ncommons-fileupload  \n1.3  \ncommons-httpclient  \ncommons-httpclient  \n3.1  \ncommons-io  \ncommons-io  \n2.5  \ncommons-lang  \ncommons-lang  \n2.6  \ncommons-logging  \ncommons-logging  \n1.1.1  \ncommons-net  \ncommons-net  \n2.2  \ncommons-pool  \ncommons-pool  \n1.5.4  \ncommons-validator  \ncommons-validator  \n1.5.0  \ndom4j  \ndom4j  \n1.6.1  \nhsqldb  \nhsqldb  \n1.8.0.10  \ninfo.ganglia.gmetric4j  \ngmetric4j  \n1.0.7  \nio.dropwizard.metrics  \nmetrics-core  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-ganglia  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-graphite  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-healthchecks  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-jetty8  \n3.1.2"
    },
    {
        "id": 1699,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "3.1.2  \nio.dropwizard.metrics  \nmetrics-healthchecks  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-jetty8  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-json  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-jvm  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-log4j  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-servlets  \n3.1.2  \nio.jsonwebtoken  \njjwt  \n0.6.0  \nio.netty  \nnetty  \n3.8.0.Final  \nio.netty  \nnetty-all  \n4.0.29.Final  \nio.prometheus  \nsimpleclient  \n0.0.16  \nio.prometheus  \nsimpleclient_common  \n0.0.16  \nio.prometheus  \nsimpleclient_dropwizard  \n0.0.16  \nio.prometheus  \nsimpleclient_pushgateway  \n0.0.16  \nio.prometheus  \nsimpleclient_servlet  \n0.0.16  \nio.prometheus.client  \nmodel  \n0.0.2  \njavax.activation  \nactivation  \n1.1  \njavax.el  \njavax.el-api  \n2.2.4  \njavax.jdo  \njdo-api  \n3.0.1  \njavax.servlet  \njavax.servlet-api  \n3.0.1  \njavax.transaction  \njta  \n1.1  \njavax.validation  \nvalidation-api  \n1.1.0.Final  \njavax.xml.bind  \njaxb-api  \n2.2.7  \njavolution  \njavolution  \n5.5.1  \njline  \njline  \n2.11  \njline  \njline  \n2.12  \njoda-time  \njoda-time  \n2.8.1  \nlog4j  \napache-log4j-extras  \n1.2.17  \nlog4j  \nlog4j  \n1.2.17  \nmysql  \nmysql-connector-java  \n5.1.27  \nnet.hydromatic  \neigenbase-properties  \n1.1.5  \nnet.java.dev.jets3t  \njets3t  \n0.7.1  \nnet.jpountz.lz4  \nlz4  \n1.3.0  \nnet.liftweb  \nlift-json_2.10"
    },
    {
        "id": 1700,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "net.java.dev.jets3t  \njets3t  \n0.7.1  \nnet.jpountz.lz4  \nlz4  \n1.3.0  \nnet.liftweb  \nlift-json_2.10  \n2.6.1  \nnet.razorvine  \npyrolite  \n4.9  \nnet.sf.jpam  \njpam  \n1.1  \nnet.sf.opencsv  \nopencsv  \n2.3  \nnet.sf.py4j  \npy4j  \n0.9  \nnet.sf.supercsv  \nsuper-csv  \n2.2.0  \nnet.sourceforge.f2j  \narpack_combined_all  \n0.1  \norg.acplt  \noncrpc  \n1.0.7  \norg.antlr  \nST4  \n4.0.4  \norg.antlr  \nantlr-runtime  \n3.4  \norg.antlr  \nantlr4-runtime  \n4.5.3  \norg.antlr  \nstringtemplate  \n3.2.1  \norg.apache.ant  \nant  \n1.9.2  \norg.apache.ant  \nant-jsch  \n1.9.2  \norg.apache.ant  \nant-launcher  \n1.9.2  \norg.apache.avro  \navro  \n1.7.7  \norg.apache.avro  \navro-ipc  \n1.7.7  \norg.apache.avro  \navro-mapred  \n1.7.7  \norg.apache.calcite  \ncalcite-avatica  \n1.2.0-incubating  \norg.apache.calcite  \ncalcite-core  \n1.2.0-incubating  \norg.apache.calcite  \ncalcite-linq4j  \n1.2.0-incubating  \norg.apache.commons  \ncommons-compress  \n1.9  \norg.apache.commons  \ncommons-csv  \n1.1  \norg.apache.commons  \ncommons-lang3  \n3.4  \norg.apache.commons  \ncommons-math  \n2.1  \norg.apache.commons  \ncommons-math3  \n3.4.1  \norg.apache.curator  \ncurator-client  \n2.4.0  \norg.apache.curator  \ncurator-framework  \n2.4.0  \norg.apache.curator  \ncurator-recipes  \n2.4.0  \norg.apache.derby  \nderby  \n10.10.2.0  \norg.apache.geronimo.specs  \ngeronimo-jms_1.1_spec  \n1.1.1"
    },
    {
        "id": 1701,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "curator-recipes  \n2.4.0  \norg.apache.derby  \nderby  \n10.10.2.0  \norg.apache.geronimo.specs  \ngeronimo-jms_1.1_spec  \n1.1.1  \norg.apache.hadoop  \nhadoop-client  \n1.2.1-0  \norg.apache.hadoop  \nhadoop-core  \n1.2.1-0  \norg.apache.httpcomponents  \nhttpclient  \n4.4.1  \norg.apache.httpcomponents  \nhttpcore  \n4.4.1  \norg.apache.ivy  \nivy  \n2.4.0  \norg.apache.mesos  \nmesos  \n0.21.1  \norg.apache.parquet  \nparquet-column  \n1.7.0  \norg.apache.parquet  \nparquet-common  \n1.7.0  \norg.apache.parquet  \nparquet-encoding  \n1.7.0  \norg.apache.parquet  \nparquet-format  \n2.3.0-incubating  \norg.apache.parquet  \nparquet-generator  \n1.7.0  \norg.apache.parquet  \nparquet-hadoop  \n1.7.0  \norg.apache.parquet  \nparquet-jackson  \n1.7.0  \norg.apache.poi  \npoi  \n3.9  \norg.apache.poi  \npoi-ooxml  \n3.9  \norg.apache.poi  \npoi-ooxml-schemas  \n3.9  \norg.apache.thrift  \nlibfb303  \n0.9.2  \norg.apache.thrift  \nlibthrift  \n0.9.2  \norg.apache.xbean  \nxbean-asm5-shaded  \n4.4  \norg.apache.xmlbeans  \nxmlbeans  \n2.3.0  \norg.apache.zookeeper  \nzookeeper  \n3.4.5  \norg.codehaus.jackson  \njackson-core-asl  \n1.8.8  \norg.codehaus.jackson  \njackson-jaxrs  \n1.8.3  \norg.codehaus.jackson  \njackson-mapper-asl  \n1.8.8  \norg.codehaus.jackson  \njackson-xc  \n1.8.3  \norg.codehaus.janino  \ncommons-compiler  \n2.7.8  \norg.codehaus.janino  \njanino  \n2.7.8  \norg.codehaus.jettison  \njettison  \n1.1"
    },
    {
        "id": 1702,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "org.codehaus.janino  \ncommons-compiler  \n2.7.8  \norg.codehaus.janino  \njanino  \n2.7.8  \norg.codehaus.jettison  \njettison  \n1.1  \norg.datanucleus  \ndatanucleus-api-jdo  \n3.2.6  \norg.datanucleus  \ndatanucleus-core  \n3.2.10  \norg.datanucleus  \ndatanucleus-rdbms  \n3.2.9  \norg.eclipse.jetty  \njetty-client  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-continuation  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-continuation  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-http  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-http  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-io  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-io  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-security  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-server  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-server  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-servlet  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-servlets  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-util  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-util  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-websocket  \n8.1.14.v20131031  \norg.eclipse.jetty.orbit  \njavax.servlet  \n3.0.0.v201112011016  \norg.eclipse.jetty.orbit  \njavax.servlet  \n3.0.0.v201112011016  \norg.fusesource.jansi  \njansi  \n1.4  \norg.fusesource.leveldbjni  \nleveldbjni-all  \n1.8  \norg.hawkular.agent"
    },
    {
        "id": 1703,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "org.fusesource.jansi  \njansi  \n1.4  \norg.fusesource.leveldbjni  \nleveldbjni-all  \n1.8  \norg.hawkular.agent  \nprometheus-scraper  \n0.20.1.Final  \norg.hibernate  \nhibernate-validator  \n5.1.1.Final  \norg.iq80.snappy  \nsnappy  \n0.2  \norg.jboss.logging  \njboss-logging  \n3.1.3.GA  \norg.jdbi  \njdbi  \n2.63.1  \norg.joda  \njoda-convert  \n1.7  \norg.jodd  \njodd-core  \n3.5.2  \norg.jpmml  \npmml-agent  \n1.1.15  \norg.jpmml  \npmml-model  \n1.1.15  \norg.jpmml  \npmml-schema  \n1.1.15  \norg.json  \njson  \n20090211  \norg.json4s  \njson4s-ast_2.10  \n3.2.10  \norg.json4s  \njson4s-core_2.10  \n3.2.10  \norg.json4s  \njson4s-jackson_2.10  \n3.2.10  \norg.mockito  \nmockito-all  \n1.9.5  \norg.objenesis  \nobjenesis  \n1.2  \norg.postgresql  \npostgresql  \n9.4-1204-jdbc41  \norg.roaringbitmap  \nRoaringBitmap  \n0.5.11  \norg.rosuda.REngine  \nREngine  \n2.1.0  \norg.scala-lang  \njline  \n2.10.6  \norg.scala-lang  \nscala-compiler_2.10  \n2.10.6  \norg.scala-lang  \nscala-library_2.10  \n2.10.6  \norg.scala-lang  \nscala-reflect_2.10  \n2.10.6  \norg.scala-lang  \nscalap_2.10  \n2.10.6  \norg.scala-sbt  \ntest-interface  \n1.0  \norg.scalacheck  \nscalacheck_2.10  \n1.12.5  \norg.scalamacros  \nquasiquotes_2.10  \n2.0.0  \norg.scalamock  \nscalamock-core_2.10  \n3.2.1  \norg.scalamock"
    },
    {
        "id": 1704,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "1.12.5  \norg.scalamacros  \nquasiquotes_2.10  \n2.0.0  \norg.scalamock  \nscalamock-core_2.10  \n3.2.1  \norg.scalamock  \nscalamock-scalatest-support_2.10  \n3.2.1  \norg.scalanlp  \nbreeze-macros_2.10  \n0.11.2  \norg.scalanlp  \nbreeze_2.10  \n0.11.2  \norg.scalatest  \nscalatest_2.10  \n2.2.4  \norg.slf4j  \njcl-over-slf4j  \n1.7.10  \norg.slf4j  \njul-to-slf4j  \n1.7.10  \norg.slf4j  \nslf4j-api  \n1.7.5  \norg.slf4j  \nslf4j-log4j12  \n1.7.5  \norg.spark-project.hive  \nhive-beeline  \n1.2.1.spark  \norg.spark-project.hive  \nhive-cli  \n1.2.1.spark  \norg.spark-project.hive  \nhive-exec  \n1.2.1.spark  \norg.spark-project.hive  \nhive-jdbc  \n1.2.1.spark  \norg.spark-project.hive  \nhive-metastore  \n1.2.1.spark  \norg.spark-project.hive  \nhive-service  \n1.2.1.spark  \norg.spark-project.spark  \nunused  \n1.0.0  \norg.spire-math  \nspire-macros_2.10  \n0.7.4  \norg.spire-math  \nspire_2.10  \n0.7.4  \norg.springframework  \nspring-core  \n4.1.4.RELEASE  \norg.springframework  \nspring-test  \n4.1.4.RELEASE  \norg.tachyonproject  \ntachyon-client  \n0.8.2  \norg.tachyonproject  \ntachyon-underfs-hdfs  \n0.8.2  \norg.tachyonproject  \ntachyon-underfs-local  \n0.8.2  \norg.tachyonproject  \ntachyon-underfs-s3  \n0.8.2  \norg.uncommons.maths  \nuncommons-maths  \n1.2.2a  \norg.xerial  \nsqlite-jdbc  \n3.8.11.2  \norg.xerial.snappy  \nsnappy-java  \n1.1.2.6  \noro  \noro  \n2.0.8"
    },
    {
        "id": 1705,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "org.xerial  \nsqlite-jdbc  \n3.8.11.2  \norg.xerial.snappy  \nsnappy-java  \n1.1.2.6  \noro  \noro  \n2.0.8  \nstax  \nstax-api  \n1.0.1  \nxmlenc  \nxmlenc  \n0.52  \nPre-installed Java and Scala libraries (Hadoop 2 cluster version)  \nGroup ID  \nArtifact ID  \nVersion  \nantlr  \nantlr  \n2.7.7  \nasm  \nasm  \n3.1  \ncom.amazonaws  \namazon-sqs-java-messaging-lib  \n1.0.0  \ncom.amazonaws  \naws-java-sdk  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-autoscaling  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cloudformation  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cloudfront  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cloudhsm  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cloudsearch  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cloudtrail  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cloudwatch  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cloudwatchmetrics  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-codedeploy  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cognitoidentity  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-cognitosync  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-config  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-core  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-datapipeline  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-directconnect  \n1.9.40  \ncom.amazonaws"
    },
    {
        "id": 1706,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "aws-java-sdk-datapipeline  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-directconnect  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-directory  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-dynamodb  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-ec2  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-ecs  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-efs  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-elasticache  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-elasticbeanstalk  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-elasticloadbalancing  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-elastictranscoder  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-emr  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-glacier  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-iam  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-importexport  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-kinesis  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-kms  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-lambda  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-logs  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-machinelearning  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-opsworks  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-rds  \n1.9.40  \ncom.amazonaws"
    },
    {
        "id": 1707,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "aws-java-sdk-opsworks  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-rds  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-redshift  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-route53  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-s3  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-ses  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-simpledb  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-simpleworkflow  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-sns  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-sqs  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-ssm  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-storagegateway  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-sts  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-support  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-swf-libraries  \n1.9.40  \ncom.amazonaws  \naws-java-sdk-workspaces  \n1.9.40  \ncom.clearspring.analytics  \nstream  \n2.7.0  \ncom.databricks  \nRserve  \n1.8-3  \ncom.databricks  \njets3t  \n0.7.1-0  \ncom.databricks  \nspark-avro_2.10  \n2.0.1  \ncom.databricks  \nspark-csv_2.10  \n1.3.2  \ncom.databricks  \nspark-redshift_2.10  \n0.6.0  \ncom.databricks.scalapb  \ncompilerplugin_2.10  \n0.4.15-9  \ncom.databricks.scalapb  \nscalapb-runtime_2.10"
    },
    {
        "id": 1708,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "0.6.0  \ncom.databricks.scalapb  \ncompilerplugin_2.10  \n0.4.15-9  \ncom.databricks.scalapb  \nscalapb-runtime_2.10  \n0.4.15-9  \ncom.esotericsoftware.kryo  \nkryo  \n2.21  \ncom.esotericsoftware.minlog  \nminlog  \n1.2  \ncom.esotericsoftware.reflectasm  \nreflectasm  \n1.07  \ncom.fasterxml  \nclassmate  \n1.0.0  \ncom.fasterxml.jackson.core  \njackson-annotations  \n2.4.5  \ncom.fasterxml.jackson.core  \njackson-core  \n2.4.5  \ncom.fasterxml.jackson.core  \njackson-databind  \n2.4.5  \ncom.fasterxml.jackson.datatype  \njackson-datatype-joda  \n2.4.5  \ncom.fasterxml.jackson.module  \njackson-module-scala_2.10  \n2.4.5  \ncom.github.fommil.netlib  \ncore  \n1.1.2  \ncom.github.rwl  \njtransforms  \n2.4.0  \ncom.github.scopt  \nscopt_2.10  \n3.3.0  \ncom.google.api-client  \ngoogle-api-client  \n1.22.0  \ncom.google.apis  \ngoogle-api-services-admin-directory  \ndirectory_v1-rev69-1.22.0  \ncom.google.code.findbugs  \njsr305  \n2.0.1  \ncom.google.code.gson  \ngson  \n2.2.4  \ncom.google.guava  \nguava  \n15.0  \ncom.google.http-client  \ngoogle-http-client  \n1.22.0  \ncom.google.http-client  \ngoogle-http-client-jackson2  \n1.22.0  \ncom.google.oauth-client  \ngoogle-oauth-client  \n1.22.0  \ncom.google.protobuf  \nprotobuf-java  \n2.6.1  \ncom.googlecode.javaewah  \nJavaEWAH  \n0.3.2  \ncom.h2database  \nh2  \n1.3.174  \ncom.jcraft  \njsch  \n0.1.50  \ncom.jolbox  \nbonecp  \n0.8.0.RELEASE"
    },
    {
        "id": 1709,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "JavaEWAH  \n0.3.2  \ncom.h2database  \nh2  \n1.3.174  \ncom.jcraft  \njsch  \n0.1.50  \ncom.jolbox  \nbonecp  \n0.8.0.RELEASE  \ncom.mchange  \nc3p0  \n0.9.5.1  \ncom.mchange  \nmchange-commons-java  \n0.2.10  \ncom.ning  \ncompress-lzf  \n1.0.3  \ncom.sun.istack  \nistack-commons-runtime  \n2.16  \ncom.sun.jersey  \njersey-client  \n1.9  \ncom.sun.jersey  \njersey-core  \n1.9  \ncom.sun.jersey  \njersey-json  \n1.9  \ncom.sun.jersey  \njersey-server  \n1.9  \ncom.sun.mail  \ngimap  \n1.5.2  \ncom.sun.mail  \njavax.mail  \n1.5.2  \ncom.sun.xml.bind  \njaxb-core  \n2.2.7  \ncom.sun.xml.bind  \njaxb-impl  \n2.2.7  \ncom.sun.xml.fastinfoset  \nFastInfoset  \n1.2.12  \ncom.thoughtworks.paranamer  \nparanamer  \n2.6  \ncom.trueaccord.lenses  \nlenses_2.10  \n0.3  \ncom.twitter  \nchill-java  \n0.5.0  \ncom.twitter  \nchill_2.10  \n0.5.0  \ncom.twitter  \nparquet-hadoop-bundle  \n1.6.0  \ncom.twitter  \nutil-app_2.10  \n6.23.0  \ncom.twitter  \nutil-core_2.10  \n6.23.0  \ncom.twitter  \nutil-jvm_2.10  \n6.23.0  \ncom.typesafe  \nconfig  \n1.2.1  \ncom.typesafe  \nscalalogging-slf4j_2.10  \n1.1.0  \ncom.typesafe.akka  \nakka-actor_2.10  \n2.3.11  \ncom.typesafe.akka  \nakka-remote_2.10  \n2.3.11  \ncom.typesafe.akka  \nakka-slf4j_2.10  \n2.3.11  \ncom.univocity  \nunivocity-parsers  \n1.5.1  \ncom.zaxxer  \nHikariCP  \n2.4.1  \ncommons-beanutils  \ncommons-beanutils  \n1.9.2  \ncommons-beanutils"
    },
    {
        "id": 1710,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "com.univocity  \nunivocity-parsers  \n1.5.1  \ncom.zaxxer  \nHikariCP  \n2.4.1  \ncommons-beanutils  \ncommons-beanutils  \n1.9.2  \ncommons-beanutils  \ncommons-beanutils-core  \n1.8.0  \ncommons-cli  \ncommons-cli  \n1.2  \ncommons-codec  \ncommons-codec  \n1.8  \ncommons-collections  \ncommons-collections  \n3.2.2  \ncommons-configuration  \ncommons-configuration  \n1.6  \ncommons-dbcp  \ncommons-dbcp  \n1.4  \ncommons-digester  \ncommons-digester  \n1.8.1  \ncommons-fileupload  \ncommons-fileupload  \n1.3  \ncommons-httpclient  \ncommons-httpclient  \n3.1  \ncommons-io  \ncommons-io  \n2.5  \ncommons-lang  \ncommons-lang  \n2.6  \ncommons-logging  \ncommons-logging  \n1.1.1  \ncommons-net  \ncommons-net  \n2.2  \ncommons-pool  \ncommons-pool  \n1.5.4  \ncommons-validator  \ncommons-validator  \n1.5.0  \ndom4j  \ndom4j  \n1.6.1  \ninfo.ganglia.gmetric4j  \ngmetric4j  \n1.0.7  \nio.dropwizard.metrics  \nmetrics-core  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-ganglia  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-graphite  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-healthchecks  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-jetty8  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-json  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-jvm  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-log4j  \n3.1.2  \nio.dropwizard.metrics  \nmetrics-servlets  \n3.1.2  \nio.jsonwebtoken  \njjwt  \n0.6.0  \nio.netty  \nnetty  \n3.8.0.Final  \nio.netty  \nnetty-all  \n4.0.29.Final  \nio.prometheus  \nsimpleclient  \n0.0.16  \nio.prometheus  \nsimpleclient_common  \n0.0.16  \nio.prometheus"
    },
    {
        "id": 1711,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "3.8.0.Final  \nio.netty  \nnetty-all  \n4.0.29.Final  \nio.prometheus  \nsimpleclient  \n0.0.16  \nio.prometheus  \nsimpleclient_common  \n0.0.16  \nio.prometheus  \nsimpleclient_dropwizard  \n0.0.16  \nio.prometheus  \nsimpleclient_pushgateway  \n0.0.16  \nio.prometheus  \nsimpleclient_servlet  \n0.0.16  \nio.prometheus.client  \nmodel  \n0.0.2  \njavax.activation  \nactivation  \n1.1  \njavax.el  \njavax.el-api  \n2.2.4  \njavax.jdo  \njdo-api  \n3.0.1  \njavax.servlet  \njavax.servlet-api  \n3.0.1  \njavax.servlet.jsp  \njsp-api  \n2.1  \njavax.transaction  \njta  \n1.1  \njavax.validation  \nvalidation-api  \n1.1.0.Final  \njavax.xml.bind  \njaxb-api  \n2.2.7  \njavolution  \njavolution  \n5.5.1  \njline  \njline  \n2.11  \njline  \njline  \n2.12  \njoda-time  \njoda-time  \n2.8.1  \nlog4j  \napache-log4j-extras  \n1.2.17  \nlog4j  \nlog4j  \n1.2.17  \nlog4j  \nlog4j  \n1.2.17  \nmysql  \nmysql-connector-java  \n5.1.27  \nnet.hydromatic  \neigenbase-properties  \n1.1.5  \nnet.java.dev.jets3t  \njets3t  \n0.7.1  \nnet.jpountz.lz4  \nlz4  \n1.3.0  \nnet.liftweb  \nlift-json_2.10  \n2.6.1  \nnet.razorvine  \npyrolite  \n4.9  \nnet.sf.jpam  \njpam  \n1.1  \nnet.sf.opencsv  \nopencsv  \n2.3  \nnet.sf.py4j  \npy4j  \n0.9  \nnet.sf.supercsv  \nsuper-csv  \n2.2.0  \nnet.sourceforge.f2j  \narpack_combined_all  \n0.1  \norg.acplt  \noncrpc  \n1.0.7  \norg.antlr  \nST4  \n4.0.4"
    },
    {
        "id": 1712,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "2.2.0  \nnet.sourceforge.f2j  \narpack_combined_all  \n0.1  \norg.acplt  \noncrpc  \n1.0.7  \norg.antlr  \nST4  \n4.0.4  \norg.antlr  \nantlr-runtime  \n3.4  \norg.antlr  \nantlr4-runtime  \n4.5.3  \norg.antlr  \nstringtemplate  \n3.2.1  \norg.apache.ant  \nant  \n1.9.2  \norg.apache.ant  \nant-jsch  \n1.9.2  \norg.apache.ant  \nant-launcher  \n1.9.2  \norg.apache.avro  \navro  \n1.7.7  \norg.apache.avro  \navro-ipc  \n1.7.7  \norg.apache.avro  \navro-mapred  \n1.7.7  \norg.apache.calcite  \ncalcite-avatica  \n1.2.0-incubating  \norg.apache.calcite  \ncalcite-core  \n1.2.0-incubating  \norg.apache.calcite  \ncalcite-linq4j  \n1.2.0-incubating  \norg.apache.commons  \ncommons-compress  \n1.9  \norg.apache.commons  \ncommons-csv  \n1.1  \norg.apache.commons  \ncommons-lang3  \n3.4  \norg.apache.commons  \ncommons-math3  \n3.4.1  \norg.apache.curator  \ncurator-client  \n2.7.1  \norg.apache.curator  \ncurator-framework  \n2.7.1  \norg.apache.curator  \ncurator-recipes  \n2.7.1  \norg.apache.derby  \nderby  \n10.10.2.0  \norg.apache.directory.api  \napi-asn1-api  \n1.0.0-M20  \norg.apache.directory.api  \napi-util  \n1.0.0-M20  \norg.apache.directory.server  \napacheds-i18n  \n2.0.0-M15  \norg.apache.directory.server  \napacheds-kerberos-codec  \n2.0.0-M15  \norg.apache.geronimo.specs  \ngeronimo-jms_1.1_spec  \n1.1.1  \norg.apache.hadoop  \nhadoop-annotations  \n2.7.3  \norg.apache.hadoop  \nhadoop-auth  \n2.7.3"
    },
    {
        "id": 1713,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "1.1.1  \norg.apache.hadoop  \nhadoop-annotations  \n2.7.3  \norg.apache.hadoop  \nhadoop-auth  \n2.7.3  \norg.apache.hadoop  \nhadoop-client  \n2.7.3  \norg.apache.hadoop  \nhadoop-common  \n2.7.3  \norg.apache.hadoop  \nhadoop-hdfs  \n2.7.3  \norg.apache.hadoop  \nhadoop-mapreduce-client-app  \n2.7.3  \norg.apache.hadoop  \nhadoop-mapreduce-client-common  \n2.7.3  \norg.apache.hadoop  \nhadoop-mapreduce-client-core  \n2.7.3  \norg.apache.hadoop  \nhadoop-mapreduce-client-jobclient  \n2.7.3  \norg.apache.hadoop  \nhadoop-mapreduce-client-shuffle  \n2.7.3  \norg.apache.hadoop  \nhadoop-yarn-api  \n2.7.3  \norg.apache.hadoop  \nhadoop-yarn-client  \n2.7.3  \norg.apache.hadoop  \nhadoop-yarn-common  \n2.7.3  \norg.apache.hadoop  \nhadoop-yarn-server-common  \n2.7.3  \norg.apache.htrace  \nhtrace-core  \n3.1.0-incubating  \norg.apache.httpcomponents  \nhttpclient  \n4.4.1  \norg.apache.httpcomponents  \nhttpcore  \n4.4.1  \norg.apache.ivy  \nivy  \n2.4.0  \norg.apache.mesos  \nmesos  \n0.21.1  \norg.apache.parquet  \nparquet-column  \n1.7.0  \norg.apache.parquet  \nparquet-common  \n1.7.0  \norg.apache.parquet  \nparquet-encoding  \n1.7.0  \norg.apache.parquet  \nparquet-format  \n2.3.0-incubating  \norg.apache.parquet  \nparquet-generator  \n1.7.0  \norg.apache.parquet  \nparquet-hadoop  \n1.7.0  \norg.apache.parquet  \nparquet-jackson  \n1.7.0  \norg.apache.poi  \npoi  \n3.9  \norg.apache.poi"
    },
    {
        "id": 1714,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "org.apache.parquet  \nparquet-hadoop  \n1.7.0  \norg.apache.parquet  \nparquet-jackson  \n1.7.0  \norg.apache.poi  \npoi  \n3.9  \norg.apache.poi  \npoi-ooxml  \n3.9  \norg.apache.poi  \npoi-ooxml-schemas  \n3.9  \norg.apache.thrift  \nlibfb303  \n0.9.2  \norg.apache.thrift  \nlibthrift  \n0.9.2  \norg.apache.xbean  \nxbean-asm5-shaded  \n4.4  \norg.apache.xmlbeans  \nxmlbeans  \n2.3.0  \norg.apache.zookeeper  \nzookeeper  \n3.4.6  \norg.codehaus.jackson  \njackson-core-asl  \n1.8.8  \norg.codehaus.jackson  \njackson-jaxrs  \n1.9.13  \norg.codehaus.jackson  \njackson-mapper-asl  \n1.8.8  \norg.codehaus.jackson  \njackson-xc  \n1.9.13  \norg.codehaus.janino  \ncommons-compiler  \n2.7.8  \norg.codehaus.janino  \njanino  \n2.7.8  \norg.codehaus.jettison  \njettison  \n1.1  \norg.datanucleus  \ndatanucleus-api-jdo  \n3.2.6  \norg.datanucleus  \ndatanucleus-core  \n3.2.10  \norg.datanucleus  \ndatanucleus-rdbms  \n3.2.9  \norg.eclipse.jetty  \njetty-client  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-continuation  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-continuation  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-http  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-http  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-io  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-io  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-security  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-server"
    },
    {
        "id": 1715,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "org.eclipse.jetty  \njetty-io  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-security  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-server  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-server  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-servlet  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-servlets  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-util  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-util  \n8.1.14.v20131031  \norg.eclipse.jetty  \njetty-websocket  \n8.1.14.v20131031  \norg.eclipse.jetty.orbit  \njavax.servlet  \n3.0.0.v201112011016  \norg.eclipse.jetty.orbit  \njavax.servlet  \n3.0.0.v201112011016  \norg.fusesource.jansi  \njansi  \n1.4  \norg.fusesource.leveldbjni  \nleveldbjni-all  \n1.8  \norg.hawkular.agent  \nprometheus-scraper  \n0.20.1.Final  \norg.hibernate  \nhibernate-validator  \n5.1.1.Final  \norg.iq80.snappy  \nsnappy  \n0.2  \norg.jboss.logging  \njboss-logging  \n3.1.3.GA  \norg.jdbi  \njdbi  \n2.63.1  \norg.joda  \njoda-convert  \n1.7  \norg.jodd  \njodd-core  \n3.5.2  \norg.jpmml  \npmml-agent  \n1.1.15  \norg.jpmml  \npmml-model  \n1.1.15  \norg.jpmml  \npmml-schema  \n1.1.15  \norg.json  \njson  \n20090211  \norg.json4s  \njson4s-ast_2.10  \n3.2.10  \norg.json4s  \njson4s-core_2.10  \n3.2.10  \norg.json4s  \njson4s-jackson_2.10  \n3.2.10  \norg.mockito  \nmockito-all"
    },
    {
        "id": 1716,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "org.json4s  \njson4s-core_2.10  \n3.2.10  \norg.json4s  \njson4s-jackson_2.10  \n3.2.10  \norg.mockito  \nmockito-all  \n1.9.5  \norg.objenesis  \nobjenesis  \n1.2  \norg.postgresql  \npostgresql  \n9.4-1204-jdbc41  \norg.roaringbitmap  \nRoaringBitmap  \n0.5.11  \norg.rosuda.REngine  \nREngine  \n2.1.0  \norg.scala-lang  \njline  \n2.10.6  \norg.scala-lang  \nscala-compiler_2.10  \n2.10.6  \norg.scala-lang  \nscala-library_2.10  \n2.10.6  \norg.scala-lang  \nscala-reflect_2.10  \n2.10.6  \norg.scala-lang  \nscalap_2.10  \n2.10.6  \norg.scala-sbt  \ntest-interface  \n1.0  \norg.scalacheck  \nscalacheck_2.10  \n1.12.5  \norg.scalamacros  \nquasiquotes_2.10  \n2.0.0  \norg.scalamock  \nscalamock-core_2.10  \n3.2.1  \norg.scalamock  \nscalamock-scalatest-support_2.10  \n3.2.1  \norg.scalanlp  \nbreeze-macros_2.10  \n0.11.2  \norg.scalanlp  \nbreeze_2.10  \n0.11.2  \norg.scalatest  \nscalatest_2.10  \n2.2.4  \norg.slf4j  \njcl-over-slf4j  \n1.7.10  \norg.slf4j  \njul-to-slf4j  \n1.7.10  \norg.slf4j  \nslf4j-api  \n1.7.5  \norg.slf4j  \nslf4j-log4j12  \n1.7.5  \norg.slf4j  \nslf4j-log4j12  \n1.7.5  \norg.spark-project.hive  \nhive-beeline  \n1.2.1.spark  \norg.spark-project.hive  \nhive-cli  \n1.2.1.spark  \norg.spark-project.hive  \nhive-exec"
    },
    {
        "id": 1717,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/cluster-images/1.6.3-db1.html",
        "content": "1.7.5  \norg.spark-project.hive  \nhive-beeline  \n1.2.1.spark  \norg.spark-project.hive  \nhive-cli  \n1.2.1.spark  \norg.spark-project.hive  \nhive-exec  \n1.2.1.spark  \norg.spark-project.hive  \nhive-jdbc  \n1.2.1.spark  \norg.spark-project.hive  \nhive-metastore  \n1.2.1.spark  \norg.spark-project.hive  \nhive-service  \n1.2.1.spark  \norg.spark-project.spark  \nunused  \n1.0.0  \norg.spire-math  \nspire-macros_2.10  \n0.7.4  \norg.spire-math  \nspire_2.10  \n0.7.4  \norg.springframework  \nspring-core  \n4.1.4.RELEASE  \norg.springframework  \nspring-test  \n4.1.4.RELEASE  \norg.tachyonproject  \ntachyon-client  \n0.8.2  \norg.tachyonproject  \ntachyon-underfs-hdfs  \n0.8.2  \norg.tachyonproject  \ntachyon-underfs-local  \n0.8.2  \norg.tachyonproject  \ntachyon-underfs-s3  \n0.8.2  \norg.uncommons.maths  \nuncommons-maths  \n1.2.2a  \norg.xerial  \nsqlite-jdbc  \n3.8.11.2  \norg.xerial.snappy  \nsnappy-java  \n1.1.2.6  \noro  \noro  \n2.0.8  \nstax  \nstax-api  \n1.0.1  \nxerces  \nxercesImpl  \n2.9.1  \nxml-apis  \nxml-apis  \n1.3.04  \nxmlenc  \nxmlenc  \n0.52"
    },
    {
        "id": 1718,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/13.0ml.html",
        "content": "Databricks Runtime 13.0 for Machine Learning (EoS)  \nNote  \nSupport for this Databricks Runtime version has ended. For the end-of-support date, see End-of-support history. For all supported Databricks Runtime versions, see Databricks Runtime release notes versions and compatibility.  \nDatabricks Runtime 13.0 for Machine Learning provides a ready-to-go environment for machine learning and data science based on Databricks Runtime 13.0 (EoS). Databricks Runtime ML contains many popular machine learning libraries, including TensorFlow, PyTorch, and XGBoost. Databricks Runtime ML includes AutoML, a tool to automatically train machine learning pipelines. Databricks Runtime ML also supports distributed deep learning training using Horovod.  \nFor more information, including instructions for creating a Databricks Runtime ML cluster, see AI and Machine Learning on Databricks.  \nNew features and improvements"
    },
    {
        "id": 1719,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/13.0ml.html",
        "content": "New features and improvements\nDatabricks Runtime 13.0 ML is built on top of Databricks Runtime 13.0. For information on what\u2019s new in Databricks Runtime 13.0, including Apache Spark MLlib and SparkR, see the Databricks Runtime 13.0 (EoS) release notes.  \nChanges to Databricks AutoML  \nIn Databricks Runtime 13.0 ML and above, Databricks AutoML requires the E2 version of the Databricks platform.  \nFor more information about Databricks AutoML, see What is AutoML?.  \nEnhancements to Databricks Feature Store  \nIn Unity Catalog-enabled workspaces on a cluster running Databricks Runtime 13.0 ML or above, you can publish both workspace and Unity Catalog feature tables to Cosmos DB online stores.  \nFor more information about Databricks Feature Store, see Feature engineering and serving.\n\nSystem environment"
    },
    {
        "id": 1720,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/13.0ml.html",
        "content": "System environment\nThe system environment in Databricks Runtime 13.0 ML differs from Databricks Runtime 13.0 as follows:  \nDBUtils: Databricks Runtime ML does not include Library utility (dbutils.library) (legacy). Use %pip commands instead. See Notebook-scoped Python libraries.  \nFor GPU clusters, Databricks Runtime ML includes the following NVIDIA GPU libraries:  \nCUDA 11.7  \ncuDNN 8.5.0.96-1  \nNCCL 2.15.1  \nTensorRT 7.2.2  \nDatabricks Runtime 13.0 ML includes XGBoost 1.7.2, which does not support GPU clusters with compute capability 5.2 and below.  \nThe miniconda package has been removed from Databricks Runtime 13.0 ML.\n\nLibraries"
    },
    {
        "id": 1721,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/13.0ml.html",
        "content": "Libraries\nThe following sections list the libraries included in Databricks Runtime 13.0 ML that differ from those included in Databricks Runtime 13.0.  \nIn this section:  \nTop-tier libraries  \nPython libraries  \nR libraries  \nJava and Scala libraries (Scala 2.12 cluster)  \nTop-tier libraries  \nDatabricks Runtime 13.0 ML includes the following top-tier libraries:  \nGraphFrames  \nHorovod and HorovodRunner  \nMLflow  \nPyTorch  \nspark-tensorflow-connector  \nTensorFlow  \nTensorBoard  \nScikit-learn  \nPython libraries  \nDatabricks Runtime 13.0 ML uses Virtualenv for Python package management and includes many popular ML packages.  \nThe following Python libraries have been introduced with Databricks Runtime 13.0 ML:  \naccelerate  \ndatasets  \nevaluate  \nydata-profiling  \nIn addition to the packages specified in the following sections, Databricks Runtime 13.0 ML also includes the following packages:  \nhyperopt 0.2.7+db3  \nsparkdl 3.0.0_db1  \nautoml 1.17.0  \nTo reproduce the Databricks Runtime ML Python environment in your local Python virtual environment, download the requirements-13.0.txt file and run pip install -r requirements-13.0.txt. This command installs all of the open source libraries that Databricks Runtime ML uses, but does not install libraries developed by Databricks, such as databricks-automl, databricks-feature-store, or the Databricks fork of hyperopt.  \nPython libraries on CPU clusters  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nabsl-py  \n1.0.0  \naccelerate  \n0.16.0  \naiohttp  \n3.8.4  \naiosignal  \n1.3.1  \nappdirs  \n1.4.4  \nargon2-cffi  \n21.3.0  \nargon2-cffi-bindings  \n21.2.0  \nastor  \n0.8.1  \nasttokens  \n2.2.1  \nastunparse  \n1.6.3  \nasync-timeout  \n4.0.2  \nattrs  \n21.4.0  \nazure-core  \n1.26.3  \nazure-cosmos  \n4.3.1b1  \nbackcall  \n0.2.0  \nbcrypt  \n3.2.0  \nbeautifulsoup4  \n4.11.1  \nblack  \n22.6.0  \nbleach  \n4.1.0"
    },
    {
        "id": 1722,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/13.0ml.html",
        "content": "azure-cosmos  \n4.3.1b1  \nbackcall  \n0.2.0  \nbcrypt  \n3.2.0  \nbeautifulsoup4  \n4.11.1  \nblack  \n22.6.0  \nbleach  \n4.1.0  \nblinker  \n1.4  \nblis  \n0.7.9  \nboto3  \n1.24.28  \nbotocore  \n1.27.28  \ncachetools  \n4.2.4  \ncatalogue  \n2.0.8  \ncategory-encoders  \n2.6.0  \ncertifi  \n2022.9.14  \ncffi  \n1.15.1  \nchardet  \n4.0.0  \ncharset-normalizer  \n2.0.4  \nclick  \n8.0.4  \ncloudpickle  \n2.0.0  \ncmdstanpy  \n1.1.0  \nconfection  \n0.0.4  \nconfigparser  \n5.2.0  \nconvertdate  \n2.4.0  \ncryptography  \n37.0.1  \ncycler  \n0.11.0  \ncymem  \n2.0.7  \nCython  \n0.29.32  \ndatabricks-automl-runtime  \n0.2.16  \ndatabricks-cli  \n0.17.4  \ndatabricks-feature-store  \n0.11.0  \ndatasets  \n2.10.0  \ndbl-tempo  \n0.1.12  \ndbus-python  \n1.2.18  \ndebugpy  \n1.5.1  \ndecorator  \n5.1.1  \ndefusedxml  \n0.7.1  \ndill  \n0.3.4  \ndiskcache  \n5.4.0  \ndistlib  \n0.3.6  \ndocstring-to-markdown  \n0.11  \nentrypoints  \n0.4  \nephem  \n4.1.4  \nevaluate  \n0.4.0  \nexecuting  \n1.2.0  \nfacets-overview  \n1.0.2  \nfastjsonschema  \n2.16.3  \nfasttext  \n0.9.2  \nfilelock  \n3.6.0  \nFlask  \n1.1.2  \nflatbuffers  \n23.3.3  \nfonttools  \n4.25.0  \nfrozenlist  \n1.3.3  \nfsspec  \n2022.7.1  \nfuture  \n0.18.2  \ngast  \n0.4.0  \ngitdb  \n4.0.10  \nGitPython  \n3.1.27  \ngoogle-auth  \n1.33.0  \ngoogle-auth-oauthlib  \n0.4.6  \ngoogle-pasta  \n0.2.0  \ngoogleapis-common-protos"
    },
    {
        "id": 1723,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/13.0ml.html",
        "content": "GitPython  \n3.1.27  \ngoogle-auth  \n1.33.0  \ngoogle-auth-oauthlib  \n0.4.6  \ngoogle-pasta  \n0.2.0  \ngoogleapis-common-protos  \n1.56.4  \ngrpcio  \n1.48.1  \ngrpcio-status  \n1.48.1  \ngunicorn  \n20.1.0  \ngviz-api  \n1.10.0  \nh5py  \n3.7.0  \nhijri-converter  \n2.2.4  \nholidays  \n0.19  \nhorovod  \n0.27.0  \nhtmlmin  \n0.1.12  \nhttplib2  \n0.20.2  \nhuggingface-hub  \n0.13.2  \nidna  \n3.3  \nImageHash  \n4.3.1  \nimbalanced-learn  \n0.8.1  \nimportlib-metadata  \n4.11.3  \nipykernel  \n6.17.1  \nipython  \n8.10.0  \nipython-genutils  \n0.2.0  \nipywidgets  \n7.7.2  \nisodate  \n0.6.1  \nitsdangerous  \n2.0.1  \njedi  \n0.18.1  \njeepney  \n0.7.1  \nJinja2  \n2.11.3  \njmespath  \n0.10.0  \njoblib  \n1.2.0  \njoblibspark  \n0.5.1  \njsonschema  \n4.16.0  \njupyter-client  \n7.3.4  \njupyter_core  \n4.11.2  \njupyterlab-pygments  \n0.1.2  \njupyterlab-widgets  \n1.0.0  \nkeras  \n2.11.0  \nkeyring  \n23.5.0  \nkiwisolver  \n1.4.2  \nkorean-lunar-calendar  \n0.3.1  \nlangcodes  \n3.3.0  \nlaunchpadlib  \n1.10.16  \nlazr.restfulclient  \n0.14.4  \nlazr.uri  \n1.0.6  \nlibclang  \n15.0.6.1  \nlightgbm  \n3.3.5  \nllvmlite  \n0.38.0  \nLunarCalendar  \n0.0.9  \nMako  \n1.2.0  \nMarkdown  \n3.3.4  \nMarkupSafe  \n2.0.1  \nmatplotlib  \n3.5.2  \nmatplotlib-inline  \n0.1.6  \nmccabe  \n0.7.0  \nmistune  \n0.8.4"
    },
    {
        "id": 1724,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/13.0ml.html",
        "content": "3.3.4  \nMarkupSafe  \n2.0.1  \nmatplotlib  \n3.5.2  \nmatplotlib-inline  \n0.1.6  \nmccabe  \n0.7.0  \nmistune  \n0.8.4  \nmleap  \n0.20.0  \nmlflow-skinny  \n2.2.1  \nmore-itertools  \n8.10.0  \nmultidict  \n6.0.4  \nmultimethod  \n1.9.1  \nmultiprocess  \n0.70.12.2  \nmurmurhash  \n1.0.9  \nmypy-extensions  \n0.4.3  \nnbclient  \n0.5.13  \nnbconvert  \n6.4.4  \nnbformat  \n5.5.0  \nnest-asyncio  \n1.5.5  \nnetworkx  \n2.8.4  \nnltk  \n3.7  \nnodeenv  \n1.7.0  \nnotebook  \n6.4.12  \nnumba  \n0.55.1  \nnumpy  \n1.21.5  \noauthlib  \n3.2.0  \nopt-einsum  \n3.3.0  \npackaging  \n21.3  \npandas  \n1.4.4  \npandas-profiling  \n3.6.6  \npandocfilters  \n1.5.0  \nparamiko  \n2.9.2  \nparso  \n0.8.3  \npathspec  \n0.9.0  \npathy  \n0.10.1  \npatsy  \n0.5.2  \npetastorm  \n0.12.1  \npexpect  \n4.8.0  \nphik  \n0.12.3  \npickleshare  \n0.7.5  \nPillow  \n9.2.0  \npip  \n22.2.2  \nplatformdirs  \n2.5.2  \nplotly  \n5.9.0  \npluggy  \n1.0.0  \npmdarima  \n2.0.2  \npreshed  \n3.0.8  \nprometheus-client  \n0.14.1  \nprompt-toolkit  \n3.0.36  \nprophet  \n1.1.2  \nprotobuf  \n3.19.4  \npsutil  \n5.9.0  \npsycopg2  \n2.9.3  \nptyprocess  \n0.7.0  \npure-eval  \n0.2.2  \npyarrow  \n7.0.0  \npyasn1  \n0.4.8  \npyasn1-modules  \n0.2.8  \npybind11  \n2.10.3  \npycparser  \n2.21  \npydantic  \n1.10.6  \npyflakes  \n3.0.1  \nPygments  \n2.11.2  \nPyGObject  \n3.42.1  \nPyJWT"
    },
    {
        "id": 1725,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/13.0ml.html",
        "content": "2.10.3  \npycparser  \n2.21  \npydantic  \n1.10.6  \npyflakes  \n3.0.1  \nPygments  \n2.11.2  \nPyGObject  \n3.42.1  \nPyJWT  \n2.3.0  \nPyMeeus  \n0.5.12  \nPyNaCl  \n1.5.0  \npyodbc  \n4.0.32  \npyparsing  \n3.0.9  \npyright  \n1.1.294  \npyrsistent  \n0.18.0  \npython-dateutil  \n2.8.2  \npython-editor  \n1.0.4  \npython-lsp-jsonrpc  \n1.0.0  \npython-lsp-server  \n1.7.1  \npytoolconfig  \n1.2.2  \npytz  \n2022.1  \nPyWavelets  \n1.3.0  \nPyYAML  \n6.0  \npyzmq  \n23.2.0  \nregex  \n2022.7.9  \nrequests  \n2.28.1  \nrequests-oauthlib  \n1.3.1  \nresponses  \n0.18.0  \nrope  \n1.7.0  \nrsa  \n4.9  \ns3transfer  \n0.6.0  \nscikit-learn  \n1.1.1  \nscipy  \n1.9.1  \nseaborn  \n0.11.2  \nSecretStorage  \n3.3.1  \nSend2Trash  \n1.8.0  \nsetuptools  \n63.4.1  \nshap  \n0.41.0  \nsimplejson  \n3.17.6  \nsix  \n1.16.0  \nslicer  \n0.0.7  \nsmart-open  \n5.2.1  \nsmmap  \n5.0.0  \nsoupsieve  \n2.3.1  \nspacy  \n3.5.0  \nspacy-legacy  \n3.0.12  \nspacy-loggers  \n1.0.4  \nspark-tensorflow-distributor  \n1.0.0  \nsqlparse  \n0.4.2  \nsrsly  \n2.4.6  \nssh-import-id  \n5.11  \nstack-data  \n0.6.2  \nstatsmodels  \n0.13.2  \ntabulate  \n0.8.10  \ntangled-up-in-unicode  \n0.2.0  \ntenacity  \n8.0.1  \ntensorboard  \n2.11.0  \ntensorboard-data-server  \n0.6.1  \ntensorboard-plugin-profile  \n2.11.1  \ntensorboard-plugin-wit  \n1.8.1  \ntensorflow-cpu"
    },
    {
        "id": 1726,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/13.0ml.html",
        "content": "2.11.0  \ntensorboard-data-server  \n0.6.1  \ntensorboard-plugin-profile  \n2.11.1  \ntensorboard-plugin-wit  \n1.8.1  \ntensorflow-cpu  \n2.11.0  \ntensorflow-estimator  \n2.11.0  \ntensorflow-io-gcs-filesystem  \n0.31.0  \ntermcolor  \n2.2.0  \nterminado  \n0.13.1  \ntestpath  \n0.6.0  \nthinc  \n8.1.9  \nthreadpoolctl  \n2.2.0  \ntokenize-rt  \n4.2.1  \ntokenizers  \n0.13.2  \ntomli  \n2.0.1  \ntorch  \n1.13.1+cpu  \ntorchvision  \n0.14.1+cpu  \ntornado  \n6.1  \ntqdm  \n4.64.1  \ntraitlets  \n5.1.1  \ntransformers  \n4.26.1  \ntypeguard  \n2.13.3  \ntyper  \n0.7.0  \ntyping_extensions  \n4.3.0  \nujson  \n5.4.0  \nunattended-upgrades  \n0.1  \nurllib3  \n1.26.11  \nvirtualenv  \n20.16.3  \nvisions  \n0.7.5  \nwadllib  \n1.3.6  \nwasabi  \n1.1.1  \nwcwidth  \n0.2.5  \nwebencodings  \n0.5.1  \nwebsocket-client  \n0.58.0  \nWerkzeug  \n2.0.3  \nwhatthepatch  \n1.0.2  \nwheel  \n0.37.1  \nwidgetsnbextension  \n3.6.1  \nwrapt  \n1.14.1  \nxgboost  \n1.7.4  \nxxhash  \n3.2.0  \nyapf  \n0.31.0  \nyarl  \n1.8.2  \nydata-profiling  \n4.1.0  \nzipp  \n3.8.0  \nPython libraries on GPU clusters  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nabsl-py  \n1.0.0  \naccelerate  \n0.16.0  \naiohttp  \n3.8.4  \naiosignal  \n1.3.1  \nappdirs  \n1.4.4  \nargon2-cffi  \n21.3.0  \nargon2-cffi-bindings  \n21.2.0  \nastor  \n0.8.1  \nasttokens  \n2.2.1  \nastunparse  \n1.6.3  \nasync-timeout  \n4.0.2"
    },
    {
        "id": 1727,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/13.0ml.html",
        "content": "argon2-cffi-bindings  \n21.2.0  \nastor  \n0.8.1  \nasttokens  \n2.2.1  \nastunparse  \n1.6.3  \nasync-timeout  \n4.0.2  \nattrs  \n21.4.0  \nazure-core  \n1.26.3  \nazure-cosmos  \n4.3.1b1  \nbackcall  \n0.2.0  \nbcrypt  \n3.2.0  \nbeautifulsoup4  \n4.11.1  \nblack  \n22.6.0  \nbleach  \n4.1.0  \nblinker  \n1.4  \nblis  \n0.7.9  \nboto3  \n1.24.28  \nbotocore  \n1.27.28  \ncachetools  \n4.2.4  \ncatalogue  \n2.0.8  \ncategory-encoders  \n2.6.0  \ncertifi  \n2022.9.14  \ncffi  \n1.15.1  \nchardet  \n4.0.0  \ncharset-normalizer  \n2.0.4  \nclick  \n8.0.4  \ncloudpickle  \n2.0.0  \ncmdstanpy  \n1.1.0  \nconfection  \n0.0.4  \nconfigparser  \n5.2.0  \nconvertdate  \n2.4.0  \ncryptography  \n37.0.1  \ncycler  \n0.11.0  \ncymem  \n2.0.7  \nCython  \n0.29.32  \ndatabricks-automl-runtime  \n0.2.16  \ndatabricks-cli  \n0.17.4  \ndatabricks-feature-store  \n0.11.0  \ndatasets  \n2.10.0  \ndbl-tempo  \n0.1.12  \ndbus-python  \n1.2.18  \ndebugpy  \n1.5.1  \ndecorator  \n5.1.1  \ndefusedxml  \n0.7.1  \ndill  \n0.3.4  \ndiskcache  \n5.4.0  \ndistlib  \n0.3.6  \ndocstring-to-markdown  \n0.11  \nentrypoints  \n0.4  \nephem  \n4.1.4  \nevaluate  \n0.4.0  \nexecuting  \n1.2.0  \nfacets-overview  \n1.0.2  \nfastjsonschema  \n2.16.3  \nfasttext  \n0.9.2  \nfilelock  \n3.6.0  \nFlask  \n1.1.2  \nflatbuffers  \n23.3.3  \nfonttools  \n4.25.0  \nfrozenlist  \n1.3.3  \nfsspec  \n2022.7.1  \nfuture  \n0.18.2  \ngast"
    },
    {
        "id": 1728,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/13.0ml.html",
        "content": "Flask  \n1.1.2  \nflatbuffers  \n23.3.3  \nfonttools  \n4.25.0  \nfrozenlist  \n1.3.3  \nfsspec  \n2022.7.1  \nfuture  \n0.18.2  \ngast  \n0.4.0  \ngitdb  \n4.0.10  \nGitPython  \n3.1.27  \ngoogle-auth  \n1.33.0  \ngoogle-auth-oauthlib  \n0.4.6  \ngoogle-pasta  \n0.2.0  \ngoogleapis-common-protos  \n1.56.4  \ngrpcio  \n1.48.1  \ngrpcio-status  \n1.48.1  \ngunicorn  \n20.1.0  \ngviz-api  \n1.10.0  \nh5py  \n3.7.0  \nhijri-converter  \n2.2.4  \nholidays  \n0.19  \nhorovod  \n0.27.0  \nhtmlmin  \n0.1.12  \nhttplib2  \n0.20.2  \nhuggingface-hub  \n0.13.1  \nidna  \n3.3  \nImageHash  \n4.3.1  \nimbalanced-learn  \n0.8.1  \nimportlib-metadata  \n4.11.3  \nipykernel  \n6.17.1  \nipython  \n8.10.0  \nipython-genutils  \n0.2.0  \nipywidgets  \n7.7.2  \nisodate  \n0.6.1  \nitsdangerous  \n2.0.1  \njedi  \n0.18.1  \njeepney  \n0.7.1  \nJinja2  \n2.11.3  \njmespath  \n0.10.0  \njoblib  \n1.2.0  \njoblibspark  \n0.5.1  \njsonschema  \n4.16.0  \njupyter-client  \n7.3.4  \njupyter_core  \n4.11.2  \njupyterlab-pygments  \n0.1.2  \njupyterlab-widgets  \n1.0.0  \nkeras  \n2.11.0  \nkeyring  \n23.5.0  \nkiwisolver  \n1.4.2  \nkorean-lunar-calendar  \n0.3.1  \nlangcodes  \n3.3.0  \nlaunchpadlib  \n1.10.16  \nlazr.restfulclient  \n0.14.4  \nlazr.uri  \n1.0.6  \nlibclang  \n15.0.6.1  \nlightgbm  \n3.3.5  \nllvmlite  \n0.38.0  \nLunarCalendar"
    },
    {
        "id": 1729,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/13.0ml.html",
        "content": "0.14.4  \nlazr.uri  \n1.0.6  \nlibclang  \n15.0.6.1  \nlightgbm  \n3.3.5  \nllvmlite  \n0.38.0  \nLunarCalendar  \n0.0.9  \nMako  \n1.2.0  \nMarkdown  \n3.3.4  \nMarkupSafe  \n2.0.1  \nmatplotlib  \n3.5.2  \nmatplotlib-inline  \n0.1.6  \nmccabe  \n0.7.0  \nmistune  \n0.8.4  \nmleap  \n0.20.0  \nmlflow-skinny  \n2.2.1  \nmore-itertools  \n8.10.0  \nmultidict  \n6.0.4  \nmultimethod  \n1.9.1  \nmultiprocess  \n0.70.12.2  \nmurmurhash  \n1.0.9  \nmypy-extensions  \n0.4.3  \nnbclient  \n0.5.13  \nnbconvert  \n6.4.4  \nnbformat  \n5.5.0  \nnest-asyncio  \n1.5.5  \nnetworkx  \n2.8.4  \nnltk  \n3.7  \nnodeenv  \n1.7.0  \nnotebook  \n6.4.12  \nnumba  \n0.55.1  \nnumpy  \n1.21.5  \noauthlib  \n3.2.0  \nopt-einsum  \n3.3.0  \npackaging  \n21.3  \npandas  \n1.4.4  \npandas-profiling  \n3.6.6  \npandocfilters  \n1.5.0  \nparamiko  \n2.9.2  \nparso  \n0.8.3  \npathspec  \n0.9.0  \npathy  \n0.10.1  \npatsy  \n0.5.2  \npetastorm  \n0.12.1  \npexpect  \n4.8.0  \nphik  \n0.12.3  \npickleshare  \n0.7.5  \nPillow  \n9.2.0  \npip  \n22.2.2  \nplatformdirs  \n2.5.2  \nplotly  \n5.9.0  \npluggy  \n1.0.0  \npmdarima  \n2.0.2  \npreshed  \n3.0.8  \nprompt-toolkit  \n3.0.36  \nprophet  \n1.1.2  \nprotobuf  \n3.19.4  \npsutil  \n5.9.0  \npsycopg2  \n2.9.3  \nptyprocess  \n0.7.0  \npure-eval  \n0.2.2  \npyarrow  \n7.0.0  \npyasn1  \n0.4.8  \npyasn1-modules  \n0.2.8"
    },
    {
        "id": 1730,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/13.0ml.html",
        "content": "2.9.3  \nptyprocess  \n0.7.0  \npure-eval  \n0.2.2  \npyarrow  \n7.0.0  \npyasn1  \n0.4.8  \npyasn1-modules  \n0.2.8  \npybind11  \n2.10.3  \npycparser  \n2.21  \npydantic  \n1.10.6  \npyflakes  \n3.0.1  \nPygments  \n2.11.2  \nPyGObject  \n3.42.1  \nPyJWT  \n2.3.0  \nPyMeeus  \n0.5.12  \nPyNaCl  \n1.5.0  \npyodbc  \n4.0.32  \npyparsing  \n3.0.9  \npyright  \n1.1.294  \npyrsistent  \n0.18.0  \npython-dateutil  \n2.8.2  \npython-editor  \n1.0.4  \npython-lsp-jsonrpc  \n1.0.0  \npython-lsp-server  \n1.7.1  \npytoolconfig  \n1.2.2  \npytz  \n2022.1  \nPyWavelets  \n1.3.0  \nPyYAML  \n6.0  \npyzmq  \n23.2.0  \nregex  \n2022.7.9  \nrequests  \n2.28.1  \nrequests-oauthlib  \n1.3.1  \nresponses  \n0.18.0  \nrope  \n1.7.0  \nrsa  \n4.9  \ns3transfer  \n0.6.0  \nscikit-learn  \n1.1.1  \nscipy  \n1.9.1  \nseaborn  \n0.11.2  \nSecretStorage  \n3.3.1  \nSend2Trash  \n1.8.0  \nsetuptools  \n63.4.1  \nshap  \n0.41.0  \nsimplejson  \n3.17.6  \nsix  \n1.16.0  \nslicer  \n0.0.7  \nsmart-open  \n5.2.1  \nsmmap  \n5.0.0  \nsoupsieve  \n2.3.1  \nspacy  \n3.5.0  \nspacy-legacy  \n3.0.12  \nspacy-loggers  \n1.0.4  \nspark-tensorflow-distributor  \n1.0.0  \nsqlparse  \n0.4.2  \nsrsly  \n2.4.6  \nssh-import-id  \n5.11  \nstack-data  \n0.6.2  \nstatsmodels  \n0.13.2  \ntabulate  \n0.8.10  \ntangled-up-in-unicode  \n0.2.0  \ntenacity  \n8.0.1"
    },
    {
        "id": 1731,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/13.0ml.html",
        "content": "5.11  \nstack-data  \n0.6.2  \nstatsmodels  \n0.13.2  \ntabulate  \n0.8.10  \ntangled-up-in-unicode  \n0.2.0  \ntenacity  \n8.0.1  \ntensorboard  \n2.11.0  \ntensorboard-data-server  \n0.6.1  \ntensorboard-plugin-profile  \n2.11.1  \ntensorboard-plugin-wit  \n1.8.1  \ntensorflow  \n2.11.0  \ntensorflow-estimator  \n2.11.0  \ntensorflow-io-gcs-filesystem  \n0.31.0  \ntermcolor  \n2.2.0  \nterminado  \n0.13.1  \ntestpath  \n0.6.0  \nthinc  \n8.1.9  \nthreadpoolctl  \n2.2.0  \ntokenize-rt  \n4.2.1  \ntokenizers  \n0.13.2  \ntomli  \n2.0.1  \ntorch  \n1.13.1+cu117  \ntorchvision  \n0.14.1+cu117  \ntornado  \n6.1  \ntqdm  \n4.64.1  \ntraitlets  \n5.1.1  \ntransformers  \n4.26.1  \ntypeguard  \n2.13.3  \ntyper  \n0.7.0  \ntyping_extensions  \n4.3.0  \nujson  \n5.4.0  \nunattended-upgrades  \n0.1  \nurllib3  \n1.26.11  \nvirtualenv  \n20.16.3  \nvisions  \n0.7.5  \nwadllib  \n1.3.6  \nwasabi  \n1.1.1  \nwcwidth  \n0.2.5  \nwebencodings  \n0.5.1  \nwebsocket-client  \n0.58.0  \nWerkzeug  \n2.0.3  \nwhatthepatch  \n1.0.2  \nwheel  \n0.37.1  \nwidgetsnbextension  \n3.6.1  \nwrapt  \n1.14.1  \nxgboost  \n1.7.4  \nxxhash  \n3.2.0  \nyapf  \n0.31.0  \nyarl  \n1.8.2  \nydata-profiling  \n4.1.0  \nzipp  \n3.8.0  \nR libraries  \nThe R libraries are identical to the R Libraries in Databricks Runtime 13.0.  \nJava and Scala libraries (Scala 2.12 cluster)"
    },
    {
        "id": 1732,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/13.0ml.html",
        "content": "4.1.0  \nzipp  \n3.8.0  \nR libraries  \nThe R libraries are identical to the R Libraries in Databricks Runtime 13.0.  \nJava and Scala libraries (Scala 2.12 cluster)  \nIn addition to Java and Scala libraries in Databricks Runtime 13.0, Databricks Runtime 13.0 ML contains the following JARs:  \nCPU clusters  \nGroup ID  \nArtifact ID  \nVersion  \ncom.typesafe.akka  \nakka-actor_2.12  \n2.5.23  \nml.combust.mleap  \nmleap-databricks-runtime_2.12  \nv0.20.0-db2  \nml.dmlc  \nxgboost4j-spark_2.12  \n1.7.3  \nml.dmlc  \nxgboost4j_2.12  \n1.7.3  \norg.graphframes  \ngraphframes_2.12  \n0.8.2-db1-spark3.2  \norg.mlflow  \nmlflow-client  \n2.2.1  \norg.scala-lang.modules  \nscala-java8-compat_2.12  \n0.8.0  \norg.tensorflow  \nspark-tensorflow-connector_2.12  \n1.15.0  \nGPU clusters  \nGroup ID  \nArtifact ID  \nVersion  \ncom.typesafe.akka  \nakka-actor_2.12  \n2.5.23  \nml.combust.mleap  \nmleap-databricks-runtime_2.12  \nv0.20.0-db2  \nml.dmlc  \nxgboost4j-gpu_2.12  \n1.7.3  \nml.dmlc  \nxgboost4j-spark-gpu_2.12  \n1.7.3  \norg.graphframes  \ngraphframes_2.12  \n0.8.2-db1-spark3.2  \norg.mlflow  \nmlflow-client  \n2.2.1  \norg.scala-lang.modules  \nscala-java8-compat_2.12  \n0.8.0  \norg.tensorflow  \nspark-tensorflow-connector_2.12  \n1.15.0"
    },
    {
        "id": 1733,
        "url": "https://docs.databricks.com/en/admin/system-tables/warehouse-events.html",
        "content": "Warehouse events system table reference  \nPreview  \nThis feature is in Public Preview. The schema must be enabled to be visible in your system catalog. For more information, see Enable system table schemas  \nIn this article, you learn how to use the warehouse events system table to monitor and manage the SQL warehouses in your workspaces. This table records a row for every time a warehouse starts, stops, runs, and scales up and down. You can use the sample queries in this article with alerts to keep you informed of changes to your warehouses.  \nThe warehouse events system table is located at system.compute.warehouse_events.  \nLogged warehouse event types\nLogged warehouse event types\nThis system table logs the following types of events:  \nSCALED_UP: A new cluster was added to the warehouse.  \nSCALED_DOWN: A cluster was removed from the warehouse.  \nSTOPPING: The warehouse is in the process of stopping.  \nRUNNING: The warehouse is actively running.  \nSTARTING: The warehouse is in the process of starting up.  \nSTOPPED: The warehouse has completely stopped running.\n\nWarehouse events schema"
    },
    {
        "id": 1734,
        "url": "https://docs.databricks.com/en/admin/system-tables/warehouse-events.html",
        "content": "Warehouse events schema\nThe warehouse_events system table uses the following schema:  \nColumn name  \nData type  \nDescription  \nExample  \naccount_id  \nstring  \nThe ID of the Databricks account.  \n7af234db-66d7-4db3-bbf0-956098224879  \nworkspace_id  \nstring  \nThe ID of the workspace where the warehouse is deployed.  \n123456789012345  \nwarehouse_id  \nstring  \nThe ID of SQL warehouse the event is related to.  \n123456789012345  \nevent_type  \nstring  \nThe type of warehouse event. Possible values are SCALED_UP, SCALED_DOWN, STOPPING, RUNNING, STARTING, and STOPPED.  \nSCALED_UP  \ncluster_count  \ninteger  \nThe number of clusters that are actively running.  \n2  \nevent_time  \ntimestamp  \nTimestamp of when the event took place in UTC.  \n2023-07-20T19:13:09.504Z\n\nSample queries"
    },
    {
        "id": 1735,
        "url": "https://docs.databricks.com/en/admin/system-tables/warehouse-events.html",
        "content": "Sample queries\nThe following sample queries are templates. Plug in whatever values make sense for your organization. You can also add alerts to these queries to help you stay informed about changes to your warehouses. See Create an alert.  \nUse the following sample queries to gain insight into warehouse behavior:  \nWhich warehouses are actively running and for how long?  \nIdentify warehouses that are upscaled longer than expected  \nWarehouses that start for the first time  \nInvestigate billing charges  \nWhich warehouses haven\u2019t been used in the last 30 days?  \nWarehouses with the most uptime in a month  \nWarehouses that spent the most time upscaled during a month  \nWhich warehouses are actively running and for how long?  \nThis query identifies which warehouses are currently active along with their running time in hours.  \nUSE CATALOG `system`; SELECT we.warehouse_id, we.event_time, TIMESTAMPDIFF(MINUTE, we.event_time, CURRENT_TIMESTAMP()) / 60.0 AS running_hours, we.cluster_count FROM compute.warehouse_events we WHERE we.event_type = 'RUNNING' AND NOT EXISTS ( SELECT 1 FROM compute.warehouse_events we2 WHERE we2.warehouse_id = we.warehouse_id AND we2.event_time > we.event_time )  \nAlert opportunity: As a workspace admin you might want to be alerted if a warehouse is running longer than expected. For example, you can use the query results to set an alert condition to trigger when the running hours exceed a certain threshold.  \nIdentify warehouses that are upscaled longer than expected  \nThis query identifies which warehouses are currently active along with their running time in hours."
    },
    {
        "id": 1736,
        "url": "https://docs.databricks.com/en/admin/system-tables/warehouse-events.html",
        "content": "Identify warehouses that are upscaled longer than expected  \nThis query identifies which warehouses are currently active along with their running time in hours.  \nuse catalog `system`; SELECT we.warehouse_id, we.event_time, TIMESTAMPDIFF(MINUTE, we.event_time, CURRENT_TIMESTAMP()) / 60.0 AS upscaled_hours, we.cluster_count FROM compute.warehouse_events we WHERE we.event_type = 'SCALED_UP' AND we.cluster_count >= 2 AND NOT EXISTS ( SELECT 1 FROM compute.warehouse_events we2 WHERE we2.warehouse_id = we.warehouse_id AND ( (we2.event_type = 'SCALED_DOWN') OR (we2.event_type = 'SCALED_UP' AND we2.cluster_count < 2) ) AND we2.event_time > we.event_time )  \nAlert opportunity: Alerting on this condition can help you monitor resources and cost. You could set an alert for when the upscaled hours exceed a certain limit.  \nWarehouses that start for the first time  \nThis query informs you about new warehouses that are starting for the first time.  \nuse catalog `system`; SELECT we.warehouse_id, we.event_time, we.cluster_count FROM compute.warehouse_events we WHERE (we.event_type = 'STARTING' OR we.event_type = 'RUNNING') AND NOT EXISTS ( SELECT 1 FROM compute.warehouse_events we2 WHERE we2.warehouse_id = we.warehouse_id AND we2.event_time < we.event_time )  \nAlert opportunity: Alerting on new warehouses can help your organization track resource allocation. For example, you could set an alert that\u2019s triggered every time a new warehouse starts.  \nInvestigate billing charges  \nIf you want to understand specifically what a warehouse was doing to generate billing charges, this query can tell you the exact dates and times the warehouse scaled up or down, or started and stopped."
    },
    {
        "id": 1737,
        "url": "https://docs.databricks.com/en/admin/system-tables/warehouse-events.html",
        "content": "Investigate billing charges  \nIf you want to understand specifically what a warehouse was doing to generate billing charges, this query can tell you the exact dates and times the warehouse scaled up or down, or started and stopped.  \nuse catalog `system`; SELECT we.warehouse_id AS warehouse_id, we.event_type AS event, we.event_time AS event_time, we.cluster_count AS cluster_count FROM compute.warehouse_events AS we WHERE we.event_type IN ( 'STARTING', 'RUNNING', 'STOPPING', 'STOPPED', 'SCALING_UP', 'SCALED_UP', 'SCALING_DOWN', 'SCALED_DOWN' ) AND MONTH(we.event_time) = 7 AND YEAR(we.event_time) = YEAR(CURRENT_DATE()) AND we.warehouse_id = '19c9d68652189278' ORDER BY event_time DESC  \nWhich warehouses haven\u2019t been used in the last 30 days?  \nThis query helps you identify unused resources, providing an opportunity for cost optimization.  \nuse catalog `system`; SELECT we.warehouse_id, we.event_time, we.event_type, we.cluster_count FROM compute.warehouse_events AS we WHERE we.warehouse_id IN ( SELECT DISTINCT warehouse_id FROM compute.warehouse_events WHERE MONTH(event_time) = 6 AND YEAR(event_time) = YEAR(CURRENT_DATE()) ) AND we.warehouse_id NOT IN ( SELECT DISTINCT warehouse_id FROM compute.warehouse_events WHERE MONTH(event_time) = 7 AND YEAR(event_time) = YEAR(CURRENT_DATE()) ) ORDER BY event_time DESC  \nAlert opportunity: Receiving an alert on unused resources could help your organization optimize costs. For example, you could set an alert that\u2019s triggered when the query detects an unused warehouse.  \nWarehouses with the most uptime in a month  \nThis query shows which warehouses have been used the most during a specific month. This query uses July as an example."
    },
    {
        "id": 1738,
        "url": "https://docs.databricks.com/en/admin/system-tables/warehouse-events.html",
        "content": "Warehouses with the most uptime in a month  \nThis query shows which warehouses have been used the most during a specific month. This query uses July as an example.  \nuse catalog `system`; SELECT warehouse_id, SUM(TIMESTAMPDIFF(MINUTE, start_time, end_time)) / 60.0 AS uptime_hours FROM ( SELECT starting.warehouse_id, starting.event_time AS start_time, ( SELECT MIN(stopping.event_time) FROM compute.warehouse_events AS stopping WHERE stopping.warehouse_id = starting.warehouse_id AND stopping.event_type = 'STOPPED' AND stopping.event_time > starting.event_time ) AS end_time FROM compute.warehouse_events AS starting WHERE starting.event_type = 'STARTING' AND MONTH(starting.event_time) = 7 AND YEAR(starting.event_time) = YEAR(CURRENT_DATE()) ) AS warehouse_uptime WHERE end_time IS NOT NULL GROUP BY warehouse_id ORDER BY uptime_hours DESC  \nAlert opportunity: You might want to keep track of high-utilization warehouses. For example, you could set an alert that\u2019s triggered when the uptime hours for a warehouse exceed a specific threshold.  \nWarehouses that spent the most time upscaled during a month  \nThis query informs you about warehouses that have spent significant time in the upscaled state during a month. This query uses July as an example."
    },
    {
        "id": 1739,
        "url": "https://docs.databricks.com/en/admin/system-tables/warehouse-events.html",
        "content": "Warehouses that spent the most time upscaled during a month  \nThis query informs you about warehouses that have spent significant time in the upscaled state during a month. This query uses July as an example.  \nuse catalog `system`; SELECT warehouse_id, SUM(TIMESTAMPDIFF(MINUTE, upscaled_time, downscaled_time)) / 60.0 AS upscaled_hours FROM ( SELECT upscaled.warehouse_id, upscaled.event_time AS upscaled_time, ( SELECT MIN(downscaled.event_time) FROM compute.warehouse_events AS downscaled WHERE downscaled.warehouse_id = upscaled.warehouse_id AND (downscaled.event_type = 'SCALED_DOWN' OR downscaled.event_type = 'STOPPED') AND downscaled.event_time > upscaled.event_time ) AS downscaled_time FROM compute.warehouse_events AS upscaled WHERE upscaled.event_type = 'SCALED_UP' AND upscaled.cluster_count >= 2 AND MONTH(upscaled.event_time) = 7 AND YEAR(upscaled.event_time) = YEAR(CURRENT_DATE()) ) AS warehouse_upscaled WHERE downscaled_time IS NOT NULL GROUP BY warehouse_id ORDER BY upscaled_hours DESC limit 0;  \nAlert opportunity: You might want to keep track of high-utilization warehouses. For example, you could set an alert that\u2019s triggered when the uptime hours for a warehouse exceed a specific threshold."
    },
    {
        "id": 1740,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "Databricks Runtime 7.4 (EoS)  \nNote  \nSupport for this Databricks Runtime version has ended. For the end-of-support date, see End-of-support history. For all supported Databricks Runtime versions, see Databricks Runtime release notes versions and compatibility.  \nDatabricks released this image in November 2020.  \nThe following release notes provide information about Databricks Runtime 7.4, powered by Apache Spark 3.0.  \nNew features"
    },
    {
        "id": 1741,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "New features\nIn this section:  \nDelta Lake features and improvements  \nAuto Loader now supports delegating file notification resources setup to admins  \nNew USAGE privilege give admins greater control over data access privileges  \nDBFS FUSE now enabled for passthrough-enabled clusters  \nDelta Lake features and improvements  \nThis release provides the following Delta Lake features and improvements:  \nNew API allows Delta Lake to verify that data added to a table satisfies constraints  \nNew API allows you to roll back a Delta table to an older version of the table  \nNew starting version allows returning only latest changes in a Delta Lake streaming source  \nImproved stability of OPTIMIZE  \nNew API allows Delta Lake to verify that data added to a table satisfies constraints  \nDelta Lake now supports CHECK constraints. When supplied, Delta Lake automatically verifies that data added to a table satisfies the specified expression. To add CHECK constraints, use the ALTER TABLE ADD CONSTRAINTS command. For details, see Constraints on Databricks.  \nNew API allows you to roll back a Delta table to an older version of the table  \nYou can now roll back your Delta tables to older versions by using the RESTORE command:  \nRESTORE <table> TO VERSION AS OF n; RESTORE <table> TO TIMESTAMP AS OF 'yyyy-MM-dd HH:mm:ss';  \nfrom delta.tables import DeltaTable DeltaTable.forName(spark, \"table_name\").restoreToVersion(n) DeltaTable.forName(spark, \"table_name\").restoreToTimestamp('yyyy-MM-dd')  \nimport io.delta.tables.DeltaTable DeltaTable.forName(spark, \"table_name\").restoreToVersion(n) DeltaTable.forName(spark, \"table_name\").restoreToTimestamp(\"yyyy-MM-dd\")  \nRESTORE creates a new commit that rolls back all changes made to your table since the version you want to restore. All existing data and metadata is restored, which includes the schema, constraints, streaming transaction IDs, COPY INTO metadata, and the table protocol version. For details, see Restore a Delta table.  \nNew starting version allows returning only latest changes in a Delta Lake streaming source  \nTo return only the latest changes, specify startingVersion as latest. For details, see Specify initial position.  \nImproved stability of OPTIMIZE"
    },
    {
        "id": 1742,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "New starting version allows returning only latest changes in a Delta Lake streaming source  \nTo return only the latest changes, specify startingVersion as latest. For details, see Specify initial position.  \nImproved stability of OPTIMIZE  \nOPTIMIZE (without any partition predicates) can scale to running on tables with tens of millions of small files. Previously the Apache Spark driver could run out of memory and OPTIMIZE wouldn\u2019t complete.OPTIMIZE now handles very large tables with tens of millions of files.  \nAuto Loader now supports delegating file notification resources setup to admins  \nA new Scala API allows admins to set up file notification resources for Auto Loader. Data engineers can now operate their Auto Loader streams with fewer permissions by delegating the initial resource setup to their admins. See Manually configure or manage file notification resources.  \nNew USAGE privilege give admins greater control over data access privileges  \nTo perform an action on an object in a database you must now be granted the USAGE privilege on that database in addition to the privileges required to perform the action. The USAGE privilege is granted for a database or a catalog. With the introduction of the USAGE privilege, a table owner can no longer unilaterally decide to share it with another user; the user must also have the USAGE privilege on the database containing the table.  \nIn workspaces that have table access control enabled, the users group automatically has the USAGE privilege for the root CATALOG.  \nFor details, see USAGE privilege.  \nDBFS FUSE now enabled for passthrough-enabled clusters  \nYou can now read and write from DBFS using the FUSE mount at /dbfs/ when you are using a High Concurrency cluster enabled for credential passthrough. Regular mounts are supported. Mounts that require passthrough credentials are not supported."
    },
    {
        "id": 1743,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "Improvements\nSpark SQL supports IFF and CHARINDEX as synonyms for IF and POSITION  \nIn Databricks Runtime IF() is a synonym for CASE WHEN <cond> THEN <expr1> ELSE <expr2> END  \nDatabricks Runtime now supports IFF() as a synonym for IF()  \nSELECT IFF(c1 = 1, 'Hello', 'World'), c1 FROM (VALUES (1), (2)) AS T(c1) => (Hello, 1) (World, 2)  \nCHARINDEX is an alternate name for the POSITION function. CHARINDEX finds the position of the first occurrence of a string within another string with an optional start index.  \nVALUES(CHARINDEX('he', 'hello from hell', 2)) => 12  \nMultiple outputs per cell enabled for Python notebooks by default  \nDatabricks Runtime 7.1 introduced support for multiple outputs per cell in Python notebooks (and %python cells within non-Python notebooks), but you were required to enable the feature for your notebook. This feature is enabled by default in Databricks Runtime 7.4. See View multiple outputs per cell.  \nAutocomplete improvements for Python notebooks  \nAutocomplete for Python shows additional type information generated from static analysis of the code using the Jedi library. You can press the Tab key to see a list of options.  \nImproved display of Spark ML vectors in Spark DataFrame preview  \nThe display format now shows labels for vector type (sparse or dense), length, indices (for sparse vectors), and values.  \nOther fixes  \nFixed a pickling issue with collections.namedtuple in notebooks.  \nFixed a pickling issue with interactively defined classes and methods.  \nFixed a bug that caused calls to mlflow.start_run() to fail on passthrough or clusters enabled for table access control.  \nLibrary upgrades  \nUpgraded Python libraries:  \njedi upgraded from 0.14.1 to 0.17.2.  \nkoalas upgraded from 1.2.0 to 1.3.0.  \nparso upgraded from 0.5.2 to 0.7.0.  \nUpgraded several installed R libraries. See Installed R Libraries."
    },
    {
        "id": 1744,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "Apache Spark\nDatabricks Runtime 7.4 includes Apache Spark 3.0.1. This release includes all Spark fixes and improvements included in Databricks Runtime 7.3 LTS (EoS), as well as the following additional bug fixes and improvements made to Spark:  \n[SPARK-33170] [SQL] Add SQL config to control fast-fail behavior in FileFormatWriter  \n[SPARK-33136] [SQL] Fix mistakenly swapped parameter in V2WriteCommand.outputResolved  \n[SPARK-33134] [SQL] Return partial results only for root JSON objects  \n[SPARK-33038] [SQL] Combine AQE initial and current plan s\u2026  \n[SPARK-33118] [SQL] CREATE TEMPORARY TABLE fails with location  \n[SPARK-33101] [ML] Make LibSVM format propagate Hadoop config from DS options to underlying HDFS file system  \n[SPARK-33035] [SQL] Updates the obsoleted entries of attribute mapping in QueryPlan#transformUpWithNewOutput  \n[SPARK-33091] [SQL] Avoid using map instead of foreach to avoid potential side effect at callers of OrcUtils.readCatalystSchema  \n[SPARK-33073] [PYTHON] Improve error handling on Pandas to Arrow conversion failures  \n[SPARK-33043] [ML] Handle spark.driver.maxResultSize=0 in RowMatrix heuristic computation  \n[SPARK-29358] [SQL] Make unionByName optionally fill missing columns with nulls  \n[SPARK-32996] [WEB-UI] Handle empty ExecutorMetrics in ExecutorMetricsJsonSerializer  \n[SPARK-32585] [SQL] Support scala enumeration in ScalaReflection  \n[SPARK-33019] [CORE] Use spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=1 by default  \n[SPARK-33018] [SQL] Fix estimate statistics issue if child has 0 bytes  \n[SPARK-32901] [CORE] Do not allocate memory while spilling UnsafeExternalSorter"
    },
    {
        "id": 1745,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "[SPARK-33018] [SQL] Fix estimate statistics issue if child has 0 bytes  \n[SPARK-32901] [CORE] Do not allocate memory while spilling UnsafeExternalSorter  \n[SPARK-33015] [SQL] Use millisToDays() in the ComputeCurrentTime rule  \n[SPARK-33015] [SQL] Compute the current date only once  \n[SPARK-32999] [SQL] Use Utils.getSimpleName to avoid hitting Malformed class name in TreeNode  \n[SPARK-32659] [SQL] Broadcast Array instead of Set in InSubqueryExec  \n[SPARK-32718] [SQL] Remove unnecessary keywords for interval units  \n[SPARK-32886] [WEBUI] fix \u2018undefined\u2019 link in event timeline view  \n[SPARK-32898] [CORE] Fix wrong executorRunTime when task killed before real start  \n[SPARK-32635] [SQL] Add a new test case in catalyst module  \n[SPARK-32930] [CORE] Replace deprecated isFile/isDirectory methods  \n[SPARK-32906] [SQL] Struct field names should not change after normalizing floats  \n[SPARK-24994] [SQL] Add UnwrapCastInBinaryComparison optimizer to simplify integral literal  \n[SPARK-32635] [SQL] Fix foldable propagation  \n[SPARK-32738] [CORE] Should reduce the number of active threads if fatal error happens in Inbox.process  \n[SPARK-32900] [CORE] Allow UnsafeExternalSorter to spill when there are nulls  \n[SPARK-32897] [PYTHON] Don\u2019t show a deprecation warning at SparkSession.builder.getOrCreate  \n[SPARK-32715] [CORE] Fix memory leak when failed to store pieces of broadcast  \n[SPARK-32715] [CORE] Fix memory leak when failed to store pieces of broadcast  \n[SPARK-32872] [CORE] Prevent BytesToBytesMap at MAX_CAPACITY from exceeding growth threshold"
    },
    {
        "id": 1746,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "[SPARK-32715] [CORE] Fix memory leak when failed to store pieces of broadcast  \n[SPARK-32872] [CORE] Prevent BytesToBytesMap at MAX_CAPACITY from exceeding growth threshold  \n[SPARK-32876] [SQL] Change default fallback versions to 3.0.1 and 2.4.7 in HiveExternalCatalogVersionsSuite  \n[SPARK-32840] [SQL] Invalid interval value can happen to be just adhesive with the unit  \n[SPARK-32819] [SQL] ignoreNullability parameter should be effective recursively  \n[SPARK-32832] [SS] Use CaseInsensitiveMap for DataStreamReader/Writer options  \n[SPARK-32794] [SS] Fixed rare corner case error in micro-batch engine with some stateful queries + no-data-batches + V1 sources  \n[SPARK-32813] [SQL] Get default config of ParquetSource vectorized reader if no active SparkSession  \n[SPARK-32823] [WEB UI] Fix the master ui resources reporting  \n[SPARK-32824] [CORE] Improve the error message when the user forgets the .amount in a resource config  \n[SPARK-32614] [SQL] Don\u2019t apply comment processing if \u2018comment\u2019 unset for CSV  \n[SPARK-32638] [SQL] Corrects references when adding aliases in WidenSetOperationTypes  \n[SPARK-32810] [SQL] CSV/JSON data sources should avoid globbing paths when inferring schema  \n[SPARK-32815] [ML] Fix LibSVM data source loading error on file paths with glob metacharacters  \n[SPARK-32753] [SQL] Only copy tags to node with no tags  \n[SPARK-32785] [SQL] Interval with dangling parts should not result null  \n[SPARK-32764] [SQL] -0.0 should be equal to 0.0  \n[SPARK-32810] [SQL] CSV/JSON data sources should avoid globbing paths when inferring schema"
    },
    {
        "id": 1747,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "[SPARK-32810] [SQL] CSV/JSON data sources should avoid globbing paths when inferring schema  \n[SPARK-32779] [SQL] Avoid using synchronized API of SessionCatalog in withClient flow, this leads to DeadLock  \n[SPARK-32791] [SQL] Non-partitioned table metric should not have dynamic partition pruning time  \n[SPARK-32767] [SQL] Bucket join should work if spark.sql.shuffle.partitions larger than bucket number  \n[SPARK-32788] [SQL] non-partitioned table scan should not have partition filter  \n[SPARK-32776] [SS] Limit in streaming should not be optimized away by PropagateEmptyRelation  \n[SPARK-32624] [SQL] Fix regression in CodegenContext.addReferenceObj on nested Scala types  \n[SPARK-32659] [SQL] Improve test for pruning DPP on non-atomic type  \n[SPARK-31511] [SQL] Make BytesToBytesMap iterators thread-safe  \n[SPARK-32693] [SQL] Compare two dataframes with same schema except nullable property  \n[SPARK-28612] [SQL] Correct method doc of DataFrameWriterV2.replace()"
    },
    {
        "id": 1748,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "System environment\nOperating System: Ubuntu 18.04.5 LTS  \nJava: Zulu 8.48.0.53-CA-linux64 (build 1.8.0_265-b11)  \nScala: 2.12.10  \nPython: 3.7.5  \nR: R version 3.6.3 (2020-02-29)  \nDelta Lake 0.7.0  \nInstalled Python libraries  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nasn1crypto  \n1.3.0  \nbackcall  \n0.1.0  \nboto3  \n1.12.0  \nbotocore  \n1.15.0  \ncertifi  \n2020.6.20  \ncffi  \n1.14.0  \nchardet  \n3.0.4  \ncryptography  \n2.8  \ncycler  \n0.10.0  \nCython  \n0.29.15  \ndecorator  \n4.4.1  \ndocutils  \n0.15.2  \nentrypoints  \n0.3  \nidna  \n2.8  \nipykernel  \n5.1.4  \nipython  \n7.12.0  \nipython-genutils  \n0.2.0  \njedi  \n0.17.2  \njmespath  \n0.10.0  \njoblib  \n0.14.1  \njupyter-client  \n5.3.4  \njupyter-core  \n4.6.1  \nkiwisolver  \n1.1.0  \nkoalas  \n1.3.0  \nmatplotlib  \n3.1.3  \nnumpy  \n1.18.1  \npandas  \n1.0.1  \nparso  \n0.7.0  \npatsy  \n0.5.1  \npexpect  \n4.8.0  \npickleshare  \n0.7.5  \npip  \n20.0.2  \nprompt-toolkit  \n3.0.3  \npsycopg2  \n2.8.4  \nptyprocess  \n0.6.0  \npyarrow  \n1.0.1  \npycparser  \n2.19  \nPygments  \n2.5.2  \nPyGObject  \n3.26.1  \npyOpenSSL  \n19.1.0  \npyparsing  \n2.4.6  \nPySocks  \n1.7.1  \npython-apt  \n1.6.5+ubuntu0.3  \npython-dateutil  \n2.8.1  \npytz  \n2019.3  \npyzmq  \n18.1.1  \nrequests  \n2.22.0  \ns3transfer  \n0.3.3  \nscikit-learn  \n0.22.1  \nscipy  \n1.4.1  \nseaborn  \n0.10.0"
    },
    {
        "id": 1749,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "2019.3  \npyzmq  \n18.1.1  \nrequests  \n2.22.0  \ns3transfer  \n0.3.3  \nscikit-learn  \n0.22.1  \nscipy  \n1.4.1  \nseaborn  \n0.10.0  \nsetuptools  \n45.2.0  \nsix  \n1.14.0  \nssh-import-id  \n5.7  \nstatsmodels  \n0.11.0  \ntornado  \n6.0.3  \ntraitlets  \n4.3.3  \nunattended-upgrades  \n0.1  \nurllib3  \n1.25.8  \nvirtualenv  \n16.7.10  \nwcwidth  \n0.1.8  \nwheel  \n0.34.2  \nInstalled R libraries  \nR libraries are installed from Microsoft CRAN snapshot on XXXX-XX-XX.  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \naskpass  \n1.1  \nassertthat  \n0.2.1  \nbackports  \n1.1.8  \nbase  \n3.6.3  \nbase64enc  \n0.1-3  \nBH  \n1.72.0-3  \nbit  \n1.1-15.2  \nbit64  \n0.9-7  \nblob  \n1.2.1  \nboot  \n1.3-25  \nbrew  \n1.0-6  \nbroom  \n0.7.0  \ncallr  \n3.4.3  \ncaret  \n6.0-86  \ncellranger  \n1.1.0  \nchron  \n2.3-55  \nclass  \n7.3-17  \ncli  \n2.0.2  \nclipr  \n0.7.0  \ncluster  \n2.1.0  \ncodetools  \n0.2-16  \ncolorspace  \n1.4-1  \ncommonmark  \n1.7  \ncompiler  \n3.6.3  \nconfig  \n0.3  \ncovr  \n3.5.0  \ncrayon  \n1.3.4  \ncrosstalk  \n1.1.0.1  \ncurl  \n4.3  \ndata.table  \n1.12.8  \ndatasets  \n3.6.3  \nDBI  \n1.1.0  \ndbplyr  \n1.4.4  \ndesc  \n1.2.0  \ndevtools  \n2.3.0  \ndigest  \n0.6.25  \ndplyr  \n0.8.5  \nDT  \n0.14  \nellipsis  \n0.3.1  \nevaluate  \n0.14  \nfansi  \n0.4.1  \nfarver  \n2.0.3  \nfastmap  \n1.0.1  \nforcats  \n0.5.0  \nforeach  \n1.5.0  \nforeign  \n0.8-76  \nforge  \n0.2.0  \nfs  \n1.4.2  \ngenerics  \n0.0.2  \nggplot2"
    },
    {
        "id": 1750,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "fastmap  \n1.0.1  \nforcats  \n0.5.0  \nforeach  \n1.5.0  \nforeign  \n0.8-76  \nforge  \n0.2.0  \nfs  \n1.4.2  \ngenerics  \n0.0.2  \nggplot2  \n3.3.2  \ngh  \n1.1.0  \ngit2r  \n0.27.1  \nglmnet  \n3.0-2  \nglobals  \n0.12.5  \nglue  \n1.4.1  \ngower  \n0.2.2  \ngraphics  \n3.6.3  \ngrDevices  \n3.6.3  \ngrid  \n3.6.3  \ngridExtra  \n2.3  \ngsubfn  \n0.7  \ngtable  \n0.3.0  \nhaven  \n2.3.1  \nhighr  \n0.8  \nhms  \n0.5.3  \nhtmltools  \n0.5.0  \nhtmlwidgets  \n1.5.1  \nhttpuv  \n1.5.4  \nhttr  \n1.4.1  \nhwriter  \n1.3.2  \nhwriterPlus  \n1.0-3  \nini  \n0.3.1  \nipred  \n0.9-9  \nisoband  \n0.2.2  \niterators  \n1.0.12  \njsonlite  \n1.7.0  \nKernSmooth  \n2.23-17  \nknitr  \n1.29  \nlabeling  \n0.3  \nlater  \n1.1.0.1  \nlattice  \n0.20-41  \nlava  \n1.6.7  \nlazyeval  \n0.2.2  \nlifecycle  \n0.2.0  \nlubridate  \n1.7.9  \nmagrittr  \n1.5  \nmarkdown  \n1.1  \nMASS  \n7.3-53  \nMatrix  \n1.2-18  \nmemoise  \n1.1.0  \nmethods  \n3.6.3  \nmgcv  \n1.8-33  \nmime  \n0.9  \nModelMetrics  \n1.2.2.2  \nmodelr  \n0.1.8  \nmunsell  \n0.5.0  \nnlme  \n3.1-149  \nnnet  \n7.3-14  \nnumDeriv  \n2016.8-1.1  \nopenssl  \n1.4.2  \nparallel  \n3.6.3  \npillar  \n1.4.6  \npkgbuild  \n1.1.0  \npkgconfig  \n2.0.3  \npkgload  \n1.1.0  \nplogr  \n0.2.0  \nplyr  \n1.8.6  \npraise  \n1.0.0  \nprettyunits  \n1.1.1  \npROC  \n1.16.2  \nprocessx"
    },
    {
        "id": 1751,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "pkgload  \n1.1.0  \nplogr  \n0.2.0  \nplyr  \n1.8.6  \npraise  \n1.0.0  \nprettyunits  \n1.1.1  \npROC  \n1.16.2  \nprocessx  \n3.4.3  \nprodlim  \n2019.11.13  \nprogress  \n1.2.2  \npromises  \n1.1.1  \nproto  \n1.0.0  \nps  \n1.3.3  \npurrr  \n0.3.4  \nr2d3  \n0.2.3  \nR6  \n2.4.1  \nrandomForest  \n4.6-14  \nrappdirs  \n0.3.1  \nrcmdcheck  \n1.3.3  \nRColorBrewer  \n1.1-2  \nRcpp  \n1.0.5  \nreadr  \n1.3.1  \nreadxl  \n1.3.1  \nrecipes  \n0.1.13  \nrematch  \n1.0.1  \nrematch2  \n2.1.2  \nremotes  \n2.1.1  \nreprex  \n0.3.0  \nreshape2  \n1.4.4  \nrex  \n1.2.0  \nrjson  \n0.2.20  \nrlang  \n0.4.7  \nrmarkdown  \n2.3  \nRODBC  \n1.3-16  \nroxygen2  \n7.1.1  \nrpart  \n4.1-15  \nrprojroot  \n1.3-2  \nRserve  \n1.8-7  \nRSQLite  \n2.2.0  \nrstudioapi  \n0.11  \nrversions  \n2.0.2  \nrvest  \n0.3.5  \nscales  \n1.1.1  \nselectr  \n0.4-2  \nsessioninfo  \n1.1.1  \nshape  \n1.4.4  \nshiny  \n1.5.0  \nsourcetools  \n0.1.7  \nsparklyr  \n1.3.1  \nSparkR  \n3.0.0  \nspatial  \n7.3-11  \nsplines  \n3.6.3  \nsqldf  \n0.4-11  \nSQUAREM  \n2020.3  \nstats  \n3.6.3  \nstats4  \n3.6.3  \nstringi  \n1.4.6  \nstringr  \n1.4.0  \nsurvival  \n3.2-7  \nsys  \n3.3  \ntcltk  \n3.6.3  \nTeachingDemos  \n2.10  \ntestthat  \n2.3.2  \ntibble  \n3.0.3  \ntidyr  \n1.1.0  \ntidyselect  \n1.1.0  \ntidyverse  \n1.3.0  \ntimeDate  \n3043.102  \ntinytex  \n0.24"
    },
    {
        "id": 1752,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "2.3.2  \ntibble  \n3.0.3  \ntidyr  \n1.1.0  \ntidyselect  \n1.1.0  \ntidyverse  \n1.3.0  \ntimeDate  \n3043.102  \ntinytex  \n0.24  \ntools  \n3.6.3  \nusethis  \n1.6.1  \nutf8  \n1.1.4  \nutils  \n3.6.3  \nuuid  \n0.1-4  \nvctrs  \n0.3.1  \nviridisLite  \n0.3.0  \nwhisker  \n0.4  \nwithr  \n2.2.0  \nxfun  \n0.15  \nxml2  \n1.3.2  \nxopen  \n1.0.0  \nxtable  \n1.8-4  \nyaml  \n2.2.1  \nInstalled Java and Scala libraries (Scala 2.12 cluster version)  \nGroup ID  \nArtifact ID  \nVersion  \nantlr  \nantlr  \n2.7.7  \ncom.amazonaws  \namazon-kinesis-client  \n1.12.0  \ncom.amazonaws  \naws-java-sdk-autoscaling  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudformation  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudfront  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudhsm  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudsearch  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudtrail  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudwatch  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cloudwatchmetrics  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-codedeploy  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cognitoidentity  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-cognitosync  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-config  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-core"
    },
    {
        "id": 1753,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "1.11.655  \ncom.amazonaws  \naws-java-sdk-config  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-core  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-datapipeline  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-directconnect  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-directory  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-dynamodb  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-ec2  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-ecs  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-efs  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-elasticache  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-elasticbeanstalk  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-elasticloadbalancing  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-elastictranscoder  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-emr  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-glacier  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-iam  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-importexport  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-kinesis  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-kms  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-lambda  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-logs  \n1.11.655"
    },
    {
        "id": 1754,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "com.amazonaws  \naws-java-sdk-lambda  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-logs  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-machinelearning  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-opsworks  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-rds  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-redshift  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-route53  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-s3  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-ses  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-simpledb  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-simpleworkflow  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-sns  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-sqs  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-ssm  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-storagegateway  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-sts  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-support  \n1.11.655  \ncom.amazonaws  \naws-java-sdk-swf-libraries  \n1.11.22  \ncom.amazonaws  \naws-java-sdk-workspaces  \n1.11.655  \ncom.amazonaws  \njmespath-java  \n1.11.655  \ncom.chuusai  \nshapeless_2.12  \n2.3.3  \ncom.clearspring.analytics  \nstream  \n2.9.6  \ncom.databricks  \nRserve"
    },
    {
        "id": 1755,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "jmespath-java  \n1.11.655  \ncom.chuusai  \nshapeless_2.12  \n2.3.3  \ncom.clearspring.analytics  \nstream  \n2.9.6  \ncom.databricks  \nRserve  \n1.8-3  \ncom.databricks  \njets3t  \n0.7.1-0  \ncom.databricks.scalapb  \ncompilerplugin_2.12  \n0.4.15-10  \ncom.databricks.scalapb  \nscalapb-runtime_2.12  \n0.4.15-10  \ncom.esotericsoftware  \nkryo-shaded  \n4.0.2  \ncom.esotericsoftware  \nminlog  \n1.3.0  \ncom.fasterxml  \nclassmate  \n1.3.4  \ncom.fasterxml.jackson.core  \njackson-annotations  \n2.10.0  \ncom.fasterxml.jackson.core  \njackson-core  \n2.10.0  \ncom.fasterxml.jackson.core  \njackson-databind  \n2.10.0  \ncom.fasterxml.jackson.dataformat  \njackson-dataformat-cbor  \n2.10.0  \ncom.fasterxml.jackson.datatype  \njackson-datatype-joda  \n2.10.0  \ncom.fasterxml.jackson.module  \njackson-module-paranamer  \n2.10.0  \ncom.fasterxml.jackson.module  \njackson-module-scala_2.12  \n2.10.0  \ncom.github.ben-manes.caffeine  \ncaffeine  \n2.3.4  \ncom.github.fommil  \njniloader  \n1.1  \ncom.github.fommil.netlib  \ncore  \n1.1.2  \ncom.github.fommil.netlib  \nnative_ref-java  \n1.1  \ncom.github.fommil.netlib  \nnative_ref-java-natives  \n1.1  \ncom.github.fommil.netlib  \nnative_system-java  \n1.1  \ncom.github.fommil.netlib  \nnative_system-java-natives  \n1.1  \ncom.github.fommil.netlib  \nnetlib-native_ref-linux-x86_64-natives  \n1.1  \ncom.github.fommil.netlib"
    },
    {
        "id": 1756,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "1.1  \ncom.github.fommil.netlib  \nnetlib-native_ref-linux-x86_64-natives  \n1.1  \ncom.github.fommil.netlib  \nnetlib-native_system-linux-x86_64-natives  \n1.1  \ncom.github.joshelser  \ndropwizard-metrics-hadoop-metrics2-reporter  \n0.1.2  \ncom.github.luben  \nzstd-jni  \n1.4.4-3  \ncom.github.wendykierp  \nJTransforms  \n3.1  \ncom.google.code.findbugs  \njsr305  \n3.0.0  \ncom.google.code.gson  \ngson  \n2.2.4  \ncom.google.flatbuffers  \nflatbuffers-java  \n1.9.0  \ncom.google.guava  \nguava  \n15.0  \ncom.google.protobuf  \nprotobuf-java  \n2.6.1  \ncom.h2database  \nh2  \n1.4.195  \ncom.helger  \nprofiler  \n1.1.1  \ncom.jcraft  \njsch  \n0.1.50  \ncom.jolbox  \nbonecp  \n0.8.0.RELEASE  \ncom.lihaoyi  \nsourcecode_2.12  \n0.1.9  \ncom.microsoft.azure  \nazure-data-lake-store-sdk  \n2.2.8  \ncom.microsoft.sqlserver  \nmssql-jdbc  \n8.2.1.jre8  \ncom.ning  \ncompress-lzf  \n1.0.3  \ncom.sun.mail  \njavax.mail  \n1.5.2  \ncom.tdunning  \njson  \n1.8  \ncom.thoughtworks.paranamer  \nparanamer  \n2.8  \ncom.trueaccord.lenses  \nlenses_2.12  \n0.4.12  \ncom.twitter  \nchill-java  \n0.9.5  \ncom.twitter  \nchill_2.12  \n0.9.5  \ncom.twitter  \nutil-app_2.12  \n7.1.0  \ncom.twitter  \nutil-core_2.12  \n7.1.0  \ncom.twitter  \nutil-function_2.12  \n7.1.0  \ncom.twitter  \nutil-jvm_2.12  \n7.1.0  \ncom.twitter"
    },
    {
        "id": 1757,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "util-core_2.12  \n7.1.0  \ncom.twitter  \nutil-function_2.12  \n7.1.0  \ncom.twitter  \nutil-jvm_2.12  \n7.1.0  \ncom.twitter  \nutil-lint_2.12  \n7.1.0  \ncom.twitter  \nutil-registry_2.12  \n7.1.0  \ncom.twitter  \nutil-stats_2.12  \n7.1.0  \ncom.typesafe  \nconfig  \n1.2.1  \ncom.typesafe.scala-logging  \nscala-logging_2.12  \n3.7.2  \ncom.univocity  \nunivocity-parsers  \n2.9.0  \ncom.zaxxer  \nHikariCP  \n3.1.0  \ncommons-beanutils  \ncommons-beanutils  \n1.9.4  \ncommons-cli  \ncommons-cli  \n1.2  \ncommons-codec  \ncommons-codec  \n1.10  \ncommons-collections  \ncommons-collections  \n3.2.2  \ncommons-configuration  \ncommons-configuration  \n1.6  \ncommons-dbcp  \ncommons-dbcp  \n1.4  \ncommons-digester  \ncommons-digester  \n1.8  \ncommons-fileupload  \ncommons-fileupload  \n1.3.3  \ncommons-httpclient  \ncommons-httpclient  \n3.1  \ncommons-io  \ncommons-io  \n2.4  \ncommons-lang  \ncommons-lang  \n2.6  \ncommons-logging  \ncommons-logging  \n1.1.3  \ncommons-net  \ncommons-net  \n3.1  \ncommons-pool  \ncommons-pool  \n1.5.4  \ninfo.ganglia.gmetric4j  \ngmetric4j  \n1.0.10  \nio.airlift  \naircompressor  \n0.10  \nio.dropwizard.metrics  \nmetrics-core  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-graphite  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-healthchecks  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jetty9  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jmx  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-json  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jvm  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-servlets"
    },
    {
        "id": 1758,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "metrics-json  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-jvm  \n4.1.1  \nio.dropwizard.metrics  \nmetrics-servlets  \n4.1.1  \nio.netty  \nnetty-all  \n4.1.47.Final  \njakarta.annotation  \njakarta.annotation-api  \n1.3.5  \njakarta.validation  \njakarta.validation-api  \n2.0.2  \njakarta.ws.rs  \njakarta.ws.rs-api  \n2.1.6  \njavax.activation  \nactivation  \n1.1.1  \njavax.el  \njavax.el-api  \n2.2.4  \njavax.jdo  \njdo-api  \n3.0.1  \njavax.servlet  \njavax.servlet-api  \n3.1.0  \njavax.servlet.jsp  \njsp-api  \n2.1  \njavax.transaction  \njta  \n1.1  \njavax.transaction  \ntransaction-api  \n1.1  \njavax.xml.bind  \njaxb-api  \n2.2.2  \njavax.xml.stream  \nstax-api  \n1.0-2  \njavolution  \njavolution  \n5.5.1  \njline  \njline  \n2.14.6  \njoda-time  \njoda-time  \n2.10.5  \nlog4j  \napache-log4j-extras  \n1.2.17  \nlog4j  \nlog4j  \n1.2.17  \nnet.razorvine  \npyrolite  \n4.30  \nnet.sf.jpam  \njpam  \n1.1  \nnet.sf.opencsv  \nopencsv  \n2.3  \nnet.sf.supercsv  \nsuper-csv  \n2.2.0  \nnet.snowflake  \nsnowflake-ingest-sdk  \n0.9.6  \nnet.snowflake  \nsnowflake-jdbc  \n3.12.8  \nnet.snowflake  \nspark-snowflake_2.12  \n2.8.1-spark_3.0  \nnet.sourceforge.f2j  \narpack_combined_all  \n0.1  \norg.acplt.remotetea  \nremotetea-oncrpc  \n1.1.2  \norg.antlr  \nST4  \n4.0.4  \norg.antlr  \nantlr-runtime  \n3.5.2"
    },
    {
        "id": 1759,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "org.acplt.remotetea  \nremotetea-oncrpc  \n1.1.2  \norg.antlr  \nST4  \n4.0.4  \norg.antlr  \nantlr-runtime  \n3.5.2  \norg.antlr  \nantlr4-runtime  \n4.7.1  \norg.antlr  \nstringtemplate  \n3.2.1  \norg.apache.ant  \nant  \n1.9.2  \norg.apache.ant  \nant-jsch  \n1.9.2  \norg.apache.ant  \nant-launcher  \n1.9.2  \norg.apache.arrow  \narrow-format  \n0.15.1  \norg.apache.arrow  \narrow-memory  \n0.15.1  \norg.apache.arrow  \narrow-vector  \n0.15.1  \norg.apache.avro  \navro  \n1.8.2  \norg.apache.avro  \navro-ipc  \n1.8.2  \norg.apache.avro  \navro-mapred-hadoop2  \n1.8.2  \norg.apache.commons  \ncommons-compress  \n1.8.1  \norg.apache.commons  \ncommons-crypto  \n1.0.0  \norg.apache.commons  \ncommons-lang3  \n3.9  \norg.apache.commons  \ncommons-math3  \n3.4.1  \norg.apache.commons  \ncommons-text  \n1.6  \norg.apache.curator  \ncurator-client  \n2.7.1  \norg.apache.curator  \ncurator-framework  \n2.7.1  \norg.apache.curator  \ncurator-recipes  \n2.7.1  \norg.apache.derby  \nderby  \n10.12.1.1  \norg.apache.directory.api  \napi-asn1-api  \n1.0.0-M20  \norg.apache.directory.api  \napi-util  \n1.0.0-M20  \norg.apache.directory.server  \napacheds-i18n  \n2.0.0-M15  \norg.apache.directory.server  \napacheds-kerberos-codec  \n2.0.0-M15  \norg.apache.hadoop  \nhadoop-annotations  \n2.7.4  \norg.apache.hadoop  \nhadoop-auth  \n2.7.4  \norg.apache.hadoop  \nhadoop-client  \n2.7.4  \norg.apache.hadoop  \nhadoop-common  \n2.7.4  \norg.apache.hadoop"
    },
    {
        "id": 1760,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "2.7.4  \norg.apache.hadoop  \nhadoop-client  \n2.7.4  \norg.apache.hadoop  \nhadoop-common  \n2.7.4  \norg.apache.hadoop  \nhadoop-hdfs  \n2.7.4  \norg.apache.hadoop  \nhadoop-mapreduce-client-app  \n2.7.4  \norg.apache.hadoop  \nhadoop-mapreduce-client-common  \n2.7.4  \norg.apache.hadoop  \nhadoop-mapreduce-client-core  \n2.7.4  \norg.apache.hadoop  \nhadoop-mapreduce-client-jobclient  \n2.7.4  \norg.apache.hadoop  \nhadoop-mapreduce-client-shuffle  \n2.7.4  \norg.apache.hadoop  \nhadoop-yarn-api  \n2.7.4  \norg.apache.hadoop  \nhadoop-yarn-client  \n2.7.4  \norg.apache.hadoop  \nhadoop-yarn-common  \n2.7.4  \norg.apache.hadoop  \nhadoop-yarn-server-common  \n2.7.4  \norg.apache.hive  \nhive-beeline  \n2.3.7  \norg.apache.hive  \nhive-cli  \n2.3.7  \norg.apache.hive  \nhive-common  \n2.3.7  \norg.apache.hive  \nhive-exec-core  \n2.3.7  \norg.apache.hive  \nhive-jdbc  \n2.3.7  \norg.apache.hive  \nhive-llap-client  \n2.3.7  \norg.apache.hive  \nhive-llap-common  \n2.3.7  \norg.apache.hive  \nhive-metastore  \n2.3.7  \norg.apache.hive  \nhive-serde  \n2.3.7  \norg.apache.hive  \nhive-shims  \n2.3.7  \norg.apache.hive  \nhive-storage-api  \n2.7.1  \norg.apache.hive  \nhive-vector-code-gen  \n2.3.7  \norg.apache.hive.shims  \nhive-shims-0.23  \n2.3.7  \norg.apache.hive.shims  \nhive-shims-common  \n2.3.7  \norg.apache.hive.shims  \nhive-shims-scheduler  \n2.3.7"
    },
    {
        "id": 1761,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "2.3.7  \norg.apache.hive.shims  \nhive-shims-common  \n2.3.7  \norg.apache.hive.shims  \nhive-shims-scheduler  \n2.3.7  \norg.apache.htrace  \nhtrace-core  \n3.1.0-incubating  \norg.apache.httpcomponents  \nhttpclient  \n4.5.6  \norg.apache.httpcomponents  \nhttpcore  \n4.4.12  \norg.apache.ivy  \nivy  \n2.4.0  \norg.apache.orc  \norc-core  \n1.5.10  \norg.apache.orc  \norc-mapreduce  \n1.5.10  \norg.apache.orc  \norc-shims  \n1.5.10  \norg.apache.parquet  \nparquet-column  \n1.10.1-databricks6  \norg.apache.parquet  \nparquet-common  \n1.10.1-databricks6  \norg.apache.parquet  \nparquet-encoding  \n1.10.1-databricks6  \norg.apache.parquet  \nparquet-format  \n2.4.0  \norg.apache.parquet  \nparquet-hadoop  \n1.10.1-databricks6  \norg.apache.parquet  \nparquet-jackson  \n1.10.1-databricks6  \norg.apache.thrift  \nlibfb303  \n0.9.3  \norg.apache.thrift  \nlibthrift  \n0.12.0  \norg.apache.velocity  \nvelocity  \n1.5  \norg.apache.xbean  \nxbean-asm7-shaded  \n4.15  \norg.apache.yetus  \naudience-annotations  \n0.5.0  \norg.apache.zookeeper  \nzookeeper  \n3.4.14  \norg.codehaus.jackson  \njackson-core-asl  \n1.9.13  \norg.codehaus.jackson  \njackson-jaxrs  \n1.9.13  \norg.codehaus.jackson  \njackson-mapper-asl  \n1.9.13  \norg.codehaus.jackson  \njackson-xc  \n1.9.13  \norg.codehaus.janino  \ncommons-compiler  \n3.0.16  \norg.codehaus.janino  \njanino  \n3.0.16  \norg.datanucleus  \ndatanucleus-api-jdo  \n4.2.4  \norg.datanucleus  \ndatanucleus-core  \n4.1.17  \norg.datanucleus"
    },
    {
        "id": 1762,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "3.0.16  \norg.datanucleus  \ndatanucleus-api-jdo  \n4.2.4  \norg.datanucleus  \ndatanucleus-core  \n4.1.17  \norg.datanucleus  \ndatanucleus-rdbms  \n4.1.19  \norg.datanucleus  \njavax.jdo  \n3.2.0-m3  \norg.eclipse.jetty  \njetty-client  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-continuation  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-http  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-io  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-jndi  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-plus  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-proxy  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-security  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-server  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-servlet  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-servlets  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-util  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-webapp  \n9.4.18.v20190429  \norg.eclipse.jetty  \njetty-xml  \n9.4.18.v20190429  \norg.fusesource.leveldbjni  \nleveldbjni-all  \n1.8  \norg.glassfish.hk2  \nhk2-api  \n2.6.1  \norg.glassfish.hk2  \nhk2-locator  \n2.6.1  \norg.glassfish.hk2  \nhk2-utils  \n2.6.1  \norg.glassfish.hk2  \nosgi-resource-locator  \n1.0.3  \norg.glassfish.hk2.external  \naopalliance-repackaged  \n2.6.1  \norg.glassfish.hk2.external  \njakarta.inject  \n2.6.1"
    },
    {
        "id": 1763,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "1.0.3  \norg.glassfish.hk2.external  \naopalliance-repackaged  \n2.6.1  \norg.glassfish.hk2.external  \njakarta.inject  \n2.6.1  \norg.glassfish.jersey.containers  \njersey-container-servlet  \n2.30  \norg.glassfish.jersey.containers  \njersey-container-servlet-core  \n2.30  \norg.glassfish.jersey.core  \njersey-client  \n2.30  \norg.glassfish.jersey.core  \njersey-common  \n2.30  \norg.glassfish.jersey.core  \njersey-server  \n2.30  \norg.glassfish.jersey.inject  \njersey-hk2  \n2.30  \norg.glassfish.jersey.media  \njersey-media-jaxb  \n2.30  \norg.hibernate.validator  \nhibernate-validator  \n6.1.0.Final  \norg.javassist  \njavassist  \n3.25.0-GA  \norg.jboss.logging  \njboss-logging  \n3.3.2.Final  \norg.jdbi  \njdbi  \n2.63.1  \norg.joda  \njoda-convert  \n1.7  \norg.jodd  \njodd-core  \n3.5.2  \norg.json4s  \njson4s-ast_2.12  \n3.6.6  \norg.json4s  \njson4s-core_2.12  \n3.6.6  \norg.json4s  \njson4s-jackson_2.12  \n3.6.6  \norg.json4s  \njson4s-scalap_2.12  \n3.6.6  \norg.lz4  \nlz4-java  \n1.7.1  \norg.mariadb.jdbc  \nmariadb-java-client  \n2.1.2  \norg.objenesis  \nobjenesis  \n2.5.1  \norg.postgresql  \npostgresql  \n42.1.4  \norg.roaringbitmap  \nRoaringBitmap  \n0.7.45  \norg.roaringbitmap  \nshims  \n0.7.45  \norg.rocksdb  \nrocksdbjni  \n6.2.2  \norg.rosuda.REngine  \nREngine  \n2.1.0  \norg.scala-lang  \nscala-compiler_2.12  \n2.12.10  \norg.scala-lang  \nscala-library_2.12  \n2.12.10"
    },
    {
        "id": 1764,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "REngine  \n2.1.0  \norg.scala-lang  \nscala-compiler_2.12  \n2.12.10  \norg.scala-lang  \nscala-library_2.12  \n2.12.10  \norg.scala-lang  \nscala-reflect_2.12  \n2.12.10  \norg.scala-lang.modules  \nscala-collection-compat_2.12  \n2.1.1  \norg.scala-lang.modules  \nscala-parser-combinators_2.12  \n1.1.2  \norg.scala-lang.modules  \nscala-xml_2.12  \n1.2.0  \norg.scala-sbt  \ntest-interface  \n1.0  \norg.scalacheck  \nscalacheck_2.12  \n1.14.2  \norg.scalactic  \nscalactic_2.12  \n3.0.8  \norg.scalanlp  \nbreeze-macros_2.12  \n1.0  \norg.scalanlp  \nbreeze_2.12  \n1.0  \norg.scalatest  \nscalatest_2.12  \n3.0.8  \norg.slf4j  \njcl-over-slf4j  \n1.7.30  \norg.slf4j  \njul-to-slf4j  \n1.7.30  \norg.slf4j  \nslf4j-api  \n1.7.30  \norg.slf4j  \nslf4j-log4j12  \n1.7.30  \norg.spark-project.spark  \nunused  \n1.0.0  \norg.springframework  \nspring-core  \n4.1.4.RELEASE  \norg.springframework  \nspring-test  \n4.1.4.RELEASE  \norg.threeten  \nthreeten-extra  \n1.5.0  \norg.tukaani  \nxz  \n1.5  \norg.typelevel  \nalgebra_2.12  \n2.0.0-M2  \norg.typelevel  \ncats-kernel_2.12  \n2.0.0-M4  \norg.typelevel  \nmachinist_2.12  \n0.6.8  \norg.typelevel  \nmacro-compat_2.12  \n1.1.1  \norg.typelevel  \nspire-macros_2.12  \n0.17.0-M1  \norg.typelevel  \nspire-platform_2.12  \n0.17.0-M1"
    },
    {
        "id": 1765,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.4.html",
        "content": "1.1.1  \norg.typelevel  \nspire-macros_2.12  \n0.17.0-M1  \norg.typelevel  \nspire-platform_2.12  \n0.17.0-M1  \norg.typelevel  \nspire-util_2.12  \n0.17.0-M1  \norg.typelevel  \nspire_2.12  \n0.17.0-M1  \norg.xerial  \nsqlite-jdbc  \n3.8.11.2  \norg.xerial.snappy  \nsnappy-java  \n1.1.7.5  \norg.yaml  \nsnakeyaml  \n1.24  \noro  \noro  \n2.0.8  \npl.edu.icm  \nJLargeArrays  \n1.5  \nsoftware.amazon.ion  \nion-java  \n1.0.2  \nstax  \nstax-api  \n1.0.1  \nxmlenc  \nxmlenc  \n0.52"
    },
    {
        "id": 1766,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.1ml.html",
        "content": "Databricks Runtime 10.1 for ML (EoS)  \nNote  \nSupport for this Databricks Runtime version has ended. For the end-of-support date, see End-of-support history. For all supported Databricks Runtime versions, see Databricks Runtime release notes versions and compatibility.  \nDatabricks Runtime 10.1 for Machine Learning provides a ready-to-go environment for machine learning and data science based on Databricks Runtime 10.1 (EoS). Databricks Runtime ML contains many popular machine learning libraries, including TensorFlow, PyTorch, and XGBoost. It also supports distributed deep learning training using Horovod.  \nFor more information, including instructions for creating a Databricks Runtime ML cluster, see AI and Machine Learning on Databricks.  \nNew features and improvements"
    },
    {
        "id": 1767,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.1ml.html",
        "content": "New features and improvements\nDatabricks Runtime 10.1 ML is built on top of Databricks Runtime 10.1. For information on what\u2019s new in Databricks Runtime 10.1, including Apache Spark MLlib and SparkR, see the Databricks Runtime 10.1 (EoS) release notes.  \nEnhancements to Databricks AutoML  \nIn Databricks Runtime 10.1, Databricks AutoML includes improved semantic type detection, new alerts for potential data issues during training, new capabilities to prevent overfitting models, and the ability to split the input dataset into train, validation, and test sets chronologically.  \nAdditional semantic type detections  \nAutoML now supports additional semantic type detection:  \nNumeric columns that contain categorical labels are treated as a categorical type.  \nString columns that contain English text are treated as a text feature.  \nYou can now also add annotations to specify a column data type. For details, see Semantic type detection.  \nAlerts during training for potential data issues  \nAutoML now detects and generates alerts for potential issues with the dataset. Example alerts include unsupported column types and high cardinality columns. These alerts appear on the experiment page under the new Alerts tab. Additional information on alerts is included in the data exploration notebook. For more information, see Run the experiment and monitor the results.  \nReduced model overfitting  \nTwo new capabilities reduce the chances of overfitting a model when using AutoML:  \nAutoML now reports test metrics in additional to validation and training metrics.  \nAutoML now uses early stopping. It stops training and tuning models if the validation metric is no longer improving.  \nSplit dataset into train/validation/test sets chronologically  \nFor classification and regression problems, you can split the dataset into train, validation, and test sets chronologically. See Split data into train, validation, and test sets for details.  \nEnhancements to Databricks Feature Store  \nDatabricks Feature Store now supports additional data types for feature tables: BinaryType, DecimalType, and MapType.  \nMlflow  \nThe following improvements are available starting in Mlflow version 1.21.0, which is included in Databricks Runtime 10.1 ML."
    },
    {
        "id": 1768,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.1ml.html",
        "content": "Mlflow  \nThe following improvements are available starting in Mlflow version 1.21.0, which is included in Databricks Runtime 10.1 ML.  \n[Models] Upgrade the fastai model flavor to support fastai v2 (2.4.1 and above).  \n[Models] Introduce an mlflow.prophet model flavor for Prophet time series models.  \n[Scoring] Fix a schema enforcement error that incorrectly cast date-like strings to datetime objects.  \nHyperopt  \nSparkTrials now supports the early_stopping_fn parameter for fmin. You can use the early stopping function to specify conditions when Hyperopt should stop hyperparameter tuning before the maximum number of evaluations is reached. For example, you can use this parameter to end tuning if the objective function is no longer decreasing. For details, see fmin().  \nMajor changes to Databricks Runtime ML Python environment  \nPython packages upgraded  \nautoml 1.3.1 => 1.4.1  \nfeature_store 0.3.4 => 0.3.5  \nholidays 0.11.2 => 0.11.3.1  \nhorovod 0.22.1 => 0.23.0  \nhyperopt 0.2.5.db2 => 0.2.5.db4  \nimbalanced-learn 0.8.0 => 0.8.1  \nlightgbm 3.1.1 => 3.3.0  \nmlflow 1.20.2 => 1.21.0  \npetastorm 0.11.2 => 0.11.3  \nplotly 5.1.0 => 5.3.0  \npytorch 1.9.0 => 1.9.1  \nspacy 3.1.2 => 3.1.3  \nsparkdl 2.2.0_db3 => 2.2.0_db4  \ntorchvision 0.10.0 => 0.10.1  \ntransformers 4.9.2 => 4.11.3  \nPython packages added  \nfasttext => 0.9.2  \ntensorboard-plugin-profile => 2.5.0"
    },
    {
        "id": 1769,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.1ml.html",
        "content": "Deprecations\nDeprecations\nMLlib automated MLflow tracking is deprecated on clusters that run Databricks Runtime 10.1 ML and above. Instead, use MLflow PySpark ML autologging by calling mlflow.pyspark.ml.autolog(). Autologging is enabled by default with Databricks Autologging.\n\nSystem environment\nSystem environment\nThe system environment in Databricks Runtime 10.1 ML differs from Databricks Runtime 10.1 as follows:  \nDBUtils: Databricks Runtime ML does not include Library utility (dbutils.library) (legacy). Use %pip commands instead. See Notebook-scoped Python libraries.  \nFor GPU clusters, Databricks Runtime ML includes the following NVIDIA GPU libraries:  \nCUDA 11.0  \ncuDNN 8.0.5.39  \nNCCL 2.10.3  \nTensorRT 7.2.2\n\nLibraries"
    },
    {
        "id": 1770,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.1ml.html",
        "content": "Libraries\nThe following sections list the libraries included in Databricks Runtime 10.1 ML that differ from those included in Databricks Runtime 10.1.  \nIn this section:  \nTop-tier libraries  \nPython libraries  \nR libraries  \nJava and Scala libraries (Scala 2.12 cluster)  \nTop-tier libraries  \nDatabricks Runtime 10.1 ML includes the following top-tier libraries:  \nGraphFrames  \nHorovod and HorovodRunner  \nMLflow  \nPyTorch  \nspark-tensorflow-connector  \nTensorFlow  \nTensorBoard  \nPython libraries  \nDatabricks Runtime 10.1 ML uses Virtualenv for Python package management and includes many popular ML packages.  \nIn addition to the packages specified in the in the following sections, Databricks Runtime 10.1 ML also includes the following packages:  \nhyperopt 0.2.5.db4  \nsparkdl 2.2.0-db4  \nfeature_store 0.3.5  \nautoml 1.4.0  \nNote  \nDatabricks Runtime 10.1 ML includes scikit-learn version 0.24 instead of version 1.0 due to incompatibility issues. The scikit-learn package interacts with many other packages in Databricks Runtime 10.1 ML.  \nYou can upgrade to scikit-learn version 1.0; however, Databricks does not support this version.  \nTo upgrade, use notebook-scoped libraries. From a notebook, run %pip install --upgrade \"scikit-learn>=1.0,<1.1\".  \nAn alternative is to use this cluster init script:  \n#!/bin/bash set -e pip install --upgrade \"scikit-learn>=1.0,<1.1\"  \nPython libraries on CPU clusters  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nabsl-py  \n0.11.0  \nAntergos Linux  \n2015.10 (ISO-Rolling)  \nappdirs  \n1.4.4  \nargon2-cffi  \n20.1.0  \nastor  \n0.8.1  \nastunparse  \n1.6.3  \nasync-generator  \n1.10  \nattrs  \n20.3.0  \nbackcall  \n0.2.0  \nbcrypt  \n3.2.0  \nbleach  \n3.3.0  \nblis  \n0.7.4  \nboto3  \n1.16.7  \nbotocore  \n1.19.7  \ncachetools  \n4.2.4  \ncatalogue"
    },
    {
        "id": 1771,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.1ml.html",
        "content": "bcrypt  \n3.2.0  \nbleach  \n3.3.0  \nblis  \n0.7.4  \nboto3  \n1.16.7  \nbotocore  \n1.19.7  \ncachetools  \n4.2.4  \ncatalogue  \n2.0.6  \ncertifi  \n2020.12.5  \ncffi  \n1.14.5  \nchardet  \n4.0.0  \nclang  \n5.0  \nclick  \n7.1.2  \ncloudpickle  \n1.6.0  \ncmdstanpy  \n0.9.68  \nconfigparser  \n5.0.1  \nconvertdate  \n2.3.2  \ncryptography  \n3.4.7  \ncycler  \n0.10.0  \ncymem  \n2.0.5  \nCython  \n0.29.23  \ndatabricks-automl-runtime  \n0.2.3  \ndatabricks-cli  \n0.14.3  \ndbus-python  \n1.2.16  \ndecorator  \n5.0.6  \ndefusedxml  \n0.7.1  \ndill  \n0.3.2  \ndiskcache  \n5.2.1  \ndistlib  \n0.3.3  \ndistro-info  \n0.23ubuntu1  \nentrypoints  \n0.3  \nephem  \n4.1  \nfacets-overview  \n1.0.0  \nfasttext  \n0.9.2  \nfilelock  \n3.0.12  \nFlask  \n1.1.2  \nflatbuffers  \n1.12  \nfsspec  \n0.9.0  \nfuture  \n0.18.2  \ngast  \n0.4.0  \ngitdb  \n4.0.7  \nGitPython  \n3.1.12  \ngoogle-auth  \n1.22.1  \ngoogle-auth-oauthlib  \n0.4.2  \ngoogle-pasta  \n0.2.0  \ngrpcio  \n1.39.0  \ngunicorn  \n20.0.4  \ngviz-api  \n1.10.0  \nh5py  \n3.1.0  \nhijri-converter  \n2.2.2  \nholidays  \n0.11.3.1  \nhorovod  \n0.23.0  \nhtmlmin  \n0.1.12  \nhuggingface-hub  \n0.0.19  \nidna  \n2.10  \nImageHash  \n4.2.1  \nimbalanced-learn  \n0.8.1  \nimportlib-metadata  \n3.10.0  \nipykernel  \n5.3.4  \nipython  \n7.22.0  \nipython-genutils  \n0.2.0  \nipywidgets  \n7.6.3  \nisodate"
    },
    {
        "id": 1772,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.1ml.html",
        "content": "importlib-metadata  \n3.10.0  \nipykernel  \n5.3.4  \nipython  \n7.22.0  \nipython-genutils  \n0.2.0  \nipywidgets  \n7.6.3  \nisodate  \n0.6.0  \nitsdangerous  \n1.1.0  \njedi  \n0.17.2  \nJinja2  \n2.11.3  \njmespath  \n0.10.0  \njoblib  \n1.0.1  \njoblibspark  \n0.3.0  \njsonschema  \n3.2.0  \njupyter-client  \n6.1.12  \njupyter-core  \n4.7.1  \njupyterlab-pygments  \n0.1.2  \njupyterlab-widgets  \n1.0.0  \nkeras  \n2.6.0  \nKeras-Preprocessing  \n1.1.2  \nkiwisolver  \n1.3.1  \nkoalas  \n1.8.2  \nkorean-lunar-calendar  \n0.2.1  \nlightgbm  \n3.3.0  \nllvmlite  \n0.37.0  \nLunarCalendar  \n0.0.9  \nMako  \n1.1.3  \nMarkdown  \n3.3.3  \nMarkupSafe  \n2.0.1  \nmatplotlib  \n3.4.2  \nmissingno  \n0.5.0  \nmistune  \n0.8.4  \nmleap  \n0.18.1  \nmlflow-skinny  \n1.21.0  \nmultimethod  \n1.6  \nmurmurhash  \n1.0.5  \nnbclient  \n0.5.3  \nnbconvert  \n6.0.7  \nnbformat  \n5.1.3  \nnest-asyncio  \n1.5.1  \nnetworkx  \n2.5  \nnltk  \n3.6.1  \nnotebook  \n6.3.0  \nnumba  \n0.54.1  \nnumpy  \n1.19.2  \noauthlib  \n3.1.0  \nopt-einsum  \n3.3.0  \npackaging  \n20.9  \npandas  \n1.2.4  \npandas-profiling  \n3.1.0  \npandocfilters  \n1.4.3  \nparamiko  \n2.7.2  \nparso  \n0.7.0  \npathy  \n0.6.0  \npatsy  \n0.5.1  \npetastorm  \n0.11.3  \npexpect  \n4.8.0  \nphik  \n0.12.0  \npickleshare  \n0.7.5  \nPillow  \n8.2.0  \npip  \n21.0.1  \nplotly  \n5.3.0  \npreshed  \n3.0.5  \nprometheus-client"
    },
    {
        "id": 1773,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.1ml.html",
        "content": "4.8.0  \nphik  \n0.12.0  \npickleshare  \n0.7.5  \nPillow  \n8.2.0  \npip  \n21.0.1  \nplotly  \n5.3.0  \npreshed  \n3.0.5  \nprometheus-client  \n0.10.1  \nprompt-toolkit  \n3.0.17  \nprophet  \n1.0.1  \nprotobuf  \n3.17.2  \npsutil  \n5.8.0  \npsycopg2  \n2.8.5  \nptyprocess  \n0.7.0  \npyarrow  \n4.0.0  \npyasn1  \n0.4.8  \npyasn1-modules  \n0.2.8  \npybind11  \n2.8.0  \npycparser  \n2.20  \npydantic  \n1.8.2  \nPygments  \n2.8.1  \nPyGObject  \n3.36.0  \nPyMeeus  \n0.5.11  \nPyNaCl  \n1.4.0  \npyodbc  \n4.0.30  \npyparsing  \n2.4.7  \npyrsistent  \n0.17.3  \npystan  \n2.19.1.1  \npython-apt  \n2.0.0+ubuntu0.20.4.6  \npython-dateutil  \n2.8.1  \npython-editor  \n1.0.4  \npytz  \n2020.5  \nPyWavelets  \n1.1.1  \nPyYAML  \n5.4.1  \npyzmq  \n20.0.0  \nregex  \n2021.4.4  \nrequests  \n2.25.1  \nrequests-oauthlib  \n1.3.0  \nrequests-unixsocket  \n0.2.0  \nrsa  \n4.7.2  \ns3transfer  \n0.3.7  \nsacremoses  \n0.0.46  \nscikit-learn  \n0.24.1  \nscipy  \n1.6.2  \nseaborn  \n0.11.1  \nSend2Trash  \n1.5.0  \nsetuptools  \n52.0.0  \nsetuptools-git  \n1.2  \nshap  \n0.39.0  \nsimplejson  \n3.17.2  \nsix  \n1.15.0  \nslicer  \n0.0.7  \nsmart-open  \n5.2.0  \nsmmap  \n3.0.5  \nspacy  \n3.1.3  \nspacy-legacy  \n3.0.8  \nspark-tensorflow-distributor  \n1.0.0  \nsqlparse  \n0.4.1  \nsrsly  \n2.4.1  \nssh-import-id  \n5.10"
    },
    {
        "id": 1774,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.1ml.html",
        "content": "spacy-legacy  \n3.0.8  \nspark-tensorflow-distributor  \n1.0.0  \nsqlparse  \n0.4.1  \nsrsly  \n2.4.1  \nssh-import-id  \n5.10  \nstatsmodels  \n0.12.2  \ntabulate  \n0.8.7  \ntangled-up-in-unicode  \n0.1.0  \ntenacity  \n6.2.0  \ntensorboard  \n2.6.0  \ntensorboard-data-server  \n0.6.1  \ntensorboard-plugin-profile  \n2.5.0  \ntensorboard-plugin-wit  \n1.8.0  \ntensorflow-cpu  \n2.6.0  \ntensorflow-estimator  \n2.6.0  \ntermcolor  \n1.1.0  \nterminado  \n0.9.4  \ntestpath  \n0.4.4  \nthinc  \n8.0.9  \nthreadpoolctl  \n2.1.0  \ntokenizers  \n0.10.3  \ntorch  \n1.9.1+cpu  \ntorchvision  \n0.10.1+cpu  \ntornado  \n6.1  \ntqdm  \n4.59.0  \ntraitlets  \n5.0.5  \ntransformers  \n4.11.3  \ntyper  \n0.3.2  \ntyping-extensions  \n3.7.4.3  \nujson  \n4.0.2  \nunattended-upgrades  \n0.1  \nurllib3  \n1.25.11  \nvirtualenv  \n20.4.1  \nvisions  \n0.7.4  \nwasabi  \n0.8.2  \nwcwidth  \n0.2.5  \nwebencodings  \n0.5.1  \nwebsocket-client  \n0.57.0  \nWerkzeug  \n1.0.1  \nwheel  \n0.36.2  \nwidgetsnbextension  \n3.5.1  \nwrapt  \n1.12.1  \nxgboost  \n1.4.2  \nzipp  \n3.4.1  \nPython libraries on GPU clusters  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nabsl-py  \n0.11.0  \nAntergos Linux  \n2015.10 (ISO-Rolling)  \nappdirs  \n1.4.4  \nargon2-cffi  \n20.1.0  \nastor  \n0.8.1  \nastunparse  \n1.6.3  \nasync-generator  \n1.10  \nattrs  \n20.3.0  \nbackcall  \n0.2.0  \nbcrypt  \n3.2.0  \nbleach  \n3.3.0  \nblis"
    },
    {
        "id": 1775,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.1ml.html",
        "content": "astunparse  \n1.6.3  \nasync-generator  \n1.10  \nattrs  \n20.3.0  \nbackcall  \n0.2.0  \nbcrypt  \n3.2.0  \nbleach  \n3.3.0  \nblis  \n0.7.4  \nboto3  \n1.16.7  \nbotocore  \n1.19.7  \ncachetools  \n4.2.4  \ncatalogue  \n2.0.6  \ncertifi  \n2020.12.5  \ncffi  \n1.14.5  \nchardet  \n4.0.0  \nclang  \n5.0  \nclick  \n7.1.2  \ncloudpickle  \n1.6.0  \ncmdstanpy  \n0.9.68  \nconfigparser  \n5.0.1  \nconvertdate  \n2.3.2  \ncryptography  \n3.4.7  \ncycler  \n0.10.0  \ncymem  \n2.0.5  \nCython  \n0.29.23  \ndatabricks-automl-runtime  \n0.2.3  \ndatabricks-cli  \n0.14.3  \ndbus-python  \n1.2.16  \ndecorator  \n5.0.6  \ndefusedxml  \n0.7.1  \ndill  \n0.3.2  \ndiskcache  \n5.2.1  \ndistlib  \n0.3.3  \ndistro-info  \n0.23ubuntu1  \nentrypoints  \n0.3  \nephem  \n4.1  \nfacets-overview  \n1.0.0  \nfasttext  \n0.9.2  \nfilelock  \n3.0.12  \nFlask  \n1.1.2  \nflatbuffers  \n1.12  \nfsspec  \n0.9.0  \nfuture  \n0.18.2  \ngast  \n0.4.0  \ngitdb  \n4.0.7  \nGitPython  \n3.1.12  \ngoogle-auth  \n1.22.1  \ngoogle-auth-oauthlib  \n0.4.2  \ngoogle-pasta  \n0.2.0  \ngrpcio  \n1.39.0  \ngunicorn  \n20.0.4  \ngviz-api  \n1.10.0  \nh5py  \n3.1.0  \nhijri-converter  \n2.2.2  \nholidays  \n0.11.3.1  \nhorovod  \n0.23.0  \nhtmlmin  \n0.1.12  \nhuggingface-hub  \n0.0.19  \nidna  \n2.10  \nImageHash  \n4.2.1  \nimbalanced-learn  \n0.8.1  \nimportlib-metadata  \n3.10.0  \nipykernel"
    },
    {
        "id": 1776,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.1ml.html",
        "content": "huggingface-hub  \n0.0.19  \nidna  \n2.10  \nImageHash  \n4.2.1  \nimbalanced-learn  \n0.8.1  \nimportlib-metadata  \n3.10.0  \nipykernel  \n5.3.4  \nipython  \n7.22.0  \nipython-genutils  \n0.2.0  \nipywidgets  \n7.6.3  \nisodate  \n0.6.0  \nitsdangerous  \n1.1.0  \njedi  \n0.17.2  \nJinja2  \n2.11.3  \njmespath  \n0.10.0  \njoblib  \n1.0.1  \njoblibspark  \n0.3.0  \njsonschema  \n3.2.0  \njupyter-client  \n6.1.12  \njupyter-core  \n4.7.1  \njupyterlab-pygments  \n0.1.2  \njupyterlab-widgets  \n1.0.0  \nkeras  \n2.6.0  \nKeras-Preprocessing  \n1.1.2  \nkiwisolver  \n1.3.1  \nkoalas  \n1.8.2  \nkorean-lunar-calendar  \n0.2.1  \nlightgbm  \n3.3.0  \nllvmlite  \n0.37.0  \nLunarCalendar  \n0.0.9  \nMako  \n1.1.3  \nMarkdown  \n3.3.3  \nMarkupSafe  \n2.0.1  \nmatplotlib  \n3.4.2  \nmissingno  \n0.5.0  \nmistune  \n0.8.4  \nmleap  \n0.18.1  \nmlflow-skinny  \n1.21.0  \nmultimethod  \n1.6  \nmurmurhash  \n1.0.5  \nnbclient  \n0.5.3  \nnbconvert  \n6.0.7  \nnbformat  \n5.1.3  \nnest-asyncio  \n1.5.1  \nnetworkx  \n2.5  \nnltk  \n3.6.1  \nnotebook  \n6.3.0  \nnumba  \n0.54.1  \nnumpy  \n1.19.2  \noauthlib  \n3.1.0  \nopt-einsum  \n3.3.0  \npackaging  \n20.9  \npandas  \n1.2.4  \npandas-profiling  \n3.1.0  \npandocfilters  \n1.4.3  \nparamiko  \n2.7.2  \nparso  \n0.7.0  \npathy  \n0.6.0  \npatsy  \n0.5.1  \npetastorm  \n0.11.3  \npexpect  \n4.8.0  \nphik  \n0.12.0  \npickleshare"
    },
    {
        "id": 1777,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.1ml.html",
        "content": "2.7.2  \nparso  \n0.7.0  \npathy  \n0.6.0  \npatsy  \n0.5.1  \npetastorm  \n0.11.3  \npexpect  \n4.8.0  \nphik  \n0.12.0  \npickleshare  \n0.7.5  \nPillow  \n8.2.0  \npip  \n21.0.1  \nplotly  \n5.3.0  \npreshed  \n3.0.5  \nprompt-toolkit  \n3.0.17  \nprophet  \n1.0.1  \nprotobuf  \n3.17.2  \npsutil  \n5.8.0  \npsycopg2  \n2.8.5  \nptyprocess  \n0.7.0  \npyarrow  \n4.0.0  \npyasn1  \n0.4.8  \npyasn1-modules  \n0.2.8  \npybind11  \n2.8.1  \npycparser  \n2.20  \npydantic  \n1.8.2  \nPygments  \n2.8.1  \nPyGObject  \n3.36.0  \nPyMeeus  \n0.5.11  \nPyNaCl  \n1.4.0  \npyodbc  \n4.0.30  \npyparsing  \n2.4.7  \npyrsistent  \n0.17.3  \npystan  \n2.19.1.1  \npython-apt  \n2.0.0+ubuntu0.20.4.6  \npython-dateutil  \n2.8.1  \npython-editor  \n1.0.4  \npytz  \n2020.5  \nPyWavelets  \n1.1.1  \nPyYAML  \n5.4.1  \npyzmq  \n20.0.0  \nregex  \n2021.4.4  \nrequests  \n2.25.1  \nrequests-oauthlib  \n1.3.0  \nrequests-unixsocket  \n0.2.0  \nrsa  \n4.7.2  \ns3transfer  \n0.3.7  \nsacremoses  \n0.0.46  \nscikit-learn  \n0.24.1  \nscipy  \n1.6.2  \nseaborn  \n0.11.1  \nSend2Trash  \n1.5.0  \nsetuptools  \n52.0.0  \nsetuptools-git  \n1.2  \nshap  \n0.39.0  \nsimplejson  \n3.17.2  \nsix  \n1.15.0  \nslicer  \n0.0.7  \nsmart-open  \n5.2.0  \nsmmap  \n3.0.5  \nspacy  \n3.1.3  \nspacy-legacy  \n3.0.8  \nspark-tensorflow-distributor  \n1.0.0"
    },
    {
        "id": 1778,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.1ml.html",
        "content": "0.0.7  \nsmart-open  \n5.2.0  \nsmmap  \n3.0.5  \nspacy  \n3.1.3  \nspacy-legacy  \n3.0.8  \nspark-tensorflow-distributor  \n1.0.0  \nsqlparse  \n0.4.1  \nsrsly  \n2.4.1  \nssh-import-id  \n5.10  \nstatsmodels  \n0.12.2  \ntabulate  \n0.8.7  \ntangled-up-in-unicode  \n0.1.0  \ntenacity  \n6.2.0  \ntensorboard  \n2.6.0  \ntensorboard-data-server  \n0.6.1  \ntensorboard-plugin-profile  \n2.5.0  \ntensorboard-plugin-wit  \n1.8.0  \ntensorflow  \n2.6.0  \ntensorflow-estimator  \n2.6.0  \ntermcolor  \n1.1.0  \nterminado  \n0.9.4  \ntestpath  \n0.4.4  \nthinc  \n8.0.9  \nthreadpoolctl  \n2.1.0  \ntokenizers  \n0.10.3  \ntorch  \n1.9.1+cu111  \ntorchvision  \n0.10.1+cu111  \ntornado  \n6.1  \ntqdm  \n4.59.0  \ntraitlets  \n5.0.5  \ntransformers  \n4.11.3  \ntyper  \n0.3.2  \ntyping-extensions  \n3.7.4.3  \nujson  \n4.0.2  \nunattended-upgrades  \n0.1  \nurllib3  \n1.25.11  \nvirtualenv  \n20.4.1  \nvisions  \n0.7.4  \nwasabi  \n0.8.2  \nwcwidth  \n0.2.5  \nwebencodings  \n0.5.1  \nwebsocket-client  \n0.57.0  \nWerkzeug  \n1.0.1  \nwheel  \n0.36.2  \nwidgetsnbextension  \n3.5.1  \nwrapt  \n1.12.1  \nxgboost  \n1.4.2  \nzipp  \n3.4.1  \nSpark packages containing Python modules  \nSpark Package  \nPython Module  \nVersion  \ngraphframes  \ngraphframes  \n0.8.2-db1-spark3.2  \nR libraries  \nThe R libraries are identical to the R Libraries in Databricks Runtime 10.1.  \nJava and Scala libraries (Scala 2.12 cluster)"
    },
    {
        "id": 1779,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.1ml.html",
        "content": "0.8.2-db1-spark3.2  \nR libraries  \nThe R libraries are identical to the R Libraries in Databricks Runtime 10.1.  \nJava and Scala libraries (Scala 2.12 cluster)  \nIn addition to Java and Scala libraries in Databricks Runtime 10.1, Databricks Runtime 10.1 ML contains the following JARs:  \nCPU clusters  \nGroup ID  \nArtifact ID  \nVersion  \ncom.typesafe.akka  \nakka-actor_2.12  \n2.5.23  \nml.combust.mleap  \nmleap-databricks-runtime_2.12  \n0.17.0-4882dc3  \nml.dmlc  \nxgboost4j-spark_2.12  \n1.4.1  \nml.dmlc  \nxgboost4j_2.12  \n1.4.1  \norg.graphframes  \ngraphframes_2.12  \n0.8.1-db6-spark3.2  \norg.mlflow  \nmlflow-client  \n1.20.2  \norg.mlflow  \nmlflow-spark  \n1.20.2  \norg.scala-lang.modules  \nscala-java8-compat_2.12  \n0.8.0  \norg.tensorflow  \nspark-tensorflow-connector_2.12  \n1.15.0  \nGPU clusters  \nGroup ID  \nArtifact ID  \nVersion  \ncom.typesafe.akka  \nakka-actor_2.12  \n2.5.23  \nml.combust.mleap  \nmleap-databricks-runtime_2.12  \n0.18.1-23eb1ef  \nml.dmlc  \nxgboost4j-gpu_2.12  \n1.4.1  \nml.dmlc  \nxgboost4j-spark-gpu_2.12  \n1.4.1-spark3.2  \norg.graphframes  \ngraphframes_2.12  \n0.8.2-db1-spark3.2  \norg.mlflow  \nmlflow-client  \n1.21.0  \norg.mlflow  \nmlflow-spark  \n1.21.0  \norg.scala-lang.modules  \nscala-java8-compat_2.12  \n0.8.0  \norg.tensorflow  \nspark-tensorflow-connector_2.12"
    },
    {
        "id": 1780,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/10.1ml.html",
        "content": "1.21.0  \norg.scala-lang.modules  \nscala-java8-compat_2.12  \n0.8.0  \norg.tensorflow  \nspark-tensorflow-connector_2.12  \n1.15.0"
    },
    {
        "id": 1781,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.3ml.html",
        "content": "Databricks Runtime 5.3 ML (EoS)  \nNote  \nSupport for this Databricks Runtime version has ended. For the end-of-support date, see End-of-support history. For all supported Databricks Runtime versions, see Databricks Runtime release notes versions and compatibility.  \nDatabricks released this image in April 2019.  \nDatabricks Runtime 5.3 ML provides a ready-to-go environment for machine learning and data science based on Databricks Runtime 5.3 (EoS). Databricks Runtime for ML contains many popular machine learning libraries, including TensorFlow, PyTorch, Keras, and XGBoost. It also supports distributed deep learning training using Horovod.  \nFor more information, including instructions for creating a Databricks Runtime ML cluster, see AI and Machine Learning on Databricks.  \nNew features"
    },
    {
        "id": 1782,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.3ml.html",
        "content": "New features\nDatabricks Runtime 5.3 ML is built on top of Databricks Runtime 5.3. For information on what\u2019s new in Databricks Runtime 5.3, see the Databricks Runtime 5.3 (EoS) release notes. In addition to library updates, Databricks Runtime 5.3 ML introduces the following new features:  \nMLflow + Apache Spark MLlib integration: Databricks Runtime 5.3 ML supports automatic logging of MLflow runs for models fit using PySpark tuning algorithms CrossValidator and TrainValidationSplit.  \nPreview  \nThis feature is in Private Preview. Contact your Databricks sales representative to learn about enabling it.  \nUpgrades the following libraries to the latest version:  \nPyArrow from 0.8.0 to 0.12.1: BinaryType is supported by Arrow-based conversion and can be used in PandasUDF.  \nHorovod from 0.15.2 to 0.16.0.  \nTensorboardX from 1.4 to 1.6.  \nThe Databricks ML Model Export API has been deprecated. Databricks recommends using MLeap instead, which provides broader coverage of MLlib model types. Find out more at MLeap ML model export."
    },
    {
        "id": 1783,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.3ml.html",
        "content": "Maintenance updates\nMaintenance updates\nSee Databricks Runtime 5.4 ML maintenance updates.\n\nSystem environment\nSystem environment\nThe system environment in Databricks Runtime 5.3 ML differs from Databricks Runtime 5.3 as follows:  \nPython: 2.7.15 for Python 2 clusters and 3.6.5 for Python 3 clusters.  \nDBUtils: Databricks Runtime 5.3 ML does not contain Library utility (dbutils.library) (legacy).  \nFor GPU clusters, the following NVIDIA GPU libraries:  \nTesla driver 396.44  \nCUDA 9.2  \nCUDNN 7.2.1\n\nLibraries"
    },
    {
        "id": 1784,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.3ml.html",
        "content": "Libraries\nThe following sections list the libraries included in Databricks Runtime 5.3 ML that differ from those included in Databricks Runtime 5.3.  \nTop-tier libraries  \nDatabricks Runtime 5.3 ML includes the following top-tier libraries:  \nGraphFrames  \nHorovod and HorovodRunner  \nPyTorch  \nspark-tensorflow-connector  \nTensorFlow  \nTensorBoard  \nPython libraries  \nDatabricks Runtime 5.3 ML uses Conda for Python package management. As a result, there are major differences in pre-installed Python libraries compared to Databricks Runtime. The following is a full list of provided Python packages and versions installed using Conda package manager.  \nLibrary  \nVersion  \nLibrary  \nVersion  \nLibrary  \nVersion  \nabsl-py  \n0.7.0  \nargparse  \n1.4.0  \nasn1crypto  \n0.24.0  \nastor  \n0.7.1  \nbackports-abc  \n0.5  \nbackports.functools-lru-cache  \n1.5  \nbackports.weakref  \n1.0.post1  \nbcrypt  \n3.1.6  \nbleach  \n2.1.3  \nboto  \n2.48.0  \nboto3  \n1.7.62  \nbotocore  \n1.10.62  \ncertifi  \n2018.04.16  \ncffi  \n1.11.5  \nchardet  \n3.0.4  \ncloudpickle  \n0.5.3  \ncolorama  \n0.3.9  \nconfigparser  \n3.5.0  \ncryptography  \n2.2.2  \ncycler  \n0.10.0  \nCython  \n0.28.2  \ndecorator  \n4.3.0  \ndocutils  \n0.14  \nentrypoints  \n0.2.3  \nenum34  \n1.1.6  \net-xmlfile  \n1.0.1  \nfuncsigs  \n1.0.2  \nfunctools32  \n3.2.3-2  \nfusepy  \n2.0.4  \nfutures  \n3.2.0  \ngast  \n0.2.2  \ngrpcio  \n1.12.1  \nh5py  \n2.8.0  \nhorovod  \n0.16.0  \nhtml5lib  \n1.0.1  \nidna  \n2.6  \nipaddress  \n1.0.22  \nipython  \n5.7.0  \nipython_genutils  \n0.2.0  \njdcal  \n1.4  \nJinja2  \n2.10  \njmespath  \n0.9.3  \njsonschema  \n2.6.0  \njupyter-client"
    },
    {
        "id": 1785,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.3ml.html",
        "content": "ipython_genutils  \n0.2.0  \njdcal  \n1.4  \nJinja2  \n2.10  \njmespath  \n0.9.3  \njsonschema  \n2.6.0  \njupyter-client  \n5.2.3  \njupyter-core  \n4.4.0  \nKeras  \n2.2.4  \nKeras-Applications  \n1.0.6  \nKeras-Preprocessing  \n1.0.5  \nkiwisolver  \n1.0.1  \nlinecache2  \n1.0.0  \nllvmlite  \n0.23.1  \nlxml  \n4.2.1  \nMarkdown  \n3.0.1  \nMarkupSafe  \n1.0  \nmatplotlib  \n2.2.2  \nmistune  \n0.8.3  \nmleap  \n0.8.1  \nmock  \n2.0.0  \nmsgpack  \n0.5.6  \nnbconvert  \n5.3.1  \nnbformat  \n4.4.0  \nnose  \n1.3.7  \nnose-exclude  \n0.5.0  \nnumba  \n0.38.0+0.g2a2b772fc.dirty  \nnumpy  \n1.14.3  \nolefile  \n0.45.1  \nopenpyxl  \n2.5.3  \npandas  \n0.23.0  \npandocfilters  \n1.4.2  \nparamiko  \n2.4.1  \npathlib2  \n2.3.2  \npatsy  \n0.5.0  \npbr  \n5.1.1  \npexpect  \n4.5.0  \npickleshare  \n0.7.4  \nPillow  \n5.1.0  \npip  \n10.0.1  \nply  \n3.11  \nprompt-toolkit  \n1.0.15  \nprotobuf  \n3.6.1  \npsutil  \n5.6.0  \npsycopg2  \n2.7.5  \nptyprocess  \n0.5.2  \npyarrow  \n0.12.1  \npyasn1  \n0.4.5  \npycparser  \n2.18  \nPygments  \n2.2.0  \nPyNaCl  \n1.3.0  \npyOpenSSL  \n18.0.0  \npyparsing  \n2.2.0  \nPySocks  \n1.6.8  \nPython  \n2.7.15  \npython-dateutil  \n2.7.3  \npytz  \n2018.4  \nPyYAML  \n3.12  \npyzmq  \n17.0.0  \nrequests  \n2.18.4  \ns3transfer  \n0.1.13  \nscandir  \n1.7  \nscikit-learn  \n0.19.1  \nscipy"
    },
    {
        "id": 1786,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.3ml.html",
        "content": "2018.4  \nPyYAML  \n3.12  \npyzmq  \n17.0.0  \nrequests  \n2.18.4  \ns3transfer  \n0.1.13  \nscandir  \n1.7  \nscikit-learn  \n0.19.1  \nscipy  \n1.1.0  \nseaborn  \n0.8.1  \nsetuptools  \n39.1.0  \nsimplegeneric  \n0.8.1  \nsingledispatch  \n3.4.0.3  \nsix  \n1.11.0  \nstatsmodels  \n0.9.0  \nsubprocess32  \n3.5.3  \ntensorboard  \n1.12.2  \ntensorboardX  \n1.6  \ntensorflow  \n1.12.0  \ntermcolor  \n1.1.0  \ntestpath  \n0.3.1  \ntorch  \n0.4.1  \ntorchvision  \n0.2.1  \ntornado  \n5.0.2  \ntraceback2  \n1.4.0  \ntraitlets  \n4.3.2  \nunittest2  \n1.1.0  \nurllib3  \n1.22  \nvirtualenv  \n16.0.0  \nwcwidth  \n0.1.7  \nwebencodings  \n0.5.1  \nWerkzeug  \n0.14.1  \nwheel  \n0.31.1  \nwrapt  \n1.10.11  \nwsgiref  \n0.1.2  \nIn addition, the following Spark packages include Python modules:  \nSpark Package  \nPython Module  \nVersion  \ngraphframes  \ngraphframes  \n0.7.0-db1-spark2.4  \nspark-deep-learning  \nsparkdl  \n1.5.0-db1-spark2.4  \ntensorframes  \ntensorframes  \n0.6.0-s_2.11  \nR libraries  \nThe R libraries are identical to the R Libraries in Databricks Runtime 5.3.  \nJava and Scala libraries (Scala 2.11 cluster)  \nIn addition to Java and Scala libraries in Databricks Runtime 5.3, Databricks Runtime 5.3 ML contains the following JARs:  \nGroup ID  \nArtifact ID  \nVersion  \ncom.databricks  \nspark-deep-learning  \n1.5.0-db1-spark2.4  \ncom.typesafe.akka  \nakka-actor_2.11  \n2.3.11  \nml.combust.mleap  \nmleap-databricks-runtime_2.11  \n0.13.0  \nml.dmlc  \nxgboost4j  \n0.81  \nml.dmlc"
    },
    {
        "id": 1787,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/5.3ml.html",
        "content": "2.3.11  \nml.combust.mleap  \nmleap-databricks-runtime_2.11  \n0.13.0  \nml.dmlc  \nxgboost4j  \n0.81  \nml.dmlc  \nxgboost4j-spark  \n0.81  \norg.graphframes  \ngraphframes_2.11  \n0.7.0-db1-spark2.4  \norg.tensorflow  \nlibtensorflow  \n1.12.0  \norg.tensorflow  \nlibtensorflow_jni  \n1.12.0  \norg.tensorflow  \nspark-tensorflow-connector_2.11  \n1.12.0  \norg.tensorflow  \ntensorflow  \n1.12.0  \norg.tensorframes  \ntensorframes  \n0.6.0-s_2.11"
    },
    {
        "id": 1788,
        "url": "https://docs.databricks.com/en/admin/system-tables/lineage.html",
        "content": "Lineage system tables reference  \nPreview  \nThis feature is in Public Preview. The schema must be enabled to be visible in your system catalog. For more information, see Enable system table schemas  \nThis article provides an overview of the two lineage system tables. These system tables build on Unity Catalog\u2019s data lineage feature, allowing you to programmatically query lineage data to fuel decision making and reports.  \nThere are two lineage system tables:  \nsystem.access.table_lineage  \nsystem.access.column_lineage  \nNote  \nBoth lineage tables represent a subset of all read/write events, as it is not always possible to capture lineage. Records are only emitted when lineage can be inferred.  \nTable lineage table\nTable lineage table\nThe table lineage system table includes a record for each read or write event on a Unity Catalog table or path. This includes but is not limited to job runs, notebook runs, and dashboards updated with the read or write event.\n\nColumn lineage table"
    },
    {
        "id": 1789,
        "url": "https://docs.databricks.com/en/admin/system-tables/lineage.html",
        "content": "Column lineage table\nThe column lineage table does not include events that do not have a source. For example, if you insert into a column using explicit values, it is not captured. If you read a column, it is captured whether or not you write the output. Column lineage is not supported for Delta Live Tables.\n\nLineage system table schema"
    },
    {
        "id": 1790,
        "url": "https://docs.databricks.com/en/admin/system-tables/lineage.html",
        "content": "Lineage system table schema\nThe lineage system tables use the following schema. The table lineage schema doesn\u2019t include source_column_name and target_column_name.  \nColumn name  \nData type  \nDescription  \nExample  \naccount_id  \nstring  \nThe ID of the Databricks account.  \n7af234db-66d7-4db3-bbf0-956098224879  \nmetastore_id  \nstring  \nThe ID of the Unity Catalog metastore.  \n5a31ba44-bbf4-4174-bf33-e1fa078e6765  \nworkspace_id  \nstring  \nThe ID of the workspace  \n123456789012345  \nentity_type  \nstring  \nThe type of entity the lineage transaction was captured from. The value is NOTEBOOK, JOB, PIPELINE, DASHBOARD_V3 (Dashboard), DBSQL_DASHBOARD (Legacy dashboard), DBSQL_QUERY, OR NULL.  \nNOTEBOOK  \nentity_id  \nstring  \nThe ID of the entity the lineage transaction was captured from. If entity_type is NULL, entity_id is NULL.  \nNotebook: 23098402394234  \nJob: 23098402394234  \nDatabricks SQL query: e9cd8a31-de2f-4206-adfa-4f6605d68d88  \nDashboard: 01ef070d110715f2b6d3061b8bda89ea  \nLegacy dashboard: e9cd8a31-de2f-4206-adfa-4f6605d68d88  \nPipeline: e9cd8a31-de2f-4206-adfa-4f6605d68d88  \nentity_run_id  \nstring  \nID to describe the unique run of the entity, or NULL. This differs for each entity type:  \nNotebook: command_run_id  \nJob: job_run_id  \nDatabricks SQL query: query_run_id  \nDashboard: query_run_id  \nLegacy dashboard: query_run_id  \nPipeline: pipeline_update_id  \nIf entity_type is NULL, entity_run_id is NULL."
    },
    {
        "id": 1791,
        "url": "https://docs.databricks.com/en/admin/system-tables/lineage.html",
        "content": "Dashboard: query_run_id  \nLegacy dashboard: query_run_id  \nPipeline: pipeline_update_id  \nIf entity_type is NULL, entity_run_id is NULL.  \nNotebook: e3cr5a10-de6f-6206-fdfa-4f5505d68d55  \nJob: 51090402394234  \nDatabricks SQL query: e9cd8a31-de2f-4206-adfa-4f6605d68d88  \nDashboard: c3ra5m10-pt6o-6206-mdfa-4f5505d68d55  \nLegacy dashboard: c3ra5m10-pt6o-6206-mdfa-4f5505d68d55  \nPipeline: c5am1e0r-on2f-4206-adfa-4f6605d68d88  \nsource_table_full_name  \nstring  \nThree-part name to identify the source table.  \ncatalog.schema.table  \nsource_table_catalog  \nstring  \nThe catalog of the source table.  \ncatalog  \nsource_table_schema  \nstring  \nThe schema of the source table.  \ncatalog.schema  \nsource_table_name  \nstring  \nThe name of the source table.  \ntable  \nsource_path  \nstring  \nLocation in cloud storage of the source table, or the path if it\u2019s reading from cloud storage directly.  \ns3://mybucket/table1  \nsource_type  \nstring  \nThe type of the source. The value is TABLE, PATH, VIEW, or STREAMING_TABLE.  \nTABLE  \nsource_column_name  \nstring  \nThe name of the source column.  \ndate  \ntarget_table_full_name  \nstring  \nThree-part name to identify the target table.  \ncatalog.schema.table  \ntarget_table_catalog  \nstring  \nThe catalog of the target table.  \ncatalog  \ntarget_table_schema  \nstring  \nThe schema of the target table.  \ncatalog.schema  \ntarget_table_name  \nstring  \nThe name of the target table.  \ntable  \ntarget_path  \nstring  \nLocation in cloud storage of the target table.  \ns3://mybucket/table1  \ntarget_type  \nstring  \nThe type of the target. The value is TABLE, PATH, VIEW,or STREAMING TABLE.  \nTABLE  \ntarget_column_name  \nstring  \nThe name of the target column.  \ndate  \ncreated_by  \nstring"
    },
    {
        "id": 1792,
        "url": "https://docs.databricks.com/en/admin/system-tables/lineage.html",
        "content": "target_type  \nstring  \nThe type of the target. The value is TABLE, PATH, VIEW,or STREAMING TABLE.  \nTABLE  \ntarget_column_name  \nstring  \nThe name of the target column.  \ndate  \ncreated_by  \nstring  \nThe user who generated this lineage. This can be a Databricks username, a Databricks service principal ID, \u201cSystem-User\u201d, or NULL if the user information cannot be captured.  \ncrampton.rods@email.com  \nevent_time  \ntimestamp  \nThe timestamp when the lineage was generated. Timezone information is recorded at the end of the value with +00:00 representing UTC.  \n2023-06-20T19:47:21.194+00:00  \nevent_date  \ndate  \nThe date when the lineage was generated. This is a partitioned column.  \n2023-06-20"
    },
    {
        "id": 1793,
        "url": "https://docs.databricks.com/en/admin/system-tables/lineage.html",
        "content": "Reading lineage system tables\nReading lineage system tables\nTake notice of the following considerations when analyzing lineage system tables:  \nFor the entity_type, Databricks supports Delta Live Tables, notebooks, jobs, Databricks SQL queries, and dashboards. Events from other entities aren\u2019t supported.  \nIf you see the entity_type as null, it means no Databricks entity is involved in the event. For example, it could be the result of a JDBC query or from a user clicking into the Sample Data tab in the Databricks UI.  \nTo determine whether the event was a read or a write, you can view the source type and the target type.  \nRead-only: The source type is not null, but target type is null.  \nWrite-only: The target type is not null, but the source type is null.  \nRead and write: The source type and the target type are not null.\n\nLineage system table example"
    },
    {
        "id": 1794,
        "url": "https://docs.databricks.com/en/admin/system-tables/lineage.html",
        "content": "Lineage system table example\nAs an example of how lineage is recorded in system tables, here is an example query followed by the lineage records the query creates:  \nCREATE OR REPLACE TABLE car_features AS SELECT *, in1+in2 as premium_feature_set FROM car_features_exterior JOIN car_features_interior USING(id, model);  \nThe record in system.access.table_lineage would look like:  \nentity_type  \nentity_id  \nsource_table_name  \ntarget_table_name  \ncreated_by  \nevent_time  \nNOTEBOOK  \n27080565267  \ncar_features_exterior  \ncar_features  \ncrampton@email.com  \n2023-01-25T16:19:58.908+0000  \nNOTEBOOK  \n27080565267  \ncar_features_interior  \ncar_features  \ncrampton@email.com  \n2023-01-25T16:19:58.908+0000  \nThe record in system.access.column_lineage would look like:  \nentity_type  \nentity_id  \nsource_table_name  \ntarget_table_name  \nsource_column_name  \ntarget_column_name  \nevent_time  \nNOTEBOOK  \n27080565267  \ncar_features_interior  \ncar_features  \nin1  \npremium_feature_set  \n2023-01-25T16:19:58.908+0000  \nNOTEBOOK  \n27080565267  \ncar_features_interior  \ncar_features  \nin2  \npremium_feature_set  \n2023-01-25T16:19:58.908+0000  \nNote  \nNot all lineage columns are shown in above example. For the full schema, see the above lineage schema."
    },
    {
        "id": 1795,
        "url": "https://docs.databricks.com/en/admin/system-tables/lineage.html",
        "content": "Troubleshooting external table queries\nWhen you reference an external table using its cloud storage path, the associated lineage record only includes the path name and not the table name. As an example, the lineage record for this query would include the path name and not the table name:  \nSELECT * FROM delta.`s3://mybucket/table1`;  \nIf you are trying to query lineage records for an external table referenced by path, you will need to filter the query using source_path or target_path instead of source_table_full_name or target_table_full_name. For example, the following query pulls all lineage records for an external table:  \nSELECT * FROM system.access.table_lineage WHERE source_path = \"s3://mybucket/table1\" OR target_path = \"s3://mybucket/table1\";  \nExample: Retrieve lineage records based on external table name  \nIf you don\u2019t want to manually retrieve the cloud storage path to find lineage, you can use the following function to get lineage data using the table name. You can also replace system.access.table_lineage with system.access.column_lineage in the function if you want to query column lineage.  \ndef getLineageForTable(table_name): table_path = spark.sql(f\"describe detail {table_name}\").select(\"location\").head()[0] df = spark.read.table(\"system.access.table_lineage\") return df.where( (df.source_table_full_name == table_name) | (df.target_table_full_name == table_name) | (df.source_path == table_path) | (df.target_path == table_path) )  \nThen use the following command to call the function and display lineage records for the external table:  \ndisplay(getLineageForTable(\"table_name\"))"
    },
    {
        "id": 1796,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.2ml.html",
        "content": "Databricks Runtime 7.2 for ML (EoS)  \nNote  \nSupport for this Databricks Runtime version has ended. For the end-of-support date, see End-of-support history. For all supported Databricks Runtime versions, see Databricks Runtime release notes versions and compatibility.  \nDatabricks released this image in August 2020.  \nDatabricks Runtime 7.2 for Machine Learning provides a ready-to-go environment for machine learning and data science based on Databricks Runtime 7.2 (EoS). Databricks Runtime ML contains many popular machine learning libraries, including TensorFlow, PyTorch, and XGBoost. It also supports distributed deep learning training using Horovod.  \nFor more information, including instructions for creating a Databricks Runtime ML cluster, see AI and Machine Learning on Databricks.  \nNew features and major changes"
    },
    {
        "id": 1797,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.2ml.html",
        "content": "New features and major changes\nDatabricks Runtime 7.2 ML is built on top of Databricks Runtime 7.2. For information on what\u2019s new in Databricks Runtime 7.2, including Apache Spark MLlib and SparkR, see the Databricks Runtime 7.2 (EoS) release notes.  \nMajor changes to Databricks Runtime ML Python environment  \nThis section describes the major changes to the installed Databricks Runtime ML Python environment compared to Databricks Runtime 7.1 for ML (EoS). You should also review the major changes to the Databricks Runtime Python environment in Databricks Runtime 7.2 (EoS). For a full list of installed Python packages and their versions, see Python libraries.  \nPython packages added  \njoblibspark: 0.2.0  \nPython packages upgraded  \nplotly 4.8.1 -> 4.8.2  \nmleap 0.16.0 -> 0.16.1  \nMajor changes to Databricks Runtime ML system environment  \nSystem packages added  \nxvfb  \nSystem packages upgraded  \nttyd 1.6.0 -> 1.6.1\n\nImprovements"
    },
    {
        "id": 1798,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.2ml.html",
        "content": "Improvements\nDatabricks Runtime 7.2 improves TensorBoard support. See the Databricks Runtime 7.2 release notes.\n\nSystem environment\nSystem environment\nThe system environment in Databricks Runtime 7.2 ML differs from Databricks Runtime 7.2 as follows:  \nDBUtils: Databricks Runtime ML does not contain Library utility (dbutils.library) (legacy). You can use %pip and %conda commands instead. See Notebook-scoped Python libraries.  \nFor GPU clusters, Databricks Runtime ML includes the following NVIDIA GPU libraries:  \nCUDA 10.1 Update 2  \ncuDNN 7.6.5  \nNCCL 2.7.3  \nTensorRT 6.0.1\n\nLibraries"
    },
    {
        "id": 1799,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.2ml.html",
        "content": "Libraries\nThe following sections list the libraries included in Databricks Runtime 7.2 ML that differ from those included in Databricks Runtime 7.2.  \nIn this section:  \nTop-tier libraries  \nPython libraries  \nR libraries  \nJava and Scala libraries (Scala 2.12 cluster)  \nTop-tier libraries  \nDatabricks Runtime 7.2 ML includes the following top-tier libraries:  \nGraphFrames  \nHorovod and HorovodRunner  \nMLflow  \nPyTorch  \nspark-tensorflow-connector  \nTensorFlow  \nTensorBoard  \nPython libraries  \nDatabricks Runtime 7.2 ML uses Conda for Python package management and includes many popular ML packages. The following section describes the Conda environment for Databricks Runtime 7.2 ML.  \nPython on CPU clusters"
    },
    {
        "id": 1800,
        "url": "https://docs.databricks.com/en/archive/runtime-release-notes/7.2ml.html",
        "content": "name: databricks-ml channels: - pytorch - defaults dependencies: - _libgcc_mutex=0.1=main - absl-py=0.9.0=py37_0 - asn1crypto=1.3.0=py37_1 - astor=0.8.0=py37_0 - backcall=0.1.0=py37_0 - backports=1.0=py_2 - bcrypt=3.1.7=py37h7b6447c_1 - blas=1.0=mkl - blinker=1.4=py37_0 - boto3=1.12.0=py_0 - botocore=1.15.0=py_0 - c-ares=1.15.0=h7b6447c_1001 - ca-certificates=2020.6.24=0 - cachetools=4.1.0=py_1 - certifi=2020.6.20=py37_0 - cffi=1.14.0=py37h2e261b9_0 - chardet=3.0.4=py37_1003 - click=7.0=py37_0 - cloudpickle=1.3.0=py_0 - configparser=3.7.4=py37_0 - cpuonly=1.0=0 - cryptography=2.8=py37h1ba5d50_0 - cycler=0.10.0=py37_0 - cython=0.29.15=py37he6710b0_0 - decorator=4.4.1=py_0 - dill=0.3.1.1=py37_1 - docutils=0.15.2=py37_0 - entrypoints=0.3=py37_0 - flask=1.1.1=py_1 - freetype=2.9.1=h8a8886c_1 - future=0.18.2=py37_1 - gast=0.3.3=py_0"
    }
]