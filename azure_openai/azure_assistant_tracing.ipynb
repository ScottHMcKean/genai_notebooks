{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de996d22-4d31-4b28-a59b-382885b41cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Azure Assistant Tracing\n",
    "\n",
    "This notebook demonstrates how we can instrument an Azure Assistant for tracing and feedback within an application. While done in a notebook there is no need to run this within Databricks, since it is designed to slot into a streamlit application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81b588f3-0076-4f9e-b6d0-fbef70944220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai mlflow --upgrade\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8ad665f-a016-4631-aa70-1cae1eb88b34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import AzureOpenAI\n",
    "import mlflow\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from mlflow.tracing.destination import Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27ef33d9-4fdd-4f74-b7ed-9c8db3d4418e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The first thing we do is setup some basic authentication. I am using a [default WorkspaceClient authentication](https://docs.databricks.com/aws/en/dev-tools/sdk-python#authenticate-the-databricks-sdk-for-python-with-your-databricks-account-or-workspace), but you may need to pass a host and token in your app or use [OAuth](https://docs.databricks.com/aws/en/dev-tools/auth/oauth-u2m) (recommended). If using OAuth, we can use the workspace client to get the user's information for conversation history and tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e6f87df-11a7-4c87-9c6f-35dd00e69079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w = WorkspaceClient()\n",
    "endpoint = \"https://dbmma.openai.azure.com/\"\n",
    "api_key = w.dbutils.secrets.get(scope=\"shm\", key=\"azure_agent_key\")\n",
    "\n",
    "# use this to get Databricks credentials for tracking\n",
    "user_name = w.current_user.me().user_name\n",
    "user_id = w.current_user.me().id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b07cdbbf-326e-470c-b090-2efec8ac4b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The first concept is to use an experiment to track our application. I would recommend either using one experiment for the whole application (i.e. Agent) and tagging each release, or using one experiment per release (i.e. Agent-V1, Agent-V2, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb0ae3e7-17fb-4e94-8c72-6323b607af4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# experiment = w.experiments.create_experiment('/Users/scott.mckean@databricks.com/experiments/azure_assistant').experiment\n",
    "\n",
    "experiment = w.experiments.get_by_name(\n",
    "  '/Users/scott.mckean@databricks.com/experiments/azure_assistant'\n",
    "  ).experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18bb776a-743b-4ee9-9b2b-88d06962bc52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This follows the standard Azure authentication and inference flow. One critical note here is that the Azure Assistant API only works with the AzureOpenAI Assistants API, which is now deprecated and being replaced by the ResponsesAPI. It should be fine for the POC though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d62f4dd3-cb7a-47be-b61e-35cddcff32c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  azure_endpoint = endpoint,\n",
    "  api_key= api_key,\n",
    "  api_version=\"2024-05-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86b380c1-0082-49db-997d-6f8e1522b3f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can either create a new assistant using the assistants.create() method or use an existing one using the assistants.retrieve() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd3221a6-6375-4eb1-980c-21e953ddf4e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.retrieve(assistant_id='asst_Mbf4tcs8E6ZOv4aOQ9nKBy7o')\n",
    "\n",
    "# or \n",
    "# assistant = client.beta.assistants.create(\n",
    "#   model=\"gpt-4o-mini\", # replace with model deployment name.\n",
    "#   instructions=\"You are an assistant that answers questions about the FORGE geothermal project in Utah.\",\n",
    "#   tools=[{\"type\":\"file_search\"}],\n",
    "#   tool_resources={\"file_search\":{\"vector_store_ids\":[\"vs_VhOIWdzPecXrOlDMWmlhhUvH\"]}},\n",
    "#   temperature=0.17,\n",
    "#   top_p=0.1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47beba2e-0ee3-4594-8642-45e0b6c8b520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We set the tracking server to Databricks and reference the experiment ID. This should be done in the application (could use environment variables for the experiment ID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eb9a151-4088-41ca-8305-ecc56e8f89bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.tracing.set_destination(\n",
    "    Databricks(experiment_id=experiment.experiment_id)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1781823-7da8-4742-b160-dabd4b29a467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is the heart of the tracing code. We wrap the default assistant code with a mlflow.start_span() context manager. Now that we have a span, we can set inputs, attributes, trace metadata, and log the outputs. \n",
    "\n",
    "It is worth noting the hierarchy here:\n",
    "Experiment -> Trace -> Span -> Assistant Calls\n",
    "\n",
    "I've done some basic quality of life conversions here to make sure we have the messages easily accessible, and the proper metadata captured, the two most important ones being the session_id and user_id, which we will need to match with our conversation history capture in Postgres or Cosmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1193e64-c96b-4e18-9fae-698ee7ab8d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_assistant_query(client, assistant, user_name, question: str):\n",
    "\n",
    "    with mlflow.start_span(\n",
    "        name='azure_assistant_query', \n",
    "        span_type='LLM'\n",
    "    ) as span:\n",
    "        thread = client.beta.threads.create()      \n",
    "        \n",
    "        span.set_inputs({\n",
    "            'messages': [{'role': 'user', 'content': question}],\n",
    "            \"user_id\": user_id,\n",
    "            \"assistant_id\": assistant.id,\n",
    "            \"thread_id\": thread.id,\n",
    "        })\n",
    "  \n",
    "        mlflow.update_current_trace(\n",
    "            client_request_id=thread.id,\n",
    "              metadata={\n",
    "                  \"mlflow.trace.user\": user_name,\n",
    "                  \"mlflow.trace.session\": thread.id, \n",
    "              }\n",
    "          )\n",
    "  \n",
    "        # Add a user question to the thread\n",
    "        _ = client.beta.threads.messages.create(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=question,\n",
    "        )\n",
    "\n",
    "        # Run the thread\n",
    "        run = client.beta.threads.runs.create(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id,\n",
    "        )\n",
    "\n",
    "        # Poll until completion\n",
    "        while run.status in [\"queued\", \"in_progress\", \"cancelling\"]:\n",
    "            time.sleep(1)\n",
    "            run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id,\n",
    "            )\n",
    "\n",
    "        # Capture status and AzureOpenAI attributes\n",
    "        mlflow.update_current_trace(state=\"OK\")\n",
    "        span.set_attributes(run.model_dump())    \n",
    "        \n",
    "        # Extract a standard dictionary of assistant responses\n",
    "        # Currently doens't include tools (build on Responses API later)\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id\n",
    "            )\n",
    "\n",
    "        output_messages = [\n",
    "            {\n",
    "                'role': x.role,\n",
    "                'content': x.content[0].text.value\n",
    "            } \n",
    "            for x in messages.data\n",
    "        ]\n",
    "\n",
    "        all_outputs = [\n",
    "            x.model_dump() \n",
    "            for x in messages.data \n",
    "            if x.role == 'assistant'\n",
    "            ]\n",
    "        \n",
    "        # Log agent response as MLflow output\n",
    "        span.set_outputs({\n",
    "            'messages': output_messages,\n",
    "            'output': all_outputs\n",
    "            })\n",
    "\n",
    "        \n",
    "        return {\n",
    "            'messages':output_messages, \n",
    "            'output':all_outputs,\n",
    "            'attributes':run.model_dump()\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6123fdb2-0cab-47c0-b153-7b6c4b40d393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we run the assistant - the tracing is automatically captured, but we get the result back so we can pull the trace and return the assistant message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a01d347-a431-4a64-91e1-4bce13b1d5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Single traced call; print concise outputs\n",
    "question = \"What wells are on the project?\"\n",
    "result = run_assistant_query(client, assistant, user_name, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2759016d-e137-4107-80dc-519772aa321b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "With the experiment ID and trace ID, we can directly query the trace and get the UUID from mlflow. We can also pull all of the user's previous threads for helping with conversation history or feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df386c4-7490-4371-87cd-89df67a64eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# search a user's previous threads\n",
    "previous_threads = mlflow.search_traces(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=['timestamp DESC'],\n",
    "    filter_string=f\"metadata.mlflow.trace.user = '{user_name}'\",\n",
    "    max_results=3\n",
    ")\n",
    "\n",
    "previous_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "937e0b96-6a48-45e8-b4a9-a9fd241aa26e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "previous_threads.iloc[0].assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ac7e6d1-d449-45d6-9690-10896304756f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# retrieve the mlflow trace id for feedback\n",
    "thread_id = result['attributes']['thread_id']\n",
    "\n",
    "# get the specific trace\n",
    "mlflow_trace_id =mlflow.search_traces(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=['timestamp DESC'],\n",
    "    filter_string=f\"metadata.mlflow.trace.session = '{thread_id}'\",\n",
    "    max_results=3\n",
    ").iloc[0].trace_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34d8da57-e7ea-4b2a-b40e-d6669b7b395c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once we have the trace ID, adding Human feedback with our application is straightforward - see an example of a boolean (thumbs up/down), a numeric score, and [LLM evaluation](https://mlflow.org/docs/2.21.3/llms/llm-evaluate/notebooks/question-answering-evaluation#custom-llm-judged-metric-for-professionalism).\n",
    "\n",
    "We can also set [expectations](https://mlflow.org/docs/latest/genai/assessments/expectations/#types-of-expectations). This can be very useful for the future if we want to use something like [DsPy and MLfLow to optimize our prompts](https://mlflow.org/docs/latest/genai/prompt-registry/optimize-prompts/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3951936a-fb35-4a7a-b419-184e3fcffedc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.entities import AssessmentSource, AssessmentSourceType\n",
    "\n",
    "# thumbs up/down button\n",
    "mlflow.log_feedback(\n",
    "    trace_id=mlflow_trace_id,\n",
    "    name=\"user_satisfaction\",\n",
    "        value=True,\n",
    "        source=AssessmentSource(\n",
    "            source_type=AssessmentSourceType.HUMAN, source_id=user_name\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# numeric score\n",
    "mlflow.log_feedback(\n",
    "    trace_id=mlflow_trace_id,\n",
    "    name=\"relevance\",\n",
    "    value=0.9,\n",
    "    source=AssessmentSource(\n",
    "        source_type=AssessmentSourceType.HUMAN, source_id=user_name\n",
    "    ),\n",
    "    rationale=\"High accuracy and clarity, slightly incomplete coverage\",\n",
    ")\n",
    "\n",
    "# llm evaluation (could be done with a job)\n",
    "mlflow.log_feedback(\n",
    "    trace_id=mlflow_trace_id,\n",
    "    name=\"llm_evaluation\",\n",
    "    value=0.3,\n",
    "    source=AssessmentSource(\n",
    "        source_type=AssessmentSourceType.LLM_JUDGE, source_id='GPT-5'\n",
    "    ),\n",
    "    rationale=\"Not as good as I could have done\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cc0ab62-a173-45bc-8921-c554338a90ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.log_expectation(\n",
    "    trace_id=mlflow_trace_id,\n",
    "    name=\"expected_behavior\",\n",
    "    value={\n",
    "        \"should_escalate\": True,\n",
    "        \"required_elements\": [\"empathy\", \"solution_offer\", \"follow_up\"],\n",
    "        \"max_response_length\": 150,\n",
    "        \"tone\": \"professional_friendly\",\n",
    "    },\n",
    "    source=AssessmentSource(\n",
    "        source_type=AssessmentSourceType.HUMAN,\n",
    "        source_id=user_name,\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "azure_assistant_tracing",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
