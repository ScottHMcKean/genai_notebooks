{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "\n",
    "# Assuming ChatDatabricks, llm_retry_strategy, mlflow, SpanType are defined elsewhere or imported as needed\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from mlflow.entities import SpanType, Document\n",
    "import mlflow\n",
    "from langchain_core.language_models.llms import create_base_retry_decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642e8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 1.5\n",
    "MAX_TOKENS = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32de8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_error_types = (Exception,)\n",
    "\n",
    "llm_retry_strategy = create_base_retry_decorator(\n",
    "    error_types=retry_error_types,\n",
    "    max_retries=5, # Maximum number of retries\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63e4367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClient:\n",
    "    def __init__(self):\n",
    "        self.dbr_llm = ChatDatabricks(\n",
    "                endpoint=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "                temperature=TEMPERATURE,\n",
    "                max_tokens=MAX_TOKENS,\n",
    "            )\n",
    "        \n",
    "        self.dbr_llm_mini = ChatDatabricks(\n",
    "                endpoint=\"databricks-meta-llama-3-1-8b-instruct\",\n",
    "                temperature=TEMPERATURE,\n",
    "                max_tokens=MAX_TOKENS,\n",
    "            )\n",
    "\n",
    "        self.vsc = VectorSearchClient()\n",
    "        self.vs_index = self.vsc.get_index(\n",
    "            endpoint_name=\"one-env-shared-endpoint-3\",      # change if needed\n",
    "            index_name=\"shm.multimodal.index\"            # change if needed\n",
    "        )\n",
    "        \n",
    "        logger.info(\"LLMClient manager initialized\")\n",
    "\n",
    "\n",
    "    @llm_retry_strategy\n",
    "    @mlflow.trace(span_type=SpanType.CHAT_MODEL, name=\"evaluate_context_sufficiency_llm\")\n",
    "    async def evaluate_context_sufficiency(\n",
    "        self, \n",
    "        user_message: str,\n",
    "        chat_history: List[Dict[str, str]]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate if existing context is sufficient to answer user's question.\n",
    "        \n",
    "        Args:\n",
    "            user_message: The user's latest question or comment\n",
    "            chat_history: List of previous conversation messages\n",
    "            initial_retrieved_context: Previous retrieved context\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with 'need_retrieval' boolean and 'reasoning'.\n",
    "        \"\"\"\n",
    "        logger.info(\"Evaluating context sufficiency for follow-up conversation\")\n",
    "\n",
    "        initial_retrieved_context = self.vs_index.similarity_search(\n",
    "            query_text=user_message,\n",
    "            columns=['enriched_text','headings']\n",
    "            )\n",
    "\n",
    "        context_text = \"\\n\\n\".join(\n",
    "            f\"Headings: {doc[1]}, Content: {doc[0]}\"\n",
    "            for doc in initial_retrieved_context['result']['data_array']\n",
    "        )\n",
    "\n",
    "        prompt =  PromptTemplate.from_template(\n",
    "            \"\"\"You are a specialized routing node. Your purpose is to determine if a `User's Current Question` requires fetching new documents, given the `Chat History` and any `Previously Retrieved Documents`.\n",
    "\n",
    "            ### Decision Logic:\n",
    "            Set `need_retrieval` to `true` if:\n",
    "            * The `User's Current Question` asks for new facts, details, or topics not present in the `Chat History` or `Previously Retrieved Documents`.\n",
    "\n",
    "            Set `need_retrieval` to `false` if:\n",
    "            * The `User's Current Question` is a rephrasing or clarification that can be answered using only the information already in the `Chat History` or `Previously Retrieved Documents`.\n",
    "\n",
    "            <BEGIN CONTEXT>\n",
    "            Chat History:\n",
    "            {chat_history}\n",
    "\n",
    "            User's Current Question:\n",
    "            {user_question}\n",
    "\n",
    "            Previously Retrieved Documents:\n",
    "            {retrieved_documents}\n",
    "            <END CONTEXT>\n",
    "\n",
    "            **Provide your assessment in JSON format.**\n",
    "            ```json\n",
    "            {{\n",
    "            \"need_retrieval\": true, // boolean: true if additional documents are likely needed, false otherwise\n",
    "            \"reasoning\": \"A concise explanation for the decision (e.g., 'Follow-up asks for new details on X not in original summary.', or 'Question is a rephrasing of previous info.')\"\n",
    "            }}\n",
    "            ```\"\"\"\n",
    "        )\n",
    "        \n",
    "        base_chain = prompt | self.dbr_llm | JsonOutputParser()\n",
    "    \n",
    "        input_dict = {\n",
    "            \"chat_history\": \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history]),\n",
    "            \"user_question\": user_message,\n",
    "            \"retrieved_documents\": context_text\n",
    "        }\n",
    "         \n",
    "        try:\n",
    "            result = await base_chain.ainvoke(input_dict)\n",
    "            if not isinstance(result, dict) or 'need_retrieval' not in result:\n",
    "                logger.warning(f\"Invalid result structure during evaluate_context_sufficiency: {result}\")\n",
    "                result = {\"need_retrieval\": True, \"reasoning\": \"Could not reliably determine context sufficiency.\"}   \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error in evaluate_context_sufficiency: {str(e)}\", exc_info=True)\n",
    "            result = {\"need_retrieval\": True, \"reasoning\": f\"Error during evaluation: {e}\"}\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1d55296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    }
   ],
   "source": [
    "client = LLMClient()\n",
    "\n",
    "user_message = \"What is the capital of France?\"\n",
    "chat_history = [{\"role\": \"user\", \"content\": \"Hi\"}, {\"role\": \"assistant\", \"content\": \"The capital of France is Nice\"}]\n",
    "\n",
    "result = await client.evaluate_context_sufficiency(user_message, chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf95467f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    }
   ],
   "source": [
    "result = await client.evaluate_context_sufficiency(user_message, chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad28e38",
   "metadata": {},
   "source": [
    "Now we take the above code and wrap it in a FastAPI app, shipped on Databricks Apps. We can then hit the backend with the requests package. See below, where we run the app locally (via `uv run uvicorn small_examples.async_llm_app:app`) and make calls to the \\evaluate endpoint every 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe52691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'ok'}\n",
      "2025-08-12 16:33:00\n",
      "200 {'need_retrieval': False, 'reasoning': \"The User's Current Question is a rephrasing of a question that can be answered using the information already in the Chat History. The chat history contains the answer to the question 'What is the capital of France?' which is 'Nice', although it's worth noting that this answer is incorrect as the capital of France is actually Paris. However, based on the decision logic provided, since the question can be answered using the chat history, no new retrieval is needed.\"}\n",
      "2025-08-12 16:33:32\n",
      "200 {'need_retrieval': False, 'reasoning': \"The User's Current Question is a rephrasing of a question that can be answered using the information already in the Chat History. The chat history contains the answer to the question 'What is the capital of France?' which is 'Nice', although it's worth noting that this answer is incorrect as the capital of France is actually Paris, not Nice. However, based on the provided context, the decision to retrieve new documents is based on whether the question can be answered by the existing information, not the accuracy of that information.\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import datetime\n",
    "\n",
    "BASE_URL = \"http://127.0.0.1:8000\"  # replace with your Databricks App URL if deployed\n",
    "\n",
    "# Health\n",
    "print(requests.get(f\"{BASE_URL}/healthz\").json())\n",
    "\n",
    "# Evaluate\n",
    "payload = {\n",
    "    \"user_message\": \"What is the capital of France?\",\n",
    "    \"chat_history\": [\n",
    "        {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The capital of France is Nice\"},\n",
    "    ],\n",
    "}\n",
    "resp = requests.post(f\"{BASE_URL}/evaluate\", json=payload, timeout=60)\n",
    "\n",
    "import time\n",
    "\n",
    "for _ in range(10):\n",
    "    resp = requests.post(f\"{BASE_URL}/evaluate\", json=payload, timeout=60)\n",
    "    print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    print(resp.status_code, resp.json())\n",
    "    time.sleep(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
