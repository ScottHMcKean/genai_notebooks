{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a65ac522-b588-4b8b-8115-b25dd21f90b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Huggingface PT Models\n",
    "This notebook registers hugging face models to Unity Catalog and deploys it via model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a32a22b-8a6e-4b07-8468-e31ed00766fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade transformers\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f4e44df-54ec-4133-8506-85cb6627f9ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Model from HuggingFace\n",
    "Our serving journey starts with how we load the model from huggingface. We leverage the 'Auto' library from the HuggingFace transformers package because of its compatibility with MLFLow and Unity Catalog. You may need an API token for huggingface if accessing a gated repo like Llama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0f6b3b-a8cf-4709-92b8-ddaff30b3c59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = dbutils.secrets.get('shm', 'hftoken')\n",
    "hf_model_id = \"microsoft/Phi-3.5-mini-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c1c25e9-9b17-4f30-bee3-7603d6012794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The pattern for deploying every huggingface model is the same: load the model via the `AutoModelForCausalLM` and `AutoTokenizer` libraries and then register it to Unity Catalog using the MLFlow.transformers package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "246944d9-34d5-48cc-9fcc-ea5f2bf2c9dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import transformers\n",
    "import re\n",
    "\n",
    "task = \"llm/v1/chat\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(hf_model_id)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(hf_model_id)\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "registry = mlflow.MlflowClient()\n",
    "\n",
    "my_uc_catalog = \"shm\"\n",
    "my_uc_schema = \"default\"\n",
    "uc_model_name = hf_model_id.split(\"/\")[-1].replace(\".\",\"\")\n",
    "\n",
    "transformers_model = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=transformers_model,\n",
    "        artifact_path=\"model\",\n",
    "        task=task,\n",
    "        registered_model_name=f\"{my_uc_catalog}.{my_uc_schema}.{uc_model_name}\",\n",
    "        metadata={\n",
    "            \"task\": task,\n",
    "            \"pretrained_model_name\": uc_model_name,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40219880-1a48-4aa6-b0a3-c5236892755e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the API endpoint and token for the current notebook context\n",
    "API_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "API_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50f7a563-577e-4a78-8491-721db01cbd8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Two Types of Provisioned Throughput\n",
    "There are two ways we can deploy models - using 'classic' model serving, or using accelerated provisioned throughput. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0450fe1-deb7-4bd5-be9a-220b783bf7a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {\"Context-Type\": \"text/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "response = requests.get(url=f\"{API_ROOT}/api/2.0/serving-endpoints/get-model-optimization-info/{my_uc_catalog}.{my_uc_schema}.{uc_model_name}/{model_info.registered_model_version}\", headers=headers)\n",
    "\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57e84d00-93d2-4f92-b6e6-bca5a20aa61c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are two ways we can deploy models - using 'classic' model serving, or using accelerated provisioned throughput. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1619bcab-55a1-44ff-ad70-5fdf04b23bf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_provisioned_throughput = response.json()['throughput_chunk_size']\n",
    "\n",
    "from mlflow.deployments import get_deploy_client\n",
    "client = get_deploy_client(\"databricks\")\n",
    "\n",
    "client.create_endpoint(\n",
    "    name=f\"shm_{uc_model_name}_acc\",\n",
    "    config={\n",
    "        \"served_entities\": [{\n",
    "            \"entity_name\": f\"shm.default.{uc_model_name}\",\n",
    "            \"entity_version\": model_info.registered_model_version,\n",
    "            \"min_provisioned_throughput\": 0, #Must be zero for scale to zero\n",
    "            \"max_provisioned_throughput\": max_provisioned_throughput,\n",
    "            \"scale_to_zero_enabled\": True\n",
    "        }]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a12080-988f-41ed-8afa-7b8619dc3ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "client = get_deploy_client(\"databricks\")\n",
    "\n",
    "client.create_endpoint(\n",
    "    name=f\"shm_{uc_model_name}_cl\",\n",
    "    config={\n",
    "        \"served_entities\": [{\n",
    "            \"entity_name\": f\"shm.default.{uc_model_name}\",\n",
    "            \"entity_version\": model_info.registered_model_version,\n",
    "            \"workload_type\": \"GPU_LARGE\",\n",
    "            \"workload_size\": \"Small\",\n",
    "            \"scale_to_zero_enabled\": True\n",
    "        }]\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "huggingface_model_deployment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
