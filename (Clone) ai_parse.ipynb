{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ca0a52c-2485-463e-9991-cb3c3b195d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Document Service\n",
    "\n",
    "This notebook forms the core of our document service. It showcases how we are going to simplify our document intelligence application using Lakebase and Serverless jobs. This is tested on Serverless Version 3 - it takes a single file or a directory and parses all the files directly into an append operation on a postgres table. We can then get embeddings and use pgvector as the backend with a langgraph Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa2dcd5f-f302-4b73-a6d2-1cd8cdaedf1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We use our Databricks user IDs as the main entry point into the workflow and authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe65912b-8c74-44d8-a0d6-bf4cff33d3b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-langchain databricks-sdk --upgrade\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e367d59-8f14-4de5-8b45-ba36d835fda7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "w = WorkspaceClient()\n",
    "me = w.current_user.me()\n",
    "print(me.id)  # This is your Databricks user ID\n",
    "print(me.user_name) \n",
    "USER_ID = me.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c47676b7-4735-42ad-9fdd-91fbd4683deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "dbutils.widgets.text(\n",
    "  \"file_path\", \n",
    "  \"/Volumes/fnf_demo/income_verification_demo/documents\"\n",
    "  )\n",
    "dbutils.widgets.text(\n",
    "  \"embedding_endpoint\", \n",
    "  'databricks-gte-large-en'\n",
    "  )\n",
    "dbutils.widgets.text(\n",
    "  \"database_instance\", \n",
    "  'shm'\n",
    "  )\n",
    "dbutils.widgets.text(\n",
    "  \"doc_id\", \n",
    "  ''\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f801ad-38b8-4eb5-82a0-20e67417bc9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = dbutils.widgets.getAll()\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa248897-5edc-46ba-a79e-4aee631c1a63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We use ai_parse_document in a serverless job as our document processing service. This could be any isolated microservice and has lots of room for optimization, but ai_parse_document does a pretty good job and can handle lots of file types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1abdb113-df99-481a-a6e7-98f59c4b9199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parsed_df = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .load(config.get(\"file_path\"))\n",
    "    .withColumn(\"user_id\", lit(USER_ID))\n",
    "    .select(\n",
    "        col(\"path\"),\n",
    "        col(\"user_id\"),\n",
    "        expr(\"ai_parse_document(content)\").alias(\"parsed\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"parsed_json\",\n",
    "        parse_json(col(\"parsed\").cast(\"string\"))\n",
    "    )\n",
    "    .select(\n",
    "        col(\"path\"),\n",
    "        col(\"user_id\"),\n",
    "        expr(\"parsed_json:document:pages\").alias(\"pages\"),\n",
    "        expr(\"parsed_json:document:elements\").alias(\"elements\"),\n",
    "        expr(\"parsed_json:document:_corrupted_data\").alias(\"_corrupted_data\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a9a736-2013-4308-8972-c8ee102dab15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(parsed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea47c15c-75b9-41cd-8121-1b2dc806ad69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1744a57-2c1f-43c7-90e9-113541b536b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To get something simple and working, I propose that we simply chunk each page for now. We can work on refining the chunking strategy in this job, but this gives a good starting point. We even wrap the embedding call here for better horizontal scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e80b7b8-2dfd-4aca-bf64-13e9890bc2a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, explode, col, concat_ws, lit, expr\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, IntegerType, StringType\n",
    "import uuid\n",
    "\n",
    "# Define schema for pages based on provided example\n",
    "page_schema = StructType([\n",
    "    StructField(\"content\", StringType()),\n",
    "    StructField(\"footer\", StringType()),\n",
    "    StructField(\"header\", StringType()),\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"page_number\", IntegerType())\n",
    "])\n",
    "\n",
    "chunked_pages = (\n",
    "    parsed_df\n",
    "    .withColumn(\n",
    "        \"pages_array\",\n",
    "        from_json(\n",
    "            col(\"pages\").cast(\"string\"),\n",
    "            ArrayType(page_schema)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"page_chunk\",\n",
    "        explode(col(\"pages_array\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"doc_id\",\n",
    "        lit(config['doc_id'])\n",
    "    )\n",
    "    .select(\n",
    "        lit(str(uuid.uuid4())).alias(\"id\"),\n",
    "        col(\"doc_id\"),\n",
    "        array(col(\"page_chunk.id\").cast(\"string\")).alias(\"page_ids\"),\n",
    "        concat_ws(\n",
    "            \"\\n\",\n",
    "            concat_ws(\"\", lit(\"Content: [\"), col(\"page_chunk.content\"), lit(\"]\")),\n",
    "            concat_ws(\"\", lit(\"Footer: [\"), col(\"page_chunk.footer\"), lit(\"]\")),\n",
    "            concat_ws(\"\", lit(\"Header: [\"), col(\"page_chunk.header\"), lit(\"]\")),\n",
    "            concat_ws(\"\", lit(\"ID: [\"), col(\"page_chunk.id\").cast(\"string\"), lit(\"]\")),\n",
    "            concat_ws(\"\", lit(\"Page Number: [\"), col(\"page_chunk.page_number\").cast(\"string\"), lit(\"]\"))\n",
    "        ).alias(\"content\")\n",
    "    )\n",
    "    .withColumn(\"embedding\", expr(f\"ai_query('{config.get('embedding_endpoint')}', content)\"))\n",
    "    .withColumn(\"metadata\", to_json(struct(col(\"_metadata\"))))\n",
    "    .withColumn(\"created_at\", current_timestamp())\n",
    ")\n",
    "\n",
    "display(chunked_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18e89699-851d-45a2-9423-6246eb415491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunked_pages_pd = chunked_pages.toPandas()\n",
    "chunked_pages_pd['embedding'] = chunked_pages_pd['embedding'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67037be3-f740-43ad-a925-b2e418c55422",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunked_pages_pd"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "(Clone) ai_parse",
   "widgets": {
    "database_instance": {
     "currentValue": "shm",
     "nuid": "c10c5b93-fb6d-47af-ab2f-91f0d0a73bc2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "shm",
      "label": null,
      "name": "database_instance",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "shm",
      "label": null,
      "name": "database_instance",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "doc_id": {
     "currentValue": "3450234095",
     "nuid": "b9abcd48-04d8-469b-b20a-8995b828c841",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "doc_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "doc_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "embedding_endpoint": {
     "currentValue": "databricks-gte-large-en",
     "nuid": "65d5b79f-7a86-4338-ad2a-48958e6e157b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_endpoint",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_endpoint",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "file_path": {
     "currentValue": "/Volumes/fnf_demo/income_verification_demo/documents/t4-2024-martinez.pdf",
     "nuid": "143d3171-7ba0-4d14-a537-59aaf785d3ba",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/fnf_demo/income_verification_demo/documents",
      "label": null,
      "name": "file_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/fnf_demo/income_verification_demo/documents",
      "label": null,
      "name": "file_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
